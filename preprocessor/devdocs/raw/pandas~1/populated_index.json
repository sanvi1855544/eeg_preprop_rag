[{"name": "10 minutes to pandas", "path": "user_guide/10min", "type": "Manual", "text": "\nThis is a short introduction to pandas, geared mainly for new users. You can\nsee more complex recipes in the Cookbook.\n\nCustomarily, we import as follows:\n\nSee the Intro to data structures section.\n\nCreating a `Series` by passing a list of values, letting pandas create a\ndefault integer index:\n\nCreating a `DataFrame` by passing a NumPy array, with a datetime index and\nlabeled columns:\n\nCreating a `DataFrame` by passing a dictionary of objects that can be\nconverted into a series-like structure:\n\nThe columns of the resulting `DataFrame` have different dtypes:\n\nIf you\u2019re using IPython, tab completion for column names (as well as public\nattributes) is automatically enabled. Here\u2019s a subset of the attributes that\nwill be completed:\n\nAs you can see, the columns `A`, `B`, `C`, and `D` are automatically tab\ncompleted. `E` and `F` are there as well; the rest of the attributes have been\ntruncated for brevity.\n\nSee the Basics section.\n\nHere is how to view the top and bottom rows of the frame:\n\nDisplay the index, columns:\n\n`DataFrame.to_numpy()` gives a NumPy representation of the underlying data.\nNote that this can be an expensive operation when your `DataFrame` has columns\nwith different data types, which comes down to a fundamental difference\nbetween pandas and NumPy: NumPy arrays have one dtype for the entire array,\nwhile pandas DataFrames have one dtype per column. When you call\n`DataFrame.to_numpy()`, pandas will find the NumPy dtype that can hold all of\nthe dtypes in the DataFrame. This may end up being `object`, which requires\ncasting every value to a Python object.\n\nFor `df`, our `DataFrame` of all floating-point values, `DataFrame.to_numpy()`\nis fast and doesn\u2019t require copying data:\n\nFor `df2`, the `DataFrame` with multiple dtypes, `DataFrame.to_numpy()` is\nrelatively expensive:\n\nNote\n\n`DataFrame.to_numpy()` does not include the index or column labels in the\noutput.\n\n`describe()` shows a quick statistic summary of your data:\n\nTransposing your data:\n\nSorting by an axis:\n\nSorting by values:\n\nNote\n\nWhile standard Python / NumPy expressions for selecting and setting are\nintuitive and come in handy for interactive work, for production code, we\nrecommend the optimized pandas data access methods, `.at`, `.iat`, `.loc` and\n`.iloc`.\n\nSee the indexing documentation Indexing and Selecting Data and MultiIndex /\nAdvanced Indexing.\n\nSelecting a single column, which yields a `Series`, equivalent to `df.A`:\n\nSelecting via `[]`, which slices the rows:\n\nSee more in Selection by Label.\n\nFor getting a cross section using a label:\n\nSelecting on a multi-axis by label:\n\nShowing label slicing, both endpoints are included:\n\nReduction in the dimensions of the returned object:\n\nFor getting a scalar value:\n\nFor getting fast access to a scalar (equivalent to the prior method):\n\nSee more in Selection by Position.\n\nSelect via the position of the passed integers:\n\nBy integer slices, acting similar to NumPy/Python:\n\nBy lists of integer position locations, similar to the NumPy/Python style:\n\nFor slicing rows explicitly:\n\nFor slicing columns explicitly:\n\nFor getting a value explicitly:\n\nFor getting fast access to a scalar (equivalent to the prior method):\n\nUsing a single column\u2019s values to select data:\n\nSelecting values from a DataFrame where a boolean condition is met:\n\nUsing the `isin()` method for filtering:\n\nSetting a new column automatically aligns the data by the indexes:\n\nSetting values by label:\n\nSetting values by position:\n\nSetting by assigning with a NumPy array:\n\nThe result of the prior setting operations:\n\nA `where` operation with setting:\n\npandas primarily uses the value `np.nan` to represent missing data. It is by\ndefault not included in computations. See the Missing Data section.\n\nReindexing allows you to change/add/delete the index on a specified axis. This\nreturns a copy of the data:\n\nTo drop any rows that have missing data:\n\nFilling missing data:\n\nTo get the boolean mask where values are `nan`:\n\nSee the Basic section on Binary Ops.\n\nOperations in general exclude missing data.\n\nPerforming a descriptive statistic:\n\nSame operation on the other axis:\n\nOperating with objects that have different dimensionality and need alignment.\nIn addition, pandas automatically broadcasts along the specified dimension:\n\nApplying functions to the data:\n\nSee more at Histogramming and Discretization.\n\nSeries is equipped with a set of string processing methods in the `str`\nattribute that make it easy to operate on each element of the array, as in the\ncode snippet below. Note that pattern-matching in `str` generally uses regular\nexpressions by default (and in some cases always uses them). See more at\nVectorized String Methods.\n\npandas provides various facilities for easily combining together Series and\nDataFrame objects with various kinds of set logic for the indexes and\nrelational algebra functionality in the case of join / merge-type operations.\n\nSee the Merging section.\n\nConcatenating pandas objects together with `concat()`:\n\nNote\n\nAdding a column to a `DataFrame` is relatively fast. However, adding a row\nrequires a copy, and may be expensive. We recommend passing a pre-built list\nof records to the `DataFrame` constructor instead of building a `DataFrame` by\niteratively appending records to it.\n\nSQL style merges. See the Database style joining section.\n\nAnother example that can be given is:\n\nBy \u201cgroup by\u201d we are referring to a process involving one or more of the\nfollowing steps:\n\nSplitting the data into groups based on some criteria\n\nApplying a function to each group independently\n\nCombining the results into a data structure\n\nSee the Grouping section.\n\nGrouping and then applying the `sum()` function to the resulting groups:\n\nGrouping by multiple columns forms a hierarchical index, and again we can\napply the `sum()` function:\n\nSee the sections on Hierarchical Indexing and Reshaping.\n\nThe `stack()` method \u201ccompresses\u201d a level in the DataFrame\u2019s columns:\n\nWith a \u201cstacked\u201d DataFrame or Series (having a `MultiIndex` as the `index`),\nthe inverse operation of `stack()` is `unstack()`, which by default unstacks\nthe last level:\n\nSee the section on Pivot Tables.\n\nWe can produce pivot tables from this data very easily:\n\npandas has simple, powerful, and efficient functionality for performing\nresampling operations during frequency conversion (e.g., converting secondly\ndata into 5-minutely data). This is extremely common in, but not limited to,\nfinancial applications. See the Time Series section.\n\nTime zone representation:\n\nConverting to another time zone:\n\nConverting between time span representations:\n\nConverting between period and timestamp enables some convenient arithmetic\nfunctions to be used. In the following example, we convert a quarterly\nfrequency with year ending in November to 9am of the end of the month\nfollowing the quarter end:\n\npandas can include categorical data in a `DataFrame`. For full docs, see the\ncategorical introduction and the API documentation.\n\nConverting the raw grades to a categorical data type:\n\nRename the categories to more meaningful names (assigning to\n`Series.cat.categories()` is in place!):\n\nReorder the categories and simultaneously add the missing categories (methods\nunder `Series.cat()` return a new `Series` by default):\n\nSorting is per order in the categories, not lexical order:\n\nGrouping by a categorical column also shows empty categories:\n\nSee the Plotting docs.\n\nWe use the standard convention for referencing the matplotlib API:\n\nThe `close()` method is used to close a figure window:\n\nIf running under Jupyter Notebook, the plot will appear on `plot()`. Otherwise\nuse matplotlib.pyplot.show to show it or matplotlib.pyplot.savefig to write it\nto a file.\n\nOn a DataFrame, the `plot()` method is a convenience to plot all of the\ncolumns with labels:\n\nWriting to a csv file:\n\nReading from a csv file:\n\nReading and writing to HDFStores.\n\nWriting to a HDF5 Store:\n\nReading from a HDF5 Store:\n\nReading and writing to MS Excel.\n\nWriting to an excel file:\n\nReading from an excel file:\n\nIf you are attempting to perform an operation you might see an exception like:\n\nSee Comparisons for an explanation and what to do.\n\nSee Gotchas as well.\n\n"}, {"name": "API reference", "path": "reference/index", "type": "General functions", "text": "\nThis page gives an overview of all public pandas objects, functions and\nmethods. All classes and functions exposed in `pandas.*` namespace are public.\n\nSome subpackages are public which include `pandas.errors`, `pandas.plotting`,\nand `pandas.testing`. Public functions in `pandas.io` and `pandas.tseries`\nsubmodules are mentioned in the documentation. `pandas.api.types` subpackage\nholds some public functions related to data types in pandas.\n\nWarning\n\nThe `pandas.core`, `pandas.compat`, and `pandas.util` top-level modules are\nPRIVATE. Stable functionality in such modules is not guaranteed.\n\n"}, {"name": "Categorical data", "path": "user_guide/categorical", "type": "Manual", "text": "\nThis is an introduction to pandas categorical data type, including a short\ncomparison with R\u2019s `factor`.\n\n`Categoricals` are a pandas data type corresponding to categorical variables\nin statistics. A categorical variable takes on a limited, and usually fixed,\nnumber of possible values (`categories`; `levels` in R). Examples are gender,\nsocial class, blood type, country affiliation, observation time or rating via\nLikert scales.\n\nIn contrast to statistical categorical variables, categorical data might have\nan order (e.g. \u2018strongly agree\u2019 vs \u2018agree\u2019 or \u2018first observation\u2019 vs. \u2018second\nobservation\u2019), but numerical operations (additions, divisions, \u2026) are not\npossible.\n\nAll values of categorical data are either in `categories` or `np.nan`. Order\nis defined by the order of `categories`, not lexical order of the values.\nInternally, the data structure consists of a `categories` array and an integer\narray of `codes` which point to the real value in the `categories` array.\n\nThe categorical data type is useful in the following cases:\n\nA string variable consisting of only a few different values. Converting such a\nstring variable to a categorical variable will save some memory, see here.\n\nThe lexical order of a variable is not the same as the logical order (\u201cone\u201d,\n\u201ctwo\u201d, \u201cthree\u201d). By converting to a categorical and specifying an order on the\ncategories, sorting and min/max will use the logical order instead of the\nlexical order, see here.\n\nAs a signal to other Python libraries that this column should be treated as a\ncategorical variable (e.g. to use suitable statistical methods or plot types).\n\nSee also the API docs on categoricals.\n\nCategorical `Series` or columns in a `DataFrame` can be created in several\nways:\n\nBy specifying `dtype=\"category\"` when constructing a `Series`:\n\nBy converting an existing `Series` or column to a `category` dtype:\n\nBy using special functions, such as `cut()`, which groups data into discrete\nbins. See the example on tiling in the docs.\n\nBy passing a `pandas.Categorical` object to a `Series` or assigning it to a\n`DataFrame`.\n\nCategorical data has a specific `category` dtype:\n\nSimilar to the previous section where a single column was converted to\ncategorical, all columns in a `DataFrame` can be batch converted to\ncategorical either during or after construction.\n\nThis can be done during construction by specifying `dtype=\"category\"` in the\n`DataFrame` constructor:\n\nNote that the categories present in each column differ; the conversion is done\ncolumn by column, so only labels present in a given column are categories:\n\nAnalogously, all columns in an existing `DataFrame` can be batch converted\nusing `DataFrame.astype()`:\n\nThis conversion is likewise done column by column:\n\nIn the examples above where we passed `dtype='category'`, we used the default\nbehavior:\n\nCategories are inferred from the data.\n\nCategories are unordered.\n\nTo control those behaviors, instead of passing `'category'`, use an instance\nof `CategoricalDtype`.\n\nSimilarly, a `CategoricalDtype` can be used with a `DataFrame` to ensure that\ncategories are consistent among all columns.\n\nNote\n\nTo perform table-wise conversion, where all labels in the entire `DataFrame`\nare used as categories for each column, the `categories` parameter can be\ndetermined programmatically by `categories =\npd.unique(df.to_numpy().ravel())`.\n\nIf you already have `codes` and `categories`, you can use the `from_codes()`\nconstructor to save the factorize step during normal constructor mode:\n\nTo get back to the original `Series` or NumPy array, use\n`Series.astype(original_dtype)` or `np.asarray(categorical)`:\n\nNote\n\nIn contrast to R\u2019s `factor` function, categorical data is not converting input\nvalues to strings; categories will end up the same data type as the original\nvalues.\n\nNote\n\nIn contrast to R\u2019s `factor` function, there is currently no way to\nassign/change labels at creation time. Use `categories` to change the\ncategories after creation time.\n\nA categorical\u2019s type is fully described by\n\n`categories`: a sequence of unique values and no missing values\n\n`ordered`: a boolean\n\nThis information can be stored in a `CategoricalDtype`. The `categories`\nargument is optional, which implies that the actual categories should be\ninferred from whatever is present in the data when the `pandas.Categorical` is\ncreated. The categories are assumed to be unordered by default.\n\nA `CategoricalDtype` can be used in any place pandas expects a `dtype`. For\nexample `pandas.read_csv()`, `pandas.DataFrame.astype()`, or in the `Series`\nconstructor.\n\nNote\n\nAs a convenience, you can use the string `'category'` in place of a\n`CategoricalDtype` when you want the default behavior of the categories being\nunordered, and equal to the set values present in the array. In other words,\n`dtype='category'` is equivalent to `dtype=CategoricalDtype()`.\n\nTwo instances of `CategoricalDtype` compare equal whenever they have the same\ncategories and order. When comparing two unordered categoricals, the order of\nthe `categories` is not considered.\n\nAll instances of `CategoricalDtype` compare equal to the string `'category'`.\n\nWarning\n\nSince `dtype='category'` is essentially `CategoricalDtype(None, False)`, and\nsince all instances `CategoricalDtype` compare equal to `'category'`, all\ninstances of `CategoricalDtype` compare equal to a `CategoricalDtype(None,\nFalse)`, regardless of `categories` or `ordered`.\n\nUsing `describe()` on categorical data will produce similar output to a\n`Series` or `DataFrame` of type `string`.\n\nCategorical data has a `categories` and a `ordered` property, which list their\npossible values and whether the ordering matters or not. These properties are\nexposed as `s.cat.categories` and `s.cat.ordered`. If you don\u2019t manually\nspecify categories and ordering, they are inferred from the passed arguments.\n\nIt\u2019s also possible to pass in the categories in a specific order:\n\nNote\n\nNew categorical data are not automatically ordered. You must explicitly pass\n`ordered=True` to indicate an ordered `Categorical`.\n\nNote\n\nThe result of `unique()` is not always the same as `Series.cat.categories`,\nbecause `Series.unique()` has a couple of guarantees, namely that it returns\ncategories in the order of appearance, and it only includes values that are\nactually present.\n\nRenaming categories is done by assigning new values to the\n`Series.cat.categories` property or by using the `rename_categories()` method:\n\nNote\n\nIn contrast to R\u2019s `factor`, categorical data can have categories of other\ntypes than string.\n\nNote\n\nBe aware that assigning new categories is an inplace operation, while most\nother operations under `Series.cat` per default return a new `Series` of dtype\n`category`.\n\nCategories must be unique or a `ValueError` is raised:\n\nCategories must also not be `NaN` or a `ValueError` is raised:\n\nAppending categories can be done by using the `add_categories()` method:\n\nRemoving categories can be done by using the `remove_categories()` method.\nValues which are removed are replaced by `np.nan`.:\n\nRemoving unused categories can also be done:\n\nIf you want to do remove and add new categories in one step (which has some\nspeed advantage), or simply set the categories to a predefined scale, use\n`set_categories()`.\n\nNote\n\nBe aware that `Categorical.set_categories()` cannot know whether some category\nis omitted intentionally or because it is misspelled or (under Python3) due to\na type difference (e.g., NumPy S1 dtype and Python strings). This can result\nin surprising behaviour!\n\nIf categorical data is ordered (`s.cat.ordered == True`), then the order of\nthe categories has a meaning and certain operations are possible. If the\ncategorical is unordered, `.min()/.max()` will raise a `TypeError`.\n\nYou can set categorical data to be ordered by using `as_ordered()` or\nunordered by using `as_unordered()`. These will by default return a new\nobject.\n\nSorting will use the order defined by categories, not any lexical order\npresent on the data type. This is even true for strings and numeric data:\n\nReordering the categories is possible via the\n`Categorical.reorder_categories()` and the `Categorical.set_categories()`\nmethods. For `Categorical.reorder_categories()`, all old categories must be\nincluded in the new categories and no new categories are allowed. This will\nnecessarily make the sort order the same as the categories order.\n\nNote\n\nNote the difference between assigning new categories and reordering the\ncategories: the first renames categories and therefore the individual values\nin the `Series`, but if the first position was sorted last, the renamed value\nwill still be sorted last. Reordering means that the way values are sorted is\ndifferent afterwards, but not that individual values in the `Series` are\nchanged.\n\nNote\n\nIf the `Categorical` is not ordered, `Series.min()` and `Series.max()` will\nraise `TypeError`. Numeric operations like `+`, `-`, `*`, `/` and operations\nbased on them (e.g. `Series.median()`, which would need to compute the mean\nbetween two values if the length of an array is even) do not work and raise a\n`TypeError`.\n\nA categorical dtyped column will participate in a multi-column sort in a\nsimilar manner to other columns. The ordering of the categorical is determined\nby the `categories` of that column.\n\nReordering the `categories` changes a future sort.\n\nComparing categorical data with other objects is possible in three cases:\n\nComparing equality (`==` and `!=`) to a list-like object (list, Series, array,\n\u2026) of the same length as the categorical data.\n\nAll comparisons (`==`, `!=`, `>`, `>=`, `<`, and `<=`) of categorical data to\nanother categorical Series, when `ordered==True` and the `categories` are the\nsame.\n\nAll comparisons of a categorical data to a scalar.\n\nAll other comparisons, especially \u201cnon-equality\u201d comparisons of two\ncategoricals with different categories or a categorical with any list-like\nobject, will raise a `TypeError`.\n\nNote\n\nAny \u201cnon-equality\u201d comparisons of categorical data with a `Series`,\n`np.array`, `list` or categorical data with different categories or ordering\nwill raise a `TypeError` because custom categories ordering could be\ninterpreted in two ways: one with taking into account the ordering and one\nwithout.\n\nComparing to a categorical with the same categories and ordering or to a\nscalar works:\n\nEquality comparisons work with any list-like object of same length and\nscalars:\n\nThis doesn\u2019t work because the categories are not the same:\n\nIf you want to do a \u201cnon-equality\u201d comparison of a categorical series with a\nlist-like object which is not categorical data, you need to be explicit and\nconvert the categorical data back to the original values:\n\nWhen you compare two unordered categoricals with the same categories, the\norder is not considered:\n\nApart from `Series.min()`, `Series.max()` and `Series.mode()`, the following\noperations are possible with categorical data:\n\n`Series` methods like `Series.value_counts()` will use all categories, even if\nsome categories are not present in the data:\n\n`DataFrame` methods like `DataFrame.sum()` also show \u201cunused\u201d categories.\n\nGroupby will also show \u201cunused\u201d categories:\n\nPivot tables:\n\nThe optimized pandas data access methods `.loc`, `.iloc`, `.at`, and `.iat`,\nwork as normal. The only difference is the return type (for getting) and that\nonly values already in `categories` can be assigned.\n\nIf the slicing operation returns either a `DataFrame` or a column of type\n`Series`, the `category` dtype is preserved.\n\nAn example where the category type is not preserved is if you take one single\nrow: the resulting `Series` is of dtype `object`:\n\nReturning a single item from categorical data will also return the value, not\na categorical of length \u201c1\u201d.\n\nNote\n\nThe is in contrast to R\u2019s `factor` function, where `factor(c(1,2,3))[1]`\nreturns a single value `factor`.\n\nTo get a single value `Series` of type `category`, you pass in a list with a\nsingle value:\n\nThe accessors `.dt` and `.str` will work if the `s.cat.categories` are of an\nappropriate type:\n\nNote\n\nThe returned `Series` (or `DataFrame`) is of the same type as if you used the\n`.str.<method>` / `.dt.<method>` on a `Series` of that type (and not of type\n`category`!).\n\nThat means, that the returned values from methods and properties on the\naccessors of a `Series` and the returned values from methods and properties on\nthe accessors of this `Series` transformed to one of type `category` will be\nequal:\n\nNote\n\nThe work is done on the `categories` and then a new `Series` is constructed.\nThis has some performance implication if you have a `Series` of type string,\nwhere lots of elements are repeated (i.e. the number of unique elements in the\n`Series` is a lot smaller than the length of the `Series`). In this case it\ncan be faster to convert the original `Series` to one of type `category` and\nuse `.str.<method>` or `.dt.<property>` on that.\n\nSetting values in a categorical column (or `Series`) works as long as the\nvalue is included in the `categories`:\n\nSetting values by assigning categorical data will also check that the\n`categories` match:\n\nAssigning a `Categorical` to parts of a column of other types will use the\nvalues:\n\nBy default, combining `Series` or `DataFrames` which contain the same\ncategories results in `category` dtype, otherwise results will depend on the\ndtype of the underlying categories. Merges that result in non-categorical\ndtypes will likely have higher memory usage. Use `.astype` or\n`union_categoricals` to ensure `category` results.\n\nThe following table summarizes the results of merging `Categoricals`:\n\narg1\n\narg2\n\nidentical\n\nresult\n\ncategory\n\ncategory\n\nTrue\n\ncategory\n\ncategory (object)\n\ncategory (object)\n\nFalse\n\nobject (dtype is inferred)\n\ncategory (int)\n\ncategory (float)\n\nFalse\n\nfloat (dtype is inferred)\n\nSee also the section on merge dtypes for notes about preserving merge dtypes\nand performance.\n\nIf you want to combine categoricals that do not necessarily have the same\ncategories, the `union_categoricals()` function will combine a list-like of\ncategoricals. The new categories will be the union of the categories being\ncombined.\n\nBy default, the resulting categories will be ordered as they appear in the\ndata. If you want the categories to be lexsorted, use `sort_categories=True`\nargument.\n\n`union_categoricals` also works with the \u201ceasy\u201d case of combining two\ncategoricals of the same categories and order information (e.g. what you could\nalso `append` for).\n\nThe below raises `TypeError` because the categories are ordered and not\nidentical.\n\nOrdered categoricals with different categories or orderings can be combined by\nusing the `ignore_ordered=True` argument.\n\n`union_categoricals()` also works with a `CategoricalIndex`, or `Series`\ncontaining categorical data, but note that the resulting array will always be\na plain `Categorical`:\n\nNote\n\n`union_categoricals` may recode the integer codes for categories when\ncombining categoricals. This is likely what you want, but if you are relying\non the exact numbering of the categories, be aware.\n\nYou can write data that contains `category` dtypes to a `HDFStore`. See here\nfor an example and caveats.\n\nIt is also possible to write data to and reading data from Stata format files.\nSee here for an example and caveats.\n\nWriting to a CSV file will convert the data, effectively removing any\ninformation about the categorical (categories and ordering). So if you read\nback the CSV file you have to convert the relevant columns back to `category`\nand assign the right categories and categories ordering.\n\nThe same holds for writing to a SQL database with `to_sql`.\n\npandas primarily uses the value `np.nan` to represent missing data. It is by\ndefault not included in computations. See the Missing Data section.\n\nMissing values should not be included in the Categorical\u2019s `categories`, only\nin the `values`. Instead, it is understood that NaN is different, and is\nalways a possibility. When working with the Categorical\u2019s `codes`, missing\nvalues will always have a code of `-1`.\n\nMethods for working with missing data, e.g. `isna()`, `fillna()`, `dropna()`,\nall work normally:\n\nThe following differences to R\u2019s factor functions can be observed:\n\nR\u2019s `levels` are named `categories`.\n\nR\u2019s `levels` are always of type string, while `categories` in pandas can be of\nany dtype.\n\nIt\u2019s not possible to specify labels at creation time. Use\n`s.cat.rename_categories(new_labels)` afterwards.\n\nIn contrast to R\u2019s `factor` function, using categorical data as the sole input\nto create a new categorical series will not remove unused categories but\ncreate a new categorical series which is equal to the passed in one!\n\nR allows for missing values to be included in its `levels` (pandas\u2019\n`categories`). pandas does not allow `NaN` categories, but missing values can\nstill be in the `values`.\n\nThe memory usage of a `Categorical` is proportional to the number of\ncategories plus the length of the data. In contrast, an `object` dtype is a\nconstant times the length of the data.\n\nNote\n\nIf the number of categories approaches the length of the data, the\n`Categorical` will use nearly the same or more memory than an equivalent\n`object` dtype representation.\n\nCurrently, categorical data and the underlying `Categorical` is implemented as\na Python object and not as a low-level NumPy array dtype. This leads to some\nproblems.\n\nNumPy itself doesn\u2019t know about the new `dtype`:\n\nDtype comparisons work:\n\nTo check if a Series contains Categorical data, use `hasattr(s, 'cat')`:\n\nUsing NumPy functions on a `Series` of type `category` should not work as\n`Categoricals` are not numeric data (even in the case that `.categories` is\nnumeric).\n\nNote\n\nIf such a function works, please file a bug at https://github.com/pandas-\ndev/pandas!\n\npandas currently does not preserve the dtype in apply functions: If you apply\nalong rows you get a `Series` of `object` `dtype` (same as getting a row ->\ngetting one element will return a basic type) and applying along columns will\nalso convert to object. `NaN` values are unaffected. You can use `fillna` to\nhandle missing values before applying a function.\n\n`CategoricalIndex` is a type of index that is useful for supporting indexing\nwith duplicates. This is a container around a `Categorical` and allows\nefficient indexing and storage of an index with a large number of duplicated\nelements. See the advanced indexing docs for a more detailed explanation.\n\nSetting the index will create a `CategoricalIndex`:\n\nConstructing a `Series` from a `Categorical` will not copy the input\n`Categorical`. This means that changes to the `Series` will in most cases\nchange the original `Categorical`:\n\nUse `copy=True` to prevent such a behaviour or simply don\u2019t reuse\n`Categoricals`:\n\nNote\n\nThis also happens in some cases when you supply a NumPy array instead of a\n`Categorical`: using an int array (e.g. `np.array([1,2,3,4])`) will exhibit\nthe same behavior, while using a string array (e.g.\n`np.array([\"a\",\"b\",\"c\",\"a\"])`) will not.\n\n"}, {"name": "Chart Visualization", "path": "user_guide/visualization", "type": "Manual", "text": "\nThis section demonstrates visualization through charting. For information on\nvisualization of tabular data please see the section on Table Visualization.\n\nWe use the standard convention for referencing the matplotlib API:\n\nWe provide the basics in pandas to easily create decent looking plots. See the\necosystem section for visualization libraries that go beyond the basics\ndocumented here.\n\nNote\n\nAll calls to `np.random` are seeded with 123456.\n\nWe will demonstrate the basics, see the cookbook for some advanced strategies.\n\nThe `plot` method on Series and DataFrame is just a simple wrapper around\n`plt.plot()`:\n\nIf the index consists of dates, it calls `gcf().autofmt_xdate()` to try to\nformat the x-axis nicely as per above.\n\nOn DataFrame, `plot()` is a convenience to plot all of the columns with\nlabels:\n\nYou can plot one column versus another using the `x` and `y` keywords in\n`plot()`:\n\nNote\n\nFor more formatting and styling options, see formatting below.\n\nPlotting methods allow for a handful of plot styles other than the default\nline plot. These methods can be provided as the `kind` keyword argument to\n`plot()`, and include:\n\n\u2018bar\u2019 or \u2018barh\u2019 for bar plots\n\n\u2018hist\u2019 for histogram\n\n\u2018box\u2019 for boxplot\n\n\u2018kde\u2019 or \u2018density\u2019 for density plots\n\n\u2018area\u2019 for area plots\n\n\u2018scatter\u2019 for scatter plots\n\n\u2018hexbin\u2019 for hexagonal bin plots\n\n\u2018pie\u2019 for pie plots\n\nFor example, a bar plot can be created the following way:\n\nYou can also create these other plots using the methods\n`DataFrame.plot.<kind>` instead of providing the `kind` keyword argument. This\nmakes it easier to discover plot methods and the specific arguments they use:\n\nIn addition to these `kind` s, there are the DataFrame.hist(), and\nDataFrame.boxplot() methods, which use a separate interface.\n\nFinally, there are several plotting functions in `pandas.plotting` that take a\n`Series` or `DataFrame` as an argument. These include:\n\nScatter Matrix\n\nAndrews Curves\n\nParallel Coordinates\n\nLag Plot\n\nAutocorrelation Plot\n\nBootstrap Plot\n\nRadViz\n\nPlots may also be adorned with errorbars or tables.\n\nFor labeled, non-time series data, you may wish to produce a bar plot:\n\nCalling a DataFrame\u2019s `plot.bar()` method produces a multiple bar plot:\n\nTo produce a stacked bar plot, pass `stacked=True`:\n\nTo get horizontal bar plots, use the `barh` method:\n\nHistograms can be drawn by using the `DataFrame.plot.hist()` and\n`Series.plot.hist()` methods.\n\nA histogram can be stacked using `stacked=True`. Bin size can be changed using\nthe `bins` keyword.\n\nYou can pass other keywords supported by matplotlib `hist`. For example,\nhorizontal and cumulative histograms can be drawn by\n`orientation='horizontal'` and `cumulative=True`.\n\nSee the `hist` method and the matplotlib hist documentation for more.\n\nThe existing interface `DataFrame.hist` to plot histogram still can be used.\n\n`DataFrame.hist()` plots the histograms of the columns on multiple subplots:\n\nThe `by` keyword can be specified to plot grouped histograms:\n\nIn addition, the `by` keyword can also be specified in\n`DataFrame.plot.hist()`.\n\nChanged in version 1.4.0.\n\nBoxplot can be drawn calling `Series.plot.box()` and `DataFrame.plot.box()`,\nor `DataFrame.boxplot()` to visualize the distribution of values within each\ncolumn.\n\nFor instance, here is a boxplot representing five trials of 10 observations of\na uniform random variable on [0,1).\n\nBoxplot can be colorized by passing `color` keyword. You can pass a `dict`\nwhose keys are `boxes`, `whiskers`, `medians` and `caps`. If some keys are\nmissing in the `dict`, default colors are used for the corresponding artists.\nAlso, boxplot has `sym` keyword to specify fliers style.\n\nWhen you pass other type of arguments via `color` keyword, it will be directly\npassed to matplotlib for all the `boxes`, `whiskers`, `medians` and `caps`\ncolorization.\n\nThe colors are applied to every boxes to be drawn. If you want more\ncomplicated colorization, you can get each drawn artists by passing\nreturn_type.\n\nAlso, you can pass other keywords supported by matplotlib `boxplot`. For\nexample, horizontal and custom-positioned boxplot can be drawn by `vert=False`\nand `positions` keywords.\n\nSee the `boxplot` method and the matplotlib boxplot documentation for more.\n\nThe existing interface `DataFrame.boxplot` to plot boxplot still can be used.\n\nYou can create a stratified boxplot using the `by` keyword argument to create\ngroupings. For instance,\n\nYou can also pass a subset of columns to plot, as well as group by multiple\ncolumns:\n\nYou could also create groupings with `DataFrame.plot.box()`, for instance:\n\nChanged in version 1.4.0.\n\nIn `boxplot`, the return type can be controlled by the `return_type`, keyword.\nThe valid choices are `{\"axes\", \"dict\", \"both\", None}`. Faceting, created by\n`DataFrame.boxplot` with the `by` keyword, will affect the output type as\nwell:\n\n`return_type`\n\nFaceted\n\nOutput type\n\n`None`\n\nNo\n\naxes\n\n`None`\n\nYes\n\n2-D ndarray of axes\n\n`'axes'`\n\nNo\n\naxes\n\n`'axes'`\n\nYes\n\nSeries of axes\n\n`'dict'`\n\nNo\n\ndict of artists\n\n`'dict'`\n\nYes\n\nSeries of dicts of artists\n\n`'both'`\n\nNo\n\nnamedtuple\n\n`'both'`\n\nYes\n\nSeries of namedtuples\n\n`Groupby.boxplot` always returns a `Series` of `return_type`.\n\nThe subplots above are split by the numeric columns first, then the value of\nthe `g` column. Below the subplots are first split by the value of `g`, then\nby the numeric columns.\n\nYou can create area plots with `Series.plot.area()` and\n`DataFrame.plot.area()`. Area plots are stacked by default. To produce stacked\narea plot, each column must be either all positive or all negative values.\n\nWhen input data contains `NaN`, it will be automatically filled by 0. If you\nwant to drop or fill by different values, use `dataframe.dropna()` or\n`dataframe.fillna()` before calling `plot`.\n\nTo produce an unstacked plot, pass `stacked=False`. Alpha value is set to 0.5\nunless otherwise specified:\n\nScatter plot can be drawn by using the `DataFrame.plot.scatter()` method.\nScatter plot requires numeric columns for the x and y axes. These can be\nspecified by the `x` and `y` keywords.\n\nTo plot multiple column groups in a single axes, repeat `plot` method\nspecifying target `ax`. It is recommended to specify `color` and `label`\nkeywords to distinguish each groups.\n\nThe keyword `c` may be given as the name of a column to provide colors for\neach point:\n\nIf a categorical column is passed to `c`, then a discrete colorbar will be\nproduced:\n\nNew in version 1.3.0.\n\nYou can pass other keywords supported by matplotlib `scatter`. The example\nbelow shows a bubble chart using a column of the `DataFrame` as the bubble\nsize.\n\nSee the `scatter` method and the matplotlib scatter documentation for more.\n\nYou can create hexagonal bin plots with `DataFrame.plot.hexbin()`. Hexbin\nplots can be a useful alternative to scatter plots if your data are too dense\nto plot each point individually.\n\nA useful keyword argument is `gridsize`; it controls the number of hexagons in\nthe x-direction, and defaults to 100. A larger `gridsize` means more, smaller\nbins.\n\nBy default, a histogram of the counts around each `(x, y)` point is computed.\nYou can specify alternative aggregations by passing values to the `C` and\n`reduce_C_function` arguments. `C` specifies the value at each `(x, y)` point\nand `reduce_C_function` is a function of one argument that reduces all the\nvalues in a bin to a single number (e.g. `mean`, `max`, `sum`, `std`). In this\nexample the positions are given by columns `a` and `b`, while the value is\ngiven by column `z`. The bins are aggregated with NumPy\u2019s `max` function.\n\nSee the `hexbin` method and the matplotlib hexbin documentation for more.\n\nYou can create a pie plot with `DataFrame.plot.pie()` or `Series.plot.pie()`.\nIf your data includes any `NaN`, they will be automatically filled with 0. A\n`ValueError` will be raised if there are any negative values in your data.\n\nFor pie plots it\u2019s best to use square figures, i.e. a figure aspect ratio 1.\nYou can create the figure with equal width and height, or force the aspect\nratio to be equal after plotting by calling `ax.set_aspect('equal')` on the\nreturned `axes` object.\n\nNote that pie plot with `DataFrame` requires that you either specify a target\ncolumn by the `y` argument or `subplots=True`. When `y` is specified, pie plot\nof selected column will be drawn. If `subplots=True` is specified, pie plots\nfor each column are drawn as subplots. A legend will be drawn in each pie\nplots by default; specify `legend=False` to hide it.\n\nYou can use the `labels` and `colors` keywords to specify the labels and\ncolors of each wedge.\n\nWarning\n\nMost pandas plots use the `label` and `color` arguments (note the lack of \u201cs\u201d\non those). To be consistent with `matplotlib.pyplot.pie()` you must use\n`labels` and `colors`.\n\nIf you want to hide wedge labels, specify `labels=None`. If `fontsize` is\nspecified, the value will be applied to wedge labels. Also, other keywords\nsupported by `matplotlib.pyplot.pie()` can be used.\n\nIf you pass values whose sum total is less than 1.0, matplotlib draws a\nsemicircle.\n\nSee the matplotlib pie documentation for more.\n\npandas tries to be pragmatic about plotting `DataFrames` or `Series` that\ncontain missing data. Missing values are dropped, left out, or filled\ndepending on the plot type.\n\nPlot Type\n\nNaN Handling\n\nLine\n\nLeave gaps at NaNs\n\nLine (stacked)\n\nFill 0\u2019s\n\nBar\n\nFill 0\u2019s\n\nScatter\n\nDrop NaNs\n\nHistogram\n\nDrop NaNs (column-wise)\n\nBox\n\nDrop NaNs (column-wise)\n\nArea\n\nFill 0\u2019s\n\nKDE\n\nDrop NaNs (column-wise)\n\nHexbin\n\nDrop NaNs\n\nPie\n\nFill 0\u2019s\n\nIf any of these defaults are not what you want, or if you want to be explicit\nabout how missing values are handled, consider using `fillna()` or `dropna()`\nbefore plotting.\n\nThese functions can be imported from `pandas.plotting` and take a `Series` or\n`DataFrame` as an argument.\n\nYou can create a scatter plot matrix using the `scatter_matrix` method in\n`pandas.plotting`:\n\nYou can create density plots using the `Series.plot.kde()` and\n`DataFrame.plot.kde()` methods.\n\nAndrews curves allow one to plot multivariate data as a large number of curves\nthat are created using the attributes of samples as coefficients for Fourier\nseries, see the Wikipedia entry for more information. By coloring these curves\ndifferently for each class it is possible to visualize data clustering. Curves\nbelonging to samples of the same class will usually be closer together and\nform larger structures.\n\nNote: The \u201cIris\u201d dataset is available here.\n\nParallel coordinates is a plotting technique for plotting multivariate data,\nsee the Wikipedia entry for an introduction. Parallel coordinates allows one\nto see clusters in data and to estimate other statistics visually. Using\nparallel coordinates points are represented as connected line segments. Each\nvertical line represents one attribute. One set of connected line segments\nrepresents one data point. Points that tend to cluster will appear closer\ntogether.\n\nLag plots are used to check if a data set or time series is random. Random\ndata should not exhibit any structure in the lag plot. Non-random structure\nimplies that the underlying data are not random. The `lag` argument may be\npassed, and when `lag=1` the plot is essentially `data[:-1]` vs. `data[1:]`.\n\nAutocorrelation plots are often used for checking randomness in time series.\nThis is done by computing autocorrelations for data values at varying time\nlags. If time series is random, such autocorrelations should be near zero for\nany and all time-lag separations. If time series is non-random then one or\nmore of the autocorrelations will be significantly non-zero. The horizontal\nlines displayed in the plot correspond to 95% and 99% confidence bands. The\ndashed line is 99% confidence band. See the Wikipedia entry for more about\nautocorrelation plots.\n\nBootstrap plots are used to visually assess the uncertainty of a statistic,\nsuch as mean, median, midrange, etc. A random subset of a specified size is\nselected from a data set, the statistic in question is computed for this\nsubset and the process is repeated a specified number of times. Resulting\nplots and histograms are what constitutes the bootstrap plot.\n\nRadViz is a way of visualizing multi-variate data. It is based on a simple\nspring tension minimization algorithm. Basically you set up a bunch of points\nin a plane. In our case they are equally spaced on a unit circle. Each point\nrepresents a single attribute. You then pretend that each sample in the data\nset is attached to each of these points by a spring, the stiffness of which is\nproportional to the numerical value of that attribute (they are normalized to\nunit interval). The point in the plane, where our sample settles to (where the\nforces acting on our sample are at an equilibrium) is where a dot representing\nour sample will be drawn. Depending on which class that sample belongs it will\nbe colored differently. See the R package Radviz for more information.\n\nNote: The \u201cIris\u201d dataset is available here.\n\nFrom version 1.5 and up, matplotlib offers a range of pre-configured plotting\nstyles. Setting the style can be used to easily give plots the general look\nthat you want. Setting the style is as easy as calling\n`matplotlib.style.use(my_plot_style)` before creating your plot. For example\nyou could write `matplotlib.style.use('ggplot')` for ggplot-style plots.\n\nYou can see the various available style names at `matplotlib.style.available`\nand it\u2019s very easy to try them out.\n\nMost plotting methods have a set of keyword arguments that control the layout\nand formatting of the returned plot:\n\nFor each kind of plot (e.g. `line`, `bar`, `scatter`) any additional arguments\nkeywords are passed along to the corresponding matplotlib function\n(`ax.plot()`, `ax.bar()`, `ax.scatter()`). These can be used to control\nadditional styling, beyond what pandas provides.\n\nYou may set the `legend` argument to `False` to hide the legend, which is\nshown by default.\n\nNew in version 1.1.0.\n\nYou may set the `xlabel` and `ylabel` arguments to give the plot custom labels\nfor x and y axis. By default, pandas will pick up index name as xlabel, while\nleaving it empty for ylabel.\n\nYou may pass `logy` to get a log-scale Y axis.\n\nSee also the `logx` and `loglog` keyword arguments.\n\nTo plot data on a secondary y-axis, use the `secondary_y` keyword:\n\nTo plot some columns in a `DataFrame`, give the column names to the\n`secondary_y` keyword:\n\nNote that the columns plotted on the secondary y-axis is automatically marked\nwith \u201c(right)\u201d in the legend. To turn off the automatic marking, use the\n`mark_right=False` keyword:\n\nChanged in version 1.0.0.\n\npandas provides custom formatters for timeseries plots. These change the\nformatting of the axis labels for dates and times. By default, the custom\nformatters are applied only to plots created by pandas with `DataFrame.plot()`\nor `Series.plot()`. To have them apply to all plots, including those made by\nmatplotlib, set the option `pd.options.plotting.matplotlib.register_converters\n= True` or use `pandas.plotting.register_matplotlib_converters()`.\n\npandas includes automatic tick resolution adjustment for regular frequency\ntime-series data. For limited cases where pandas cannot infer the frequency\ninformation (e.g., in an externally created `twinx`), you can choose to\nsuppress this behavior for alignment purposes.\n\nHere is the default behavior, notice how the x-axis tick labeling is\nperformed:\n\nUsing the `x_compat` parameter, you can suppress this behavior:\n\nIf you have more than one plot that needs to be suppressed, the `use` method\nin `pandas.plotting.plot_params` can be used in a `with` statement:\n\n`TimedeltaIndex` now uses the native matplotlib tick locator methods, it is\nuseful to call the automatic date tick adjustment from matplotlib for figures\nwhose ticklabels overlap.\n\nSee the `autofmt_xdate` method and the matplotlib documentation for more.\n\nEach `Series` in a `DataFrame` can be plotted on a different axis with the\n`subplots` keyword:\n\nThe layout of subplots can be specified by the `layout` keyword. It can accept\n`(rows, columns)`. The `layout` keyword can be used in `hist` and `boxplot`\nalso. If the input is invalid, a `ValueError` will be raised.\n\nThe number of axes which can be contained by rows x columns specified by\n`layout` must be larger than the number of required subplots. If layout can\ncontain more axes than required, blank axes are not drawn. Similar to a NumPy\narray\u2019s `reshape` method, you can use `-1` for one dimension to automatically\ncalculate the number of rows or columns needed, given the other.\n\nThe above example is identical to using:\n\nThe required number of columns (3) is inferred from the number of series to\nplot and the given number of rows (2).\n\nYou can pass multiple axes created beforehand as list-like via `ax` keyword.\nThis allows more complicated layouts. The passed axes must be the same number\nas the subplots being drawn.\n\nWhen multiple axes are passed via the `ax` keyword, `layout`, `sharex` and\n`sharey` keywords don\u2019t affect to the output. You should explicitly pass\n`sharex=False` and `sharey=False`, otherwise you will see a warning.\n\nAnother option is passing an `ax` argument to `Series.plot()` to plot on a\nparticular axis:\n\nPlotting with error bars is supported in `DataFrame.plot()` and\n`Series.plot()`.\n\nHorizontal and vertical error bars can be supplied to the `xerr` and `yerr`\nkeyword arguments to `plot()`. The error values can be specified using a\nvariety of formats:\n\nAs a `DataFrame` or `dict` of errors with column names matching the `columns`\nattribute of the plotting `DataFrame` or matching the `name` attribute of the\n`Series`.\n\nAs a `str` indicating which of the columns of plotting `DataFrame` contain the\nerror values.\n\nAs raw values (`list`, `tuple`, or `np.ndarray`). Must be the same length as\nthe plotting `DataFrame`/`Series`.\n\nHere is an example of one way to easily plot group means with standard\ndeviations from the raw data.\n\nAsymmetrical error bars are also supported, however raw error values must be\nprovided in this case. For a `N` length `Series`, a `2xN` array should be\nprovided indicating lower and upper (or left and right) errors. For a `MxN`\n`DataFrame`, asymmetrical errors should be in a `Mx2xN` array.\n\nHere is an example of one way to plot the min/max range using asymmetrical\nerror bars.\n\nPlotting with matplotlib table is now supported in `DataFrame.plot()` and\n`Series.plot()` with a `table` keyword. The `table` keyword can accept `bool`,\n`DataFrame` or `Series`. The simple way to draw a table is to specify\n`table=True`. Data will be transposed to meet matplotlib\u2019s default layout.\n\nAlso, you can pass a different `DataFrame` or `Series` to the `table` keyword.\nThe data will be drawn as displayed in print method (not transposed\nautomatically). If required, it should be transposed manually as seen in the\nexample below.\n\nThere also exists a helper function `pandas.plotting.table`, which creates a\ntable from `DataFrame` or `Series`, and adds it to an `matplotlib.Axes`\ninstance. This function can accept keywords which the matplotlib table has.\n\nNote: You can get table instances on the axes using `axes.tables` property for\nfurther decorations. See the matplotlib table documentation for more.\n\nA potential issue when plotting a large number of columns is that it can be\ndifficult to distinguish some series due to repetition in the default colors.\nTo remedy this, `DataFrame` plotting supports the use of the `colormap`\nargument, which accepts either a Matplotlib colormap or a string that is a\nname of a colormap registered with Matplotlib. A visualization of the default\nmatplotlib colormaps is available here.\n\nAs matplotlib does not directly support colormaps for line-based plots, the\ncolors are selected based on an even spacing determined by the number of\ncolumns in the `DataFrame`. There is no consideration made for background\ncolor, so some colormaps will produce lines that are not easily visible.\n\nTo use the cubehelix colormap, we can pass `colormap='cubehelix'`.\n\nAlternatively, we can pass the colormap itself:\n\nColormaps can also be used other plot types, like bar charts:\n\nParallel coordinates charts:\n\nAndrews curves charts:\n\nIn some situations it may still be preferable or necessary to prepare plots\ndirectly with matplotlib, for instance when a certain type of plot or\ncustomization is not (yet) supported by pandas. `Series` and `DataFrame`\nobjects behave like arrays and can therefore be passed directly to matplotlib\nfunctions without explicit casts.\n\npandas also automatically registers formatters and locators that recognize\ndate indices, thereby extending date and time support to practically all plot\ntypes available in matplotlib. Although this formatting does not provide the\nsame level of refinement you would get when plotting via pandas, it can be\nfaster when plotting a large number of points.\n\nStarting in version 0.25, pandas can be extended with third-party plotting\nbackends. The main idea is letting users select a plotting backend different\nthan the provided one based on Matplotlib.\n\nThis can be done by passing \u2018backend.module\u2019 as the argument `backend` in\n`plot` function. For example:\n\nAlternatively, you can also set this option globally, do you don\u2019t need to\nspecify the keyword in each `plot` call. For example:\n\nOr:\n\nThis would be more or less equivalent to:\n\nThe backend module can then use other visualization tools (Bokeh, Altair,\nhvplot,\u2026) to generate the plots. Some libraries implementing a backend for\npandas are listed on the ecosystem Visualization page.\n\nDevelopers guide can be found at\nhttps://pandas.pydata.org/docs/dev/development/extending.html#plotting-\nbackends\n\n"}, {"name": "Computational tools", "path": "user_guide/computation", "type": "Manual", "text": "\n`Series` and `DataFrame` have a method `pct_change()` to compute the percent\nchange over a given number of periods (using `fill_method` to fill NA/null\nvalues before computing the percent change).\n\n`Series.cov()` can be used to compute covariance between series (excluding\nmissing values).\n\nAnalogously, `DataFrame.cov()` to compute pairwise covariances among the\nseries in the DataFrame, also excluding NA/null values.\n\nNote\n\nAssuming the missing data are missing at random this results in an estimate\nfor the covariance matrix which is unbiased. However, for many applications\nthis estimate may not be acceptable because the estimated covariance matrix is\nnot guaranteed to be positive semi-definite. This could lead to estimated\ncorrelations having absolute values which are greater than one, and/or a non-\ninvertible covariance matrix. See Estimation of covariance matrices for more\ndetails.\n\n`DataFrame.cov` also supports an optional `min_periods` keyword that specifies\nthe required minimum number of observations for each column pair in order to\nhave a valid result.\n\nCorrelation may be computed using the `corr()` method. Using the `method`\nparameter, several methods for computing correlations are provided:\n\nMethod name\n\nDescription\n\n`pearson (default)`\n\nStandard correlation coefficient\n\n`kendall`\n\nKendall Tau correlation coefficient\n\n`spearman`\n\nSpearman rank correlation coefficient\n\nAll of these are currently computed using pairwise complete observations.\nWikipedia has articles covering the above correlation coefficients:\n\nPearson correlation coefficient\n\nKendall rank correlation coefficient\n\nSpearman\u2019s rank correlation coefficient\n\nNote\n\nPlease see the caveats associated with this method of calculating correlation\nmatrices in the covariance section.\n\nNote that non-numeric columns will be automatically excluded from the\ncorrelation calculation.\n\nLike `cov`, `corr` also supports the optional `min_periods` keyword:\n\nThe `method` argument can also be a callable for a generic correlation\ncalculation. In this case, it should be a single function that produces a\nsingle value from two ndarray inputs. Suppose we wanted to compute the\ncorrelation based on histogram intersection:\n\nA related method `corrwith()` is implemented on DataFrame to compute the\ncorrelation between like-labeled Series contained in different DataFrame\nobjects.\n\nThe `rank()` method produces a data ranking with ties being assigned the mean\nof the ranks (by default) for the group:\n\n`rank()` is also a DataFrame method and can rank either the rows (`axis=0`) or\nthe columns (`axis=1`). `NaN` values are excluded from the ranking.\n\n`rank` optionally takes a parameter `ascending` which by default is true; when\nfalse, data is reverse-ranked, with larger values assigned a smaller rank.\n\n`rank` supports different tie-breaking methods, specified with the `method`\nparameter:\n\n`average` : average rank of tied group\n\n`min` : lowest rank in the group\n\n`max` : highest rank in the group\n\n`first` : ranks assigned in the order they appear in the array\n\nSee the window operations user guide for an overview of windowing functions.\n\n"}, {"name": "Cookbook", "path": "user_guide/cookbook", "type": "Manual", "text": "\nThis is a repository for short and sweet examples and links for useful pandas\nrecipes. We encourage users to add to this documentation.\n\nAdding interesting links and/or inline examples to this section is a great\nFirst Pull Request.\n\nSimplified, condensed, new-user friendly, in-line examples have been inserted\nwhere possible to augment the Stack-Overflow and GitHub links. Many of the\nlinks contain expanded information, above what the in-line examples offer.\n\npandas (pd) and NumPy (np) are the only two abbreviated imported modules. The\nrest are kept explicitly imported for newer users.\n\nThese are some neat pandas `idioms`\n\nif-then/if-then-else on one column, and assignment to another one or more\ncolumns:\n\nAn if-then on one column\n\nAn if-then with assignment to 2 columns:\n\nAdd another line with different logic, to do the -else\n\nOr use pandas where after you\u2019ve set up a mask\n\nif-then-else using NumPy\u2019s where()\n\nSplit a frame with a boolean criterion\n\nSelect with multi-column criteria\n\n\u2026and (without assignment returns a Series)\n\n\u2026or (without assignment returns a Series)\n\n\u2026or (with assignment modifies the DataFrame.)\n\nSelect rows with data closest to certain value using argsort\n\nDynamically reduce a list of criteria using a binary operators\n\nOne could hard code:\n\n\u2026Or it can be done with a list of dynamically built criteria\n\nThe indexing docs.\n\nUsing both row labels and value conditionals\n\nUse loc for label-oriented slicing and iloc positional slicing GH2904\n\nThere are 2 explicit slicing methods, with a third general case\n\nPositional-oriented (Python slicing style : exclusive of end)\n\nLabel-oriented (Non-Python slicing style : inclusive of end)\n\nGeneral (Either slicing style : depends on if the slice contains labels or\npositions)\n\nAmbiguity arises when an index consists of integers with a non-zero start or\nnon-unit increment.\n\nUsing inverse operator (~) to take the complement of a mask\n\nEfficiently and dynamically creating new columns using applymap\n\nKeep other columns when using min() with groupby\n\nMethod 1 : idxmin() to get the index of the minimums\n\nMethod 2 : sort then take first of each\n\nNotice the same results, with the exception of the index.\n\nThe multindexing docs.\n\nCreating a MultiIndex from a labeled frame\n\nPerforming arithmetic with a MultiIndex that needs broadcasting\n\nSlicing a MultiIndex with xs\n\nTo take the cross section of the 1st level and 1st axis the index:\n\n\u2026and now the 2nd level of the 1st axis.\n\nSlicing a MultiIndex with xs, method #2\n\nSetting portions of a MultiIndex with xs\n\nSort by specific column or an ordered list of columns, with a MultiIndex\n\nPartial selection, the need for sortedness GH2995\n\nPrepending a level to a multiindex\n\nFlatten Hierarchical columns\n\nThe missing data docs.\n\nFill forward a reversed timeseries\n\ncumsum reset at NaN values\n\nUsing replace with backrefs\n\nThe grouping docs.\n\nBasic grouping with apply\n\nUnlike agg, apply\u2019s callable is passed a sub-DataFrame which gives you access\nto all the columns\n\nUsing get_group\n\nApply to different items in a group\n\nExpanding apply\n\nReplacing some values with mean of the rest of a group\n\nSort groups by aggregated data\n\nCreate multiple aggregated columns\n\nCreate a value counts column and reassign back to the DataFrame\n\nShift groups of the values in a column based on the index\n\nSelect row with maximum value from each group\n\nGrouping like Python\u2019s itertools.groupby\n\nAlignment and to-date\n\nRolling Computation window based on values instead of counts\n\nRolling Mean by Time Interval\n\nSplitting a frame\n\nCreate a list of dataframes, split using a delineation based on logic included\nin rows.\n\nThe Pivot docs.\n\nPartial sums and subtotals\n\nFrequency table like plyr in R\n\nPlot pandas DataFrame with year over year data\n\nTo create year and month cross tabulation:\n\nRolling apply to organize - Turning embedded lists into a MultiIndex frame\n\nRolling apply with a DataFrame returning a Series\n\nRolling Apply to multiple columns where function calculates a Series before a\nScalar from the Series is returned\n\nRolling apply with a DataFrame returning a Scalar\n\nRolling Apply to multiple columns where function returns a Scalar (Volume\nWeighted Average Price)\n\nBetween times\n\nUsing indexer between time\n\nConstructing a datetime range that excludes weekends and includes only certain\ntimes\n\nVectorized Lookup\n\nAggregation and plotting time series\n\nTurn a matrix with hours in columns and days in rows into a continuous row\nsequence in the form of a time series. How to rearrange a Python pandas\nDataFrame?\n\nDealing with duplicates when reindexing a timeseries to a specified frequency\n\nCalculate the first day of the month for each entry in a DatetimeIndex\n\nThe Resample docs.\n\nUsing Grouper instead of TimeGrouper for time grouping of values\n\nTime grouping with some missing values\n\nValid frequency arguments to Grouper Timeseries\n\nGrouping using a MultiIndex\n\nUsing TimeGrouper and another grouping to create subgroups, then apply a\ncustom function GH3791\n\nResampling with custom periods\n\nResample intraday frame without adding new days\n\nResample minute data\n\nResample with groupby\n\nThe Join docs.\n\nConcatenate two dataframes with overlapping index (emulate R rbind)\n\nDepending on df construction, `ignore_index` may be needed\n\nSelf Join of a DataFrame GH2996\n\nHow to set the index and join\n\nKDB like asof join\n\nJoin with a criteria based on the values\n\nUsing searchsorted to merge based on values inside a range\n\nThe Plotting docs.\n\nMake Matplotlib look like R\n\nSetting x-axis major and minor labels\n\nPlotting multiple charts in an IPython Jupyter notebook\n\nCreating a multi-line plot\n\nPlotting a heatmap\n\nAnnotate a time-series plot\n\nAnnotate a time-series plot #2\n\nGenerate Embedded plots in excel files using Pandas, Vincent and xlsxwriter\n\nBoxplot for each quartile of a stratifying variable\n\nPerformance comparison of SQL vs HDF5\n\nThe CSV docs\n\nread_csv in action\n\nappending to a csv\n\nReading a csv chunk-by-chunk\n\nReading only certain rows of a csv chunk-by-chunk\n\nReading the first few lines of a frame\n\nReading a file that is compressed but not by `gzip/bz2` (the native compressed\nformats which `read_csv` understands). This example shows a `WinZipped` file,\nbut is a general application of opening the file within a context manager and\nusing that handle to read. See here\n\nInferring dtypes from a file\n\nDealing with bad lines GH2886\n\nWrite a multi-row index CSV without writing duplicates\n\nThe best way to combine multiple files into a single DataFrame is to read the\nindividual frames one by one, put all of the individual frames into a list,\nand then combine the frames in the list using `pd.concat()`:\n\nYou can use the same approach to read all files matching a pattern. Here is an\nexample using `glob`:\n\nFinally, this strategy will work with the other `pd.read_*(...)` functions\ndescribed in the io docs.\n\nParsing date components in multi-columns is faster with a format\n\nThe SQL docs\n\nReading from databases with SQL\n\nThe Excel docs\n\nReading from a filelike handle\n\nModifying formatting in XlsxWriter output\n\nLoading only visible sheets GH19842#issuecomment-892150745\n\nReading HTML tables from a server that cannot handle the default request\nheader\n\nThe HDFStores docs\n\nSimple queries with a Timestamp Index\n\nManaging heterogeneous data using a linked multiple table hierarchy GH3032\n\nMerging on-disk tables with millions of rows\n\nAvoiding inconsistencies when writing to a store from multiple\nprocesses/threads\n\nDe-duplicating a large store by chunks, essentially a recursive reduction\noperation. Shows a function for taking in data from csv file and creating a\nstore by chunks, with date parsing as well. See here\n\nCreating a store chunk-by-chunk from a csv file\n\nAppending to a store, while creating a unique index\n\nLarge Data work flows\n\nReading in a sequence of files, then providing a global unique index to a\nstore while appending\n\nGroupby on a HDFStore with low group density\n\nGroupby on a HDFStore with high group density\n\nHierarchical queries on a HDFStore\n\nCounting with a HDFStore\n\nTroubleshoot HDFStore exceptions\n\nSetting min_itemsize with strings\n\nUsing ptrepack to create a completely-sorted-index on a store\n\nStoring Attributes to a group node\n\nYou can create or load a HDFStore in-memory by passing the `driver` parameter\nto PyTables. Changes are only written to disk when the HDFStore is closed.\n\npandas readily accepts NumPy record arrays, if you need to read in a binary\nfile consisting of an array of C structs. For example, given this C program in\na file called `main.c` compiled with `gcc main.c -std=gnu99` on a 64-bit\nmachine,\n\nthe following Python code will read the binary file `'binary.dat'` into a\npandas `DataFrame`, where each element of the struct corresponds to a column\nin the frame:\n\nNote\n\nThe offsets of the structure elements may be different depending on the\narchitecture of the machine on which the file was created. Using a raw binary\nfile format like this for general data storage is not recommended, as it is\nnot cross platform. We recommended either HDF5 or parquet, both of which are\nsupported by pandas\u2019 IO facilities.\n\nNumerical integration (sample-based) of a time series\n\nOften it\u2019s useful to obtain the lower (or upper) triangular form of a\ncorrelation matrix calculated from `DataFrame.corr()`. This can be achieved by\npassing a boolean mask to `where` as follows:\n\nThe `method` argument within `DataFrame.corr` can accept a callable in\naddition to the named correlation types. Here we compute the distance\ncorrelation matrix for a `DataFrame` object.\n\nThe Timedeltas docs.\n\nUsing timedeltas\n\nAdding and subtracting deltas and dates\n\nAnother example\n\nValues can be set to NaT using np.nan, similar to datetime\n\nTo create a dataframe from every combination of some given values, like R\u2019s\n`expand.grid()` function, we can create a dict where the keys are column names\nand the values are lists of the data values:\n\n"}, {"name": "DataFrame", "path": "reference/frame", "type": "DataFrame", "text": "\n`DataFrame`([data, index, columns, dtype, copy])\n\nTwo-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nAxes\n\n`DataFrame.index`\n\nThe index (row labels) of the DataFrame.\n\n`DataFrame.columns`\n\nThe column labels of the DataFrame.\n\n`DataFrame.dtypes`\n\nReturn the dtypes in the DataFrame.\n\n`DataFrame.info`([verbose, buf, max_cols, ...])\n\nPrint a concise summary of a DataFrame.\n\n`DataFrame.select_dtypes`([include, exclude])\n\nReturn a subset of the DataFrame's columns based on the column dtypes.\n\n`DataFrame.values`\n\nReturn a Numpy representation of the DataFrame.\n\n`DataFrame.axes`\n\nReturn a list representing the axes of the DataFrame.\n\n`DataFrame.ndim`\n\nReturn an int representing the number of axes / array dimensions.\n\n`DataFrame.size`\n\nReturn an int representing the number of elements in this object.\n\n`DataFrame.shape`\n\nReturn a tuple representing the dimensionality of the DataFrame.\n\n`DataFrame.memory_usage`([index, deep])\n\nReturn the memory usage of each column in bytes.\n\n`DataFrame.empty`\n\nIndicator whether Series/DataFrame is empty.\n\n`DataFrame.set_flags`(*[, copy, ...])\n\nReturn a new object with updated flags.\n\n`DataFrame.astype`(dtype[, copy, errors])\n\nCast a pandas object to a specified dtype `dtype`.\n\n`DataFrame.convert_dtypes`([infer_objects, ...])\n\nConvert columns to best possible dtypes using dtypes supporting `pd.NA`.\n\n`DataFrame.infer_objects`()\n\nAttempt to infer better dtypes for object columns.\n\n`DataFrame.copy`([deep])\n\nMake a copy of this object's indices and data.\n\n`DataFrame.bool`()\n\nReturn the bool of a single element Series or DataFrame.\n\n`DataFrame.head`([n])\n\nReturn the first n rows.\n\n`DataFrame.at`\n\nAccess a single value for a row/column label pair.\n\n`DataFrame.iat`\n\nAccess a single value for a row/column pair by integer position.\n\n`DataFrame.loc`\n\nAccess a group of rows and columns by label(s) or a boolean array.\n\n`DataFrame.iloc`\n\nPurely integer-location based indexing for selection by position.\n\n`DataFrame.insert`(loc, column, value[, ...])\n\nInsert column into DataFrame at specified location.\n\n`DataFrame.__iter__`()\n\nIterate over info axis.\n\n`DataFrame.items`()\n\nIterate over (column name, Series) pairs.\n\n`DataFrame.iteritems`()\n\nIterate over (column name, Series) pairs.\n\n`DataFrame.keys`()\n\nGet the 'info axis' (see Indexing for more).\n\n`DataFrame.iterrows`()\n\nIterate over DataFrame rows as (index, Series) pairs.\n\n`DataFrame.itertuples`([index, name])\n\nIterate over DataFrame rows as namedtuples.\n\n`DataFrame.lookup`(row_labels, col_labels)\n\n(DEPRECATED) Label-based \"fancy indexing\" function for DataFrame.\n\n`DataFrame.pop`(item)\n\nReturn item and drop from frame.\n\n`DataFrame.tail`([n])\n\nReturn the last n rows.\n\n`DataFrame.xs`(key[, axis, level, drop_level])\n\nReturn cross-section from the Series/DataFrame.\n\n`DataFrame.get`(key[, default])\n\nGet item from object for given key (ex: DataFrame column).\n\n`DataFrame.isin`(values)\n\nWhether each element in the DataFrame is contained in values.\n\n`DataFrame.where`(cond[, other, inplace, ...])\n\nReplace values where the condition is False.\n\n`DataFrame.mask`(cond[, other, inplace, axis, ...])\n\nReplace values where the condition is True.\n\n`DataFrame.query`(expr[, inplace])\n\nQuery the columns of a DataFrame with a boolean expression.\n\nFor more information on `.at`, `.iat`, `.loc`, and `.iloc`, see the indexing\ndocumentation.\n\n`DataFrame.add`(other[, axis, level, fill_value])\n\nGet Addition of dataframe and other, element-wise (binary operator add).\n\n`DataFrame.sub`(other[, axis, level, fill_value])\n\nGet Subtraction of dataframe and other, element-wise (binary operator sub).\n\n`DataFrame.mul`(other[, axis, level, fill_value])\n\nGet Multiplication of dataframe and other, element-wise (binary operator mul).\n\n`DataFrame.div`(other[, axis, level, fill_value])\n\nGet Floating division of dataframe and other, element-wise (binary operator\ntruediv).\n\n`DataFrame.truediv`(other[, axis, level, ...])\n\nGet Floating division of dataframe and other, element-wise (binary operator\ntruediv).\n\n`DataFrame.floordiv`(other[, axis, level, ...])\n\nGet Integer division of dataframe and other, element-wise (binary operator\nfloordiv).\n\n`DataFrame.mod`(other[, axis, level, fill_value])\n\nGet Modulo of dataframe and other, element-wise (binary operator mod).\n\n`DataFrame.pow`(other[, axis, level, fill_value])\n\nGet Exponential power of dataframe and other, element-wise (binary operator\npow).\n\n`DataFrame.dot`(other)\n\nCompute the matrix multiplication between the DataFrame and other.\n\n`DataFrame.radd`(other[, axis, level, fill_value])\n\nGet Addition of dataframe and other, element-wise (binary operator radd).\n\n`DataFrame.rsub`(other[, axis, level, fill_value])\n\nGet Subtraction of dataframe and other, element-wise (binary operator rsub).\n\n`DataFrame.rmul`(other[, axis, level, fill_value])\n\nGet Multiplication of dataframe and other, element-wise (binary operator\nrmul).\n\n`DataFrame.rdiv`(other[, axis, level, fill_value])\n\nGet Floating division of dataframe and other, element-wise (binary operator\nrtruediv).\n\n`DataFrame.rtruediv`(other[, axis, level, ...])\n\nGet Floating division of dataframe and other, element-wise (binary operator\nrtruediv).\n\n`DataFrame.rfloordiv`(other[, axis, level, ...])\n\nGet Integer division of dataframe and other, element-wise (binary operator\nrfloordiv).\n\n`DataFrame.rmod`(other[, axis, level, fill_value])\n\nGet Modulo of dataframe and other, element-wise (binary operator rmod).\n\n`DataFrame.rpow`(other[, axis, level, fill_value])\n\nGet Exponential power of dataframe and other, element-wise (binary operator\nrpow).\n\n`DataFrame.lt`(other[, axis, level])\n\nGet Less than of dataframe and other, element-wise (binary operator lt).\n\n`DataFrame.gt`(other[, axis, level])\n\nGet Greater than of dataframe and other, element-wise (binary operator gt).\n\n`DataFrame.le`(other[, axis, level])\n\nGet Less than or equal to of dataframe and other, element-wise (binary\noperator le).\n\n`DataFrame.ge`(other[, axis, level])\n\nGet Greater than or equal to of dataframe and other, element-wise (binary\noperator ge).\n\n`DataFrame.ne`(other[, axis, level])\n\nGet Not equal to of dataframe and other, element-wise (binary operator ne).\n\n`DataFrame.eq`(other[, axis, level])\n\nGet Equal to of dataframe and other, element-wise (binary operator eq).\n\n`DataFrame.combine`(other, func[, fill_value, ...])\n\nPerform column-wise combine with another DataFrame.\n\n`DataFrame.combine_first`(other)\n\nUpdate null elements with value in the same location in other.\n\n`DataFrame.apply`(func[, axis, raw, ...])\n\nApply a function along an axis of the DataFrame.\n\n`DataFrame.applymap`(func[, na_action])\n\nApply a function to a Dataframe elementwise.\n\n`DataFrame.pipe`(func, *args, **kwargs)\n\nApply chainable functions that expect Series or DataFrames.\n\n`DataFrame.agg`([func, axis])\n\nAggregate using one or more operations over the specified axis.\n\n`DataFrame.aggregate`([func, axis])\n\nAggregate using one or more operations over the specified axis.\n\n`DataFrame.transform`(func[, axis])\n\nCall `func` on self producing a DataFrame with the same axis shape as self.\n\n`DataFrame.groupby`([by, axis, level, ...])\n\nGroup DataFrame using a mapper or by a Series of columns.\n\n`DataFrame.rolling`(window[, min_periods, ...])\n\nProvide rolling window calculations.\n\n`DataFrame.expanding`([min_periods, center, ...])\n\nProvide expanding window calculations.\n\n`DataFrame.ewm`([com, span, halflife, alpha, ...])\n\nProvide exponentially weighted (EW) calculations.\n\n`DataFrame.abs`()\n\nReturn a Series/DataFrame with absolute numeric value of each element.\n\n`DataFrame.all`([axis, bool_only, skipna, level])\n\nReturn whether all elements are True, potentially over an axis.\n\n`DataFrame.any`([axis, bool_only, skipna, level])\n\nReturn whether any element is True, potentially over an axis.\n\n`DataFrame.clip`([lower, upper, axis, inplace])\n\nTrim values at input threshold(s).\n\n`DataFrame.corr`([method, min_periods])\n\nCompute pairwise correlation of columns, excluding NA/null values.\n\n`DataFrame.corrwith`(other[, axis, drop, method])\n\nCompute pairwise correlation.\n\n`DataFrame.count`([axis, level, numeric_only])\n\nCount non-NA cells for each column or row.\n\n`DataFrame.cov`([min_periods, ddof])\n\nCompute pairwise covariance of columns, excluding NA/null values.\n\n`DataFrame.cummax`([axis, skipna])\n\nReturn cumulative maximum over a DataFrame or Series axis.\n\n`DataFrame.cummin`([axis, skipna])\n\nReturn cumulative minimum over a DataFrame or Series axis.\n\n`DataFrame.cumprod`([axis, skipna])\n\nReturn cumulative product over a DataFrame or Series axis.\n\n`DataFrame.cumsum`([axis, skipna])\n\nReturn cumulative sum over a DataFrame or Series axis.\n\n`DataFrame.describe`([percentiles, include, ...])\n\nGenerate descriptive statistics.\n\n`DataFrame.diff`([periods, axis])\n\nFirst discrete difference of element.\n\n`DataFrame.eval`(expr[, inplace])\n\nEvaluate a string describing operations on DataFrame columns.\n\n`DataFrame.kurt`([axis, skipna, level, ...])\n\nReturn unbiased kurtosis over requested axis.\n\n`DataFrame.kurtosis`([axis, skipna, level, ...])\n\nReturn unbiased kurtosis over requested axis.\n\n`DataFrame.mad`([axis, skipna, level])\n\nReturn the mean absolute deviation of the values over the requested axis.\n\n`DataFrame.max`([axis, skipna, level, ...])\n\nReturn the maximum of the values over the requested axis.\n\n`DataFrame.mean`([axis, skipna, level, ...])\n\nReturn the mean of the values over the requested axis.\n\n`DataFrame.median`([axis, skipna, level, ...])\n\nReturn the median of the values over the requested axis.\n\n`DataFrame.min`([axis, skipna, level, ...])\n\nReturn the minimum of the values over the requested axis.\n\n`DataFrame.mode`([axis, numeric_only, dropna])\n\nGet the mode(s) of each element along the selected axis.\n\n`DataFrame.pct_change`([periods, fill_method, ...])\n\nPercentage change between the current and a prior element.\n\n`DataFrame.prod`([axis, skipna, level, ...])\n\nReturn the product of the values over the requested axis.\n\n`DataFrame.product`([axis, skipna, level, ...])\n\nReturn the product of the values over the requested axis.\n\n`DataFrame.quantile`([q, axis, numeric_only, ...])\n\nReturn values at the given quantile over requested axis.\n\n`DataFrame.rank`([axis, method, numeric_only, ...])\n\nCompute numerical data ranks (1 through n) along axis.\n\n`DataFrame.round`([decimals])\n\nRound a DataFrame to a variable number of decimal places.\n\n`DataFrame.sem`([axis, skipna, level, ddof, ...])\n\nReturn unbiased standard error of the mean over requested axis.\n\n`DataFrame.skew`([axis, skipna, level, ...])\n\nReturn unbiased skew over requested axis.\n\n`DataFrame.sum`([axis, skipna, level, ...])\n\nReturn the sum of the values over the requested axis.\n\n`DataFrame.std`([axis, skipna, level, ddof, ...])\n\nReturn sample standard deviation over requested axis.\n\n`DataFrame.var`([axis, skipna, level, ddof, ...])\n\nReturn unbiased variance over requested axis.\n\n`DataFrame.nunique`([axis, dropna])\n\nCount number of distinct elements in specified axis.\n\n`DataFrame.value_counts`([subset, normalize, ...])\n\nReturn a Series containing counts of unique rows in the DataFrame.\n\n`DataFrame.add_prefix`(prefix)\n\nPrefix labels with string prefix.\n\n`DataFrame.add_suffix`(suffix)\n\nSuffix labels with string suffix.\n\n`DataFrame.align`(other[, join, axis, level, ...])\n\nAlign two objects on their axes with the specified join method.\n\n`DataFrame.at_time`(time[, asof, axis])\n\nSelect values at particular time of day (e.g., 9:30AM).\n\n`DataFrame.between_time`(start_time, end_time)\n\nSelect values between particular times of the day (e.g., 9:00-9:30 AM).\n\n`DataFrame.drop`([labels, axis, index, ...])\n\nDrop specified labels from rows or columns.\n\n`DataFrame.drop_duplicates`([subset, keep, ...])\n\nReturn DataFrame with duplicate rows removed.\n\n`DataFrame.duplicated`([subset, keep])\n\nReturn boolean Series denoting duplicate rows.\n\n`DataFrame.equals`(other)\n\nTest whether two objects contain the same elements.\n\n`DataFrame.filter`([items, like, regex, axis])\n\nSubset the dataframe rows or columns according to the specified index labels.\n\n`DataFrame.first`(offset)\n\nSelect initial periods of time series data based on a date offset.\n\n`DataFrame.head`([n])\n\nReturn the first n rows.\n\n`DataFrame.idxmax`([axis, skipna])\n\nReturn index of first occurrence of maximum over requested axis.\n\n`DataFrame.idxmin`([axis, skipna])\n\nReturn index of first occurrence of minimum over requested axis.\n\n`DataFrame.last`(offset)\n\nSelect final periods of time series data based on a date offset.\n\n`DataFrame.reindex`([labels, index, columns, ...])\n\nConform Series/DataFrame to new index with optional filling logic.\n\n`DataFrame.reindex_like`(other[, method, ...])\n\nReturn an object with matching indices as other object.\n\n`DataFrame.rename`([mapper, index, columns, ...])\n\nAlter axes labels.\n\n`DataFrame.rename_axis`([mapper, index, ...])\n\nSet the name of the axis for the index or columns.\n\n`DataFrame.reset_index`([level, drop, ...])\n\nReset the index, or a level of it.\n\n`DataFrame.sample`([n, frac, replace, ...])\n\nReturn a random sample of items from an axis of object.\n\n`DataFrame.set_axis`(labels[, axis, inplace])\n\nAssign desired index to given axis.\n\n`DataFrame.set_index`(keys[, drop, append, ...])\n\nSet the DataFrame index using existing columns.\n\n`DataFrame.tail`([n])\n\nReturn the last n rows.\n\n`DataFrame.take`(indices[, axis, is_copy])\n\nReturn the elements in the given positional indices along an axis.\n\n`DataFrame.truncate`([before, after, axis, copy])\n\nTruncate a Series or DataFrame before and after some index value.\n\n`DataFrame.backfill`([axis, inplace, limit, ...])\n\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\n`DataFrame.bfill`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\n`DataFrame.dropna`([axis, how, thresh, ...])\n\nRemove missing values.\n\n`DataFrame.ffill`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\n`DataFrame.fillna`([value, method, axis, ...])\n\nFill NA/NaN values using the specified method.\n\n`DataFrame.interpolate`([method, axis, limit, ...])\n\nFill NaN values using an interpolation method.\n\n`DataFrame.isna`()\n\nDetect missing values.\n\n`DataFrame.isnull`()\n\nDataFrame.isnull is an alias for DataFrame.isna.\n\n`DataFrame.notna`()\n\nDetect existing (non-missing) values.\n\n`DataFrame.notnull`()\n\nDataFrame.notnull is an alias for DataFrame.notna.\n\n`DataFrame.pad`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\n`DataFrame.replace`([to_replace, value, ...])\n\nReplace values given in to_replace with value.\n\n`DataFrame.droplevel`(level[, axis])\n\nReturn Series/DataFrame with requested index / column level(s) removed.\n\n`DataFrame.pivot`([index, columns, values])\n\nReturn reshaped DataFrame organized by given index / column values.\n\n`DataFrame.pivot_table`([values, index, ...])\n\nCreate a spreadsheet-style pivot table as a DataFrame.\n\n`DataFrame.reorder_levels`(order[, axis])\n\nRearrange index levels using input order.\n\n`DataFrame.sort_values`(by[, axis, ascending, ...])\n\nSort by the values along either axis.\n\n`DataFrame.sort_index`([axis, level, ...])\n\nSort object by labels (along an axis).\n\n`DataFrame.nlargest`(n, columns[, keep])\n\nReturn the first n rows ordered by columns in descending order.\n\n`DataFrame.nsmallest`(n, columns[, keep])\n\nReturn the first n rows ordered by columns in ascending order.\n\n`DataFrame.swaplevel`([i, j, axis])\n\nSwap levels i and j in a `MultiIndex`.\n\n`DataFrame.stack`([level, dropna])\n\nStack the prescribed level(s) from columns to index.\n\n`DataFrame.unstack`([level, fill_value])\n\nPivot a level of the (necessarily hierarchical) index labels.\n\n`DataFrame.swapaxes`(axis1, axis2[, copy])\n\nInterchange axes and swap values axes appropriately.\n\n`DataFrame.melt`([id_vars, value_vars, ...])\n\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers\nset.\n\n`DataFrame.explode`(column[, ignore_index])\n\nTransform each element of a list-like to a row, replicating index values.\n\n`DataFrame.squeeze`([axis])\n\nSqueeze 1 dimensional axis objects into scalars.\n\n`DataFrame.to_xarray`()\n\nReturn an xarray object from the pandas object.\n\n`DataFrame.T`\n\n`DataFrame.transpose`(*args[, copy])\n\nTranspose index and columns.\n\n`DataFrame.append`(other[, ignore_index, ...])\n\nAppend rows of other to the end of caller, returning a new object.\n\n`DataFrame.assign`(**kwargs)\n\nAssign new columns to a DataFrame.\n\n`DataFrame.compare`(other[, align_axis, ...])\n\nCompare to another DataFrame and show the differences.\n\n`DataFrame.join`(other[, on, how, lsuffix, ...])\n\nJoin columns of another DataFrame.\n\n`DataFrame.merge`(right[, how, on, left_on, ...])\n\nMerge DataFrame or named Series objects with a database-style join.\n\n`DataFrame.update`(other[, join, overwrite, ...])\n\nModify in place using non-NA values from another DataFrame.\n\n`DataFrame.asfreq`(freq[, method, how, ...])\n\nConvert time series to specified frequency.\n\n`DataFrame.asof`(where[, subset])\n\nReturn the last row(s) without any NaNs before where.\n\n`DataFrame.shift`([periods, freq, axis, ...])\n\nShift index by desired number of periods with an optional time freq.\n\n`DataFrame.slice_shift`([periods, axis])\n\n(DEPRECATED) Equivalent to shift without copying data.\n\n`DataFrame.tshift`([periods, freq, axis])\n\n(DEPRECATED) Shift the time index, using the index's frequency if available.\n\n`DataFrame.first_valid_index`()\n\nReturn index for first non-NA value or None, if no NA value is found.\n\n`DataFrame.last_valid_index`()\n\nReturn index for last non-NA value or None, if no NA value is found.\n\n`DataFrame.resample`(rule[, axis, closed, ...])\n\nResample time-series data.\n\n`DataFrame.to_period`([freq, axis, copy])\n\nConvert DataFrame from DatetimeIndex to PeriodIndex.\n\n`DataFrame.to_timestamp`([freq, how, axis, copy])\n\nCast to DatetimeIndex of timestamps, at beginning of period.\n\n`DataFrame.tz_convert`(tz[, axis, level, copy])\n\nConvert tz-aware axis to target time zone.\n\n`DataFrame.tz_localize`(tz[, axis, level, ...])\n\nLocalize tz-naive index of a Series or DataFrame to target time zone.\n\nFlags refer to attributes of the pandas object. Properties of the dataset\n(like the date is was recorded, the URL it was accessed from, etc.) should be\nstored in `DataFrame.attrs`.\n\n`Flags`(obj, *, allows_duplicate_labels)\n\nFlags that apply to pandas objects.\n\n`DataFrame.attrs` is a dictionary for storing global metadata for this\nDataFrame.\n\nWarning\n\n`DataFrame.attrs` is considered experimental and may change without warning.\n\n`DataFrame.attrs`\n\nDictionary of global attributes of this dataset.\n\n`DataFrame.plot` is both a callable method and a namespace attribute for\nspecific plotting methods of the form `DataFrame.plot.<kind>`.\n\n`DataFrame.plot`([x, y, kind, ax, ....])\n\nDataFrame plotting accessor and method\n\n`DataFrame.plot.area`([x, y])\n\nDraw a stacked area plot.\n\n`DataFrame.plot.bar`([x, y])\n\nVertical bar plot.\n\n`DataFrame.plot.barh`([x, y])\n\nMake a horizontal bar plot.\n\n`DataFrame.plot.box`([by])\n\nMake a box plot of the DataFrame columns.\n\n`DataFrame.plot.density`([bw_method, ind])\n\nGenerate Kernel Density Estimate plot using Gaussian kernels.\n\n`DataFrame.plot.hexbin`(x, y[, C, ...])\n\nGenerate a hexagonal binning plot.\n\n`DataFrame.plot.hist`([by, bins])\n\nDraw one histogram of the DataFrame's columns.\n\n`DataFrame.plot.kde`([bw_method, ind])\n\nGenerate Kernel Density Estimate plot using Gaussian kernels.\n\n`DataFrame.plot.line`([x, y])\n\nPlot Series or DataFrame as lines.\n\n`DataFrame.plot.pie`(**kwargs)\n\nGenerate a pie plot.\n\n`DataFrame.plot.scatter`(x, y[, s, c])\n\nCreate a scatter plot with varying marker point size and color.\n\n`DataFrame.boxplot`([column, by, ax, ...])\n\nMake a box plot from DataFrame columns.\n\n`DataFrame.hist`([column, by, grid, ...])\n\nMake a histogram of the DataFrame's columns.\n\nSparse-dtype specific methods and attributes are provided under the\n`DataFrame.sparse` accessor.\n\n`DataFrame.sparse.density`\n\nRatio of non-sparse points to total (dense) data points.\n\n`DataFrame.sparse.from_spmatrix`(data[, ...])\n\nCreate a new DataFrame from a scipy sparse matrix.\n\n`DataFrame.sparse.to_coo`()\n\nReturn the contents of the frame as a sparse SciPy COO matrix.\n\n`DataFrame.sparse.to_dense`()\n\nConvert a DataFrame with sparse values to dense.\n\n`DataFrame.from_dict`(data[, orient, dtype, ...])\n\nConstruct DataFrame from dict of array-like or dicts.\n\n`DataFrame.from_records`(data[, index, ...])\n\nConvert structured or record ndarray to DataFrame.\n\n`DataFrame.to_parquet`([path, engine, ...])\n\nWrite a DataFrame to the binary parquet format.\n\n`DataFrame.to_pickle`(path[, compression, ...])\n\nPickle (serialize) object to file.\n\n`DataFrame.to_csv`([path_or_buf, sep, na_rep, ...])\n\nWrite object to a comma-separated values (csv) file.\n\n`DataFrame.to_hdf`(path_or_buf, key[, mode, ...])\n\nWrite the contained data to an HDF5 file using HDFStore.\n\n`DataFrame.to_sql`(name, con[, schema, ...])\n\nWrite records stored in a DataFrame to a SQL database.\n\n`DataFrame.to_dict`([orient, into])\n\nConvert the DataFrame to a dictionary.\n\n`DataFrame.to_excel`(excel_writer[, ...])\n\nWrite object to an Excel sheet.\n\n`DataFrame.to_json`([path_or_buf, orient, ...])\n\nConvert the object to a JSON string.\n\n`DataFrame.to_html`([buf, columns, col_space, ...])\n\nRender a DataFrame as an HTML table.\n\n`DataFrame.to_feather`(path, **kwargs)\n\nWrite a DataFrame to the binary Feather format.\n\n`DataFrame.to_latex`([buf, columns, ...])\n\nRender object to a LaTeX tabular, longtable, or nested table.\n\n`DataFrame.to_stata`(path[, convert_dates, ...])\n\nExport DataFrame object to Stata dta format.\n\n`DataFrame.to_gbq`(destination_table[, ...])\n\nWrite a DataFrame to a Google BigQuery table.\n\n`DataFrame.to_records`([index, column_dtypes, ...])\n\nConvert DataFrame to a NumPy record array.\n\n`DataFrame.to_string`([buf, columns, ...])\n\nRender a DataFrame to a console-friendly tabular output.\n\n`DataFrame.to_clipboard`([excel, sep])\n\nCopy object to the system clipboard.\n\n`DataFrame.to_markdown`([buf, mode, index, ...])\n\nPrint DataFrame in Markdown-friendly format.\n\n`DataFrame.style`\n\nReturns a Styler object.\n\n"}, {"name": "Date offsets", "path": "reference/offset_frequency", "type": "Data offsets", "text": "\n`DateOffset`\n\nStandard kind of date increment used for a date range.\n\n`DateOffset.freqstr`\n\n`DateOffset.kwds`\n\n`DateOffset.name`\n\n`DateOffset.nanos`\n\n`DateOffset.normalize`\n\n`DateOffset.rule_code`\n\n`DateOffset.n`\n\n`DateOffset.is_month_start`\n\n`DateOffset.is_month_end`\n\n`DateOffset.apply`\n\n`DateOffset.apply_index`(other)\n\n`DateOffset.copy`\n\n`DateOffset.isAnchored`\n\n`DateOffset.onOffset`\n\n`DateOffset.is_anchored`\n\n`DateOffset.is_on_offset`\n\n`DateOffset.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`DateOffset.is_month_start`\n\n`DateOffset.is_month_end`\n\n`DateOffset.is_quarter_start`\n\n`DateOffset.is_quarter_end`\n\n`DateOffset.is_year_start`\n\n`DateOffset.is_year_end`\n\n`BusinessDay`\n\nDateOffset subclass representing possibly n business days.\n\nAlias:\n\n`BDay`\n\nalias of `pandas._libs.tslibs.offsets.BusinessDay`\n\n`BusinessDay.freqstr`\n\n`BusinessDay.kwds`\n\n`BusinessDay.name`\n\n`BusinessDay.nanos`\n\n`BusinessDay.normalize`\n\n`BusinessDay.rule_code`\n\n`BusinessDay.n`\n\n`BusinessDay.weekmask`\n\n`BusinessDay.holidays`\n\n`BusinessDay.calendar`\n\n`BusinessDay.apply`\n\n`BusinessDay.apply_index`(other)\n\n`BusinessDay.copy`\n\n`BusinessDay.isAnchored`\n\n`BusinessDay.onOffset`\n\n`BusinessDay.is_anchored`\n\n`BusinessDay.is_on_offset`\n\n`BusinessDay.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`BusinessDay.is_month_start`\n\n`BusinessDay.is_month_end`\n\n`BusinessDay.is_quarter_start`\n\n`BusinessDay.is_quarter_end`\n\n`BusinessDay.is_year_start`\n\n`BusinessDay.is_year_end`\n\n`BusinessHour`\n\nDateOffset subclass representing possibly n business hours.\n\n`BusinessHour.freqstr`\n\n`BusinessHour.kwds`\n\n`BusinessHour.name`\n\n`BusinessHour.nanos`\n\n`BusinessHour.normalize`\n\n`BusinessHour.rule_code`\n\n`BusinessHour.n`\n\n`BusinessHour.start`\n\n`BusinessHour.end`\n\n`BusinessHour.weekmask`\n\n`BusinessHour.holidays`\n\n`BusinessHour.calendar`\n\n`BusinessHour.apply`\n\n`BusinessHour.apply_index`(other)\n\n`BusinessHour.copy`\n\n`BusinessHour.isAnchored`\n\n`BusinessHour.onOffset`\n\n`BusinessHour.is_anchored`\n\n`BusinessHour.is_on_offset`\n\n`BusinessHour.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`BusinessHour.is_month_start`\n\n`BusinessHour.is_month_end`\n\n`BusinessHour.is_quarter_start`\n\n`BusinessHour.is_quarter_end`\n\n`BusinessHour.is_year_start`\n\n`BusinessHour.is_year_end`\n\n`CustomBusinessDay`\n\nDateOffset subclass representing custom business days excluding holidays.\n\nAlias:\n\n`CDay`\n\nalias of `pandas._libs.tslibs.offsets.CustomBusinessDay`\n\n`CustomBusinessDay.freqstr`\n\n`CustomBusinessDay.kwds`\n\n`CustomBusinessDay.name`\n\n`CustomBusinessDay.nanos`\n\n`CustomBusinessDay.normalize`\n\n`CustomBusinessDay.rule_code`\n\n`CustomBusinessDay.n`\n\n`CustomBusinessDay.weekmask`\n\n`CustomBusinessDay.calendar`\n\n`CustomBusinessDay.holidays`\n\n`CustomBusinessDay.apply_index`\n\n`CustomBusinessDay.apply`\n\n`CustomBusinessDay.copy`\n\n`CustomBusinessDay.isAnchored`\n\n`CustomBusinessDay.onOffset`\n\n`CustomBusinessDay.is_anchored`\n\n`CustomBusinessDay.is_on_offset`\n\n`CustomBusinessDay.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`CustomBusinessDay.is_month_start`\n\n`CustomBusinessDay.is_month_end`\n\n`CustomBusinessDay.is_quarter_start`\n\n`CustomBusinessDay.is_quarter_end`\n\n`CustomBusinessDay.is_year_start`\n\n`CustomBusinessDay.is_year_end`\n\n`CustomBusinessHour`\n\nDateOffset subclass representing possibly n custom business days.\n\n`CustomBusinessHour.freqstr`\n\n`CustomBusinessHour.kwds`\n\n`CustomBusinessHour.name`\n\n`CustomBusinessHour.nanos`\n\n`CustomBusinessHour.normalize`\n\n`CustomBusinessHour.rule_code`\n\n`CustomBusinessHour.n`\n\n`CustomBusinessHour.weekmask`\n\n`CustomBusinessHour.calendar`\n\n`CustomBusinessHour.holidays`\n\n`CustomBusinessHour.start`\n\n`CustomBusinessHour.end`\n\n`CustomBusinessHour.apply`\n\n`CustomBusinessHour.apply_index`(other)\n\n`CustomBusinessHour.copy`\n\n`CustomBusinessHour.isAnchored`\n\n`CustomBusinessHour.onOffset`\n\n`CustomBusinessHour.is_anchored`\n\n`CustomBusinessHour.is_on_offset`\n\n`CustomBusinessHour.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`CustomBusinessHour.is_month_start`\n\n`CustomBusinessHour.is_month_end`\n\n`CustomBusinessHour.is_quarter_start`\n\n`CustomBusinessHour.is_quarter_end`\n\n`CustomBusinessHour.is_year_start`\n\n`CustomBusinessHour.is_year_end`\n\n`MonthEnd`\n\nDateOffset of one month end.\n\n`MonthEnd.freqstr`\n\n`MonthEnd.kwds`\n\n`MonthEnd.name`\n\n`MonthEnd.nanos`\n\n`MonthEnd.normalize`\n\n`MonthEnd.rule_code`\n\n`MonthEnd.n`\n\n`MonthEnd.apply`\n\n`MonthEnd.apply_index`(other)\n\n`MonthEnd.copy`\n\n`MonthEnd.isAnchored`\n\n`MonthEnd.onOffset`\n\n`MonthEnd.is_anchored`\n\n`MonthEnd.is_on_offset`\n\n`MonthEnd.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`MonthEnd.is_month_start`\n\n`MonthEnd.is_month_end`\n\n`MonthEnd.is_quarter_start`\n\n`MonthEnd.is_quarter_end`\n\n`MonthEnd.is_year_start`\n\n`MonthEnd.is_year_end`\n\n`MonthBegin`\n\nDateOffset of one month at beginning.\n\n`MonthBegin.freqstr`\n\n`MonthBegin.kwds`\n\n`MonthBegin.name`\n\n`MonthBegin.nanos`\n\n`MonthBegin.normalize`\n\n`MonthBegin.rule_code`\n\n`MonthBegin.n`\n\n`MonthBegin.apply`\n\n`MonthBegin.apply_index`(other)\n\n`MonthBegin.copy`\n\n`MonthBegin.isAnchored`\n\n`MonthBegin.onOffset`\n\n`MonthBegin.is_anchored`\n\n`MonthBegin.is_on_offset`\n\n`MonthBegin.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`MonthBegin.is_month_start`\n\n`MonthBegin.is_month_end`\n\n`MonthBegin.is_quarter_start`\n\n`MonthBegin.is_quarter_end`\n\n`MonthBegin.is_year_start`\n\n`MonthBegin.is_year_end`\n\n`BusinessMonthEnd`\n\nDateOffset increments between the last business day of the month.\n\nAlias:\n\n`BMonthEnd`\n\nalias of `pandas._libs.tslibs.offsets.BusinessMonthEnd`\n\n`BusinessMonthEnd.freqstr`\n\n`BusinessMonthEnd.kwds`\n\n`BusinessMonthEnd.name`\n\n`BusinessMonthEnd.nanos`\n\n`BusinessMonthEnd.normalize`\n\n`BusinessMonthEnd.rule_code`\n\n`BusinessMonthEnd.n`\n\n`BusinessMonthEnd.apply`\n\n`BusinessMonthEnd.apply_index`(other)\n\n`BusinessMonthEnd.copy`\n\n`BusinessMonthEnd.isAnchored`\n\n`BusinessMonthEnd.onOffset`\n\n`BusinessMonthEnd.is_anchored`\n\n`BusinessMonthEnd.is_on_offset`\n\n`BusinessMonthEnd.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`BusinessMonthEnd.is_month_start`\n\n`BusinessMonthEnd.is_month_end`\n\n`BusinessMonthEnd.is_quarter_start`\n\n`BusinessMonthEnd.is_quarter_end`\n\n`BusinessMonthEnd.is_year_start`\n\n`BusinessMonthEnd.is_year_end`\n\n`BusinessMonthBegin`\n\nDateOffset of one month at the first business day.\n\nAlias:\n\n`BMonthBegin`\n\nalias of `pandas._libs.tslibs.offsets.BusinessMonthBegin`\n\n`BusinessMonthBegin.freqstr`\n\n`BusinessMonthBegin.kwds`\n\n`BusinessMonthBegin.name`\n\n`BusinessMonthBegin.nanos`\n\n`BusinessMonthBegin.normalize`\n\n`BusinessMonthBegin.rule_code`\n\n`BusinessMonthBegin.n`\n\n`BusinessMonthBegin.apply`\n\n`BusinessMonthBegin.apply_index`(other)\n\n`BusinessMonthBegin.copy`\n\n`BusinessMonthBegin.isAnchored`\n\n`BusinessMonthBegin.onOffset`\n\n`BusinessMonthBegin.is_anchored`\n\n`BusinessMonthBegin.is_on_offset`\n\n`BusinessMonthBegin.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`BusinessMonthBegin.is_month_start`\n\n`BusinessMonthBegin.is_month_end`\n\n`BusinessMonthBegin.is_quarter_start`\n\n`BusinessMonthBegin.is_quarter_end`\n\n`BusinessMonthBegin.is_year_start`\n\n`BusinessMonthBegin.is_year_end`\n\n`CustomBusinessMonthEnd`\n\nAttributes\n\nAlias:\n\n`CBMonthEnd`\n\nalias of `pandas._libs.tslibs.offsets.CustomBusinessMonthEnd`\n\n`CustomBusinessMonthEnd.freqstr`\n\n`CustomBusinessMonthEnd.kwds`\n\n`CustomBusinessMonthEnd.m_offset`\n\n`CustomBusinessMonthEnd.name`\n\n`CustomBusinessMonthEnd.nanos`\n\n`CustomBusinessMonthEnd.normalize`\n\n`CustomBusinessMonthEnd.rule_code`\n\n`CustomBusinessMonthEnd.n`\n\n`CustomBusinessMonthEnd.weekmask`\n\n`CustomBusinessMonthEnd.calendar`\n\n`CustomBusinessMonthEnd.holidays`\n\n`CustomBusinessMonthEnd.apply`\n\n`CustomBusinessMonthEnd.apply_index`(other)\n\n`CustomBusinessMonthEnd.copy`\n\n`CustomBusinessMonthEnd.isAnchored`\n\n`CustomBusinessMonthEnd.onOffset`\n\n`CustomBusinessMonthEnd.is_anchored`\n\n`CustomBusinessMonthEnd.is_on_offset`\n\n`CustomBusinessMonthEnd.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`CustomBusinessMonthEnd.is_month_start`\n\n`CustomBusinessMonthEnd.is_month_end`\n\n`CustomBusinessMonthEnd.is_quarter_start`\n\n`CustomBusinessMonthEnd.is_quarter_end`\n\n`CustomBusinessMonthEnd.is_year_start`\n\n`CustomBusinessMonthEnd.is_year_end`\n\n`CustomBusinessMonthBegin`\n\nAttributes\n\nAlias:\n\n`CBMonthBegin`\n\nalias of `pandas._libs.tslibs.offsets.CustomBusinessMonthBegin`\n\n`CustomBusinessMonthBegin.freqstr`\n\n`CustomBusinessMonthBegin.kwds`\n\n`CustomBusinessMonthBegin.m_offset`\n\n`CustomBusinessMonthBegin.name`\n\n`CustomBusinessMonthBegin.nanos`\n\n`CustomBusinessMonthBegin.normalize`\n\n`CustomBusinessMonthBegin.rule_code`\n\n`CustomBusinessMonthBegin.n`\n\n`CustomBusinessMonthBegin.weekmask`\n\n`CustomBusinessMonthBegin.calendar`\n\n`CustomBusinessMonthBegin.holidays`\n\n`CustomBusinessMonthBegin.apply`\n\n`CustomBusinessMonthBegin.apply_index`(other)\n\n`CustomBusinessMonthBegin.copy`\n\n`CustomBusinessMonthBegin.isAnchored`\n\n`CustomBusinessMonthBegin.onOffset`\n\n`CustomBusinessMonthBegin.is_anchored`\n\n`CustomBusinessMonthBegin.is_on_offset`\n\n`CustomBusinessMonthBegin.__call__`(*args, ...)\n\nCall self as a function.\n\n`CustomBusinessMonthBegin.is_month_start`\n\n`CustomBusinessMonthBegin.is_month_end`\n\n`CustomBusinessMonthBegin.is_quarter_start`\n\n`CustomBusinessMonthBegin.is_quarter_end`\n\n`CustomBusinessMonthBegin.is_year_start`\n\n`CustomBusinessMonthBegin.is_year_end`\n\n`SemiMonthEnd`\n\nTwo DateOffset's per month repeating on the last day of the month and\nday_of_month.\n\n`SemiMonthEnd.freqstr`\n\n`SemiMonthEnd.kwds`\n\n`SemiMonthEnd.name`\n\n`SemiMonthEnd.nanos`\n\n`SemiMonthEnd.normalize`\n\n`SemiMonthEnd.rule_code`\n\n`SemiMonthEnd.n`\n\n`SemiMonthEnd.day_of_month`\n\n`SemiMonthEnd.apply`\n\n`SemiMonthEnd.apply_index`(other)\n\n`SemiMonthEnd.copy`\n\n`SemiMonthEnd.isAnchored`\n\n`SemiMonthEnd.onOffset`\n\n`SemiMonthEnd.is_anchored`\n\n`SemiMonthEnd.is_on_offset`\n\n`SemiMonthEnd.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`SemiMonthEnd.is_month_start`\n\n`SemiMonthEnd.is_month_end`\n\n`SemiMonthEnd.is_quarter_start`\n\n`SemiMonthEnd.is_quarter_end`\n\n`SemiMonthEnd.is_year_start`\n\n`SemiMonthEnd.is_year_end`\n\n`SemiMonthBegin`\n\nTwo DateOffset's per month repeating on the first day of the month and\nday_of_month.\n\n`SemiMonthBegin.freqstr`\n\n`SemiMonthBegin.kwds`\n\n`SemiMonthBegin.name`\n\n`SemiMonthBegin.nanos`\n\n`SemiMonthBegin.normalize`\n\n`SemiMonthBegin.rule_code`\n\n`SemiMonthBegin.n`\n\n`SemiMonthBegin.day_of_month`\n\n`SemiMonthBegin.apply`\n\n`SemiMonthBegin.apply_index`(other)\n\n`SemiMonthBegin.copy`\n\n`SemiMonthBegin.isAnchored`\n\n`SemiMonthBegin.onOffset`\n\n`SemiMonthBegin.is_anchored`\n\n`SemiMonthBegin.is_on_offset`\n\n`SemiMonthBegin.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`SemiMonthBegin.is_month_start`\n\n`SemiMonthBegin.is_month_end`\n\n`SemiMonthBegin.is_quarter_start`\n\n`SemiMonthBegin.is_quarter_end`\n\n`SemiMonthBegin.is_year_start`\n\n`SemiMonthBegin.is_year_end`\n\n`Week`\n\nWeekly offset.\n\n`Week.freqstr`\n\n`Week.kwds`\n\n`Week.name`\n\n`Week.nanos`\n\n`Week.normalize`\n\n`Week.rule_code`\n\n`Week.n`\n\n`Week.weekday`\n\n`Week.apply`\n\n`Week.apply_index`(other)\n\n`Week.copy`\n\n`Week.isAnchored`\n\n`Week.onOffset`\n\n`Week.is_anchored`\n\n`Week.is_on_offset`\n\n`Week.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Week.is_month_start`\n\n`Week.is_month_end`\n\n`Week.is_quarter_start`\n\n`Week.is_quarter_end`\n\n`Week.is_year_start`\n\n`Week.is_year_end`\n\n`WeekOfMonth`\n\nDescribes monthly dates like \"the Tuesday of the 2nd week of each month\".\n\n`WeekOfMonth.freqstr`\n\n`WeekOfMonth.kwds`\n\n`WeekOfMonth.name`\n\n`WeekOfMonth.nanos`\n\n`WeekOfMonth.normalize`\n\n`WeekOfMonth.rule_code`\n\n`WeekOfMonth.n`\n\n`WeekOfMonth.week`\n\n`WeekOfMonth.apply`\n\n`WeekOfMonth.apply_index`(other)\n\n`WeekOfMonth.copy`\n\n`WeekOfMonth.isAnchored`\n\n`WeekOfMonth.onOffset`\n\n`WeekOfMonth.is_anchored`\n\n`WeekOfMonth.is_on_offset`\n\n`WeekOfMonth.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`WeekOfMonth.weekday`\n\n`WeekOfMonth.is_month_start`\n\n`WeekOfMonth.is_month_end`\n\n`WeekOfMonth.is_quarter_start`\n\n`WeekOfMonth.is_quarter_end`\n\n`WeekOfMonth.is_year_start`\n\n`WeekOfMonth.is_year_end`\n\n`LastWeekOfMonth`\n\nDescribes monthly dates in last week of month like \"the last Tuesday of each\nmonth\".\n\n`LastWeekOfMonth.freqstr`\n\n`LastWeekOfMonth.kwds`\n\n`LastWeekOfMonth.name`\n\n`LastWeekOfMonth.nanos`\n\n`LastWeekOfMonth.normalize`\n\n`LastWeekOfMonth.rule_code`\n\n`LastWeekOfMonth.n`\n\n`LastWeekOfMonth.weekday`\n\n`LastWeekOfMonth.week`\n\n`LastWeekOfMonth.apply`\n\n`LastWeekOfMonth.apply_index`(other)\n\n`LastWeekOfMonth.copy`\n\n`LastWeekOfMonth.isAnchored`\n\n`LastWeekOfMonth.onOffset`\n\n`LastWeekOfMonth.is_anchored`\n\n`LastWeekOfMonth.is_on_offset`\n\n`LastWeekOfMonth.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`LastWeekOfMonth.is_month_start`\n\n`LastWeekOfMonth.is_month_end`\n\n`LastWeekOfMonth.is_quarter_start`\n\n`LastWeekOfMonth.is_quarter_end`\n\n`LastWeekOfMonth.is_year_start`\n\n`LastWeekOfMonth.is_year_end`\n\n`BQuarterEnd`\n\nDateOffset increments between the last business day of each Quarter.\n\n`BQuarterEnd.freqstr`\n\n`BQuarterEnd.kwds`\n\n`BQuarterEnd.name`\n\n`BQuarterEnd.nanos`\n\n`BQuarterEnd.normalize`\n\n`BQuarterEnd.rule_code`\n\n`BQuarterEnd.n`\n\n`BQuarterEnd.startingMonth`\n\n`BQuarterEnd.apply`\n\n`BQuarterEnd.apply_index`(other)\n\n`BQuarterEnd.copy`\n\n`BQuarterEnd.isAnchored`\n\n`BQuarterEnd.onOffset`\n\n`BQuarterEnd.is_anchored`\n\n`BQuarterEnd.is_on_offset`\n\n`BQuarterEnd.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`BQuarterEnd.is_month_start`\n\n`BQuarterEnd.is_month_end`\n\n`BQuarterEnd.is_quarter_start`\n\n`BQuarterEnd.is_quarter_end`\n\n`BQuarterEnd.is_year_start`\n\n`BQuarterEnd.is_year_end`\n\n`BQuarterBegin`\n\nDateOffset increments between the first business day of each Quarter.\n\n`BQuarterBegin.freqstr`\n\n`BQuarterBegin.kwds`\n\n`BQuarterBegin.name`\n\n`BQuarterBegin.nanos`\n\n`BQuarterBegin.normalize`\n\n`BQuarterBegin.rule_code`\n\n`BQuarterBegin.n`\n\n`BQuarterBegin.startingMonth`\n\n`BQuarterBegin.apply`\n\n`BQuarterBegin.apply_index`(other)\n\n`BQuarterBegin.copy`\n\n`BQuarterBegin.isAnchored`\n\n`BQuarterBegin.onOffset`\n\n`BQuarterBegin.is_anchored`\n\n`BQuarterBegin.is_on_offset`\n\n`BQuarterBegin.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`BQuarterBegin.is_month_start`\n\n`BQuarterBegin.is_month_end`\n\n`BQuarterBegin.is_quarter_start`\n\n`BQuarterBegin.is_quarter_end`\n\n`BQuarterBegin.is_year_start`\n\n`BQuarterBegin.is_year_end`\n\n`QuarterEnd`\n\nDateOffset increments between Quarter end dates.\n\n`QuarterEnd.freqstr`\n\n`QuarterEnd.kwds`\n\n`QuarterEnd.name`\n\n`QuarterEnd.nanos`\n\n`QuarterEnd.normalize`\n\n`QuarterEnd.rule_code`\n\n`QuarterEnd.n`\n\n`QuarterEnd.startingMonth`\n\n`QuarterEnd.apply`\n\n`QuarterEnd.apply_index`(other)\n\n`QuarterEnd.copy`\n\n`QuarterEnd.isAnchored`\n\n`QuarterEnd.onOffset`\n\n`QuarterEnd.is_anchored`\n\n`QuarterEnd.is_on_offset`\n\n`QuarterEnd.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`QuarterEnd.is_month_start`\n\n`QuarterEnd.is_month_end`\n\n`QuarterEnd.is_quarter_start`\n\n`QuarterEnd.is_quarter_end`\n\n`QuarterEnd.is_year_start`\n\n`QuarterEnd.is_year_end`\n\n`QuarterBegin`\n\nDateOffset increments between Quarter start dates.\n\n`QuarterBegin.freqstr`\n\n`QuarterBegin.kwds`\n\n`QuarterBegin.name`\n\n`QuarterBegin.nanos`\n\n`QuarterBegin.normalize`\n\n`QuarterBegin.rule_code`\n\n`QuarterBegin.n`\n\n`QuarterBegin.startingMonth`\n\n`QuarterBegin.apply`\n\n`QuarterBegin.apply_index`(other)\n\n`QuarterBegin.copy`\n\n`QuarterBegin.isAnchored`\n\n`QuarterBegin.onOffset`\n\n`QuarterBegin.is_anchored`\n\n`QuarterBegin.is_on_offset`\n\n`QuarterBegin.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`QuarterBegin.is_month_start`\n\n`QuarterBegin.is_month_end`\n\n`QuarterBegin.is_quarter_start`\n\n`QuarterBegin.is_quarter_end`\n\n`QuarterBegin.is_year_start`\n\n`QuarterBegin.is_year_end`\n\n`BYearEnd`\n\nDateOffset increments between the last business day of the year.\n\n`BYearEnd.freqstr`\n\n`BYearEnd.kwds`\n\n`BYearEnd.name`\n\n`BYearEnd.nanos`\n\n`BYearEnd.normalize`\n\n`BYearEnd.rule_code`\n\n`BYearEnd.n`\n\n`BYearEnd.month`\n\n`BYearEnd.apply`\n\n`BYearEnd.apply_index`(other)\n\n`BYearEnd.copy`\n\n`BYearEnd.isAnchored`\n\n`BYearEnd.onOffset`\n\n`BYearEnd.is_anchored`\n\n`BYearEnd.is_on_offset`\n\n`BYearEnd.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`BYearEnd.is_month_start`\n\n`BYearEnd.is_month_end`\n\n`BYearEnd.is_quarter_start`\n\n`BYearEnd.is_quarter_end`\n\n`BYearEnd.is_year_start`\n\n`BYearEnd.is_year_end`\n\n`BYearBegin`\n\nDateOffset increments between the first business day of the year.\n\n`BYearBegin.freqstr`\n\n`BYearBegin.kwds`\n\n`BYearBegin.name`\n\n`BYearBegin.nanos`\n\n`BYearBegin.normalize`\n\n`BYearBegin.rule_code`\n\n`BYearBegin.n`\n\n`BYearBegin.month`\n\n`BYearBegin.apply`\n\n`BYearBegin.apply_index`(other)\n\n`BYearBegin.copy`\n\n`BYearBegin.isAnchored`\n\n`BYearBegin.onOffset`\n\n`BYearBegin.is_anchored`\n\n`BYearBegin.is_on_offset`\n\n`BYearBegin.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`BYearBegin.is_month_start`\n\n`BYearBegin.is_month_end`\n\n`BYearBegin.is_quarter_start`\n\n`BYearBegin.is_quarter_end`\n\n`BYearBegin.is_year_start`\n\n`BYearBegin.is_year_end`\n\n`YearEnd`\n\nDateOffset increments between calendar year ends.\n\n`YearEnd.freqstr`\n\n`YearEnd.kwds`\n\n`YearEnd.name`\n\n`YearEnd.nanos`\n\n`YearEnd.normalize`\n\n`YearEnd.rule_code`\n\n`YearEnd.n`\n\n`YearEnd.month`\n\n`YearEnd.apply`\n\n`YearEnd.apply_index`(other)\n\n`YearEnd.copy`\n\n`YearEnd.isAnchored`\n\n`YearEnd.onOffset`\n\n`YearEnd.is_anchored`\n\n`YearEnd.is_on_offset`\n\n`YearEnd.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`YearEnd.is_month_start`\n\n`YearEnd.is_month_end`\n\n`YearEnd.is_quarter_start`\n\n`YearEnd.is_quarter_end`\n\n`YearEnd.is_year_start`\n\n`YearEnd.is_year_end`\n\n`YearBegin`\n\nDateOffset increments between calendar year begin dates.\n\n`YearBegin.freqstr`\n\n`YearBegin.kwds`\n\n`YearBegin.name`\n\n`YearBegin.nanos`\n\n`YearBegin.normalize`\n\n`YearBegin.rule_code`\n\n`YearBegin.n`\n\n`YearBegin.month`\n\n`YearBegin.apply`\n\n`YearBegin.apply_index`(other)\n\n`YearBegin.copy`\n\n`YearBegin.isAnchored`\n\n`YearBegin.onOffset`\n\n`YearBegin.is_anchored`\n\n`YearBegin.is_on_offset`\n\n`YearBegin.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`YearBegin.is_month_start`\n\n`YearBegin.is_month_end`\n\n`YearBegin.is_quarter_start`\n\n`YearBegin.is_quarter_end`\n\n`YearBegin.is_year_start`\n\n`YearBegin.is_year_end`\n\n`FY5253`\n\nDescribes 52-53 week fiscal year.\n\n`FY5253.freqstr`\n\n`FY5253.kwds`\n\n`FY5253.name`\n\n`FY5253.nanos`\n\n`FY5253.normalize`\n\n`FY5253.rule_code`\n\n`FY5253.n`\n\n`FY5253.startingMonth`\n\n`FY5253.variation`\n\n`FY5253.weekday`\n\n`FY5253.apply`\n\n`FY5253.apply_index`(other)\n\n`FY5253.copy`\n\n`FY5253.get_rule_code_suffix`\n\n`FY5253.get_year_end`\n\n`FY5253.isAnchored`\n\n`FY5253.onOffset`\n\n`FY5253.is_anchored`\n\n`FY5253.is_on_offset`\n\n`FY5253.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`FY5253.is_month_start`\n\n`FY5253.is_month_end`\n\n`FY5253.is_quarter_start`\n\n`FY5253.is_quarter_end`\n\n`FY5253.is_year_start`\n\n`FY5253.is_year_end`\n\n`FY5253Quarter`\n\nDateOffset increments between business quarter dates for 52-53 week fiscal\nyear (also known as a 4-4-5 calendar).\n\n`FY5253Quarter.freqstr`\n\n`FY5253Quarter.kwds`\n\n`FY5253Quarter.name`\n\n`FY5253Quarter.nanos`\n\n`FY5253Quarter.normalize`\n\n`FY5253Quarter.rule_code`\n\n`FY5253Quarter.n`\n\n`FY5253Quarter.qtr_with_extra_week`\n\n`FY5253Quarter.startingMonth`\n\n`FY5253Quarter.variation`\n\n`FY5253Quarter.weekday`\n\n`FY5253Quarter.apply`\n\n`FY5253Quarter.apply_index`(other)\n\n`FY5253Quarter.copy`\n\n`FY5253Quarter.get_rule_code_suffix`\n\n`FY5253Quarter.get_weeks`\n\n`FY5253Quarter.isAnchored`\n\n`FY5253Quarter.onOffset`\n\n`FY5253Quarter.is_anchored`\n\n`FY5253Quarter.is_on_offset`\n\n`FY5253Quarter.year_has_extra_week`\n\n`FY5253Quarter.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`FY5253Quarter.is_month_start`\n\n`FY5253Quarter.is_month_end`\n\n`FY5253Quarter.is_quarter_start`\n\n`FY5253Quarter.is_quarter_end`\n\n`FY5253Quarter.is_year_start`\n\n`FY5253Quarter.is_year_end`\n\n`Easter`\n\nDateOffset for the Easter holiday using logic defined in dateutil.\n\n`Easter.freqstr`\n\n`Easter.kwds`\n\n`Easter.name`\n\n`Easter.nanos`\n\n`Easter.normalize`\n\n`Easter.rule_code`\n\n`Easter.n`\n\n`Easter.apply`\n\n`Easter.apply_index`(other)\n\n`Easter.copy`\n\n`Easter.isAnchored`\n\n`Easter.onOffset`\n\n`Easter.is_anchored`\n\n`Easter.is_on_offset`\n\n`Easter.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Easter.is_month_start`\n\n`Easter.is_month_end`\n\n`Easter.is_quarter_start`\n\n`Easter.is_quarter_end`\n\n`Easter.is_year_start`\n\n`Easter.is_year_end`\n\n`Tick`\n\nAttributes\n\n`Tick.delta`\n\n`Tick.freqstr`\n\n`Tick.kwds`\n\n`Tick.name`\n\n`Tick.nanos`\n\n`Tick.normalize`\n\n`Tick.rule_code`\n\n`Tick.n`\n\n`Tick.copy`\n\n`Tick.isAnchored`\n\n`Tick.onOffset`\n\n`Tick.is_anchored`\n\n`Tick.is_on_offset`\n\n`Tick.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Tick.apply`\n\n`Tick.apply_index`(other)\n\n`Tick.is_month_start`\n\n`Tick.is_month_end`\n\n`Tick.is_quarter_start`\n\n`Tick.is_quarter_end`\n\n`Tick.is_year_start`\n\n`Tick.is_year_end`\n\n`Day`\n\nAttributes\n\n`Day.delta`\n\n`Day.freqstr`\n\n`Day.kwds`\n\n`Day.name`\n\n`Day.nanos`\n\n`Day.normalize`\n\n`Day.rule_code`\n\n`Day.n`\n\n`Day.copy`\n\n`Day.isAnchored`\n\n`Day.onOffset`\n\n`Day.is_anchored`\n\n`Day.is_on_offset`\n\n`Day.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Day.apply`\n\n`Day.apply_index`(other)\n\n`Day.is_month_start`\n\n`Day.is_month_end`\n\n`Day.is_quarter_start`\n\n`Day.is_quarter_end`\n\n`Day.is_year_start`\n\n`Day.is_year_end`\n\n`Hour`\n\nAttributes\n\n`Hour.delta`\n\n`Hour.freqstr`\n\n`Hour.kwds`\n\n`Hour.name`\n\n`Hour.nanos`\n\n`Hour.normalize`\n\n`Hour.rule_code`\n\n`Hour.n`\n\n`Hour.copy`\n\n`Hour.isAnchored`\n\n`Hour.onOffset`\n\n`Hour.is_anchored`\n\n`Hour.is_on_offset`\n\n`Hour.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Hour.apply`\n\n`Hour.apply_index`(other)\n\n`Hour.is_month_start`\n\n`Hour.is_month_end`\n\n`Hour.is_quarter_start`\n\n`Hour.is_quarter_end`\n\n`Hour.is_year_start`\n\n`Hour.is_year_end`\n\n`Minute`\n\nAttributes\n\n`Minute.delta`\n\n`Minute.freqstr`\n\n`Minute.kwds`\n\n`Minute.name`\n\n`Minute.nanos`\n\n`Minute.normalize`\n\n`Minute.rule_code`\n\n`Minute.n`\n\n`Minute.copy`\n\n`Minute.isAnchored`\n\n`Minute.onOffset`\n\n`Minute.is_anchored`\n\n`Minute.is_on_offset`\n\n`Minute.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Minute.apply`\n\n`Minute.apply_index`(other)\n\n`Minute.is_month_start`\n\n`Minute.is_month_end`\n\n`Minute.is_quarter_start`\n\n`Minute.is_quarter_end`\n\n`Minute.is_year_start`\n\n`Minute.is_year_end`\n\n`Second`\n\nAttributes\n\n`Second.delta`\n\n`Second.freqstr`\n\n`Second.kwds`\n\n`Second.name`\n\n`Second.nanos`\n\n`Second.normalize`\n\n`Second.rule_code`\n\n`Second.n`\n\n`Second.copy`\n\n`Second.isAnchored`\n\n`Second.onOffset`\n\n`Second.is_anchored`\n\n`Second.is_on_offset`\n\n`Second.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Second.apply`\n\n`Second.apply_index`(other)\n\n`Second.is_month_start`\n\n`Second.is_month_end`\n\n`Second.is_quarter_start`\n\n`Second.is_quarter_end`\n\n`Second.is_year_start`\n\n`Second.is_year_end`\n\n`Milli`\n\nAttributes\n\n`Milli.delta`\n\n`Milli.freqstr`\n\n`Milli.kwds`\n\n`Milli.name`\n\n`Milli.nanos`\n\n`Milli.normalize`\n\n`Milli.rule_code`\n\n`Milli.n`\n\n`Milli.copy`\n\n`Milli.isAnchored`\n\n`Milli.onOffset`\n\n`Milli.is_anchored`\n\n`Milli.is_on_offset`\n\n`Milli.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Milli.apply`\n\n`Milli.apply_index`(other)\n\n`Milli.is_month_start`\n\n`Milli.is_month_end`\n\n`Milli.is_quarter_start`\n\n`Milli.is_quarter_end`\n\n`Milli.is_year_start`\n\n`Milli.is_year_end`\n\n`Micro`\n\nAttributes\n\n`Micro.delta`\n\n`Micro.freqstr`\n\n`Micro.kwds`\n\n`Micro.name`\n\n`Micro.nanos`\n\n`Micro.normalize`\n\n`Micro.rule_code`\n\n`Micro.n`\n\n`Micro.copy`\n\n`Micro.isAnchored`\n\n`Micro.onOffset`\n\n`Micro.is_anchored`\n\n`Micro.is_on_offset`\n\n`Micro.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Micro.apply`\n\n`Micro.apply_index`(other)\n\n`Micro.is_month_start`\n\n`Micro.is_month_end`\n\n`Micro.is_quarter_start`\n\n`Micro.is_quarter_end`\n\n`Micro.is_year_start`\n\n`Micro.is_year_end`\n\n`Nano`\n\nAttributes\n\n`Nano.delta`\n\n`Nano.freqstr`\n\n`Nano.kwds`\n\n`Nano.name`\n\n`Nano.nanos`\n\n`Nano.normalize`\n\n`Nano.rule_code`\n\n`Nano.n`\n\n`Nano.copy`\n\n`Nano.isAnchored`\n\n`Nano.onOffset`\n\n`Nano.is_anchored`\n\n`Nano.is_on_offset`\n\n`Nano.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Nano.apply`\n\n`Nano.apply_index`(other)\n\n`Nano.is_month_start`\n\n`Nano.is_month_end`\n\n`Nano.is_quarter_start`\n\n`Nano.is_quarter_end`\n\n`Nano.is_year_start`\n\n`Nano.is_year_end`\n\n"}, {"name": "Duplicate Labels", "path": "user_guide/duplicates", "type": "Manual", "text": "\n`Index` objects are not required to be unique; you can have duplicate row or\ncolumn labels. This may be a bit confusing at first. If you\u2019re familiar with\nSQL, you know that row labels are similar to a primary key on a table, and you\nwould never want duplicates in a SQL table. But one of pandas\u2019 roles is to\nclean messy, real-world data before it goes to some downstream system. And\nreal-world data has duplicates, even in fields that are supposed to be unique.\n\nThis section describes how duplicate labels change the behavior of certain\noperations, and how prevent duplicates from arising during operations, or to\ndetect them if they do.\n\nSome pandas methods (`Series.reindex()` for example) just don\u2019t work with\nduplicates present. The output can\u2019t be determined, and so pandas raises.\n\nOther methods, like indexing, can give very surprising results. Typically\nindexing with a scalar will reduce dimensionality. Slicing a `DataFrame` with\na scalar will return a `Series`. Slicing a `Series` with a scalar will return\na scalar. But with duplicates, this isn\u2019t the case.\n\nWe have duplicates in the columns. If we slice `'B'`, we get back a `Series`\n\nBut slicing `'A'` returns a `DataFrame`\n\nThis applies to row labels as well\n\nYou can check whether an `Index` (storing the row or column labels) is unique\nwith `Index.is_unique`:\n\nNote\n\nChecking whether an index is unique is somewhat expensive for large datasets.\npandas does cache this result, so re-checking on the same index is very fast.\n\n`Index.duplicated()` will return a boolean ndarray indicating whether a label\nis repeated.\n\nWhich can be used as a boolean filter to drop duplicate rows.\n\nIf you need additional logic to handle duplicate labels, rather than just\ndropping the repeats, using `groupby()` on the index is a common trick. For\nexample, we\u2019ll resolve duplicates by taking the average of all rows with the\nsame label.\n\nNew in version 1.2.0.\n\nAs noted above, handling duplicates is an important feature when reading in\nraw data. That said, you may want to avoid introducing duplicates as part of a\ndata processing pipeline (from methods like `pandas.concat()`, `rename()`,\netc.). Both `Series` and `DataFrame` disallow duplicate labels by calling\n`.set_flags(allows_duplicate_labels=False)`. (the default is to allow them).\nIf there are duplicate labels, an exception will be raised.\n\nThis applies to both row and column labels for a `DataFrame`\n\nThis attribute can be checked or set with `allows_duplicate_labels`, which\nindicates whether that object can have duplicate labels.\n\n`DataFrame.set_flags()` can be used to return a new `DataFrame` with\nattributes like `allows_duplicate_labels` set to some value\n\nThe new `DataFrame` returned is a view on the same data as the old\n`DataFrame`. Or the property can just be set directly on the same object\n\nWhen processing raw, messy data you might initially read in the messy data\n(which potentially has duplicate labels), deduplicate, and then disallow\nduplicates going forward, to ensure that your data pipeline doesn\u2019t introduce\nduplicates.\n\nSetting `allows_duplicate_labels=True` on a `Series` or `DataFrame` with\nduplicate labels or performing an operation that introduces duplicate labels\non a `Series` or `DataFrame` that disallows duplicates will raise an\n`errors.DuplicateLabelError`.\n\nThis error message contains the labels that are duplicated, and the numeric\npositions of all the duplicates (including the \u201coriginal\u201d) in the `Series` or\n`DataFrame`\n\nIn general, disallowing duplicates is \u201csticky\u201d. It\u2019s preserved through\noperations.\n\nWarning\n\nThis is an experimental feature. Currently, many methods fail to propagate the\n`allows_duplicate_labels` value. In future versions it is expected that every\nmethod taking or returning one or more DataFrame or Series objects will\npropagate `allows_duplicate_labels`.\n\n"}, {"name": "Enhancing performance", "path": "user_guide/enhancingperf", "type": "Manual", "text": "\nIn this part of the tutorial, we will investigate how to speed up certain\nfunctions operating on pandas `DataFrames` using three different techniques:\nCython, Numba and `pandas.eval()`. We will see a speed improvement of ~200\nwhen we use Cython and Numba on a test function operating row-wise on the\n`DataFrame`. Using `pandas.eval()` we will speed up a sum by an order of ~2.\n\nNote\n\nIn addition to following the steps in this tutorial, users interested in\nenhancing performance are highly encouraged to install the recommended\ndependencies for pandas. These dependencies are often not installed by\ndefault, but will offer speed improvements if present.\n\nFor many use cases writing pandas in pure Python and NumPy is sufficient. In\nsome computationally heavy applications however, it can be possible to achieve\nsizable speed-ups by offloading work to cython.\n\nThis tutorial assumes you have refactored as much as possible in Python, for\nexample by trying to remove for-loops and making use of NumPy vectorization.\nIt\u2019s always worth optimising in Python first.\n\nThis tutorial walks through a \u201ctypical\u201d process of cythonizing a slow\ncomputation. We use an example from the Cython documentation but in the\ncontext of pandas. Our final cythonized solution is around 100 times faster\nthan the pure Python solution.\n\nWe have a `DataFrame` to which we want to apply a function row-wise.\n\nHere\u2019s the function in pure Python:\n\nWe achieve our result by using `apply` (row-wise):\n\nBut clearly this isn\u2019t fast enough for us. Let\u2019s take a look and see where the\ntime is spent during this operation (limited to the most time consuming four\ncalls) using the prun ipython magic function:\n\nBy far the majority of time is spend inside either `integrate_f` or `f`, hence\nwe\u2019ll concentrate our efforts cythonizing these two functions.\n\nFirst we\u2019re going to need to import the Cython magic function to IPython:\n\nNow, let\u2019s simply copy our functions over to Cython as is (the suffix is here\nto distinguish between function versions):\n\nNote\n\nIf you\u2019re having trouble pasting the above into your ipython, you may need to\nbe using bleeding edge IPython for paste to play well with cell magics.\n\nAlready this has shaved a third off, not too bad for a simple copy and paste.\n\nWe get another huge improvement simply by providing type information:\n\nNow, we\u2019re talking! It\u2019s now over ten times faster than the original Python\nimplementation, and we haven\u2019t really modified the code. Let\u2019s have another\nlook at what\u2019s eating up time:\n\nIt\u2019s calling series\u2026 a lot! It\u2019s creating a Series from each row, and get-ting\nfrom both the index and the series (three times for each row). Function calls\nare expensive in Python, so maybe we could minimize these by cythonizing the\napply part.\n\nNote\n\nWe are now passing ndarrays into the Cython function, fortunately Cython plays\nvery nicely with NumPy.\n\nThe implementation is simple, it creates an array of zeros and loops over the\nrows, applying our `integrate_f_typed`, and putting this in the zeros array.\n\nWarning\n\nYou can not pass a `Series` directly as a `ndarray` typed parameter to a\nCython function. Instead pass the actual `ndarray` using the\n`Series.to_numpy()`. The reason is that the Cython definition is specific to\nan ndarray and not the passed `Series`.\n\nSo, do not do this:\n\nBut rather, use `Series.to_numpy()` to get the underlying `ndarray`:\n\nNote\n\nLoops like this would be extremely slow in Python, but in Cython looping over\nNumPy arrays is fast.\n\nWe\u2019ve gotten another big improvement. Let\u2019s check again where the time is\nspent:\n\nAs one might expect, the majority of the time is now spent in\n`apply_integrate_f`, so if we wanted to make anymore efficiencies we must\ncontinue to concentrate our efforts here.\n\nThere is still hope for improvement. Here\u2019s an example of using some more\nadvanced Cython techniques:\n\nEven faster, with the caveat that a bug in our Cython code (an off-by-one\nerror, for example) might cause a segfault because memory access isn\u2019t\nchecked. For more about `boundscheck` and `wraparound`, see the Cython docs on\ncompiler directives.\n\nAn alternative to statically compiling Cython code is to use a dynamic just-\nin-time (JIT) compiler with Numba.\n\nNumba allows you to write a pure Python function which can be JIT compiled to\nnative machine instructions, similar in performance to C, C++ and Fortran, by\ndecorating your function with `@jit`.\n\nNumba works by generating optimized machine code using the LLVM compiler\ninfrastructure at import time, runtime, or statically (using the included pycc\ntool). Numba supports compilation of Python to run on either CPU or GPU\nhardware and is designed to integrate with the Python scientific software\nstack.\n\nNote\n\nThe `@jit` compilation will add overhead to the runtime of the function, so\nperformance benefits may not be realized especially when using small data\nsets. Consider caching your function to avoid compilation overhead each time\nyour function is run.\n\nNumba can be used in 2 ways with pandas:\n\nSpecify the `engine=\"numba\"` keyword in select pandas methods\n\nDefine your own Python function decorated with `@jit` and pass the underlying\nNumPy array of `Series` or `Dataframe` (using `to_numpy()`) into the function\n\nIf Numba is installed, one can specify `engine=\"numba\"` in select pandas\nmethods to execute the method using Numba. Methods that support\n`engine=\"numba\"` will also have an `engine_kwargs` keyword that accepts a\ndictionary that allows one to specify `\"nogil\"`, `\"nopython\"` and `\"parallel\"`\nkeys with boolean values to pass into the `@jit` decorator. If `engine_kwargs`\nis not specified, it defaults to `{\"nogil\": False, \"nopython\": True,\n\"parallel\": False}` unless otherwise specified.\n\nIn terms of performance, the first time a function is run using the Numba\nengine will be slow as Numba will have some function compilation overhead.\nHowever, the JIT compiled functions are cached, and subsequent calls will be\nfast. In general, the Numba engine is performant with a larger amount of data\npoints (e.g. 1+ million).\n\nA custom Python function decorated with `@jit` can be used with pandas objects\nby passing their NumPy array representations with `to_numpy()`.\n\nIn this example, using Numba was faster than Cython.\n\nNumba can also be used to write vectorized functions that do not require the\nuser to explicitly loop over the observations of a vector; a vectorized\nfunction will be applied to each row automatically. Consider the following\nexample of doubling each observation:\n\nNumba is best at accelerating functions that apply numerical functions to\nNumPy arrays. If you try to `@jit` a function that contains unsupported Python\nor NumPy code, compilation will revert object mode which will mostly likely\nnot speed up your function. If you would prefer that Numba throw an error if\nit cannot compile a function in a way that speeds up your code, pass Numba the\nargument `nopython=True` (e.g. `@jit(nopython=True)`). For more on\ntroubleshooting Numba modes, see the Numba troubleshooting page.\n\nUsing `parallel=True` (e.g. `@jit(parallel=True)`) may result in a `SIGABRT`\nif the threading layer leads to unsafe behavior. You can first specify a safe\nthreading layer before running a JIT function with `parallel=True`.\n\nGenerally if the you encounter a segfault (`SIGSEGV`) while using Numba,\nplease report the issue to the Numba issue tracker.\n\nThe top-level function `pandas.eval()` implements expression evaluation of\n`Series` and `DataFrame` objects.\n\nNote\n\nTo benefit from using `eval()` you need to install `numexpr`. See the\nrecommended dependencies section for more details.\n\nThe point of using `eval()` for expression evaluation rather than plain Python\nis two-fold: 1) large `DataFrame` objects are evaluated more efficiently and\n2) large arithmetic and boolean expressions are evaluated all at once by the\nunderlying engine (by default `numexpr` is used for evaluation).\n\nNote\n\nYou should not use `eval()` for simple expressions or for expressions\ninvolving small DataFrames. In fact, `eval()` is many orders of magnitude\nslower for smaller expressions/objects than plain ol\u2019 Python. A good rule of\nthumb is to only use `eval()` when you have a `DataFrame` with more than\n10,000 rows.\n\n`eval()` supports all arithmetic expressions supported by the engine in\naddition to some extensions available only in pandas.\n\nNote\n\nThe larger the frame and the larger the expression the more speedup you will\nsee from using `eval()`.\n\nThese operations are supported by `pandas.eval()`:\n\nArithmetic operations except for the left shift (`<<`) and right shift (`>>`)\noperators, e.g., `df + 2 * pi / s ** 4 % 42 - the_golden_ratio`\n\nComparison operations, including chained comparisons, e.g., `2 < df < df2`\n\nBoolean operations, e.g., `df < df2 and df3 < df4 or not df_bool`\n\n`list` and `tuple` literals, e.g., `[1, 2]` or `(1, 2)`\n\nAttribute access, e.g., `df.a`\n\nSubscript expressions, e.g., `df[0]`\n\nSimple variable evaluation, e.g., `pd.eval(\"df\")` (this is not very useful)\n\nMath functions: `sin`, `cos`, `exp`, `log`, `expm1`, `log1p`, `sqrt`, `sinh`,\n`cosh`, `tanh`, `arcsin`, `arccos`, `arctan`, `arccosh`, `arcsinh`, `arctanh`,\n`abs`, `arctan2` and `log10`.\n\nThis Python syntax is not allowed:\n\nExpressions\n\nFunction calls other than math functions.\n\n`is`/`is not` operations\n\n`if` expressions\n\n`lambda` expressions\n\n`list`/`set`/`dict` comprehensions\n\nLiteral `dict` and `set` expressions\n\n`yield` expressions\n\nGenerator expressions\n\nBoolean expressions consisting of only scalar values\n\nStatements\n\nNeither simple nor compound statements are allowed. This includes things like\n`for`, `while`, and `if`.\n\n`pandas.eval()` works well with expressions containing large arrays.\n\nFirst let\u2019s create a few decent-sized arrays to play with:\n\nNow let\u2019s compare adding them together using plain ol\u2019 Python versus `eval()`:\n\nNow let\u2019s do the same thing but with comparisons:\n\n`eval()` also works with unaligned pandas objects:\n\nNote\n\nOperations such as\n\nshould be performed in Python. An exception will be raised if you try to\nperform any boolean/bitwise operations with scalar operands that are not of\ntype `bool` or `np.bool_`. Again, you should perform these kinds of operations\nin plain Python.\n\nIn addition to the top level `pandas.eval()` function you can also evaluate an\nexpression in the \u201ccontext\u201d of a `DataFrame`.\n\nAny expression that is a valid `pandas.eval()` expression is also a valid\n`DataFrame.eval()` expression, with the added benefit that you don\u2019t have to\nprefix the name of the `DataFrame` to the column(s) you\u2019re interested in\nevaluating.\n\nIn addition, you can perform assignment of columns within an expression. This\nallows for formulaic evaluation. The assignment target can be a new column\nname or an existing column name, and it must be a valid Python identifier.\n\nThe `inplace` keyword determines whether this assignment will performed on the\noriginal `DataFrame` or return a copy with the new column.\n\nWhen `inplace` is set to `False`, the default, a copy of the `DataFrame` with\nthe new or modified columns is returned and the original frame is unchanged.\n\nAs a convenience, multiple assignments can be performed by using a multi-line\nstring.\n\nThe equivalent in standard Python would be\n\nThe `query` method has a `inplace` keyword which determines whether the query\nmodifies the original frame.\n\nYou must explicitly reference any local variable that you want to use in an\nexpression by placing the `@` character in front of the name. For example,\n\nIf you don\u2019t prefix the local variable with `@`, pandas will raise an\nexception telling you the variable is undefined.\n\nWhen using `DataFrame.eval()` and `DataFrame.query()`, this allows you to have\na local variable and a `DataFrame` column with the same name in an expression.\n\nWith `pandas.eval()` you cannot use the `@` prefix at all, because it isn\u2019t\ndefined in that context. pandas will let you know this if you try to use `@`\nin a top-level call to `pandas.eval()`. For example,\n\nIn this case, you should simply refer to the variables like you would in\nstandard Python.\n\nThere are two different parsers and two different engines you can use as the\nbackend.\n\nThe default `'pandas'` parser allows a more intuitive syntax for expressing\nquery-like operations (comparisons, conjunctions and disjunctions). In\nparticular, the precedence of the `&` and `|` operators is made equal to the\nprecedence of the corresponding boolean operations `and` and `or`.\n\nFor example, the above conjunction can be written without parentheses.\nAlternatively, you can use the `'python'` parser to enforce strict Python\nsemantics.\n\nThe same expression can be \u201canded\u201d together with the word `and` as well:\n\nThe `and` and `or` operators here have the same precedence that they would in\nvanilla Python.\n\nThere\u2019s also the option to make `eval()` operate identical to plain ol\u2019\nPython.\n\nNote\n\nUsing the `'python'` engine is generally not useful, except for testing other\nevaluation engines against it. You will achieve no performance benefits using\n`eval()` with `engine='python'` and in fact may incur a performance hit.\n\nYou can see this by using `pandas.eval()` with the `'python'` engine. It is a\nbit slower (not by much) than evaluating the same expression in Python\n\n`eval()` is intended to speed up certain kinds of operations. In particular,\nthose operations involving complex expressions with large `DataFrame`/`Series`\nobjects should see a significant performance benefit. Here is a plot showing\nthe running time of `pandas.eval()` as function of the size of the frame\ninvolved in the computation. The two lines are two different engines.\n\nNote\n\nOperations with smallish objects (around 15k-20k rows) are faster using plain\nPython:\n\nThis plot was created using a `DataFrame` with 3 columns each containing\nfloating point values generated using `numpy.random.randn()`.\n\nExpressions that would result in an object dtype or involve datetime\noperations (because of `NaT`) must be evaluated in Python space. The main\nreason for this behavior is to maintain backwards compatibility with versions\nof NumPy < 1.7. In those versions of NumPy a call to `ndarray.astype(str)`\nwill truncate any strings that are more than 60 characters in length. Second,\nwe can\u2019t pass `object` arrays to `numexpr` thus string comparisons must be\nevaluated in Python space.\n\nThe upshot is that this only applies to object-dtype expressions. So, if you\nhave an expression\u2013for example\n\nthe numeric part of the comparison (`nums == 1`) will be evaluated by\n`numexpr`.\n\nIn general, `DataFrame.query()`/`pandas.eval()` will evaluate the\nsubexpressions that can be evaluated by `numexpr` and those that must be\nevaluated in Python space transparently to the user. This is done by inferring\nthe result type of an expression from its arguments and operators.\n\n"}, {"name": "Essential basic functionality", "path": "user_guide/basics", "type": "Manual", "text": "\nHere we discuss a lot of the essential functionality common to the pandas data\nstructures. To begin, let\u2019s create some example objects like we did in the 10\nminutes to pandas section:\n\nTo view a small sample of a Series or DataFrame object, use the `head()` and\n`tail()` methods. The default number of elements to display is five, but you\nmay pass a custom number.\n\npandas objects have a number of attributes enabling you to access the metadata\n\nshape: gives the axis dimensions of the object, consistent with ndarray\n\nSeries: index (only axis)\n\nDataFrame: index (rows) and columns\n\nNote, these attributes can be safely assigned to!\n\npandas objects (`Index`, `Series`, `DataFrame`) can be thought of as\ncontainers for arrays, which hold the actual data and do the actual\ncomputation. For many types, the underlying array is a `numpy.ndarray`.\nHowever, pandas and 3rd party libraries may extend NumPy\u2019s type system to add\nsupport for custom arrays (see dtypes).\n\nTo get the actual data inside a `Index` or `Series`, use the `.array` property\n\n`array` will always be an `ExtensionArray`. The exact details of what an\n`ExtensionArray` is and why pandas uses them are a bit beyond the scope of\nthis introduction. See dtypes for more.\n\nIf you know you need a NumPy array, use `to_numpy()` or `numpy.asarray()`.\n\nWhen the Series or Index is backed by an `ExtensionArray`, `to_numpy()` may\ninvolve copying data and coercing values. See dtypes for more.\n\n`to_numpy()` gives some control over the `dtype` of the resulting\n`numpy.ndarray`. For example, consider datetimes with timezones. NumPy doesn\u2019t\nhave a dtype to represent timezone-aware datetimes, so there are two possibly\nuseful representations:\n\nAn object-dtype `numpy.ndarray` with `Timestamp` objects, each with the\ncorrect `tz`\n\nA `datetime64[ns]` -dtype `numpy.ndarray`, where the values have been\nconverted to UTC and the timezone discarded\n\nTimezones may be preserved with `dtype=object`\n\nOr thrown away with `dtype='datetime64[ns]'`\n\nGetting the \u201craw data\u201d inside a `DataFrame` is possibly a bit more complex.\nWhen your `DataFrame` only has a single data type for all the columns,\n`DataFrame.to_numpy()` will return the underlying data:\n\nIf a DataFrame contains homogeneously-typed data, the ndarray can actually be\nmodified in-place, and the changes will be reflected in the data structure.\nFor heterogeneous data (e.g. some of the DataFrame\u2019s columns are not all the\nsame dtype), this will not be the case. The values attribute itself, unlike\nthe axis labels, cannot be assigned to.\n\nNote\n\nWhen working with heterogeneous data, the dtype of the resulting ndarray will\nbe chosen to accommodate all of the data involved. For example, if strings are\ninvolved, the result will be of object dtype. If there are only floats and\nintegers, the resulting array will be of float dtype.\n\nIn the past, pandas recommended `Series.values` or `DataFrame.values` for\nextracting the data from a Series or DataFrame. You\u2019ll still find references\nto these in old code bases and online. Going forward, we recommend avoiding\n`.values` and using `.array` or `.to_numpy()`. `.values` has the following\ndrawbacks:\n\nWhen your Series contains an extension type, it\u2019s unclear whether\n`Series.values` returns a NumPy array or the extension array. `Series.array`\nwill always return an `ExtensionArray`, and will never copy data.\n`Series.to_numpy()` will always return a NumPy array, potentially at the cost\nof copying / coercing values.\n\nWhen your DataFrame contains a mixture of data types, `DataFrame.values` may\ninvolve copying data and coercing values to a common dtype, a relatively\nexpensive operation. `DataFrame.to_numpy()`, being a method, makes it clearer\nthat the returned NumPy array may not be a view on the same data in the\nDataFrame.\n\npandas has support for accelerating certain types of binary numerical and\nboolean operations using the `numexpr` library and the `bottleneck` libraries.\n\nThese libraries are especially useful when dealing with large data sets, and\nprovide large speedups. `numexpr` uses smart chunking, caching, and multiple\ncores. `bottleneck` is a set of specialized cython routines that are\nespecially fast when dealing with arrays that have `nans`.\n\nHere is a sample (using 100 column x 100,000 row `DataFrames`):\n\nOperation\n\n0.11.0 (ms)\n\nPrior Version (ms)\n\nRatio to Prior\n\n`df1 > df2`\n\n13.32\n\n125.35\n\n0.1063\n\n`df1 * df2`\n\n21.71\n\n36.63\n\n0.5928\n\n`df1 + df2`\n\n22.04\n\n36.50\n\n0.6039\n\nYou are highly encouraged to install both libraries. See the section\nRecommended Dependencies for more installation info.\n\nThese are both enabled to be used by default, you can control this by setting\nthe options:\n\nWith binary operations between pandas data structures, there are two key\npoints of interest:\n\nBroadcasting behavior between higher- (e.g. DataFrame) and lower-dimensional\n(e.g. Series) objects.\n\nMissing data in computations.\n\nWe will demonstrate how to manage these issues independently, though they can\nbe handled simultaneously.\n\nDataFrame has the methods `add()`, `sub()`, `mul()`, `div()` and related\nfunctions `radd()`, `rsub()`, \u2026 for carrying out binary operations. For\nbroadcasting behavior, Series input is of primary interest. Using these\nfunctions, you can use to either match on the index or columns via the axis\nkeyword:\n\nFurthermore you can align a level of a MultiIndexed DataFrame with a Series.\n\nSeries and Index also support the `divmod()` builtin. This function takes the\nfloor division and modulo operation at the same time returning a two-tuple of\nthe same type as the left hand side. For example:\n\nWe can also do elementwise `divmod()`:\n\nIn Series and DataFrame, the arithmetic functions have the option of inputting\na fill_value, namely a value to substitute when at most one of the values at a\nlocation are missing. For example, when adding two DataFrame objects, you may\nwish to treat NaN as 0 unless both DataFrames are missing that value, in which\ncase the result will be NaN (you can later replace NaN with some other value\nusing `fillna` if you wish).\n\nSeries and DataFrame have the binary comparison methods `eq`, `ne`, `lt`,\n`gt`, `le`, and `ge` whose behavior is analogous to the binary arithmetic\noperations described above:\n\nThese operations produce a pandas object of the same type as the left-hand-\nside input that is of dtype `bool`. These `boolean` objects can be used in\nindexing operations, see the section on Boolean indexing.\n\nYou can apply the reductions: `empty`, `any()`, `all()`, and `bool()` to\nprovide a way to summarize a boolean result.\n\nYou can reduce to a final boolean value.\n\nYou can test if a pandas object is empty, via the `empty` property.\n\nTo evaluate single-element pandas objects in a boolean context, use the method\n`bool()`:\n\nWarning\n\nYou might be tempted to do the following:\n\nOr\n\nThese will both raise errors, as you are trying to compare multiple values.:\n\nSee gotchas for a more detailed discussion.\n\nOften you may find that there is more than one way to compute the same result.\nAs a simple example, consider `df + df` and `df * 2`. To test that these two\ncomputations produce the same result, given the tools shown above, you might\nimagine using `(df + df == df * 2).all()`. But in fact, this expression is\nFalse:\n\nNotice that the boolean DataFrame `df + df == df * 2` contains some False\nvalues! This is because NaNs do not compare as equals:\n\nSo, NDFrames (such as Series and DataFrames) have an `equals()` method for\ntesting equality, with NaNs in corresponding locations treated as equal.\n\nNote that the Series or DataFrame index needs to be in the same order for\nequality to be True:\n\nYou can conveniently perform element-wise comparisons when comparing a pandas\ndata structure with a scalar value:\n\npandas also handles element-wise comparisons between different array-like\nobjects of the same length:\n\nTrying to compare `Index` or `Series` objects of different lengths will raise\na ValueError:\n\nNote that this is different from the NumPy behavior where a comparison can be\nbroadcast:\n\nor it can return False if broadcasting can not be done:\n\nA problem occasionally arising is the combination of two similar data sets\nwhere values in one are preferred over the other. An example would be two data\nseries representing a particular economic indicator where one is considered to\nbe of \u201chigher quality\u201d. However, the lower quality series might extend further\nback in history or have more complete data coverage. As such, we would like to\ncombine two DataFrame objects where missing values in one DataFrame are\nconditionally filled with like-labeled values from the other DataFrame. The\nfunction implementing this operation is `combine_first()`, which we\nillustrate:\n\nThe `combine_first()` method above calls the more general\n`DataFrame.combine()`. This method takes another DataFrame and a combiner\nfunction, aligns the input DataFrame and then passes the combiner function\npairs of Series (i.e., columns whose names are the same).\n\nSo, for instance, to reproduce `combine_first()` as above:\n\nThere exists a large number of methods for computing descriptive statistics\nand other related operations on Series, DataFrame. Most of these are\naggregations (hence producing a lower-dimensional result) like `sum()`,\n`mean()`, and `quantile()`, but some of them, like `cumsum()` and `cumprod()`,\nproduce an object of the same size. Generally speaking, these methods take an\naxis argument, just like ndarray.{sum, std, \u2026}, but the axis can be specified\nby name or integer:\n\nSeries: no axis argument needed\n\nDataFrame: \u201cindex\u201d (axis=0, default), \u201ccolumns\u201d (axis=1)\n\nFor example:\n\nAll such methods have a `skipna` option signaling whether to exclude missing\ndata (`True` by default):\n\nCombined with the broadcasting / arithmetic behavior, one can describe various\nstatistical procedures, like standardization (rendering data zero mean and\nstandard deviation of 1), very concisely:\n\nNote that methods like `cumsum()` and `cumprod()` preserve the location of\n`NaN` values. This is somewhat different from `expanding()` and `rolling()`\nsince `NaN` behavior is furthermore dictated by a `min_periods` parameter.\n\nHere is a quick reference summary table of common functions. Each also takes\nan optional `level` parameter which applies only if the object has a\nhierarchical index.\n\nFunction\n\nDescription\n\n`count`\n\nNumber of non-NA observations\n\n`sum`\n\nSum of values\n\n`mean`\n\nMean of values\n\n`mad`\n\nMean absolute deviation\n\n`median`\n\nArithmetic median of values\n\n`min`\n\nMinimum\n\n`max`\n\nMaximum\n\n`mode`\n\nMode\n\n`abs`\n\nAbsolute Value\n\n`prod`\n\nProduct of values\n\n`std`\n\nBessel-corrected sample standard deviation\n\n`var`\n\nUnbiased variance\n\n`sem`\n\nStandard error of the mean\n\n`skew`\n\nSample skewness (3rd moment)\n\n`kurt`\n\nSample kurtosis (4th moment)\n\n`quantile`\n\nSample quantile (value at %)\n\n`cumsum`\n\nCumulative sum\n\n`cumprod`\n\nCumulative product\n\n`cummax`\n\nCumulative maximum\n\n`cummin`\n\nCumulative minimum\n\nNote that by chance some NumPy methods, like `mean`, `std`, and `sum`, will\nexclude NAs on Series input by default:\n\n`Series.nunique()` will return the number of unique non-NA values in a Series:\n\nThere is a convenient `describe()` function which computes a variety of\nsummary statistics about a Series or the columns of a DataFrame (excluding NAs\nof course):\n\nYou can select specific percentiles to include in the output:\n\nBy default, the median is always included.\n\nFor a non-numerical Series object, `describe()` will give a simple summary of\nthe number of unique values and most frequently occurring values:\n\nNote that on a mixed-type DataFrame object, `describe()` will restrict the\nsummary to include only numerical columns or, if none are, only categorical\ncolumns:\n\nThis behavior can be controlled by providing a list of types as\n`include`/`exclude` arguments. The special value `all` can also be used:\n\nThat feature relies on select_dtypes. Refer to there for details about\naccepted inputs.\n\nThe `idxmin()` and `idxmax()` functions on Series and DataFrame compute the\nindex labels with the minimum and maximum corresponding values:\n\nWhen there are multiple rows (or columns) matching the minimum or maximum\nvalue, `idxmin()` and `idxmax()` return the first matching index:\n\nNote\n\n`idxmin` and `idxmax` are called `argmin` and `argmax` in NumPy.\n\nThe `value_counts()` Series method and top-level function computes a histogram\nof a 1D array of values. It can also be used as a function on regular arrays:\n\nNew in version 1.1.0.\n\nThe `value_counts()` method can be used to count combinations across multiple\ncolumns. By default all columns are used but a subset can be selected using\nthe `subset` argument.\n\nSimilarly, you can get the most frequently occurring value(s), i.e. the mode,\nof the values in a Series or DataFrame:\n\nContinuous values can be discretized using the `cut()` (bins based on values)\nand `qcut()` (bins based on sample quantiles) functions:\n\n`qcut()` computes sample quantiles. For example, we could slice up some\nnormally distributed data into equal-size quartiles like so:\n\nWe can also pass infinite values to define the bins:\n\nTo apply your own or another library\u2019s functions to pandas objects, you should\nbe aware of the three methods below. The appropriate method to use depends on\nwhether your function expects to operate on an entire `DataFrame` or `Series`,\nrow- or column-wise, or elementwise.\n\nTablewise Function Application: `pipe()`\n\nRow or Column-wise Function Application: `apply()`\n\nAggregation API: `agg()` and `transform()`\n\nApplying Elementwise Functions: `applymap()`\n\n`DataFrames` and `Series` can be passed into functions. However, if the\nfunction needs to be called in a chain, consider using the `pipe()` method.\n\nFirst some setup:\n\n`extract_city_name` and `add_country_name` are functions taking and returning\n`DataFrames`.\n\nNow compare the following:\n\nIs equivalent to:\n\npandas encourages the second style, which is known as method chaining. `pipe`\nmakes it easy to use your own or another library\u2019s functions in method chains,\nalongside pandas\u2019 methods.\n\nIn the example above, the functions `extract_city_name` and `add_country_name`\neach expected a `DataFrame` as the first positional argument. What if the\nfunction you wish to apply takes its data as, say, the second argument? In\nthis case, provide `pipe` with a tuple of `(callable, data_keyword)`. `.pipe`\nwill route the `DataFrame` to the argument specified in the tuple.\n\nFor example, we can fit a regression using statsmodels. Their API expects a\nformula first and a `DataFrame` as the second argument, `data`. We pass in the\nfunction, keyword pair `(sm.ols, 'data')` to `pipe`:\n\nThe pipe method is inspired by unix pipes and more recently dplyr and\nmagrittr, which have introduced the popular `(%>%)` (read pipe) operator for\nR. The implementation of `pipe` here is quite clean and feels right at home in\nPython. We encourage you to view the source code of `pipe()`.\n\nArbitrary functions can be applied along the axes of a DataFrame using the\n`apply()` method, which, like the descriptive statistics methods, takes an\noptional `axis` argument:\n\nThe `apply()` method will also dispatch on a string method name.\n\nThe return type of the function passed to `apply()` affects the type of the\nfinal output from `DataFrame.apply` for the default behaviour:\n\nIf the applied function returns a `Series`, the final output is a `DataFrame`.\nThe columns match the index of the `Series` returned by the applied function.\n\nIf the applied function returns any other type, the final output is a\n`Series`.\n\nThis default behaviour can be overridden using the `result_type`, which\naccepts three options: `reduce`, `broadcast`, and `expand`. These will\ndetermine how list-likes return values expand (or not) to a `DataFrame`.\n\n`apply()` combined with some cleverness can be used to answer many questions\nabout a data set. For example, suppose we wanted to extract the date where the\nmaximum value for each column occurred:\n\nYou may also pass additional arguments and keyword arguments to the `apply()`\nmethod. For instance, consider the following function you would like to apply:\n\nYou may then apply this function as follows:\n\nAnother useful feature is the ability to pass Series methods to carry out some\nSeries operation on each column or row:\n\nFinally, `apply()` takes an argument `raw` which is False by default, which\nconverts each row or column into a Series before applying the function. When\nset to True, the passed function will instead receive an ndarray object, which\nhas positive performance implications if you do not need the indexing\nfunctionality.\n\nThe aggregation API allows one to express possibly multiple aggregation\noperations in a single concise way. This API is similar across pandas objects,\nsee groupby API, the window API, and the resample API. The entry point for\naggregation is `DataFrame.aggregate()`, or the alias `DataFrame.agg()`.\n\nWe will use a similar starting frame from above:\n\nUsing a single function is equivalent to `apply()`. You can also pass named\nmethods as strings. These will return a `Series` of the aggregated output:\n\nSingle aggregations on a `Series` this will return a scalar value:\n\nYou can pass multiple aggregation arguments as a list. The results of each of\nthe passed functions will be a row in the resulting `DataFrame`. These are\nnaturally named from the aggregation function.\n\nMultiple functions yield multiple rows:\n\nOn a `Series`, multiple functions return a `Series`, indexed by the function\nnames:\n\nPassing a `lambda` function will yield a `<lambda>` named row:\n\nPassing a named function will yield that name for the row:\n\nPassing a dictionary of column names to a scalar or a list of scalars, to\n`DataFrame.agg` allows you to customize which functions are applied to which\ncolumns. Note that the results are not in any particular order, you can use an\n`OrderedDict` instead to guarantee ordering.\n\nPassing a list-like will generate a `DataFrame` output. You will get a matrix-\nlike output of all of the aggregators. The output will consist of all unique\nfunctions. Those that are not noted for a particular column will be `NaN`:\n\nDeprecated since version 1.4.0: Attempting to determine which columns cannot\nbe aggregated and silently dropping them from the results is deprecated and\nwill be removed in a future version. If any porition of the columns or\noperations provided fail, the call to `.agg` will raise.\n\nWhen presented with mixed dtypes that cannot aggregate, `.agg` will only take\nthe valid aggregations. This is similar to how `.groupby.agg` works.\n\nWith `.agg()` it is possible to easily create a custom describe function,\nsimilar to the built in describe function.\n\nThe `transform()` method returns an object that is indexed the same (same\nsize) as the original. This API allows you to provide multiple operations at\nthe same time rather than one-by-one. Its API is quite similar to the `.agg`\nAPI.\n\nWe create a frame similar to the one used in the above sections.\n\nTransform the entire frame. `.transform()` allows input functions as: a NumPy\nfunction, a string function name or a user defined function.\n\nHere `transform()` received a single function; this is equivalent to a ufunc\napplication.\n\nPassing a single function to `.transform()` with a `Series` will yield a\nsingle `Series` in return.\n\nPassing multiple functions will yield a column MultiIndexed DataFrame. The\nfirst level will be the original frame column names; the second level will be\nthe names of the transforming functions.\n\nPassing multiple functions to a Series will yield a DataFrame. The resulting\ncolumn names will be the transforming functions.\n\nPassing a dict of functions will allow selective transforming per column.\n\nPassing a dict of lists will generate a MultiIndexed DataFrame with these\nselective transforms.\n\nSince not all functions can be vectorized (accept NumPy arrays and return\nanother array or value), the methods `applymap()` on DataFrame and analogously\n`map()` on Series accept any Python function taking a single value and\nreturning a single value. For example:\n\n`Series.map()` has an additional feature; it can be used to easily \u201clink\u201d or\n\u201cmap\u201d values defined by a secondary series. This is closely related to\nmerging/joining functionality:\n\n`reindex()` is the fundamental data alignment method in pandas. It is used to\nimplement nearly all other features relying on label-alignment functionality.\nTo reindex means to conform the data to match a given set of labels along a\nparticular axis. This accomplishes several things:\n\nReorders the existing data to match a new set of labels\n\nInserts missing value (NA) markers in label locations where no data for that\nlabel existed\n\nIf specified, fill data for missing labels using logic (highly relevant to\nworking with time series data)\n\nHere is a simple example:\n\nHere, the `f` label was not contained in the Series and hence appears as `NaN`\nin the result.\n\nWith a DataFrame, you can simultaneously reindex the index and columns:\n\nYou may also use `reindex` with an `axis` keyword:\n\nNote that the `Index` objects containing the actual axis labels can be shared\nbetween objects. So if we have a Series and a DataFrame, the following can be\ndone:\n\nThis means that the reindexed Series\u2019s index is the same Python object as the\nDataFrame\u2019s index.\n\n`DataFrame.reindex()` also supports an \u201caxis-style\u201d calling convention, where\nyou specify a single `labels` argument and the `axis` it applies to.\n\nSee also\n\nMultiIndex / Advanced Indexing is an even more concise way of doing\nreindexing.\n\nNote\n\nWhen writing performance-sensitive code, there is a good reason to spend some\ntime becoming a reindexing ninja: many operations are faster on pre-aligned\ndata. Adding two unaligned DataFrames internally triggers a reindexing step.\nFor exploratory analysis you will hardly notice the difference (because\n`reindex` has been heavily optimized), but when CPU cycles matter sprinkling a\nfew explicit `reindex` calls here and there can have an impact.\n\nYou may wish to take an object and reindex its axes to be labeled the same as\nanother object. While the syntax for this is straightforward albeit verbose,\nit is a common enough operation that the `reindex_like()` method is available\nto make this simpler:\n\nThe `align()` method is the fastest way to simultaneously align two objects.\nIt supports a `join` argument (related to joining and merging):\n\n`join='outer'`: take the union of the indexes (default)\n\n`join='left'`: use the calling object\u2019s index\n\n`join='right'`: use the passed object\u2019s index\n\n`join='inner'`: intersect the indexes\n\nIt returns a tuple with both of the reindexed Series:\n\nFor DataFrames, the join method will be applied to both the index and the\ncolumns by default:\n\nYou can also pass an `axis` option to only align on the specified axis:\n\nIf you pass a Series to `DataFrame.align()`, you can choose to align both\nobjects either on the DataFrame\u2019s index or columns using the `axis` argument:\n\n`reindex()` takes an optional parameter `method` which is a filling method\nchosen from the following table:\n\nMethod\n\nAction\n\npad / ffill\n\nFill values forward\n\nbfill / backfill\n\nFill values backward\n\nnearest\n\nFill from the nearest index value\n\nWe illustrate these fill methods on a simple Series:\n\nThese methods require that the indexes are ordered increasing or decreasing.\n\nNote that the same result could have been achieved using fillna (except for\n`method='nearest'`) or interpolate:\n\n`reindex()` will raise a ValueError if the index is not monotonically\nincreasing or decreasing. `fillna()` and `interpolate()` will not perform any\nchecks on the order of the index.\n\nThe `limit` and `tolerance` arguments provide additional control over filling\nwhile reindexing. Limit specifies the maximum count of consecutive matches:\n\nIn contrast, tolerance specifies the maximum distance between the index and\nindexer values:\n\nNotice that when used on a `DatetimeIndex`, `TimedeltaIndex` or `PeriodIndex`,\n`tolerance` will coerced into a `Timedelta` if possible. This allows you to\nspecify tolerance with appropriate strings.\n\nA method closely related to `reindex` is the `drop()` function. It removes a\nset of labels from an axis:\n\nNote that the following also works, but is a bit less obvious / clean:\n\nThe `rename()` method allows you to relabel an axis based on some mapping (a\ndict or Series) or an arbitrary function.\n\nIf you pass a function, it must return a value when called with any of the\nlabels (and must produce a set of unique values). A dict or Series can also be\nused:\n\nIf the mapping doesn\u2019t include a column/index label, it isn\u2019t renamed. Note\nthat extra labels in the mapping don\u2019t throw an error.\n\n`DataFrame.rename()` also supports an \u201caxis-style\u201d calling convention, where\nyou specify a single `mapper` and the `axis` to apply that mapping to.\n\nThe `rename()` method also provides an `inplace` named parameter that is by\ndefault `False` and copies the underlying data. Pass `inplace=True` to rename\nthe data in place.\n\nFinally, `rename()` also accepts a scalar or list-like for altering the\n`Series.name` attribute.\n\nThe methods `DataFrame.rename_axis()` and `Series.rename_axis()` allow\nspecific names of a `MultiIndex` to be changed (as opposed to the labels).\n\nThe behavior of basic iteration over pandas objects depends on the type. When\niterating over a Series, it is regarded as array-like, and basic iteration\nproduces the values. DataFrames follow the dict-like convention of iterating\nover the \u201ckeys\u201d of the objects.\n\nIn short, basic iteration (`for i in object`) produces:\n\nSeries: values\n\nDataFrame: column labels\n\nThus, for example, iterating over a DataFrame gives you the column names:\n\npandas objects also have the dict-like `items()` method to iterate over the\n(key, value) pairs.\n\nTo iterate over the rows of a DataFrame, you can use the following methods:\n\n`iterrows()`: Iterate over the rows of a DataFrame as (index, Series) pairs.\nThis converts the rows to Series objects, which can change the dtypes and has\nsome performance implications.\n\n`itertuples()`: Iterate over the rows of a DataFrame as namedtuples of the\nvalues. This is a lot faster than `iterrows()`, and is in most cases\npreferable to use to iterate over the values of a DataFrame.\n\nWarning\n\nIterating through pandas objects is generally slow. In many cases, iterating\nmanually over the rows is not needed and can be avoided with one of the\nfollowing approaches:\n\nLook for a vectorized solution: many operations can be performed using built-\nin methods or NumPy functions, (boolean) indexing, \u2026\n\nWhen you have a function that cannot work on the full DataFrame/Series at\nonce, it is better to use `apply()` instead of iterating over the values. See\nthe docs on function application.\n\nIf you need to do iterative manipulations on the values but performance is\nimportant, consider writing the inner loop with cython or numba. See the\nenhancing performance section for some examples of this approach.\n\nWarning\n\nYou should never modify something you are iterating over. This is not\nguaranteed to work in all cases. Depending on the data types, the iterator\nreturns a copy and not a view, and writing to it will have no effect!\n\nFor example, in the following case setting the value has no effect:\n\nConsistent with the dict-like interface, `items()` iterates through key-value\npairs:\n\nSeries: (index, scalar value) pairs\n\nDataFrame: (column, Series) pairs\n\nFor example:\n\n`iterrows()` allows you to iterate through the rows of a DataFrame as Series\nobjects. It returns an iterator yielding each index value along with a Series\ncontaining the data in each row:\n\nNote\n\nBecause `iterrows()` returns a Series for each row, it does not preserve\ndtypes across the rows (dtypes are preserved across columns for DataFrames).\nFor example,\n\nAll values in `row`, returned as a Series, are now upcasted to floats, also\nthe original integer value in column `x`:\n\nTo preserve dtypes while iterating over the rows, it is better to use\n`itertuples()` which returns namedtuples of the values and which is generally\nmuch faster than `iterrows()`.\n\nFor instance, a contrived way to transpose the DataFrame would be:\n\nThe `itertuples()` method will return an iterator yielding a namedtuple for\neach row in the DataFrame. The first element of the tuple will be the row\u2019s\ncorresponding index value, while the remaining values are the row values.\n\nFor instance:\n\nThis method does not convert the row to a Series object; it merely returns the\nvalues inside a namedtuple. Therefore, `itertuples()` preserves the data type\nof the values and is generally faster as `iterrows()`.\n\nNote\n\nThe column names will be renamed to positional names if they are invalid\nPython identifiers, repeated, or start with an underscore. With a large number\nof columns (>255), regular tuples are returned.\n\n`Series` has an accessor to succinctly return datetime like properties for the\nvalues of the Series, if it is a datetime/period like Series. This will return\na Series, indexed like the existing Series.\n\nThis enables nice expressions like this:\n\nYou can easily produces tz aware transformations:\n\nYou can also chain these types of operations:\n\nYou can also format datetime values as strings with `Series.dt.strftime()`\nwhich supports the same format as the standard `strftime()`.\n\nThe `.dt` accessor works for period and timedelta dtypes.\n\nNote\n\n`Series.dt` will raise a `TypeError` if you access with a non-datetime-like\nvalues.\n\nSeries is equipped with a set of string processing methods that make it easy\nto operate on each element of the array. Perhaps most importantly, these\nmethods exclude missing/NA values automatically. These are accessed via the\nSeries\u2019s `str` attribute and generally have names matching the equivalent\n(scalar) built-in string methods. For example:\n\nPowerful pattern-matching methods are provided as well, but note that pattern-\nmatching generally uses regular expressions by default (and in some cases\nalways uses them).\n\nNote\n\nPrior to pandas 1.0, string methods were only available on `object` -dtype\n`Series`. pandas 1.0 added the `StringDtype` which is dedicated to strings.\nSee Text data types for more.\n\nPlease see Vectorized String Methods for a complete description.\n\npandas supports three kinds of sorting: sorting by index labels, sorting by\ncolumn values, and sorting by a combination of both.\n\nThe `Series.sort_index()` and `DataFrame.sort_index()` methods are used to\nsort a pandas object by its index levels.\n\nNew in version 1.1.0.\n\nSorting by index also supports a `key` parameter that takes a callable\nfunction to apply to the index being sorted. For `MultiIndex` objects, the key\nis applied per-level to the levels specified by `level`.\n\nFor information on key sorting by value, see value sorting.\n\nThe `Series.sort_values()` method is used to sort a `Series` by its values.\nThe `DataFrame.sort_values()` method is used to sort a `DataFrame` by its\ncolumn or row values. The optional `by` parameter to `DataFrame.sort_values()`\nmay used to specify one or more columns to use to determine the sorted order.\n\nThe `by` parameter can take a list of column names, e.g.:\n\nThese methods have special treatment of NA values via the `na_position`\nargument:\n\nNew in version 1.1.0.\n\nSorting also supports a `key` parameter that takes a callable function to\napply to the values being sorted.\n\n`key` will be given the `Series` of values and should return a `Series` or\narray of the same shape with the transformed values. For `DataFrame` objects,\nthe key is applied per column, so the key should still expect a Series and\nreturn a Series, e.g.\n\nThe name or type of each column can be used to apply different functions to\ndifferent columns.\n\nStrings passed as the `by` parameter to `DataFrame.sort_values()` may refer to\neither columns or index level names.\n\nSort by \u2018second\u2019 (index) and \u2018A\u2019 (column)\n\nNote\n\nIf a string matches both a column name and an index level name then a warning\nis issued and the column takes precedence. This will result in an ambiguity\nerror in a future version.\n\nSeries has the `searchsorted()` method, which works similarly to\n`numpy.ndarray.searchsorted()`.\n\n`Series` has the `nsmallest()` and `nlargest()` methods which return the\nsmallest or largest \\\\(n\\\\) values. For a large `Series` this can be much\nfaster than sorting the entire Series and calling `head(n)` on the result.\n\n`DataFrame` also has the `nlargest` and `nsmallest` methods.\n\nYou must be explicit about sorting when the column is a MultiIndex, and fully\nspecify all levels to `by`.\n\nThe `copy()` method on pandas objects copies the underlying data (though not\nthe axis indexes, since they are immutable) and returns a new object. Note\nthat it is seldom necessary to copy objects. For example, there are only a\nhandful of ways to alter a DataFrame in-place:\n\nInserting, deleting, or modifying a column.\n\nAssigning to the `index` or `columns` attributes.\n\nFor homogeneous data, directly modifying the values via the `values` attribute\nor advanced indexing.\n\nTo be clear, no pandas method has the side effect of modifying your data;\nalmost every method returns a new object, leaving the original object\nuntouched. If the data is modified, it is because you did so explicitly.\n\nFor the most part, pandas uses NumPy arrays and dtypes for Series or\nindividual columns of a DataFrame. NumPy provides support for `float`, `int`,\n`bool`, `timedelta64[ns]` and `datetime64[ns]` (note that NumPy does not\nsupport timezone-aware datetimes).\n\npandas and third-party libraries extend NumPy\u2019s type system in a few places.\nThis section describes the extensions pandas has made internally. See\nExtension types for how to write your own extension that works with pandas.\nSee Extension data types for a list of third-party libraries that have\nimplemented an extension.\n\nThe following table lists all of pandas extension types. For methods requiring\n`dtype` arguments, strings can be specified as indicated. See the respective\ndocumentation sections for more on each type.\n\nKind of Data\n\nData Type\n\nScalar\n\nArray\n\nString Aliases\n\ntz-aware datetime\n\n`DatetimeTZDtype`\n\n`Timestamp`\n\n`arrays.DatetimeArray`\n\n`'datetime64[ns, <tz>]'`\n\nCategorical\n\n`CategoricalDtype`\n\n(none)\n\n`Categorical`\n\n`'category'`\n\nperiod (time spans)\n\n`PeriodDtype`\n\n`Period`\n\n`arrays.PeriodArray` `'Period[<freq>]'`\n\n`'period[<freq>]'`,\n\nsparse\n\n`SparseDtype`\n\n(none)\n\n`arrays.SparseArray`\n\n`'Sparse'`, `'Sparse[int]'`, `'Sparse[float]'`\n\nintervals\n\n`IntervalDtype`\n\n`Interval`\n\n`arrays.IntervalArray`\n\n`'interval'`, `'Interval'`, `'Interval[<numpy_dtype>]'`,\n`'Interval[datetime64[ns, <tz>]]'`, `'Interval[timedelta64[<freq>]]'`\n\nnullable integer\n\n`Int64Dtype`, \u2026\n\n(none)\n\n`arrays.IntegerArray`\n\n`'Int8'`, `'Int16'`, `'Int32'`, `'Int64'`, `'UInt8'`, `'UInt16'`, `'UInt32'`,\n`'UInt64'`\n\nStrings\n\n`StringDtype`\n\n`str`\n\n`arrays.StringArray`\n\n`'string'`\n\nBoolean (with NA)\n\n`BooleanDtype`\n\n`bool`\n\n`arrays.BooleanArray`\n\n`'boolean'`\n\npandas has two ways to store strings.\n\n`object` dtype, which can hold any Python object, including strings.\n\n`StringDtype`, which is dedicated to strings.\n\nGenerally, we recommend using `StringDtype`. See Text data types for more.\n\nFinally, arbitrary objects may be stored using the `object` dtype, but should\nbe avoided to the extent possible (for performance and interoperability with\nother libraries and methods. See object conversion).\n\nA convenient `dtypes` attribute for DataFrame returns a Series with the data\ntype of each column.\n\nOn a `Series` object, use the `dtype` attribute.\n\nIf a pandas object contains data with multiple dtypes in a single column, the\ndtype of the column will be chosen to accommodate all of the data types\n(`object` is the most general).\n\nThe number of columns of each type in a `DataFrame` can be found by calling\n`DataFrame.dtypes.value_counts()`.\n\nNumeric dtypes will propagate and can coexist in DataFrames. If a dtype is\npassed (either directly via the `dtype` keyword, a passed `ndarray`, or a\npassed `Series`), then it will be preserved in DataFrame operations.\nFurthermore, different numeric dtypes will NOT be combined. The following\nexample will give you a taste.\n\nBy default integer types are `int64` and float types are `float64`, regardless\nof platform (32-bit or 64-bit). The following will all result in `int64`\ndtypes.\n\nNote that Numpy will choose platform-dependent types when creating arrays. The\nfollowing WILL result in `int32` on 32-bit platform.\n\nTypes can potentially be upcasted when combined with other types, meaning they\nare promoted from the current type (e.g. `int` to `float`).\n\n`DataFrame.to_numpy()` will return the lower-common-denominator of the dtypes,\nmeaning the dtype that can accommodate ALL of the types in the resulting\nhomogeneous dtyped NumPy array. This can force some upcasting.\n\nYou can use the `astype()` method to explicitly convert dtypes from one to\nanother. These will by default return a copy, even if the dtype was unchanged\n(pass `copy=False` to change this behavior). In addition, they will raise an\nexception if the astype operation is invalid.\n\nUpcasting is always according to the NumPy rules. If two different dtypes are\ninvolved in an operation, then the more general one will be used as the result\nof the operation.\n\nConvert a subset of columns to a specified type using `astype()`.\n\nConvert certain columns to a specific dtype by passing a dict to `astype()`.\n\nNote\n\nWhen trying to convert a subset of columns to a specified type using\n`astype()` and `loc()`, upcasting occurs.\n\n`loc()` tries to fit in what we are assigning to the current dtypes, while\n`[]` will overwrite them taking the dtype from the right hand side. Therefore\nthe following piece of code produces the unintended result.\n\npandas offers various functions to try to force conversion of types from the\n`object` dtype to other types. In cases where the data is already of the\ncorrect type, but stored in an `object` array, the `DataFrame.infer_objects()`\nand `Series.infer_objects()` methods can be used to soft convert to the\ncorrect type.\n\nBecause the data was transposed the original inference stored all columns as\nobject, which `infer_objects` will correct.\n\nThe following functions are available for one dimensional object arrays or\nscalars to perform hard conversion of objects to a specified type:\n\n`to_numeric()` (conversion to numeric dtypes)\n\n`to_datetime()` (conversion to datetime objects)\n\n`to_timedelta()` (conversion to timedelta objects)\n\nTo force a conversion, we can pass in an `errors` argument, which specifies\nhow pandas should deal with elements that cannot be converted to desired dtype\nor object. By default, `errors='raise'`, meaning that any errors encountered\nwill be raised during the conversion process. However, if `errors='coerce'`,\nthese errors will be ignored and pandas will convert problematic elements to\n`pd.NaT` (for datetime and timedelta) or `np.nan` (for numeric). This might be\nuseful if you are reading in data which is mostly of the desired dtype (e.g.\nnumeric, datetime), but occasionally has non-conforming elements intermixed\nthat you want to represent as missing:\n\nThe `errors` parameter has a third option of `errors='ignore'`, which will\nsimply return the passed in data if it encounters any errors with the\nconversion to a desired data type:\n\nIn addition to object conversion, `to_numeric()` provides another argument\n`downcast`, which gives the option of downcasting the newly (or already)\nnumeric data to a smaller dtype, which can conserve memory:\n\nAs these methods apply only to one-dimensional arrays, lists or scalars; they\ncannot be used directly on multi-dimensional objects such as DataFrames.\nHowever, with `apply()`, we can \u201capply\u201d the function over each column\nefficiently:\n\nPerforming selection operations on `integer` type data can easily upcast the\ndata to `floating`. The dtype of the input data will be preserved in cases\nwhere `nans` are not introduced. See also Support for integer NA.\n\nWhile float dtypes are unchanged.\n\nThe `select_dtypes()` method implements subsetting of columns based on their\n`dtype`.\n\nFirst, let\u2019s create a `DataFrame` with a slew of different dtypes:\n\nAnd the dtypes:\n\n`select_dtypes()` has two parameters `include` and `exclude` that allow you to\nsay \u201cgive me the columns with these dtypes\u201d (`include`) and/or \u201cgive the\ncolumns without these dtypes\u201d (`exclude`).\n\nFor example, to select `bool` columns:\n\nYou can also pass the name of a dtype in the NumPy dtype hierarchy:\n\n`select_dtypes()` also works with generic dtypes as well.\n\nFor example, to select all numeric and boolean columns while excluding\nunsigned integers:\n\nTo select string columns you must use the `object` dtype:\n\nTo see all the child dtypes of a generic `dtype` like `numpy.number` you can\ndefine a function that returns a tree of child dtypes:\n\nAll NumPy dtypes are subclasses of `numpy.generic`:\n\nNote\n\npandas also defines the types `category`, and `datetime64[ns, tz]`, which are\nnot integrated into the normal NumPy hierarchy and won\u2019t show up with the\nabove function.\n\n"}, {"name": "Extensions", "path": "reference/extensions", "type": "Extensions", "text": "\nThese are primarily intended for library authors looking to extend pandas\nobjects.\n\n`api.extensions.register_extension_dtype`(cls)\n\nRegister an ExtensionType with pandas as class decorator.\n\n`api.extensions.register_dataframe_accessor`(name)\n\nRegister a custom accessor on DataFrame objects.\n\n`api.extensions.register_series_accessor`(name)\n\nRegister a custom accessor on Series objects.\n\n`api.extensions.register_index_accessor`(name)\n\nRegister a custom accessor on Index objects.\n\n`api.extensions.ExtensionDtype`()\n\nA custom data type, to be paired with an ExtensionArray.\n\n`api.extensions.ExtensionArray`()\n\nAbstract base class for custom 1-D array types.\n\n`arrays.PandasArray`(values[, copy])\n\nA pandas ExtensionArray for NumPy data.\n\nAdditionally, we have some utility methods for ensuring your object behaves\ncorrectly.\n\n`api.indexers.check_array_indexer`(array, indexer)\n\nCheck if indexer is a valid array indexer for array.\n\nThe sentinel `pandas.api.extensions.no_default` is used as the default value\nin some methods. Use an `is` comparison to check if the user provides a non-\ndefault value.\n\n"}, {"name": "Frequently Asked Questions (FAQ)", "path": "user_guide/gotchas", "type": "Manual", "text": "\nThe memory usage of a `DataFrame` (including the index) is shown when calling\nthe `info()`. A configuration option, `display.memory_usage` (see the list of\noptions), specifies if the `DataFrame`\u2019s memory usage will be displayed when\ninvoking the `df.info()` method.\n\nFor example, the memory usage of the `DataFrame` below is shown when calling\n`info()`:\n\nThe `+` symbol indicates that the true memory usage could be higher, because\npandas does not count the memory used by values in columns with\n`dtype=object`.\n\nPassing `memory_usage='deep'` will enable a more accurate memory usage report,\naccounting for the full usage of the contained objects. This is optional as it\ncan be expensive to do this deeper introspection.\n\nBy default the display option is set to `True` but can be explicitly\noverridden by passing the `memory_usage` argument when invoking `df.info()`.\n\nThe memory usage of each column can be found by calling the `memory_usage()`\nmethod. This returns a `Series` with an index represented by column names and\nmemory usage of each column shown in bytes. For the `DataFrame` above, the\nmemory usage of each column and the total memory usage can be found with the\n`memory_usage` method:\n\nBy default the memory usage of the `DataFrame`\u2019s index is shown in the\nreturned `Series`, the memory usage of the index can be suppressed by passing\nthe `index=False` argument:\n\nThe memory usage displayed by the `info()` method utilizes the\n`memory_usage()` method to determine the memory usage of a `DataFrame` while\nalso formatting the output in human-readable units (base-2 representation;\ni.e. 1KB = 1024 bytes).\n\nSee also Categorical Memory Usage.\n\npandas follows the NumPy convention of raising an error when you try to\nconvert something to a `bool`. This happens in an `if`-statement or when using\nthe boolean operations: `and`, `or`, and `not`. It is not clear what the\nresult of the following code should be:\n\nShould it be `True` because it\u2019s not zero-length, or `False` because there are\n`False` values? It is unclear, so instead, pandas raises a `ValueError`:\n\nYou need to explicitly choose what you want to do with the `DataFrame`, e.g.\nuse `any()`, `all()` or `empty()`. Alternatively, you might want to compare if\nthe pandas object is `None`:\n\nBelow is how to check if any of the values are `True`:\n\nTo evaluate single-element pandas objects in a boolean context, use the method\n`bool()`:\n\nBitwise boolean operators like `==` and `!=` return a boolean `Series`, which\nis almost always what you want anyways.\n\nSee boolean comparisons for more examples.\n\nUsing the Python `in` operator on a `Series` tests for membership in the\nindex, not membership among the values.\n\nIf this behavior is surprising, keep in mind that using `in` on a Python\ndictionary tests keys, not values, and `Series` are dict-like. To test for\nmembership in the values, use the method `isin()`:\n\nFor `DataFrames`, likewise, `in` applies to the column axis, testing for\nmembership in the list of column names.\n\nThis section applies to pandas methods that take a UDF. In particular, the\nmethods `.apply`, `.aggregate`, `.transform`, and `.filter`.\n\nIt is a general rule in programming that one should not mutate a container\nwhile it is being iterated over. Mutation will invalidate the iterator,\ncausing unexpected behavior. Consider the example:\n\nOne probably would have expected that the result would be `[1, 3, 5]`. When\nusing a pandas method that takes a UDF, internally pandas is often iterating\nover the `DataFrame` or other pandas object. Therefore, if the UDF mutates\n(changes) the `DataFrame`, unexpected behavior can arise.\n\nHere is a similar example with `DataFrame.apply()`:\n\nTo resolve this issue, one can make a copy so that the mutation does not apply\nto the container being iterated over.\n\nFor lack of `NA` (missing) support from the ground up in NumPy and Python in\ngeneral, we were given the difficult choice between either:\n\nA masked array solution: an array of data and an array of boolean values\nindicating whether a value is there or is missing.\n\nUsing a special sentinel value, bit pattern, or set of sentinel values to\ndenote `NA` across the dtypes.\n\nFor many reasons we chose the latter. After years of production use it has\nproven, at least in my opinion, to be the best decision given the state of\naffairs in NumPy and Python in general. The special value `NaN` (Not-A-Number)\nis used everywhere as the `NA` value, and there are API functions `isna` and\n`notna` which can be used across the dtypes to detect NA values.\n\nHowever, it comes with it a couple of trade-offs which I most certainly have\nnot ignored.\n\nIn the absence of high performance `NA` support being built into NumPy from\nthe ground up, the primary casualty is the ability to represent NAs in integer\narrays. For example:\n\nThis trade-off is made largely for memory and performance reasons, and also so\nthat the resulting `Series` continues to be \u201cnumeric\u201d.\n\nIf you need to represent integers with possibly missing values, use one of the\nnullable-integer extension dtypes provided by pandas\n\n`Int8Dtype`\n\n`Int16Dtype`\n\n`Int32Dtype`\n\n`Int64Dtype`\n\nSee Nullable integer data type for more.\n\nWhen introducing NAs into an existing `Series` or `DataFrame` via `reindex()`\nor some other means, boolean and integer types will be promoted to a different\ndtype in order to store the NAs. The promotions are summarized in this table:\n\nTypeclass\n\nPromotion dtype for storing NAs\n\n`floating`\n\nno change\n\n`object`\n\nno change\n\n`integer`\n\ncast to `float64`\n\n`boolean`\n\ncast to `object`\n\nWhile this may seem like a heavy trade-off, I have found very few cases where\nthis is an issue in practice i.e. storing values greater than 2**53. Some\nexplanation for the motivation is in the next section.\n\nMany people have suggested that NumPy should simply emulate the `NA` support\npresent in the more domain-specific statistical programming language R. Part\nof the reason is the NumPy type hierarchy:\n\nTypeclass\n\nDtypes\n\n`numpy.floating`\n\n`float16, float32, float64, float128`\n\n`numpy.integer`\n\n`int8, int16, int32, int64`\n\n`numpy.unsignedinteger`\n\n`uint8, uint16, uint32, uint64`\n\n`numpy.object_`\n\n`object_`\n\n`numpy.bool_`\n\n`bool_`\n\n`numpy.character`\n\n`string_, unicode_`\n\nThe R language, by contrast, only has a handful of built-in data types:\n`integer`, `numeric` (floating-point), `character`, and `boolean`. `NA` types\nare implemented by reserving special bit patterns for each type to be used as\nthe missing value. While doing this with the full NumPy type hierarchy would\nbe possible, it would be a more substantial trade-off (especially for the 8-\nand 16-bit data types) and implementation undertaking.\n\nAn alternate approach is that of using masked arrays. A masked array is an\narray of data with an associated boolean mask denoting whether each value\nshould be considered `NA` or not. I am personally not in love with this\napproach as I feel that overall it places a fairly heavy burden on the user\nand the library implementer. Additionally, it exacts a fairly high performance\ncost when working with numerical data compared with the simple approach of\nusing `NaN`. Thus, I have chosen the Pythonic \u201cpracticality beats purity\u201d\napproach and traded integer `NA` capability for a much simpler approach of\nusing a special value in float and object arrays to denote `NA`, and promoting\ninteger arrays to floating when NAs must be introduced.\n\nFor `Series` and `DataFrame` objects, `var()` normalizes by `N-1` to produce\nunbiased estimates of the sample variance, while NumPy\u2019s `var` normalizes by\nN, which measures the variance of the sample. Note that `cov()` normalizes by\n`N-1` in both pandas and NumPy.\n\nAs of pandas 0.11, pandas is not 100% thread safe. The known issues relate to\nthe `copy()` method. If you are doing a lot of copying of `DataFrame` objects\nshared among threads, we recommend holding locks inside the threads where the\ndata copying occurs.\n\nSee this link for more information.\n\nOccasionally you may have to deal with data that were created on a machine\nwith a different byte order than the one on which you are running Python. A\ncommon symptom of this issue is an error like:\n\nTo deal with this issue you should convert the underlying NumPy array to the\nnative system byte order before passing it to `Series` or `DataFrame`\nconstructors using something similar to the following:\n\nSee the NumPy documentation on byte order for more details.\n\n"}, {"name": "General functions", "path": "reference/general_functions", "type": "Input/output", "text": "\n`melt`(frame[, id_vars, value_vars, var_name, ...])\n\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers\nset.\n\n`pivot`(data[, index, columns, values])\n\nReturn reshaped DataFrame organized by given index / column values.\n\n`pivot_table`(data[, values, index, columns, ...])\n\nCreate a spreadsheet-style pivot table as a DataFrame.\n\n`crosstab`(index, columns[, values, rownames, ...])\n\nCompute a simple cross tabulation of two (or more) factors.\n\n`cut`(x, bins[, right, labels, retbins, ...])\n\nBin values into discrete intervals.\n\n`qcut`(x, q[, labels, retbins, precision, ...])\n\nQuantile-based discretization function.\n\n`merge`(left, right[, how, on, left_on, ...])\n\nMerge DataFrame or named Series objects with a database-style join.\n\n`merge_ordered`(left, right[, on, left_on, ...])\n\nPerform a merge for ordered data with optional filling/interpolation.\n\n`merge_asof`(left, right[, on, left_on, ...])\n\nPerform a merge by key distance.\n\n`concat`(objs[, axis, join, ignore_index, ...])\n\nConcatenate pandas objects along a particular axis with optional set logic\nalong the other axes.\n\n`get_dummies`(data[, prefix, prefix_sep, ...])\n\nConvert categorical variable into dummy/indicator variables.\n\n`factorize`(values[, sort, na_sentinel, size_hint])\n\nEncode the object as an enumerated type or categorical variable.\n\n`unique`(values)\n\nReturn unique values based on a hash table.\n\n`wide_to_long`(df, stubnames, i, j[, sep, suffix])\n\nUnpivot a DataFrame from wide to long format.\n\n`isna`(obj)\n\nDetect missing values for an array-like object.\n\n`isnull`(obj)\n\nDetect missing values for an array-like object.\n\n`notna`(obj)\n\nDetect non-missing values for an array-like object.\n\n`notnull`(obj)\n\nDetect non-missing values for an array-like object.\n\n`to_numeric`(arg[, errors, downcast])\n\nConvert argument to a numeric type.\n\n`to_datetime`(arg[, errors, dayfirst, ...])\n\nConvert argument to datetime.\n\n`to_timedelta`(arg[, unit, errors])\n\nConvert argument to timedelta.\n\n`date_range`([start, end, periods, freq, tz, ...])\n\nReturn a fixed frequency DatetimeIndex.\n\n`bdate_range`([start, end, periods, freq, tz, ...])\n\nReturn a fixed frequency DatetimeIndex, with business day as the default\nfrequency.\n\n`period_range`([start, end, periods, freq, name])\n\nReturn a fixed frequency PeriodIndex.\n\n`timedelta_range`([start, end, periods, freq, ...])\n\nReturn a fixed frequency TimedeltaIndex, with day as the default frequency.\n\n`infer_freq`(index[, warn])\n\nInfer the most likely frequency given the input index.\n\n`interval_range`([start, end, periods, freq, ...])\n\nReturn a fixed frequency IntervalIndex.\n\n`eval`(expr[, parser, engine, truediv, ...])\n\nEvaluate a Python expression as a string using various backends.\n\n`util.hash_array`(vals[, encoding, hash_key, ...])\n\nGiven a 1d array, return an array of deterministic integers.\n\n`util.hash_pandas_object`(obj[, index, ...])\n\nReturn a data hash of the Index/Series/DataFrame.\n\n`test`([extra_args])\n\nRun the pandas test suite using pytest.\n\n"}, {"name": "General utility functions", "path": "reference/general_utility_functions", "type": "Input/output", "text": "\n`describe_option`(pat[, _print_desc])\n\nPrints the description for one or more registered options.\n\n`reset_option`(pat)\n\nReset one or more options to their default value.\n\n`get_option`(pat)\n\nRetrieves the value of the specified option.\n\n`set_option`(pat, value)\n\nSets the value of the specified option.\n\n`option_context`(*args)\n\nContext manager to temporarily set options in the with statement context.\n\n`testing.assert_frame_equal`(left, right[, ...])\n\nCheck that left and right DataFrame are equal.\n\n`testing.assert_series_equal`(left, right[, ...])\n\nCheck that left and right Series are equal.\n\n`testing.assert_index_equal`(left, right[, ...])\n\nCheck that left and right Index are equal.\n\n`testing.assert_extension_array_equal`(left, right)\n\nCheck that left and right ExtensionArrays are equal.\n\n`errors.AbstractMethodError`(class_instance[, ...])\n\nRaise this error instead of NotImplementedError for abstract methods while\nkeeping compatibility with Python 2 and Python 3.\n\n`errors.AccessorRegistrationWarning`\n\nWarning for attribute conflicts in accessor registration.\n\n`errors.DtypeWarning`\n\nWarning raised when reading different dtypes in a column from a file.\n\n`errors.DuplicateLabelError`\n\nError raised when an operation would introduce duplicate labels.\n\n`errors.EmptyDataError`\n\nException that is thrown in pd.read_csv (by both the C and Python engines)\nwhen empty data or header is encountered.\n\n`errors.InvalidIndexError`\n\nException raised when attempting to use an invalid index key.\n\n`errors.IntCastingNaNError`\n\nRaised when attempting an astype operation on an array with NaN to an integer\ndtype.\n\n`errors.MergeError`\n\nError raised when problems arise during merging due to problems with input\ndata.\n\n`errors.NullFrequencyError`\n\nError raised when a null freq attribute is used in an operation that needs a\nnon-null frequency, particularly DatetimeIndex.shift, TimedeltaIndex.shift,\nPeriodIndex.shift.\n\n`errors.NumbaUtilError`\n\nError raised for unsupported Numba engine routines.\n\n`errors.OptionError`\n\nException for pandas.options, backwards compatible with KeyError checks.\n\n`errors.OutOfBoundsDatetime`\n\n`errors.OutOfBoundsTimedelta`\n\nRaised when encountering a timedelta value that cannot be represented as a\ntimedelta64[ns].\n\n`errors.ParserError`\n\nException that is raised by an error encountered in parsing file contents.\n\n`errors.ParserWarning`\n\nWarning raised when reading a file that doesn't use the default 'c' parser.\n\n`errors.PerformanceWarning`\n\nWarning raised when there is a possible performance impact.\n\n`errors.UnsortedIndexError`\n\nError raised when attempting to get a slice of a MultiIndex, and the index has\nnot been lexsorted.\n\n`errors.UnsupportedFunctionCall`\n\nException raised when attempting to call a numpy function on a pandas object,\nbut that function is not supported by the object e.g.\n\n`api.types.union_categoricals`(to_union[, ...])\n\nCombine list-like of Categorical-like, unioning categories.\n\n`api.types.infer_dtype`\n\nEfficiently infer the type of a passed val, or list-like array of values.\n\n`api.types.pandas_dtype`(dtype)\n\nConvert input into a pandas only dtype object or a numpy dtype object.\n\n`api.types.is_bool_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of a boolean dtype.\n\n`api.types.is_categorical_dtype`(arr_or_dtype)\n\nCheck whether an array-like or dtype is of the Categorical dtype.\n\n`api.types.is_complex_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of a complex dtype.\n\n`api.types.is_datetime64_any_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of the datetime64 dtype.\n\n`api.types.is_datetime64_dtype`(arr_or_dtype)\n\nCheck whether an array-like or dtype is of the datetime64 dtype.\n\n`api.types.is_datetime64_ns_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of the datetime64[ns] dtype.\n\n`api.types.is_datetime64tz_dtype`(arr_or_dtype)\n\nCheck whether an array-like or dtype is of a DatetimeTZDtype dtype.\n\n`api.types.is_extension_type`(arr)\n\n(DEPRECATED) Check whether an array-like is of a pandas extension class\ninstance.\n\n`api.types.is_extension_array_dtype`(arr_or_dtype)\n\nCheck if an object is a pandas extension array type.\n\n`api.types.is_float_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of a float dtype.\n\n`api.types.is_int64_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of the int64 dtype.\n\n`api.types.is_integer_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of an integer dtype.\n\n`api.types.is_interval_dtype`(arr_or_dtype)\n\nCheck whether an array-like or dtype is of the Interval dtype.\n\n`api.types.is_numeric_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of a numeric dtype.\n\n`api.types.is_object_dtype`(arr_or_dtype)\n\nCheck whether an array-like or dtype is of the object dtype.\n\n`api.types.is_period_dtype`(arr_or_dtype)\n\nCheck whether an array-like or dtype is of the Period dtype.\n\n`api.types.is_signed_integer_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of a signed integer dtype.\n\n`api.types.is_string_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of the string dtype.\n\n`api.types.is_timedelta64_dtype`(arr_or_dtype)\n\nCheck whether an array-like or dtype is of the timedelta64 dtype.\n\n`api.types.is_timedelta64_ns_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of the timedelta64[ns] dtype.\n\n`api.types.is_unsigned_integer_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of an unsigned integer dtype.\n\n`api.types.is_sparse`(arr)\n\nCheck whether an array-like is a 1-D pandas sparse array.\n\n`api.types.is_dict_like`(obj)\n\nCheck if the object is dict-like.\n\n`api.types.is_file_like`(obj)\n\nCheck if the object is a file-like object.\n\n`api.types.is_list_like`\n\nCheck if the object is list-like.\n\n`api.types.is_named_tuple`(obj)\n\nCheck if the object is a named tuple.\n\n`api.types.is_iterator`\n\nCheck if the object is an iterator.\n\n`api.types.is_bool`\n\nReturn True if given object is boolean.\n\n`api.types.is_categorical`(arr)\n\nCheck whether an array-like is a Categorical instance.\n\n`api.types.is_complex`\n\nReturn True if given object is complex.\n\n`api.types.is_float`\n\nReturn True if given object is float.\n\n`api.types.is_hashable`(obj)\n\nReturn True if hash(obj) will succeed, False otherwise.\n\n`api.types.is_integer`\n\nReturn True if given object is integer.\n\n`api.types.is_interval`\n\n`api.types.is_number`(obj)\n\nCheck if the object is a number.\n\n`api.types.is_re`(obj)\n\nCheck if the object is a regex pattern instance.\n\n`api.types.is_re_compilable`(obj)\n\nCheck if the object can be compiled into a regex pattern instance.\n\n`api.types.is_scalar`\n\nReturn True if given object is scalar.\n\n`show_versions`([as_json])\n\nProvide useful information, important for bug reports.\n\n"}, {"name": "Group by: split-apply-combine", "path": "user_guide/groupby", "type": "Manual", "text": "\nBy \u201cgroup by\u201d we are referring to a process involving one or more of the\nfollowing steps:\n\nSplitting the data into groups based on some criteria.\n\nApplying a function to each group independently.\n\nCombining the results into a data structure.\n\nOut of these, the split step is the most straightforward. In fact, in many\nsituations we may wish to split the data set into groups and do something with\nthose groups. In the apply step, we might wish to do one of the following:\n\nAggregation: compute a summary statistic (or statistics) for each group. Some\nexamples:\n\nCompute group sums or means.\n\nCompute group sizes / counts.\n\nTransformation: perform some group-specific computations and return a like-\nindexed object. Some examples:\n\nStandardize data (zscore) within a group.\n\nFilling NAs within groups with a value derived from each group.\n\nFiltration: discard some groups, according to a group-wise computation that\nevaluates True or False. Some examples:\n\nDiscard data that belongs to groups with only a few members.\n\nFilter out data based on the group sum or mean.\n\nSome combination of the above: GroupBy will examine the results of the apply\nstep and try to return a sensibly combined result if it doesn\u2019t fit into\neither of the above two categories.\n\nSince the set of object instance methods on pandas data structures are\ngenerally rich and expressive, we often simply want to invoke, say, a\nDataFrame function on each group. The name GroupBy should be quite familiar to\nthose who have used a SQL-based tool (or `itertools`), in which you can write\ncode like:\n\nWe aim to make operations like this natural and easy to express using pandas.\nWe\u2019ll address each area of GroupBy functionality then provide some non-trivial\nexamples / use cases.\n\nSee the cookbook for some advanced strategies.\n\npandas objects can be split on any of their axes. The abstract definition of\ngrouping is to provide a mapping of labels to group names. To create a GroupBy\nobject (more on what the GroupBy object is later), you may do the following:\n\nThe mapping can be specified many different ways:\n\nA Python function, to be called on each of the axis labels.\n\nA list or NumPy array of the same length as the selected axis.\n\nA dict or `Series`, providing a `label -> group name` mapping.\n\nFor `DataFrame` objects, a string indicating either a column name or an index\nlevel name to be used to group.\n\n`df.groupby('A')` is just syntactic sugar for `df.groupby(df['A'])`.\n\nA list of any of the above things.\n\nCollectively we refer to the grouping objects as the keys. For example,\nconsider the following `DataFrame`:\n\nNote\n\nA string passed to `groupby` may refer to either a column or an index level.\nIf a string matches both a column name and an index level name, a `ValueError`\nwill be raised.\n\nOn a DataFrame, we obtain a GroupBy object by calling `groupby()`. We could\nnaturally group by either the `A` or `B` columns, or both:\n\nIf we also have a MultiIndex on columns `A` and `B`, we can group by all but\nthe specified columns\n\nThese will split the DataFrame on its index (rows). We could also split by the\ncolumns:\n\npandas `Index` objects support duplicate values. If a non-unique index is used\nas the group key in a groupby operation, all values for the same index value\nwill be considered to be in one group and thus the output of aggregation\nfunctions will only contain unique index values:\n\nNote that no splitting occurs until it\u2019s needed. Creating the GroupBy object\nonly verifies that you\u2019ve passed a valid mapping.\n\nNote\n\nMany kinds of complicated data manipulations can be expressed in terms of\nGroupBy operations (though can\u2019t be guaranteed to be the most efficient). You\ncan get quite creative with the label mapping functions.\n\nBy default the group keys are sorted during the `groupby` operation. You may\nhowever pass `sort=False` for potential speedups:\n\nNote that `groupby` will preserve the order in which observations are sorted\nwithin each group. For example, the groups created by `groupby()` below are in\nthe order they appeared in the original `DataFrame`:\n\nNew in version 1.1.0.\n\nBy default `NA` values are excluded from group keys during the `groupby`\noperation. However, in case you want to include `NA` values in group keys, you\ncould pass `dropna=False` to achieve it.\n\nThe default setting of `dropna` argument is `True` which means `NA` are not\nincluded in group keys.\n\nThe `groups` attribute is a dict whose keys are the computed unique groups and\ncorresponding values being the axis labels belonging to each group. In the\nabove example we have:\n\nCalling the standard Python `len` function on the GroupBy object just returns\nthe length of the `groups` dict, so it is largely just a convenience:\n\n`GroupBy` will tab complete column names (and other attributes):\n\nWith hierarchically-indexed data, it\u2019s quite natural to group by one of the\nlevels of the hierarchy.\n\nLet\u2019s create a Series with a two-level `MultiIndex`.\n\nWe can then group by one of the levels in `s`.\n\nIf the MultiIndex has names specified, these can be passed instead of the\nlevel number:\n\nGrouping with multiple levels is supported.\n\nIndex level names may be supplied as keys.\n\nMore on the `sum` function and aggregation later.\n\nA DataFrame may be grouped by a combination of columns and index levels by\nspecifying the column names as strings and the index levels as `pd.Grouper`\nobjects.\n\nThe following example groups `df` by the `second` index level and the `A`\ncolumn.\n\nIndex levels may also be specified by name.\n\nIndex level names may be specified as keys directly to `groupby`.\n\nOnce you have created the GroupBy object from a DataFrame, you might want to\ndo something different for each of the columns. Thus, using `[]` similar to\ngetting a column from a DataFrame, you can do:\n\nThis is mainly syntactic sugar for the alternative and much more verbose:\n\nAdditionally this method avoids recomputing the internal grouping information\nderived from the passed key.\n\nWith the GroupBy object in hand, iterating through the grouped data is very\nnatural and functions similarly to `itertools.groupby()`:\n\nIn the case of grouping by multiple keys, the group name will be a tuple:\n\nSee Iterating through groups.\n\nA single group can be selected using `get_group()`:\n\nOr for an object grouped on multiple columns:\n\nOnce the GroupBy object has been created, several methods are available to\nperform a computation on the grouped data. These operations are similar to the\naggregating API, window API, and resample API.\n\nAn obvious one is aggregation via the `aggregate()` or equivalently `agg()`\nmethod:\n\nAs you can see, the result of the aggregation will have the group names as the\nnew index along the grouped axis. In the case of multiple keys, the result is\na MultiIndex by default, though this can be changed by using the `as_index`\noption:\n\nNote that you could use the `reset_index` DataFrame function to achieve the\nsame result as the column names are stored in the resulting `MultiIndex`:\n\nAnother simple aggregation example is to compute the size of each group. This\nis included in GroupBy as the `size` method. It returns a Series whose index\nare the group names and whose values are the sizes of each group.\n\nAnother aggregation example is to compute the number of unique values of each\ngroup. This is similar to the `value_counts` function, except that it only\ncounts unique values.\n\nNote\n\nAggregation functions will not return the groups that you are aggregating over\nif they are named columns, when `as_index=True`, the default. The grouped\ncolumns will be the indices of the returned object.\n\nPassing `as_index=False` will return the groups that you are aggregating over,\nif they are named columns.\n\nAggregating functions are the ones that reduce the dimension of the returned\nobjects. Some common aggregating functions are tabulated below:\n\nFunction\n\nDescription\n\n`mean()`\n\nCompute mean of groups\n\n`sum()`\n\nCompute sum of group values\n\n`size()`\n\nCompute group sizes\n\n`count()`\n\nCompute count of group\n\n`std()`\n\nStandard deviation of groups\n\n`var()`\n\nCompute variance of groups\n\n`sem()`\n\nStandard error of the mean of groups\n\n`describe()`\n\nGenerates descriptive statistics\n\n`first()`\n\nCompute first of group values\n\n`last()`\n\nCompute last of group values\n\n`nth()`\n\nTake nth value, or a subset if n is a list\n\n`min()`\n\nCompute min of group values\n\n`max()`\n\nCompute max of group values\n\nThe aggregating functions above will exclude NA values. Any function which\nreduces a `Series` to a scalar value is an aggregation function and will work,\na trivial example is `df.groupby('A').agg(lambda ser: 1)`. Note that `nth()`\ncan act as a reducer or a filter, see here.\n\nWith grouped `Series` you can also pass a list or dict of functions to do\naggregation with, outputting a DataFrame:\n\nOn a grouped `DataFrame`, you can pass a list of functions to apply to each\ncolumn, which produces an aggregated result with a hierarchical index:\n\nThe resulting aggregations are named for the functions themselves. If you need\nto rename, then you can add in a chained operation for a `Series` like this:\n\nFor a grouped `DataFrame`, you can rename in a similar manner:\n\nNote\n\nIn general, the output column names should be unique. You can\u2019t apply the same\nfunction (or two functions with the same name) to the same column.\n\npandas does allow you to provide multiple lambdas. In this case, pandas will\nmangle the name of the (nameless) lambda functions, appending `_<i>` to each\nsubsequent lambda.\n\nNew in version 0.25.0.\n\nTo support column-specific aggregation with control over the output column\nnames, pandas accepts the special syntax in `GroupBy.agg()`, known as \u201cnamed\naggregation\u201d, where\n\nThe keywords are the output column names\n\nThe values are tuples whose first element is the column to select and the\nsecond element is the aggregation to apply to that column. pandas provides the\n`pandas.NamedAgg` namedtuple with the fields `['column', 'aggfunc']` to make\nit clearer what the arguments are. As usual, the aggregation can be a callable\nor a string alias.\n\n`pandas.NamedAgg` is just a `namedtuple`. Plain tuples are allowed as well.\n\nIf your desired output column names are not valid Python keywords, construct a\ndictionary and unpack the keyword arguments\n\nAdditional keyword arguments are not passed through to the aggregation\nfunctions. Only pairs of `(column, aggfunc)` should be passed as `**kwargs`.\nIf your aggregation functions requires additional arguments, partially apply\nthem with `functools.partial()`.\n\nNote\n\nFor Python 3.5 and earlier, the order of `**kwargs` in a functions was not\npreserved. This means that the output column ordering would not be consistent.\nTo ensure consistent ordering, the keys (and so output columns) will always be\nsorted for Python 3.5.\n\nNamed aggregation is also valid for Series groupby aggregations. In this case\nthere\u2019s no column selection, so the values are just the functions.\n\nBy passing a dict to `aggregate` you can apply a different aggregation to the\ncolumns of a DataFrame:\n\nThe function names can also be strings. In order for a string to be valid it\nmust be either implemented on GroupBy or available via dispatching:\n\nSome common aggregations, currently only `sum`, `mean`, `std`, and `sem`, have\noptimized Cython implementations:\n\nOf course `sum` and `mean` are implemented on pandas objects, so the above\ncode would work even without the special versions via dispatching (see below).\n\nUsers can also provide their own functions for custom aggregations. When\naggregating with a User-Defined Function (UDF), the UDF should not mutate the\nprovided `Series`, see Mutating with User Defined Function (UDF) methods for\nmore information.\n\nThe resulting dtype will reflect that of the aggregating function. If the\nresults from different groups have different dtypes, then a common dtype will\nbe determined in the same way as `DataFrame` construction.\n\nThe `transform` method returns an object that is indexed the same (same size)\nas the one being grouped. The transform function must:\n\nReturn a result that is either the same size as the group chunk or\nbroadcastable to the size of the group chunk (e.g., a scalar,\n`grouped.transform(lambda x: x.iloc[-1])`).\n\nOperate column-by-column on the group chunk. The transform is applied to the\nfirst group chunk using chunk.apply.\n\nNot perform in-place operations on the group chunk. Group chunks should be\ntreated as immutable, and changes to a group chunk may produce unexpected\nresults. For example, when using `fillna`, `inplace` must be `False`\n(`grouped.transform(lambda x: x.fillna(inplace=False))`).\n\n(Optionally) operates on the entire group chunk. If this is supported, a fast\npath is used starting from the second chunk.\n\nSimilar to Aggregations with User-Defined Functions, the resulting dtype will\nreflect that of the transformation function. If the results from different\ngroups have different dtypes, then a common dtype will be determined in the\nsame way as `DataFrame` construction.\n\nSuppose we wished to standardize the data within each group:\n\nWe would expect the result to now have mean 0 and standard deviation 1 within\neach group, which we can easily check:\n\nWe can also visually compare the original and transformed data sets.\n\nTransformation functions that have lower dimension outputs are broadcast to\nmatch the shape of the input array.\n\nAlternatively, the built-in methods could be used to produce the same outputs.\n\nAnother common data transform is to replace missing data with the group mean.\n\nWe can verify that the group means have not changed in the transformed data\nand that the transformed data contains no NAs.\n\nNote\n\nSome functions will automatically transform the input when applied to a\nGroupBy object, but returning an object of the same shape as the original.\nPassing `as_index=False` will not affect these transformation methods.\n\nFor example: `fillna, ffill, bfill, shift.`.\n\nIt is possible to use `resample()`, `expanding()` and `rolling()` as methods\non groupbys.\n\nThe example below will apply the `rolling()` method on the samples of the\ncolumn B based on the groups of column A.\n\nThe `expanding()` method will accumulate a given operation (`sum()` in the\nexample) for all the members of each particular group.\n\nSuppose you want to use the `resample()` method to get a daily frequency in\neach group of your dataframe and wish to complete the missing values with the\n`ffill()` method.\n\nThe `filter` method returns a subset of the original object. Suppose we want\nto take only elements that belong to groups with a group sum greater than 2.\n\nThe argument of `filter` must be a function that, applied to the group as a\nwhole, returns `True` or `False`.\n\nAnother useful operation is filtering out elements that belong to groups with\nonly a couple members.\n\nAlternatively, instead of dropping the offending groups, we can return a like-\nindexed objects where the groups that do not pass the filter are filled with\nNaNs.\n\nFor DataFrames with multiple columns, filters should explicitly specify a\ncolumn as the filter criterion.\n\nNote\n\nSome functions when applied to a groupby object will act as a filter on the\ninput, returning a reduced shape of the original (and potentially eliminating\ngroups), but with the index unchanged. Passing `as_index=False` will not\naffect these transformation methods.\n\nFor example: `head, tail`.\n\nWhen doing an aggregation or transformation, you might just want to call an\ninstance method on each data group. This is pretty easy to do by passing\nlambda functions:\n\nBut, it\u2019s rather verbose and can be untidy if you need to pass additional\narguments. Using a bit of metaprogramming cleverness, GroupBy now has the\nability to \u201cdispatch\u201d method calls to the groups:\n\nWhat is actually happening here is that a function wrapper is being generated.\nWhen invoked, it takes any passed arguments and invokes the function with any\narguments on each group (in the above example, the `std` function). The\nresults are then combined together much in the style of `agg` and `transform`\n(it actually uses `apply` to infer the gluing, documented next). This enables\nsome operations to be carried out rather succinctly:\n\nIn this example, we chopped the collection of time series into yearly chunks\nthen independently called fillna on the groups.\n\nThe `nlargest` and `nsmallest` methods work on `Series` style groupbys:\n\nSome operations on the grouped data might not fit into either the aggregate or\ntransform categories. Or, you may simply want GroupBy to infer how to combine\nthe results. For these, use the `apply` function, which can be substituted for\nboth `aggregate` and `transform` in many standard use cases. However, `apply`\ncan handle some exceptional use cases, for example:\n\nThe dimension of the returned result can also change:\n\n`apply` on a Series can operate on a returned value from the applied function,\nthat is itself a series, and possibly upcast the result to a DataFrame:\n\nNote\n\n`apply` can act as a reducer, transformer, or filter function, depending on\nexactly what is passed to it. So depending on the path taken, and exactly what\nyou are grouping. Thus the grouped columns(s) may be included in the output as\nwell as set the indices.\n\nSimilar to Aggregations with User-Defined Functions, the resulting dtype will\nreflect that of the apply function. If the results from different groups have\ndifferent dtypes, then a common dtype will be determined in the same way as\n`DataFrame` construction.\n\nNew in version 1.1.\n\nIf Numba is installed as an optional dependency, the `transform` and\n`aggregate` methods support `engine='numba'` and `engine_kwargs` arguments.\nSee enhancing performance with Numba for general usage of the arguments and\nperformance considerations.\n\nThe function signature must start with `values, index` exactly as the data\nbelonging to each group will be passed into `values`, and the group index will\nbe passed into `index`.\n\nWarning\n\nWhen using `engine='numba'`, there will be no \u201cfall back\u201d behavior internally.\nThe group data and group index will be passed as NumPy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n\nAgain consider the example DataFrame we\u2019ve been looking at:\n\nSuppose we wish to compute the standard deviation grouped by the `A` column.\nThere is a slight problem, namely that we don\u2019t care about the data in column\n`B`. We refer to this as a \u201cnuisance\u201d column. If the passed aggregation\nfunction can\u2019t be applied to some columns, the troublesome columns will be\n(silently) dropped. Thus, this does not pose any problems:\n\nNote that `df.groupby('A').colname.std().` is more efficient than\n`df.groupby('A').std().colname`, so if the result of an aggregation function\nis only interesting over one column (here `colname`), it may be filtered\nbefore applying the aggregation function.\n\nNote\n\nAny object column, also if it contains numerical values such as `Decimal`\nobjects, is considered as a \u201cnuisance\u201d columns. They are excluded from\naggregate functions automatically in groupby.\n\nIf you do wish to include decimal or object columns in an aggregation with\nother non-nuisance data types, you must do so explicitly.\n\nWhen using a `Categorical` grouper (as a single grouper, or as part of\nmultiple groupers), the `observed` keyword controls whether to return a\ncartesian product of all possible groupers values (`observed=False`) or only\nthose that are observed groupers (`observed=True`).\n\nShow all values:\n\nShow only the observed values:\n\nThe returned dtype of the grouped will always include all of the categories\nthat were grouped.\n\nIf there are any NaN or NaT values in the grouping key, these will be\nautomatically excluded. In other words, there will never be an \u201cNA group\u201d or\n\u201cNaT group\u201d. This was not the case in older versions of pandas, but users were\ngenerally discarding the NA group anyway (and supporting it was an\nimplementation headache).\n\nCategorical variables represented as instance of pandas\u2019s `Categorical` class\ncan be used as group keys. If so, the order of the levels will be preserved:\n\nYou may need to specify a bit more data to properly group. You can use the\n`pd.Grouper` to provide this local control.\n\nGroupby a specific column with the desired frequency. This is like resampling.\n\nYou have an ambiguous specification in that you have a named index and a\ncolumn that could be potential groupers.\n\nJust like for a DataFrame or Series you can call head and tail on a groupby:\n\nThis shows the first or last n rows from each group.\n\nTo select from a DataFrame or Series the nth item, use `nth()`. This is a\nreduction method, and will return a single row (or no row) per group if you\npass an int for n:\n\nIf you want to select the nth not-null item, use the `dropna` kwarg. For a\nDataFrame this should be either `'any'` or `'all'` just like you would pass to\ndropna:\n\nAs with other methods, passing `as_index=False`, will achieve a filtration,\nwhich returns the grouped row.\n\nYou can also select multiple rows from each group by specifying multiple nth\nvalues as a list of ints.\n\nTo see the order in which each row appears within its group, use the\n`cumcount` method:\n\nTo see the ordering of the groups (as opposed to the order of rows within a\ngroup given by `cumcount`) you can use `ngroup()`.\n\nNote that the numbers given to the groups match the order in which the groups\nwould be seen when iterating over the groupby object, not the order they are\nfirst observed.\n\nGroupby also works with some plotting methods. For example, suppose we suspect\nthat some features in a DataFrame may differ by group, in this case, the\nvalues in column 1 where the group is \u201cB\u201d are 3 higher on average.\n\nWe can easily visualize this with a boxplot:\n\nThe result of calling `boxplot` is a dictionary whose keys are the values of\nour grouping column `g` (\u201cA\u201d and \u201cB\u201d). The values of the resulting dictionary\ncan be controlled by the `return_type` keyword of `boxplot`. See the\nvisualization documentation for more.\n\nWarning\n\nFor historical reasons, `df.groupby(\"g\").boxplot()` is not equivalent to\n`df.boxplot(by=\"g\")`. See here for an explanation.\n\nSimilar to the functionality provided by `DataFrame` and `Series`, functions\nthat take `GroupBy` objects can be chained together using a `pipe` method to\nallow for a cleaner, more readable syntax. To read about `.pipe` in general\nterms, see here.\n\nCombining `.groupby` and `.pipe` is often useful when you need to reuse\nGroupBy objects.\n\nAs an example, imagine having a DataFrame with columns for stores, products,\nrevenue and quantity sold. We\u2019d like to do a groupwise calculation of prices\n(i.e. revenue/quantity) per store and per product. We could do this in a\nmulti-step operation, but expressing it in terms of piping can make the code\nmore readable. First we set the data:\n\nNow, to find prices per store/product, we can simply do:\n\nPiping can also be expressive when you want to deliver a grouped object to\nsome arbitrary function, for example:\n\nwhere `mean` takes a GroupBy object and finds the mean of the Revenue and\nQuantity columns respectively for each Store-Product combination. The `mean`\nfunction can be any function that takes in a GroupBy object; the `.pipe` will\npass the GroupBy object as a parameter into the function you specify.\n\nRegroup columns of a DataFrame according to their sum, and sum the aggregated\nones.\n\nBy using `ngroup()`, we can extract information about the groups in a way\nsimilar to `factorize()` (as described further in the reshaping API) but which\napplies naturally to multiple columns of mixed type and different sources.\nThis can be useful as an intermediate categorical-like step in processing,\nwhen the relationships between the group rows are more important than their\ncontent, or as input to an algorithm which only accepts the integer encoding.\n(For more information about support in pandas for full categorical data, see\nthe Categorical introduction and the API documentation.)\n\nResampling produces new hypothetical samples (resamples) from already existing\nobserved data or from a model that generates data. These new samples are\nsimilar to the pre-existing samples.\n\nIn order to resample to work on indices that are non-datetimelike, the\nfollowing procedure can be utilized.\n\nIn the following examples, df.index // 5 returns a binary array which is used\nto determine what gets selected for the groupby operation.\n\nNote\n\nThe below example shows how we can downsample by consolidation of samples into\nfewer samples. Here by using df.index // 5, we are aggregating the samples in\nbins. By applying std() function, we aggregate the information contained in\nmany samples into a small subset of values which is their standard deviation\nthereby reducing the number of samples.\n\nGroup DataFrame columns, compute a set of metrics and return a named Series.\nThe Series name is used as the name for the column index. This is especially\nuseful in conjunction with reshaping operations such as stacking in which the\ncolumn index name will be used as the name of the inserted column:\n\n"}, {"name": "GroupBy", "path": "reference/groupby", "type": "GroupBy", "text": "\nGroupBy objects are returned by groupby calls: `pandas.DataFrame.groupby()`,\n`pandas.Series.groupby()`, etc.\n\n`GroupBy.__iter__`()\n\nGroupby iterator.\n\n`GroupBy.groups`\n\nDict {group name -> group labels}.\n\n`GroupBy.indices`\n\nDict {group name -> group indices}.\n\n`GroupBy.get_group`(name[, obj])\n\nConstruct DataFrame from group with provided name.\n\n`Grouper`(*args, **kwargs)\n\nA Grouper allows the user to specify a groupby instruction for an object.\n\n`GroupBy.apply`(func, *args, **kwargs)\n\nApply function `func` group-wise and combine the results together.\n\n`GroupBy.agg`(func, *args, **kwargs)\n\n`SeriesGroupBy.aggregate`([func, engine, ...])\n\nAggregate using one or more operations over the specified axis.\n\n`DataFrameGroupBy.aggregate`([func, engine, ...])\n\nAggregate using one or more operations over the specified axis.\n\n`SeriesGroupBy.transform`(func, *args[, ...])\n\nCall function producing a like-indexed Series on each group and return a\nSeries having the same indexes as the original object filled with the\ntransformed values.\n\n`DataFrameGroupBy.transform`(func, *args[, ...])\n\nCall function producing a like-indexed DataFrame on each group and return a\nDataFrame having the same indexes as the original object filled with the\ntransformed values.\n\n`GroupBy.pipe`(func, *args, **kwargs)\n\nApply a function func with arguments to this GroupBy object and return the\nfunction's result.\n\n`GroupBy.all`([skipna])\n\nReturn True if all values in the group are truthful, else False.\n\n`GroupBy.any`([skipna])\n\nReturn True if any value in the group is truthful, else False.\n\n`GroupBy.bfill`([limit])\n\nBackward fill the values.\n\n`GroupBy.backfill`([limit])\n\nBackward fill the values.\n\n`GroupBy.count`()\n\nCompute count of group, excluding missing values.\n\n`GroupBy.cumcount`([ascending])\n\nNumber each item in each group from 0 to the length of that group - 1.\n\n`GroupBy.cummax`([axis])\n\nCumulative max for each group.\n\n`GroupBy.cummin`([axis])\n\nCumulative min for each group.\n\n`GroupBy.cumprod`([axis])\n\nCumulative product for each group.\n\n`GroupBy.cumsum`([axis])\n\nCumulative sum for each group.\n\n`GroupBy.ffill`([limit])\n\nForward fill the values.\n\n`GroupBy.first`([numeric_only, min_count])\n\nCompute first of group values.\n\n`GroupBy.head`([n])\n\nReturn first n rows of each group.\n\n`GroupBy.last`([numeric_only, min_count])\n\nCompute last of group values.\n\n`GroupBy.max`([numeric_only, min_count])\n\nCompute max of group values.\n\n`GroupBy.mean`([numeric_only, engine, ...])\n\nCompute mean of groups, excluding missing values.\n\n`GroupBy.median`([numeric_only])\n\nCompute median of groups, excluding missing values.\n\n`GroupBy.min`([numeric_only, min_count])\n\nCompute min of group values.\n\n`GroupBy.ngroup`([ascending])\n\nNumber each group from 0 to the number of groups - 1.\n\n`GroupBy.nth`(n[, dropna])\n\nTake the nth row from each group if n is an int, otherwise a subset of rows.\n\n`GroupBy.ohlc`()\n\nCompute open, high, low and close values of a group, excluding missing values.\n\n`GroupBy.pad`([limit])\n\nForward fill the values.\n\n`GroupBy.prod`([numeric_only, min_count])\n\nCompute prod of group values.\n\n`GroupBy.rank`([method, ascending, na_option, ...])\n\nProvide the rank of values within each group.\n\n`GroupBy.pct_change`([periods, fill_method, ...])\n\nCalculate pct_change of each value to previous entry in group.\n\n`GroupBy.size`()\n\nCompute group sizes.\n\n`GroupBy.sem`([ddof])\n\nCompute standard error of the mean of groups, excluding missing values.\n\n`GroupBy.std`([ddof, engine, engine_kwargs])\n\nCompute standard deviation of groups, excluding missing values.\n\n`GroupBy.sum`([numeric_only, min_count, ...])\n\nCompute sum of group values.\n\n`GroupBy.var`([ddof, engine, engine_kwargs])\n\nCompute variance of groups, excluding missing values.\n\n`GroupBy.tail`([n])\n\nReturn last n rows of each group.\n\nThe following methods are available in both `SeriesGroupBy` and\n`DataFrameGroupBy` objects, but may differ slightly, usually in that the\n`DataFrameGroupBy` version usually permits the specification of an axis\nargument, and often an argument indicating whether to restrict application to\ncolumns of a specific data type.\n\n`DataFrameGroupBy.all`([skipna])\n\nReturn True if all values in the group are truthful, else False.\n\n`DataFrameGroupBy.any`([skipna])\n\nReturn True if any value in the group is truthful, else False.\n\n`DataFrameGroupBy.backfill`([limit])\n\nBackward fill the values.\n\n`DataFrameGroupBy.bfill`([limit])\n\nBackward fill the values.\n\n`DataFrameGroupBy.corr`\n\nCompute pairwise correlation of columns, excluding NA/null values.\n\n`DataFrameGroupBy.count`()\n\nCompute count of group, excluding missing values.\n\n`DataFrameGroupBy.cov`\n\nCompute pairwise covariance of columns, excluding NA/null values.\n\n`DataFrameGroupBy.cumcount`([ascending])\n\nNumber each item in each group from 0 to the length of that group - 1.\n\n`DataFrameGroupBy.cummax`([axis])\n\nCumulative max for each group.\n\n`DataFrameGroupBy.cummin`([axis])\n\nCumulative min for each group.\n\n`DataFrameGroupBy.cumprod`([axis])\n\nCumulative product for each group.\n\n`DataFrameGroupBy.cumsum`([axis])\n\nCumulative sum for each group.\n\n`DataFrameGroupBy.describe`(**kwargs)\n\nGenerate descriptive statistics.\n\n`DataFrameGroupBy.diff`\n\nFirst discrete difference of element.\n\n`DataFrameGroupBy.ffill`([limit])\n\nForward fill the values.\n\n`DataFrameGroupBy.fillna`\n\nFill NA/NaN values using the specified method.\n\n`DataFrameGroupBy.filter`(func[, dropna])\n\nReturn a copy of a DataFrame excluding filtered elements.\n\n`DataFrameGroupBy.hist`\n\nMake a histogram of the DataFrame's columns.\n\n`DataFrameGroupBy.idxmax`([axis, skipna])\n\nReturn index of first occurrence of maximum over requested axis.\n\n`DataFrameGroupBy.idxmin`([axis, skipna])\n\nReturn index of first occurrence of minimum over requested axis.\n\n`DataFrameGroupBy.mad`\n\nReturn the mean absolute deviation of the values over the requested axis.\n\n`DataFrameGroupBy.nunique`([dropna])\n\nReturn DataFrame with counts of unique elements in each position.\n\n`DataFrameGroupBy.pad`([limit])\n\nForward fill the values.\n\n`DataFrameGroupBy.pct_change`([periods, ...])\n\nCalculate pct_change of each value to previous entry in group.\n\n`DataFrameGroupBy.plot`\n\nClass implementing the .plot attribute for groupby objects.\n\n`DataFrameGroupBy.quantile`([q, interpolation])\n\nReturn group values at the given quantile, a la numpy.percentile.\n\n`DataFrameGroupBy.rank`([method, ascending, ...])\n\nProvide the rank of values within each group.\n\n`DataFrameGroupBy.resample`(rule, *args, **kwargs)\n\nProvide resampling when using a TimeGrouper.\n\n`DataFrameGroupBy.sample`([n, frac, replace, ...])\n\nReturn a random sample of items from each group.\n\n`DataFrameGroupBy.shift`([periods, freq, ...])\n\nShift each group by periods observations.\n\n`DataFrameGroupBy.size`()\n\nCompute group sizes.\n\n`DataFrameGroupBy.skew`\n\nReturn unbiased skew over requested axis.\n\n`DataFrameGroupBy.take`\n\nReturn the elements in the given positional indices along an axis.\n\n`DataFrameGroupBy.tshift`\n\n(DEPRECATED) Shift the time index, using the index's frequency if available.\n\n`DataFrameGroupBy.value_counts`([subset, ...])\n\nReturn a Series or DataFrame containing counts of unique rows.\n\nThe following methods are available only for `SeriesGroupBy` objects.\n\n`SeriesGroupBy.hist`\n\nDraw histogram of the input series using matplotlib.\n\n`SeriesGroupBy.nlargest`([n, keep])\n\nReturn the largest n elements.\n\n`SeriesGroupBy.nsmallest`([n, keep])\n\nReturn the smallest n elements.\n\n`SeriesGroupBy.nunique`([dropna])\n\nReturn number of unique elements in the group.\n\n`SeriesGroupBy.unique`\n\nReturn unique values of Series object.\n\n`SeriesGroupBy.value_counts`([normalize, ...])\n\n`SeriesGroupBy.is_monotonic_increasing`\n\nAlias for is_monotonic.\n\n`SeriesGroupBy.is_monotonic_decreasing`\n\nReturn boolean if values in the object are monotonic_decreasing.\n\nThe following methods are available only for `DataFrameGroupBy` objects.\n\n`DataFrameGroupBy.corrwith`\n\nCompute pairwise correlation.\n\n`DataFrameGroupBy.boxplot`([subplots, column, ...])\n\nMake box plots from DataFrameGroupBy data.\n\n"}, {"name": "Index objects", "path": "reference/indexing", "type": "Index Objects", "text": "\nMany of these methods or variants thereof are available on the objects that\ncontain an index (Series/DataFrame) and those should most likely be used\nbefore calling these methods directly.\n\n`Index`([data, dtype, copy, name, tupleize_cols])\n\nImmutable sequence used for indexing and alignment.\n\n`Index.values`\n\nReturn an array representing the data in the Index.\n\n`Index.is_monotonic`\n\nAlias for is_monotonic_increasing.\n\n`Index.is_monotonic_increasing`\n\nReturn if the index is monotonic increasing (only equal or increasing) values.\n\n`Index.is_monotonic_decreasing`\n\nReturn if the index is monotonic decreasing (only equal or decreasing) values.\n\n`Index.is_unique`\n\nReturn if the index has unique values.\n\n`Index.has_duplicates`\n\nCheck if the Index has duplicate values.\n\n`Index.hasnans`\n\nReturn True if there are any NaNs.\n\n`Index.dtype`\n\nReturn the dtype object of the underlying data.\n\n`Index.inferred_type`\n\nReturn a string of the type inferred from the values.\n\n`Index.is_all_dates`\n\nWhether or not the index values only consist of dates.\n\n`Index.shape`\n\nReturn a tuple of the shape of the underlying data.\n\n`Index.name`\n\nReturn Index or MultiIndex name.\n\n`Index.names`\n\n`Index.nbytes`\n\nReturn the number of bytes in the underlying data.\n\n`Index.ndim`\n\nNumber of dimensions of the underlying data, by definition 1.\n\n`Index.size`\n\nReturn the number of elements in the underlying data.\n\n`Index.empty`\n\n`Index.T`\n\nReturn the transpose, which is by definition self.\n\n`Index.memory_usage`([deep])\n\nMemory usage of the values.\n\n`Index.all`(*args, **kwargs)\n\nReturn whether all elements are Truthy.\n\n`Index.any`(*args, **kwargs)\n\nReturn whether any element is Truthy.\n\n`Index.argmin`([axis, skipna])\n\nReturn int position of the smallest value in the Series.\n\n`Index.argmax`([axis, skipna])\n\nReturn int position of the largest value in the Series.\n\n`Index.copy`([name, deep, dtype, names])\n\nMake a copy of this object.\n\n`Index.delete`(loc)\n\nMake new Index with passed location(-s) deleted.\n\n`Index.drop`(labels[, errors])\n\nMake new Index with passed list of labels deleted.\n\n`Index.drop_duplicates`([keep])\n\nReturn Index with duplicate values removed.\n\n`Index.duplicated`([keep])\n\nIndicate duplicate index values.\n\n`Index.equals`(other)\n\nDetermine if two Index object are equal.\n\n`Index.factorize`([sort, na_sentinel])\n\nEncode the object as an enumerated type or categorical variable.\n\n`Index.identical`(other)\n\nSimilar to equals, but checks that object attributes and types are also equal.\n\n`Index.insert`(loc, item)\n\nMake new Index inserting new item at location.\n\n`Index.is_`(other)\n\nMore flexible, faster check like `is` but that works through views.\n\n`Index.is_boolean`()\n\nCheck if the Index only consists of booleans.\n\n`Index.is_categorical`()\n\nCheck if the Index holds categorical data.\n\n`Index.is_floating`()\n\nCheck if the Index is a floating type.\n\n`Index.is_integer`()\n\nCheck if the Index only consists of integers.\n\n`Index.is_interval`()\n\nCheck if the Index holds Interval objects.\n\n`Index.is_mixed`()\n\nCheck if the Index holds data with mixed data types.\n\n`Index.is_numeric`()\n\nCheck if the Index only consists of numeric data.\n\n`Index.is_object`()\n\nCheck if the Index is of the object dtype.\n\n`Index.min`([axis, skipna])\n\nReturn the minimum value of the Index.\n\n`Index.max`([axis, skipna])\n\nReturn the maximum value of the Index.\n\n`Index.reindex`(target[, method, level, ...])\n\nCreate index with target's values.\n\n`Index.rename`(name[, inplace])\n\nAlter Index or MultiIndex name.\n\n`Index.repeat`(repeats[, axis])\n\nRepeat elements of a Index.\n\n`Index.where`(cond[, other])\n\nReplace values where the condition is False.\n\n`Index.take`(indices[, axis, allow_fill, ...])\n\nReturn a new Index of the values selected by the indices.\n\n`Index.putmask`(mask, value)\n\nReturn a new Index of the values set with the mask.\n\n`Index.unique`([level])\n\nReturn unique values in the index.\n\n`Index.nunique`([dropna])\n\nReturn number of unique elements in the object.\n\n`Index.value_counts`([normalize, sort, ...])\n\nReturn a Series containing counts of unique values.\n\n`Index.set_names`(names[, level, inplace])\n\nSet Index or MultiIndex name.\n\n`Index.droplevel`([level])\n\nReturn index with requested level(s) removed.\n\n`Index.fillna`([value, downcast])\n\nFill NA/NaN values with the specified value.\n\n`Index.dropna`([how])\n\nReturn Index without NA/NaN values.\n\n`Index.isna`()\n\nDetect missing values.\n\n`Index.notna`()\n\nDetect existing (non-missing) values.\n\n`Index.astype`(dtype[, copy])\n\nCreate an Index with values cast to dtypes.\n\n`Index.item`()\n\nReturn the first element of the underlying data as a Python scalar.\n\n`Index.map`(mapper[, na_action])\n\nMap values using an input mapping or function.\n\n`Index.ravel`([order])\n\nReturn an ndarray of the flattened values of the underlying data.\n\n`Index.to_list`()\n\nReturn a list of the values.\n\n`Index.to_native_types`([slicer])\n\n(DEPRECATED) Format specified values of self and return them.\n\n`Index.to_series`([index, name])\n\nCreate a Series with both index and values equal to the index keys.\n\n`Index.to_frame`([index, name])\n\nCreate a DataFrame with a column containing the Index.\n\n`Index.view`([cls])\n\n`Index.argsort`(*args, **kwargs)\n\nReturn the integer indices that would sort the index.\n\n`Index.searchsorted`(value[, side, sorter])\n\nFind indices where elements should be inserted to maintain order.\n\n`Index.sort_values`([return_indexer, ...])\n\nReturn a sorted copy of the index.\n\n`Index.shift`([periods, freq])\n\nShift index by desired number of time frequency increments.\n\n`Index.append`(other)\n\nAppend a collection of Index options together.\n\n`Index.join`(other[, how, level, ...])\n\nCompute join_index and indexers to conform data structures to the new index.\n\n`Index.intersection`(other[, sort])\n\nForm the intersection of two Index objects.\n\n`Index.union`(other[, sort])\n\nForm the union of two Index objects.\n\n`Index.difference`(other[, sort])\n\nReturn a new Index with elements of index not in other.\n\n`Index.symmetric_difference`(other[, ...])\n\nCompute the symmetric difference of two Index objects.\n\n`Index.asof`(label)\n\nReturn the label from the index, or, if not present, the previous one.\n\n`Index.asof_locs`(where, mask)\n\nReturn the locations (indices) of labels in the index.\n\n`Index.get_indexer`(target[, method, limit, ...])\n\nCompute indexer and mask for new index given the current index.\n\n`Index.get_indexer_for`(target)\n\nGuaranteed return of an indexer even when non-unique.\n\n`Index.get_indexer_non_unique`(target)\n\nCompute indexer and mask for new index given the current index.\n\n`Index.get_level_values`(level)\n\nReturn an Index of values for requested level.\n\n`Index.get_loc`(key[, method, tolerance])\n\nGet integer location, slice or boolean mask for requested label.\n\n`Index.get_slice_bound`(label, side[, kind])\n\nCalculate slice bound that corresponds to given label.\n\n`Index.get_value`(series, key)\n\nFast lookup of value from 1-dimensional ndarray.\n\n`Index.isin`(values[, level])\n\nReturn a boolean array where the index values are in values.\n\n`Index.slice_indexer`([start, end, step, kind])\n\nCompute the slice indexer for input labels and step.\n\n`Index.slice_locs`([start, end, step, kind])\n\nCompute slice locations for input labels.\n\n`RangeIndex`([start, stop, step, dtype, copy, ...])\n\nImmutable Index implementing a monotonic integer range.\n\n`Int64Index`([data, dtype, copy, name])\n\n(DEPRECATED) Immutable sequence used for indexing and alignment.\n\n`UInt64Index`([data, dtype, copy, name])\n\n(DEPRECATED) Immutable sequence used for indexing and alignment.\n\n`Float64Index`([data, dtype, copy, name])\n\n(DEPRECATED) Immutable sequence used for indexing and alignment.\n\n`RangeIndex.start`\n\nThe value of the start parameter (`0` if this was not supplied).\n\n`RangeIndex.stop`\n\nThe value of the stop parameter.\n\n`RangeIndex.step`\n\nThe value of the step parameter (`1` if this was not supplied).\n\n`RangeIndex.from_range`(data[, name, dtype])\n\nCreate RangeIndex from a range object.\n\n`CategoricalIndex`([data, categories, ...])\n\nIndex based on an underlying `Categorical`.\n\n`CategoricalIndex.codes`\n\nThe category codes of this categorical.\n\n`CategoricalIndex.categories`\n\nThe categories of this categorical.\n\n`CategoricalIndex.ordered`\n\nWhether the categories have an ordered relationship.\n\n`CategoricalIndex.rename_categories`(*args, ...)\n\nRename categories.\n\n`CategoricalIndex.reorder_categories`(*args, ...)\n\nReorder categories as specified in new_categories.\n\n`CategoricalIndex.add_categories`(*args, **kwargs)\n\nAdd new categories.\n\n`CategoricalIndex.remove_categories`(*args, ...)\n\nRemove the specified categories.\n\n`CategoricalIndex.remove_unused_categories`(...)\n\nRemove categories which are not used.\n\n`CategoricalIndex.set_categories`(*args, **kwargs)\n\nSet the categories to the specified new_categories.\n\n`CategoricalIndex.as_ordered`(*args, **kwargs)\n\nSet the Categorical to be ordered.\n\n`CategoricalIndex.as_unordered`(*args, **kwargs)\n\nSet the Categorical to be unordered.\n\n`CategoricalIndex.map`(mapper)\n\nMap values using input an input mapping or function.\n\n`CategoricalIndex.equals`(other)\n\nDetermine if two CategoricalIndex objects contain the same elements.\n\n`IntervalIndex`(data[, closed, dtype, copy, ...])\n\nImmutable index of intervals that are closed on the same side.\n\n`IntervalIndex.from_arrays`(left, right[, ...])\n\nConstruct from two arrays defining the left and right bounds.\n\n`IntervalIndex.from_tuples`(data[, closed, ...])\n\nConstruct an IntervalIndex from an array-like of tuples.\n\n`IntervalIndex.from_breaks`(breaks[, closed, ...])\n\nConstruct an IntervalIndex from an array of splits.\n\n`IntervalIndex.left`\n\n`IntervalIndex.right`\n\n`IntervalIndex.mid`\n\n`IntervalIndex.closed`\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\n`IntervalIndex.length`\n\n`IntervalIndex.values`\n\nReturn an array representing the data in the Index.\n\n`IntervalIndex.is_empty`\n\nIndicates if an interval is empty, meaning it contains no points.\n\n`IntervalIndex.is_non_overlapping_monotonic`\n\nReturn True if the IntervalArray is non-overlapping (no Intervals share\npoints) and is either monotonic increasing or monotonic decreasing, else\nFalse.\n\n`IntervalIndex.is_overlapping`\n\nReturn True if the IntervalIndex has overlapping intervals, else False.\n\n`IntervalIndex.get_loc`(key[, method, tolerance])\n\nGet integer location, slice or boolean mask for requested label.\n\n`IntervalIndex.get_indexer`(target[, method, ...])\n\nCompute indexer and mask for new index given the current index.\n\n`IntervalIndex.set_closed`(*args, **kwargs)\n\nReturn an IntervalArray identical to the current one, but closed on the\nspecified side.\n\n`IntervalIndex.contains`(*args, **kwargs)\n\nCheck elementwise if the Intervals contain the value.\n\n`IntervalIndex.overlaps`(*args, **kwargs)\n\nCheck elementwise if an Interval overlaps the values in the IntervalArray.\n\n`IntervalIndex.to_tuples`(*args, **kwargs)\n\nReturn an ndarray of tuples of the form (left, right).\n\n`MultiIndex`([levels, codes, sortorder, ...])\n\nA multi-level, or hierarchical, index object for pandas objects.\n\n`IndexSlice`\n\nCreate an object to more easily perform multi-index slicing.\n\n`MultiIndex.from_arrays`(arrays[, sortorder, ...])\n\nConvert arrays to MultiIndex.\n\n`MultiIndex.from_tuples`(tuples[, sortorder, ...])\n\nConvert list of tuples to MultiIndex.\n\n`MultiIndex.from_product`(iterables[, ...])\n\nMake a MultiIndex from the cartesian product of multiple iterables.\n\n`MultiIndex.from_frame`(df[, sortorder, names])\n\nMake a MultiIndex from a DataFrame.\n\n`MultiIndex.names`\n\nNames of levels in MultiIndex.\n\n`MultiIndex.levels`\n\n`MultiIndex.codes`\n\n`MultiIndex.nlevels`\n\nInteger number of levels in this MultiIndex.\n\n`MultiIndex.levshape`\n\nA tuple with the length of each level.\n\n`MultiIndex.dtypes`\n\nReturn the dtypes as a Series for the underlying MultiIndex.\n\n`MultiIndex.set_levels`(levels[, level, ...])\n\nSet new levels on MultiIndex.\n\n`MultiIndex.set_codes`(codes[, level, ...])\n\nSet new codes on MultiIndex.\n\n`MultiIndex.to_flat_index`()\n\nConvert a MultiIndex to an Index of Tuples containing the level values.\n\n`MultiIndex.to_frame`([index, name])\n\nCreate a DataFrame with the levels of the MultiIndex as columns.\n\n`MultiIndex.sortlevel`([level, ascending, ...])\n\nSort MultiIndex at the requested level.\n\n`MultiIndex.droplevel`([level])\n\nReturn index with requested level(s) removed.\n\n`MultiIndex.swaplevel`([i, j])\n\nSwap level i with level j.\n\n`MultiIndex.reorder_levels`(order)\n\nRearrange levels using input order.\n\n`MultiIndex.remove_unused_levels`()\n\nCreate new MultiIndex from current that removes unused levels.\n\n`MultiIndex.get_loc`(key[, method])\n\nGet location for a label or a tuple of labels.\n\n`MultiIndex.get_locs`(seq)\n\nGet location for a sequence of labels.\n\n`MultiIndex.get_loc_level`(key[, level, ...])\n\nGet location and sliced index for requested label(s)/level(s).\n\n`MultiIndex.get_indexer`(target[, method, ...])\n\nCompute indexer and mask for new index given the current index.\n\n`MultiIndex.get_level_values`(level)\n\nReturn vector of label values for requested level.\n\n`DatetimeIndex`([data, freq, tz, normalize, ...])\n\nImmutable ndarray-like of datetime64 data.\n\n`DatetimeIndex.year`\n\nThe year of the datetime.\n\n`DatetimeIndex.month`\n\nThe month as January=1, December=12.\n\n`DatetimeIndex.day`\n\nThe day of the datetime.\n\n`DatetimeIndex.hour`\n\nThe hours of the datetime.\n\n`DatetimeIndex.minute`\n\nThe minutes of the datetime.\n\n`DatetimeIndex.second`\n\nThe seconds of the datetime.\n\n`DatetimeIndex.microsecond`\n\nThe microseconds of the datetime.\n\n`DatetimeIndex.nanosecond`\n\nThe nanoseconds of the datetime.\n\n`DatetimeIndex.date`\n\nReturns numpy array of python `datetime.date` objects.\n\n`DatetimeIndex.time`\n\nReturns numpy array of `datetime.time` objects.\n\n`DatetimeIndex.timetz`\n\nReturns numpy array of `datetime.time` objects with timezone information.\n\n`DatetimeIndex.dayofyear`\n\nThe ordinal day of the year.\n\n`DatetimeIndex.day_of_year`\n\nThe ordinal day of the year.\n\n`DatetimeIndex.weekofyear`\n\n(DEPRECATED) The week ordinal of the year.\n\n`DatetimeIndex.week`\n\n(DEPRECATED) The week ordinal of the year.\n\n`DatetimeIndex.dayofweek`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`DatetimeIndex.day_of_week`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`DatetimeIndex.weekday`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`DatetimeIndex.quarter`\n\nThe quarter of the date.\n\n`DatetimeIndex.tz`\n\nReturn the timezone.\n\n`DatetimeIndex.freq`\n\nReturn the frequency object if it is set, otherwise None.\n\n`DatetimeIndex.freqstr`\n\nReturn the frequency object as a string if its set, otherwise None.\n\n`DatetimeIndex.is_month_start`\n\nIndicates whether the date is the first day of the month.\n\n`DatetimeIndex.is_month_end`\n\nIndicates whether the date is the last day of the month.\n\n`DatetimeIndex.is_quarter_start`\n\nIndicator for whether the date is the first day of a quarter.\n\n`DatetimeIndex.is_quarter_end`\n\nIndicator for whether the date is the last day of a quarter.\n\n`DatetimeIndex.is_year_start`\n\nIndicate whether the date is the first day of a year.\n\n`DatetimeIndex.is_year_end`\n\nIndicate whether the date is the last day of the year.\n\n`DatetimeIndex.is_leap_year`\n\nBoolean indicator if the date belongs to a leap year.\n\n`DatetimeIndex.inferred_freq`\n\nTries to return a string representing a frequency guess, generated by\ninfer_freq.\n\n`DatetimeIndex.indexer_at_time`(time[, asof])\n\nReturn index locations of values at particular time of day (e.g.\n\n`DatetimeIndex.indexer_between_time`(...[, ...])\n\nReturn index locations of values between particular times of day (e.g.,\n9:00-9:30AM).\n\n`DatetimeIndex.normalize`(*args, **kwargs)\n\nConvert times to midnight.\n\n`DatetimeIndex.strftime`(*args, **kwargs)\n\nConvert to Index using specified date_format.\n\n`DatetimeIndex.snap`([freq])\n\nSnap time stamps to nearest occurring frequency.\n\n`DatetimeIndex.tz_convert`(tz)\n\nConvert tz-aware Datetime Array/Index from one time zone to another.\n\n`DatetimeIndex.tz_localize`(tz[, ambiguous, ...])\n\nLocalize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index.\n\n`DatetimeIndex.round`(*args, **kwargs)\n\nPerform round operation on the data to the specified freq.\n\n`DatetimeIndex.floor`(*args, **kwargs)\n\nPerform floor operation on the data to the specified freq.\n\n`DatetimeIndex.ceil`(*args, **kwargs)\n\nPerform ceil operation on the data to the specified freq.\n\n`DatetimeIndex.month_name`(*args, **kwargs)\n\nReturn the month names of the DateTimeIndex with specified locale.\n\n`DatetimeIndex.day_name`(*args, **kwargs)\n\nReturn the day names of the DateTimeIndex with specified locale.\n\n`DatetimeIndex.to_period`(*args, **kwargs)\n\nCast to PeriodArray/Index at a particular frequency.\n\n`DatetimeIndex.to_perioddelta`(freq)\n\nCalculate TimedeltaArray of difference between index values and index\nconverted to PeriodArray at specified freq.\n\n`DatetimeIndex.to_pydatetime`(*args, **kwargs)\n\nReturn Datetime Array/Index as object ndarray of datetime.datetime objects.\n\n`DatetimeIndex.to_series`([keep_tz, index, name])\n\nCreate a Series with both index and values equal to the index keys useful with\nmap for returning an indexer based on an index.\n\n`DatetimeIndex.to_frame`([index, name])\n\nCreate a DataFrame with a column containing the Index.\n\n`DatetimeIndex.mean`(*args, **kwargs)\n\nReturn the mean value of the Array.\n\n`DatetimeIndex.std`(*args, **kwargs)\n\nReturn sample standard deviation over requested axis.\n\n`TimedeltaIndex`([data, unit, freq, closed, ...])\n\nImmutable ndarray of timedelta64 data, represented internally as int64, and\nwhich can be boxed to timedelta objects.\n\n`TimedeltaIndex.days`\n\nNumber of days for each element.\n\n`TimedeltaIndex.seconds`\n\nNumber of seconds (>= 0 and less than 1 day) for each element.\n\n`TimedeltaIndex.microseconds`\n\nNumber of microseconds (>= 0 and less than 1 second) for each element.\n\n`TimedeltaIndex.nanoseconds`\n\nNumber of nanoseconds (>= 0 and less than 1 microsecond) for each element.\n\n`TimedeltaIndex.components`\n\nReturn a dataframe of the components (days, hours, minutes, seconds,\nmilliseconds, microseconds, nanoseconds) of the Timedeltas.\n\n`TimedeltaIndex.inferred_freq`\n\nTries to return a string representing a frequency guess, generated by\ninfer_freq.\n\n`TimedeltaIndex.to_pytimedelta`(*args, **kwargs)\n\nReturn Timedelta Array/Index as object ndarray of datetime.timedelta objects.\n\n`TimedeltaIndex.to_series`([index, name])\n\nCreate a Series with both index and values equal to the index keys.\n\n`TimedeltaIndex.round`(*args, **kwargs)\n\nPerform round operation on the data to the specified freq.\n\n`TimedeltaIndex.floor`(*args, **kwargs)\n\nPerform floor operation on the data to the specified freq.\n\n`TimedeltaIndex.ceil`(*args, **kwargs)\n\nPerform ceil operation on the data to the specified freq.\n\n`TimedeltaIndex.to_frame`([index, name])\n\nCreate a DataFrame with a column containing the Index.\n\n`TimedeltaIndex.mean`(*args, **kwargs)\n\nReturn the mean value of the Array.\n\n`PeriodIndex`([data, ordinal, freq, dtype, ...])\n\nImmutable ndarray holding ordinal values indicating regular periods in time.\n\n`PeriodIndex.day`\n\nThe days of the period.\n\n`PeriodIndex.dayofweek`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`PeriodIndex.day_of_week`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`PeriodIndex.dayofyear`\n\nThe ordinal day of the year.\n\n`PeriodIndex.day_of_year`\n\nThe ordinal day of the year.\n\n`PeriodIndex.days_in_month`\n\nThe number of days in the month.\n\n`PeriodIndex.daysinmonth`\n\nThe number of days in the month.\n\n`PeriodIndex.end_time`\n\n`PeriodIndex.freq`\n\nReturn the frequency object if it is set, otherwise None.\n\n`PeriodIndex.freqstr`\n\nReturn the frequency object as a string if its set, otherwise None.\n\n`PeriodIndex.hour`\n\nThe hour of the period.\n\n`PeriodIndex.is_leap_year`\n\nLogical indicating if the date belongs to a leap year.\n\n`PeriodIndex.minute`\n\nThe minute of the period.\n\n`PeriodIndex.month`\n\nThe month as January=1, December=12.\n\n`PeriodIndex.quarter`\n\nThe quarter of the date.\n\n`PeriodIndex.qyear`\n\n`PeriodIndex.second`\n\nThe second of the period.\n\n`PeriodIndex.start_time`\n\n`PeriodIndex.week`\n\nThe week ordinal of the year.\n\n`PeriodIndex.weekday`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`PeriodIndex.weekofyear`\n\nThe week ordinal of the year.\n\n`PeriodIndex.year`\n\nThe year of the period.\n\n`PeriodIndex.asfreq`([freq, how])\n\nConvert the PeriodArray to the specified frequency freq.\n\n`PeriodIndex.strftime`(*args, **kwargs)\n\nConvert to Index using specified date_format.\n\n`PeriodIndex.to_timestamp`([freq, how])\n\nCast to DatetimeArray/Index.\n\n"}, {"name": "Indexing and selecting data", "path": "user_guide/indexing", "type": "Manual", "text": "\nThe axis labeling information in pandas objects serves many purposes:\n\nIdentifies data (i.e. provides metadata) using known indicators, important for\nanalysis, visualization, and interactive console display.\n\nEnables automatic and explicit data alignment.\n\nAllows intuitive getting and setting of subsets of the data set.\n\nIn this section, we will focus on the final point: namely, how to slice, dice,\nand generally get and set subsets of pandas objects. The primary focus will be\non Series and DataFrame as they have received more development attention in\nthis area.\n\nNote\n\nThe Python and NumPy indexing operators `[]` and attribute operator `.`\nprovide quick and easy access to pandas data structures across a wide range of\nuse cases. This makes interactive work intuitive, as there\u2019s little new to\nlearn if you already know how to deal with Python dictionaries and NumPy\narrays. However, since the type of the data to be accessed isn\u2019t known in\nadvance, directly using standard operators has some optimization limits. For\nproduction code, we recommended that you take advantage of the optimized\npandas data access methods exposed in this chapter.\n\nWarning\n\nWhether a copy or a reference is returned for a setting operation, may depend\non the context. This is sometimes called `chained assignment` and should be\navoided. See Returning a View versus Copy.\n\nSee the MultiIndex / Advanced Indexing for `MultiIndex` and more advanced\nindexing documentation.\n\nSee the cookbook for some advanced strategies.\n\nObject selection has had a number of user-requested additions in order to\nsupport more explicit location based indexing. pandas now supports three types\nof multi-axis indexing.\n\n`.loc` is primarily label based, but may also be used with a boolean array.\n`.loc` will raise `KeyError` when the items are not found. Allowed inputs are:\n\nA single label, e.g. `5` or `'a'` (Note that `5` is interpreted as a label of\nthe index. This use is not an integer position along the index.).\n\nA list or array of labels `['a', 'b', 'c']`.\n\nA slice object with labels `'a':'f'` (Note that contrary to usual Python\nslices, both the start and the stop are included, when present in the index!\nSee Slicing with labels and Endpoints are inclusive.)\n\nA boolean array (any `NA` values will be treated as `False`).\n\nA `callable` function with one argument (the calling Series or DataFrame) and\nthat returns valid output for indexing (one of the above).\n\nSee more at Selection by Label.\n\n`.iloc` is primarily integer position based (from `0` to `length-1` of the\naxis), but may also be used with a boolean array. `.iloc` will raise\n`IndexError` if a requested indexer is out-of-bounds, except slice indexers\nwhich allow out-of-bounds indexing. (this conforms with Python/NumPy slice\nsemantics). Allowed inputs are:\n\nAn integer e.g. `5`.\n\nA list or array of integers `[4, 3, 0]`.\n\nA slice object with ints `1:7`.\n\nA boolean array (any `NA` values will be treated as `False`).\n\nA `callable` function with one argument (the calling Series or DataFrame) and\nthat returns valid output for indexing (one of the above).\n\nSee more at Selection by Position, Advanced Indexing and Advanced\nHierarchical.\n\n`.loc`, `.iloc`, and also `[]` indexing can accept a `callable` as indexer.\nSee more at Selection By Callable.\n\nGetting values from an object with multi-axes selection uses the following\nnotation (using `.loc` as an example, but the following applies to `.iloc` as\nwell). Any of the axes accessors may be the null slice `:`. Axes left out of\nthe specification are assumed to be `:`, e.g. `p.loc['a']` is equivalent to\n`p.loc['a', :, :]`.\n\nObject Type\n\nIndexers\n\nSeries\n\n`s.loc[indexer]`\n\nDataFrame\n\n`df.loc[row_indexer,column_indexer]`\n\nAs mentioned when introducing the data structures in the last section, the\nprimary function of indexing with `[]` (a.k.a. `__getitem__` for those\nfamiliar with implementing class behavior in Python) is selecting out lower-\ndimensional slices. The following table shows return type values when indexing\npandas objects with `[]`:\n\nObject Type\n\nSelection\n\nReturn Value Type\n\nSeries\n\n`series[label]`\n\nscalar value\n\nDataFrame\n\n`frame[colname]`\n\n`Series` corresponding to colname\n\nHere we construct a simple time series data set to use for illustrating the\nindexing functionality:\n\nNote\n\nNone of the indexing functionality is time series specific unless specifically\nstated.\n\nThus, as per above, we have the most basic indexing using `[]`:\n\nYou can pass a list of columns to `[]` to select columns in that order. If a\ncolumn is not contained in the DataFrame, an exception will be raised.\nMultiple columns can also be set in this manner:\n\nYou may find this useful for applying a transform (in-place) to a subset of\nthe columns.\n\nWarning\n\npandas aligns all AXES when setting `Series` and `DataFrame` from `.loc`, and\n`.iloc`.\n\nThis will not modify `df` because the column alignment is before value\nassignment.\n\nThe correct way to swap column values is by using raw values:\n\nYou may access an index on a `Series` or column on a `DataFrame` directly as\nan attribute:\n\nWarning\n\nYou can use this access only if the index element is a valid Python\nidentifier, e.g. `s.1` is not allowed. See here for an explanation of valid\nidentifiers.\n\nThe attribute will not be available if it conflicts with an existing method\nname, e.g. `s.min` is not allowed, but `s['min']` is possible.\n\nSimilarly, the attribute will not be available if it conflicts with any of the\nfollowing list: `index`, `major_axis`, `minor_axis`, `items`.\n\nIn any of these cases, standard indexing will still work, e.g. `s['1']`,\n`s['min']`, and `s['index']` will access the corresponding element or column.\n\nIf you are using the IPython environment, you may also use tab-completion to\nsee these accessible attributes.\n\nYou can also assign a `dict` to a row of a `DataFrame`:\n\nYou can use attribute access to modify an existing element of a Series or\ncolumn of a DataFrame, but be careful; if you try to use attribute access to\ncreate a new column, it creates a new attribute rather than a new column. In\n0.21.0 and later, this will raise a `UserWarning`:\n\nThe most robust and consistent way of slicing ranges along arbitrary axes is\ndescribed in the Selection by Position section detailing the `.iloc` method.\nFor now, we explain the semantics of slicing using the `[]` operator.\n\nWith Series, the syntax works exactly as with an ndarray, returning a slice of\nthe values and the corresponding labels:\n\nNote that setting works as well:\n\nWith DataFrame, slicing inside of `[]` slices the rows. This is provided\nlargely as a convenience since it is such a common operation.\n\nWarning\n\nWhether a copy or a reference is returned for a setting operation, may depend\non the context. This is sometimes called `chained assignment` and should be\navoided. See Returning a View versus Copy.\n\nWarning\n\n`.loc` is strict when you present slicers that are not compatible (or\nconvertible) with the index type. For example using integers in a\n`DatetimeIndex`. These will raise a `TypeError`.\n\nString likes in slicing can be convertible to the type of the index and lead\nto natural slicing.\n\nWarning\n\nChanged in version 1.0.0.\n\npandas will raise a `KeyError` if indexing with a list with missing labels.\nSee list-like Using loc with missing keys in a list is Deprecated.\n\npandas provides a suite of methods in order to have purely label based\nindexing. This is a strict inclusion based protocol. Every label asked for\nmust be in the index, or a `KeyError` will be raised. When slicing, both the\nstart bound AND the stop bound are included, if present in the index. Integers\nare valid labels, but they refer to the label and not the position.\n\nThe `.loc` attribute is the primary access method. The following are valid\ninputs:\n\nA single label, e.g. `5` or `'a'` (Note that `5` is interpreted as a label of\nthe index. This use is not an integer position along the index.).\n\nA list or array of labels `['a', 'b', 'c']`.\n\nA slice object with labels `'a':'f'` (Note that contrary to usual Python\nslices, both the start and the stop are included, when present in the index!\nSee Slicing with labels.\n\nA boolean array.\n\nA `callable`, see Selection By Callable.\n\nNote that setting works as well:\n\nWith a DataFrame:\n\nAccessing via label slices:\n\nFor getting a cross section using a label (equivalent to `df.xs('a')`):\n\nFor getting values with a boolean array:\n\nNA values in a boolean array propagate as `False`:\n\nChanged in version 1.0.2.\n\nFor getting a value explicitly:\n\nWhen using `.loc` with slices, if both the start and the stop labels are\npresent in the index, then elements located between the two (including them)\nare returned:\n\nIf at least one of the two is absent, but the index is sorted, and can be\ncompared against start and stop labels, then slicing will still work as\nexpected, by selecting labels which rank between the two:\n\nHowever, if at least one of the two is absent and the index is not sorted, an\nerror will be raised (since doing otherwise would be computationally\nexpensive, as well as potentially ambiguous for mixed type indexes). For\ninstance, in the above example, `s.loc[1:6]` would raise `KeyError`.\n\nFor the rationale behind this behavior, see Endpoints are inclusive.\n\nAlso, if the index has duplicate labels and either the start or the stop label\nis duplicated, an error will be raised. For instance, in the above example,\n`s.loc[2:5]` would raise a `KeyError`.\n\nFor more information about duplicate labels, see Duplicate Labels.\n\nWarning\n\nWhether a copy or a reference is returned for a setting operation, may depend\non the context. This is sometimes called `chained assignment` and should be\navoided. See Returning a View versus Copy.\n\npandas provides a suite of methods in order to get purely integer based\nindexing. The semantics follow closely Python and NumPy slicing. These are\n`0-based` indexing. When slicing, the start bound is included, while the upper\nbound is excluded. Trying to use a non-integer, even a valid label will raise\nan `IndexError`.\n\nThe `.iloc` attribute is the primary access method. The following are valid\ninputs:\n\nAn integer e.g. `5`.\n\nA list or array of integers `[4, 3, 0]`.\n\nA slice object with ints `1:7`.\n\nA boolean array.\n\nA `callable`, see Selection By Callable.\n\nNote that setting works as well:\n\nWith a DataFrame:\n\nSelect via integer slicing:\n\nSelect via integer list:\n\nFor getting a cross section using an integer position (equiv to `df.xs(1)`):\n\nOut of range slice indexes are handled gracefully just as in Python/NumPy.\n\nNote that using slices that go out of bounds can result in an empty axis (e.g.\nan empty DataFrame being returned).\n\nA single indexer that is out of bounds will raise an `IndexError`. A list of\nindexers where any element is out of bounds will raise an `IndexError`.\n\n`.loc`, `.iloc`, and also `[]` indexing can accept a `callable` as indexer.\nThe `callable` must be a function with one argument (the calling Series or\nDataFrame) that returns valid output for indexing.\n\nYou can use callable indexing in `Series`.\n\nUsing these methods / indexers, you can chain data selection operations\nwithout using a temporary variable.\n\nIf you wish to get the 0th and the 2nd elements from the index in the \u2018A\u2019\ncolumn, you can do:\n\nThis can also be expressed using `.iloc`, by explicitly getting locations on\nthe indexers, and using positional indexing to select things.\n\nFor getting multiple indexers, using `.get_indexer`:\n\nWarning\n\nChanged in version 1.0.0.\n\nUsing `.loc` or `[]` with a list with one or more missing labels will no\nlonger reindex, in favor of `.reindex`.\n\nIn prior versions, using `.loc[list-of-labels]` would work as long as at least\n1 of the keys was found (otherwise it would raise a `KeyError`). This behavior\nwas changed and will now raise a `KeyError` if at least one label is missing.\nThe recommended alternative is to use `.reindex()`.\n\nFor example.\n\nSelection with all keys found is unchanged.\n\nPrevious behavior\n\nCurrent behavior\n\nThe idiomatic way to achieve selecting potentially not-found elements is via\n`.reindex()`. See also the section on reindexing.\n\nAlternatively, if you want to select only valid keys, the following is\nidiomatic and efficient; it is guaranteed to preserve the dtype of the\nselection.\n\nHaving a duplicated index will raise for a `.reindex()`:\n\nGenerally, you can intersect the desired labels with the current axis, and\nthen reindex.\n\nHowever, this would still raise if your resulting index is duplicated.\n\nA random selection of rows or columns from a Series or DataFrame with the\n`sample()` method. The method will sample rows by default, and accepts a\nspecific number of rows/columns to return, or a fraction of rows.\n\nBy default, `sample` will return each row at most once, but one can also\nsample with replacement using the `replace` option:\n\nBy default, each row has an equal probability of being selected, but if you\nwant rows to have different probabilities, you can pass the `sample` function\nsampling weights as `weights`. These weights can be a list, a NumPy array, or\na Series, but they must be of the same length as the object you are sampling.\nMissing values will be treated as a weight of zero, and inf values are not\nallowed. If weights do not sum to 1, they will be re-normalized by dividing\nall weights by the sum of the weights. For example:\n\nWhen applied to a DataFrame, you can use a column of the DataFrame as sampling\nweights (provided you are sampling rows and not columns) by simply passing the\nname of the column as a string.\n\n`sample` also allows users to sample columns instead of rows using the `axis`\nargument.\n\nFinally, one can also set a seed for `sample`\u2019s random number generator using\nthe `random_state` argument, which will accept either an integer (as a seed)\nor a NumPy RandomState object.\n\nThe `.loc/[]` operations can perform enlargement when setting a non-existent\nkey for that axis.\n\nIn the `Series` case this is effectively an appending operation.\n\nA `DataFrame` can be enlarged on either axis via `.loc`.\n\nThis is like an `append` operation on the `DataFrame`.\n\nSince indexing with `[]` must handle a lot of cases (single-label access,\nslicing, boolean indexing, etc.), it has a bit of overhead in order to figure\nout what you\u2019re asking for. If you only want to access a scalar value, the\nfastest way is to use the `at` and `iat` methods, which are implemented on all\nof the data structures.\n\nSimilarly to `loc`, `at` provides label based scalar lookups, while, `iat`\nprovides integer based lookups analogously to `iloc`\n\nYou can also set using these same indexers.\n\n`at` may enlarge the object in-place as above if the indexer is missing.\n\nAnother common operation is the use of boolean vectors to filter the data. The\noperators are: `|` for `or`, `&` for `and`, and `~` for `not`. These must be\ngrouped by using parentheses, since by default Python will evaluate an\nexpression such as `df['A'] > 2 & df['B'] < 3` as `df['A'] > (2 & df['B']) <\n3`, while the desired evaluation order is `(df['A'] > 2) & (df['B'] < 3)`.\n\nUsing a boolean vector to index a Series works exactly as in a NumPy ndarray:\n\nYou may select rows from a DataFrame using a boolean vector the same length as\nthe DataFrame\u2019s index (for example, something derived from one of the columns\nof the DataFrame):\n\nList comprehensions and the `map` method of Series can also be used to produce\nmore complex criteria:\n\nWith the choice methods Selection by Label, Selection by Position, and\nAdvanced Indexing you may select along more than one axis using boolean\nvectors combined with other indexing expressions.\n\nWarning\n\n`iloc` supports two kinds of boolean indexing. If the indexer is a boolean\n`Series`, an error will be raised. For instance, in the following example,\n`df.iloc[s.values, 1]` is ok. The boolean indexer is an array. But `df.iloc[s,\n1]` would raise `ValueError`.\n\nConsider the `isin()` method of `Series`, which returns a boolean vector that\nis true wherever the `Series` elements exist in the passed list. This allows\nyou to select rows where one or more columns have values you want:\n\nThe same method is available for `Index` objects and is useful for the cases\nwhen you don\u2019t know which of the sought labels are in fact present:\n\nIn addition to that, `MultiIndex` allows selecting a separate level to use in\nthe membership check:\n\nDataFrame also has an `isin()` method. When calling `isin`, pass a set of\nvalues as either an array or dict. If values is an array, `isin` returns a\nDataFrame of booleans that is the same shape as the original DataFrame, with\nTrue wherever the element is in the sequence of values.\n\nOftentimes you\u2019ll want to match certain values with certain columns. Just make\nvalues a `dict` where the key is the column, and the value is a list of items\nyou want to check for.\n\nTo return the DataFrame of booleans where the values are not in the original\nDataFrame, use the `~` operator:\n\nCombine DataFrame\u2019s `isin` with the `any()` and `all()` methods to quickly\nselect subsets of your data that meet a given criteria. To select a row where\neach column meets its own criterion:\n\nSelecting values from a Series with a boolean vector generally returns a\nsubset of the data. To guarantee that selection output has the same shape as\nthe original data, you can use the `where` method in `Series` and `DataFrame`.\n\nTo return only the selected rows:\n\nTo return a Series of the same shape as the original:\n\nSelecting values from a DataFrame with a boolean criterion now also preserves\ninput data shape. `where` is used under the hood as the implementation. The\ncode below is equivalent to `df.where(df < 0)`.\n\nIn addition, `where` takes an optional `other` argument for replacement of\nvalues where the condition is False, in the returned copy.\n\nYou may wish to set values based on some boolean criteria. This can be done\nintuitively like so:\n\nBy default, `where` returns a modified copy of the data. There is an optional\nparameter `inplace` so that the original data can be modified without creating\na copy:\n\nNote\n\nThe signature for `DataFrame.where()` differs from `numpy.where()`. Roughly\n`df1.where(m, df2)` is equivalent to `np.where(m, df1, df2)`.\n\nAlignment\n\nFurthermore, `where` aligns the input boolean condition (ndarray or\nDataFrame), such that partial selection with setting is possible. This is\nanalogous to partial setting via `.loc` (but on the contents rather than the\naxis labels).\n\nWhere can also accept `axis` and `level` parameters to align the input when\nperforming the `where`.\n\nThis is equivalent to (but faster than) the following.\n\n`where` can accept a callable as condition and `other` arguments. The function\nmust be with one argument (the calling Series or DataFrame) and that returns\nvalid output as condition and `other` argument.\n\n`mask()` is the inverse boolean operation of `where`.\n\nAn alternative to `where()` is to use `numpy.where()`. Combined with setting a\nnew column, you can use it to enlarge a DataFrame where the values are\ndetermined conditionally.\n\nConsider you have two choices to choose from in the following DataFrame. And\nyou want to set a new column color to \u2018green\u2019 when the second column has \u2018Z\u2019.\nYou can do the following:\n\nIf you have multiple conditions, you can use `numpy.select()` to achieve that.\nSay corresponding to three conditions there are three choice of colors, with a\nfourth color as a fallback, you can do the following.\n\n`DataFrame` objects have a `query()` method that allows selection using an\nexpression.\n\nYou can get the value of the frame where column `b` has values between the\nvalues of columns `a` and `c`. For example:\n\nDo the same thing but fall back on a named index if there is no column with\nthe name `a`.\n\nIf instead you don\u2019t want to or cannot name your index, you can use the name\n`index` in your query expression:\n\nNote\n\nIf the name of your index overlaps with a column name, the column name is\ngiven precedence. For example,\n\nYou can still use the index in a query expression by using the special\nidentifier \u2018index\u2019:\n\nIf for some reason you have a column named `index`, then you can refer to the\nindex as `ilevel_0` as well, but at this point you should consider renaming\nyour columns to something less ambiguous.\n\nYou can also use the levels of a `DataFrame` with a `MultiIndex` as if they\nwere columns in the frame:\n\nIf the levels of the `MultiIndex` are unnamed, you can refer to them using\nspecial names:\n\nThe convention is `ilevel_0`, which means \u201cindex level 0\u201d for the 0th level of\nthe `index`.\n\nA use case for `query()` is when you have a collection of `DataFrame` objects\nthat have a subset of column names (or index levels/names) in common. You can\npass the same query to both frames without having to specify which frame\nyou\u2019re interested in querying\n\nFull numpy-like syntax:\n\nSlightly nicer by removing the parentheses (comparison operators bind tighter\nthan `&` and `|`):\n\nUse English instead of symbols:\n\nPretty close to how you might write it on paper:\n\n`query()` also supports special use of Python\u2019s `in` and `not in` comparison\noperators, providing a succinct syntax for calling the `isin` method of a\n`Series` or `DataFrame`.\n\nYou can combine this with other expressions for very succinct queries:\n\nNote\n\nNote that `in` and `not in` are evaluated in Python, since `numexpr` has no\nequivalent of this operation. However, only the `in`/`not in` expression\nitself is evaluated in vanilla Python. For example, in the expression\n\n`(b + c + d)` is evaluated by `numexpr` and then the `in` operation is\nevaluated in plain Python. In general, any operations that can be evaluated\nusing `numexpr` will be.\n\nComparing a `list` of values to a column using `==`/`!=` works similarly to\n`in`/`not in`.\n\nYou can negate boolean expressions with the word `not` or the `~` operator.\n\nOf course, expressions can be arbitrarily complex too:\n\n`DataFrame.query()` using `numexpr` is slightly faster than Python for large\nframes.\n\nNote\n\nYou will only see the performance benefits of using the `numexpr` engine with\n`DataFrame.query()` if your frame has more than approximately 200,000 rows.\n\nThis plot was created using a `DataFrame` with 3 columns each containing\nfloating point values generated using `numpy.random.randn()`.\n\nIf you want to identify and remove duplicate rows in a DataFrame, there are\ntwo methods that will help: `duplicated` and `drop_duplicates`. Each takes as\nan argument the columns to use to identify duplicated rows.\n\n`duplicated` returns a boolean vector whose length is the number of rows, and\nwhich indicates whether a row is duplicated.\n\n`drop_duplicates` removes duplicate rows.\n\nBy default, the first observed row of a duplicate set is considered unique,\nbut each method has a `keep` parameter to specify targets to be kept.\n\n`keep='first'` (default): mark / drop duplicates except for the first\noccurrence.\n\n`keep='last'`: mark / drop duplicates except for the last occurrence.\n\n`keep=False`: mark / drop all duplicates.\n\nAlso, you can pass a list of columns to identify duplications.\n\nTo drop duplicates by index value, use `Index.duplicated` then perform\nslicing. The same set of options are available for the `keep` parameter.\n\nEach of Series or DataFrame have a `get` method which can return a default\nvalue.\n\nSometimes you want to extract a set of values given a sequence of row labels\nand column labels, this can be achieved by `pandas.factorize` and NumPy\nindexing. For instance:\n\nFormerly this could be achieved with the dedicated `DataFrame.lookup` method\nwhich was deprecated in version 1.2.0.\n\nThe pandas `Index` class and its subclasses can be viewed as implementing an\nordered multiset. Duplicates are allowed. However, if you try to convert an\n`Index` object with duplicate entries into a `set`, an exception will be\nraised.\n\n`Index` also provides the infrastructure necessary for lookups, data\nalignment, and reindexing. The easiest way to create an `Index` directly is to\npass a `list` or other sequence to `Index`:\n\nYou can also pass a `name` to be stored in the index:\n\nThe name, if set, will be shown in the console display:\n\nIndexes are \u201cmostly immutable\u201d, but it is possible to set and change their\n`name` attribute. You can use the `rename`, `set_names` to set these\nattributes directly, and they default to returning a copy.\n\nSee Advanced Indexing for usage of MultiIndexes.\n\n`set_names`, `set_levels`, and `set_codes` also take an optional `level`\nargument\n\nThe two main operations are `union` and `intersection`. Difference is provided\nvia the `.difference()` method.\n\nAlso available is the `symmetric_difference` operation, which returns elements\nthat appear in either `idx1` or `idx2`, but not in both. This is equivalent to\nthe Index created by `idx1.difference(idx2).union(idx2.difference(idx1))`,\nwith duplicates dropped.\n\nNote\n\nThe resulting index from a set operation will be sorted in ascending order.\n\nWhen performing `Index.union()` between indexes with different dtypes, the\nindexes must be cast to a common dtype. Typically, though not always, this is\nobject dtype. The exception is when performing a union between integer and\nfloat data. In this case, the integer values are converted to float\n\nImportant\n\nEven though `Index` can hold missing values (`NaN`), it should be avoided if\nyou do not want any unexpected results. For example, some operations exclude\nmissing values implicitly.\n\n`Index.fillna` fills missing values with specified scalar value.\n\nOccasionally you will load or create a data set into a DataFrame and want to\nadd an index after you\u2019ve already done so. There are a couple of different\nways.\n\nDataFrame has a `set_index()` method which takes a column name (for a regular\n`Index`) or a list of column names (for a `MultiIndex`). To create a new, re-\nindexed DataFrame:\n\nThe `append` keyword option allow you to keep the existing index and append\nthe given columns to a MultiIndex:\n\nOther options in `set_index` allow you not drop the index columns or to add\nthe index in-place (without creating a new object):\n\nAs a convenience, there is a new function on DataFrame called `reset_index()`\nwhich transfers the index values into the DataFrame\u2019s columns and sets a\nsimple integer index. This is the inverse operation of `set_index()`.\n\nThe output is more similar to a SQL table or a record array. The names for the\ncolumns derived from the index are the ones stored in the `names` attribute.\n\nYou can use the `level` keyword to remove only a portion of the index:\n\n`reset_index` takes an optional parameter `drop` which if true simply discards\nthe index, instead of putting index values in the DataFrame\u2019s columns.\n\nIf you create an index yourself, you can just assign it to the `index` field:\n\nWhen setting values in a pandas object, care must be taken to avoid what is\ncalled `chained indexing`. Here is an example.\n\nCompare these two access methods:\n\nThese both yield the same results, so which should you use? It is instructive\nto understand the order of operations on these and why method 2 (`.loc`) is\nmuch preferred over method 1 (chained `[]`).\n\n`dfmi['one']` selects the first level of the columns and returns a DataFrame\nthat is singly-indexed. Then another Python operation\n`dfmi_with_one['second']` selects the series indexed by `'second'`. This is\nindicated by the variable `dfmi_with_one` because pandas sees these operations\nas separate events. e.g. separate calls to `__getitem__`, so it has to treat\nthem as linear operations, they happen one after another.\n\nContrast this to `df.loc[:,('one','second')]` which passes a nested tuple of\n`(slice(None),('one','second'))` to a single call to `__getitem__`. This\nallows pandas to deal with this as a single entity. Furthermore this order of\noperations can be significantly faster, and allows one to index both axes if\nso desired.\n\nThe problem in the previous section is just a performance issue. What\u2019s up\nwith the `SettingWithCopy` warning? We don\u2019t usually throw warnings around\nwhen you do something that might cost a few extra milliseconds!\n\nBut it turns out that assigning to the product of chained indexing has\ninherently unpredictable results. To see this, think about how the Python\ninterpreter executes this code:\n\nBut this code is handled differently:\n\nSee that `__getitem__` in there? Outside of simple cases, it\u2019s very hard to\npredict whether it will return a view or a copy (it depends on the memory\nlayout of the array, about which pandas makes no guarantees), and therefore\nwhether the `__setitem__` will modify `dfmi` or a temporary object that gets\nthrown out immediately afterward. That\u2019s what `SettingWithCopy` is warning you\nabout!\n\nNote\n\nYou may be wondering whether we should be concerned about the `loc` property\nin the first example. But `dfmi.loc` is guaranteed to be `dfmi` itself with\nmodified indexing behavior, so `dfmi.loc.__getitem__` / `dfmi.loc.__setitem__`\noperate on `dfmi` directly. Of course, `dfmi.loc.__getitem__(idx)` may be a\nview or a copy of `dfmi`.\n\nSometimes a `SettingWithCopy` warning will arise at times when there\u2019s no\nobvious chained indexing going on. These are the bugs that `SettingWithCopy`\nis designed to catch! pandas is probably trying to warn you that you\u2019ve done\nthis:\n\nYikes!\n\nWhen you use chained indexing, the order and type of the indexing operation\npartially determine whether the result is a slice into the original object, or\na copy of the slice.\n\npandas has the `SettingWithCopyWarning` because assigning to a copy of a slice\nis frequently not intentional, but a mistake caused by chained indexing\nreturning a copy where a slice was expected.\n\nIf you would like pandas to be more or less trusting about assignment to a\nchained indexing expression, you can set the option `mode.chained_assignment`\nto one of these values:\n\n`'warn'`, the default, means a `SettingWithCopyWarning` is printed.\n\n`'raise'` means pandas will raise a `SettingWithCopyException` you have to\ndeal with.\n\n`None` will suppress the warnings entirely.\n\nThis however is operating on a copy and will not work.\n\nA chained assignment can also crop up in setting in a mixed dtype frame.\n\nNote\n\nThese setting rules apply to all of `.loc/.iloc`.\n\nThe following is the recommended access method using `.loc` for multiple items\n(using `mask`) and a single item using a fixed index:\n\nThe following can work at times, but it is not guaranteed to, and therefore\nshould be avoided:\n\nLast, the subsequent example will not work at all, and so should be avoided:\n\nWarning\n\nThe chained assignment warnings / exceptions are aiming to inform the user of\na possibly invalid assignment. There may be false positives; situations where\na chained assignment is inadvertently reported.\n\n"}, {"name": "Input/output", "path": "reference/io", "type": "Input/output", "text": "\n`read_pickle`(filepath_or_buffer[, ...])\n\nLoad pickled pandas object (or any object) from file.\n\n`DataFrame.to_pickle`(path[, compression, ...])\n\nPickle (serialize) object to file.\n\n`read_table`(filepath_or_buffer[, sep, ...])\n\nRead general delimited file into DataFrame.\n\n`read_csv`(filepath_or_buffer[, sep, ...])\n\nRead a comma-separated values (csv) file into DataFrame.\n\n`DataFrame.to_csv`([path_or_buf, sep, na_rep, ...])\n\nWrite object to a comma-separated values (csv) file.\n\n`read_fwf`(filepath_or_buffer[, colspecs, ...])\n\nRead a table of fixed-width formatted lines into DataFrame.\n\n`read_clipboard`([sep])\n\nRead text from clipboard and pass to read_csv.\n\n`DataFrame.to_clipboard`([excel, sep])\n\nCopy object to the system clipboard.\n\n`read_excel`(io[, sheet_name, header, names, ...])\n\nRead an Excel file into a pandas DataFrame.\n\n`DataFrame.to_excel`(excel_writer[, ...])\n\nWrite object to an Excel sheet.\n\n`ExcelFile.parse`([sheet_name, header, names, ...])\n\nParse specified sheet(s) into a DataFrame.\n\n`Styler.to_excel`(excel_writer[, sheet_name, ...])\n\nWrite Styler to an Excel sheet.\n\n`ExcelWriter`(path[, engine, date_format, ...])\n\nClass for writing DataFrame objects into excel sheets.\n\n`read_json`([path_or_buf, orient, typ, dtype, ...])\n\nConvert a JSON string to pandas object.\n\n`json_normalize`(data[, record_path, meta, ...])\n\nNormalize semi-structured JSON data into a flat table.\n\n`DataFrame.to_json`([path_or_buf, orient, ...])\n\nConvert the object to a JSON string.\n\n`build_table_schema`(data[, index, ...])\n\nCreate a Table schema from `data`.\n\n`read_html`(io[, match, flavor, header, ...])\n\nRead HTML tables into a `list` of `DataFrame` objects.\n\n`DataFrame.to_html`([buf, columns, col_space, ...])\n\nRender a DataFrame as an HTML table.\n\n`Styler.to_html`([buf, table_uuid, ...])\n\nWrite Styler to a file, buffer or string in HTML-CSS format.\n\n`read_xml`(path_or_buffer[, xpath, ...])\n\nRead XML document into a `DataFrame` object.\n\n`DataFrame.to_xml`([path_or_buffer, index, ...])\n\nRender a DataFrame to an XML document.\n\n`DataFrame.to_latex`([buf, columns, ...])\n\nRender object to a LaTeX tabular, longtable, or nested table.\n\n`Styler.to_latex`([buf, column_format, ...])\n\nWrite Styler to a file, buffer or string in LaTeX format.\n\n`read_hdf`(path_or_buf[, key, mode, errors, ...])\n\nRead from the store, close it if we opened it.\n\n`HDFStore.put`(key, value[, format, index, ...])\n\nStore object in HDFStore.\n\n`HDFStore.append`(key, value[, format, axes, ...])\n\nAppend to Table in file.\n\n`HDFStore.get`(key)\n\nRetrieve pandas object stored in file.\n\n`HDFStore.select`(key[, where, start, stop, ...])\n\nRetrieve pandas object stored in file, optionally based on where criteria.\n\n`HDFStore.info`()\n\nPrint detailed information on the store.\n\n`HDFStore.keys`([include])\n\nReturn a list of keys corresponding to objects stored in HDFStore.\n\n`HDFStore.groups`()\n\nReturn a list of all the top-level nodes.\n\n`HDFStore.walk`([where])\n\nWalk the pytables group hierarchy for pandas objects.\n\nWarning\n\nOne can store a subclass of `DataFrame` or `Series` to HDF5, but the type of\nthe subclass is lost upon storing.\n\n`read_feather`(path[, columns, use_threads, ...])\n\nLoad a feather-format object from the file path.\n\n`DataFrame.to_feather`(path, **kwargs)\n\nWrite a DataFrame to the binary Feather format.\n\n`read_parquet`(path[, engine, columns, ...])\n\nLoad a parquet object from the file path, returning a DataFrame.\n\n`DataFrame.to_parquet`([path, engine, ...])\n\nWrite a DataFrame to the binary parquet format.\n\n`read_orc`(path[, columns])\n\nLoad an ORC object from the file path, returning a DataFrame.\n\n`read_sas`(filepath_or_buffer[, format, ...])\n\nRead SAS files stored as either XPORT or SAS7BDAT format files.\n\n`read_spss`(path[, usecols, convert_categoricals])\n\nLoad an SPSS file from the file path, returning a DataFrame.\n\n`read_sql_table`(table_name, con[, schema, ...])\n\nRead SQL database table into a DataFrame.\n\n`read_sql_query`(sql, con[, index_col, ...])\n\nRead SQL query into a DataFrame.\n\n`read_sql`(sql, con[, index_col, ...])\n\nRead SQL query or database table into a DataFrame.\n\n`DataFrame.to_sql`(name, con[, schema, ...])\n\nWrite records stored in a DataFrame to a SQL database.\n\n`read_gbq`(query[, project_id, index_col, ...])\n\nLoad data from Google BigQuery.\n\n`read_stata`(filepath_or_buffer[, ...])\n\nRead Stata file into DataFrame.\n\n`DataFrame.to_stata`(path[, convert_dates, ...])\n\nExport DataFrame object to Stata dta format.\n\n`StataReader.data_label`\n\nReturn data label of Stata file.\n\n`StataReader.value_labels`()\n\nReturn a dict, associating each variable name a dict, associating each value\nits corresponding label.\n\n`StataReader.variable_labels`()\n\nReturn variable labels as a dict, associating each variable name with\ncorresponding label.\n\n`StataWriter.write_file`()\n\nExport DataFrame object to Stata dta format.\n\n"}, {"name": "Intro to data structures", "path": "user_guide/dsintro", "type": "Manual", "text": "\nWe\u2019ll start with a quick, non-comprehensive overview of the fundamental data\nstructures in pandas to get you started. The fundamental behavior about data\ntypes, indexing, and axis labeling / alignment apply across all of the\nobjects. To get started, import NumPy and load pandas into your namespace:\n\nHere is a basic tenet to keep in mind: data alignment is intrinsic. The link\nbetween labels and data will not be broken unless done so explicitly by you.\n\nWe\u2019ll give a brief intro to the data structures, then consider all of the\nbroad categories of functionality and methods in separate sections.\n\n`Series` is a one-dimensional labeled array capable of holding any data type\n(integers, strings, floating point numbers, Python objects, etc.). The axis\nlabels are collectively referred to as the index. The basic method to create a\nSeries is to call:\n\nHere, `data` can be many different things:\n\na Python dict\n\nan ndarray\n\na scalar value (like 5)\n\nThe passed index is a list of axis labels. Thus, this separates into a few\ncases depending on what data is:\n\nFrom ndarray\n\nIf `data` is an ndarray, index must be the same length as data. If no index is\npassed, one will be created having values `[0, ..., len(data) - 1]`.\n\nNote\n\npandas supports non-unique index values. If an operation that does not support\nduplicate index values is attempted, an exception will be raised at that time.\nThe reason for being lazy is nearly all performance-based (there are many\ninstances in computations, like parts of GroupBy, where the index is not\nused).\n\nFrom dict\n\nSeries can be instantiated from dicts:\n\nNote\n\nWhen the data is a dict, and an index is not passed, the `Series` index will\nbe ordered by the dict\u2019s insertion order, if you\u2019re using Python version >=\n3.6 and pandas version >= 0.23.\n\nIf you\u2019re using Python < 3.6 or pandas < 0.23, and an index is not passed, the\n`Series` index will be the lexically ordered list of dict keys.\n\nIn the example above, if you were on a Python version lower than 3.6 or a\npandas version lower than 0.23, the `Series` would be ordered by the lexical\norder of the dict keys (i.e. `['a', 'b', 'c']` rather than `['b', 'a', 'c']`).\n\nIf an index is passed, the values in data corresponding to the labels in the\nindex will be pulled out.\n\nNote\n\nNaN (not a number) is the standard missing data marker used in pandas.\n\nFrom scalar value\n\nIf `data` is a scalar value, an index must be provided. The value will be\nrepeated to match the length of index.\n\n`Series` acts very similarly to a `ndarray`, and is a valid argument to most\nNumPy functions. However, operations such as slicing will also slice the\nindex.\n\nNote\n\nWe will address array-based indexing like `s[[4, 3, 1]]` in section on\nindexing.\n\nLike a NumPy array, a pandas Series has a `dtype`.\n\nThis is often a NumPy dtype. However, pandas and 3rd-party libraries extend\nNumPy\u2019s type system in a few places, in which case the dtype would be an\n`ExtensionDtype`. Some examples within pandas are Categorical data and\nNullable integer data type. See dtypes for more.\n\nIf you need the actual array backing a `Series`, use `Series.array`.\n\nAccessing the array can be useful when you need to do some operation without\nthe index (to disable automatic alignment, for example).\n\n`Series.array` will always be an `ExtensionArray`. Briefly, an ExtensionArray\nis a thin wrapper around one or more concrete arrays like a `numpy.ndarray`.\npandas knows how to take an `ExtensionArray` and store it in a `Series` or a\ncolumn of a `DataFrame`. See dtypes for more.\n\nWhile Series is ndarray-like, if you need an actual ndarray, then use\n`Series.to_numpy()`.\n\nEven if the Series is backed by a `ExtensionArray`, `Series.to_numpy()` will\nreturn a NumPy ndarray.\n\nA Series is like a fixed-size dict in that you can get and set values by index\nlabel:\n\nIf a label is not contained, an exception is raised:\n\nUsing the `get` method, a missing label will return None or specified default:\n\nSee also the section on attribute access.\n\nWhen working with raw NumPy arrays, looping through value-by-value is usually\nnot necessary. The same is true when working with Series in pandas. Series can\nalso be passed into most NumPy methods expecting an ndarray.\n\nA key difference between Series and ndarray is that operations between Series\nautomatically align the data based on label. Thus, you can write computations\nwithout giving consideration to whether the Series involved have the same\nlabels.\n\nThe result of an operation between unaligned Series will have the union of the\nindexes involved. If a label is not found in one Series or the other, the\nresult will be marked as missing `NaN`. Being able to write code without doing\nany explicit data alignment grants immense freedom and flexibility in\ninteractive data analysis and research. The integrated data alignment features\nof the pandas data structures set pandas apart from the majority of related\ntools for working with labeled data.\n\nNote\n\nIn general, we chose to make the default result of operations between\ndifferently indexed objects yield the union of the indexes in order to avoid\nloss of information. Having an index label, though the data is missing, is\ntypically important information as part of a computation. You of course have\nthe option of dropping labels with missing data via the dropna function.\n\nSeries can also have a `name` attribute:\n\nThe Series `name` will be assigned automatically in many cases, in particular\nwhen taking 1D slices of DataFrame as you will see below.\n\nYou can rename a Series with the `pandas.Series.rename()` method.\n\nNote that `s` and `s2` refer to different objects.\n\nDataFrame is a 2-dimensional labeled data structure with columns of\npotentially different types. You can think of it like a spreadsheet or SQL\ntable, or a dict of Series objects. It is generally the most commonly used\npandas object. Like Series, DataFrame accepts many different kinds of input:\n\nDict of 1D ndarrays, lists, dicts, or Series\n\n2-D numpy.ndarray\n\nStructured or record ndarray\n\nA `Series`\n\nAnother `DataFrame`\n\nAlong with the data, you can optionally pass index (row labels) and columns\n(column labels) arguments. If you pass an index and / or columns, you are\nguaranteeing the index and / or columns of the resulting DataFrame. Thus, a\ndict of Series plus a specific index will discard all data not matching up to\nthe passed index.\n\nIf axis labels are not passed, they will be constructed from the input data\nbased on common sense rules.\n\nNote\n\nWhen the data is a dict, and `columns` is not specified, the `DataFrame`\ncolumns will be ordered by the dict\u2019s insertion order, if you are using Python\nversion >= 3.6 and pandas >= 0.23.\n\nIf you are using Python < 3.6 or pandas < 0.23, and `columns` is not\nspecified, the `DataFrame` columns will be the lexically ordered list of dict\nkeys.\n\nThe resulting index will be the union of the indexes of the various Series. If\nthere are any nested dicts, these will first be converted to Series. If no\ncolumns are passed, the columns will be the ordered list of dict keys.\n\nThe row and column labels can be accessed respectively by accessing the index\nand columns attributes:\n\nNote\n\nWhen a particular set of columns is passed along with a dict of data, the\npassed columns override the keys in the dict.\n\nThe ndarrays must all be the same length. If an index is passed, it must\nclearly also be the same length as the arrays. If no index is passed, the\nresult will be `range(n)`, where `n` is the array length.\n\nThis case is handled identically to a dict of arrays.\n\nNote\n\nDataFrame is not intended to work exactly like a 2-dimensional NumPy ndarray.\n\nYou can automatically create a MultiIndexed frame by passing a tuples\ndictionary.\n\nThe result will be a DataFrame with the same index as the input Series, and\nwith one column whose name is the original name of the Series (only if no\nother column name provided).\n\nThe field names of the first `namedtuple` in the list determine the columns of\nthe `DataFrame`. The remaining namedtuples (or tuples) are simply unpacked and\ntheir values are fed into the rows of the `DataFrame`. If any of those tuples\nis shorter than the first `namedtuple` then the later columns in the\ncorresponding row are marked as missing values. If any are longer than the\nfirst `namedtuple`, a `ValueError` is raised.\n\nNew in version 1.1.0.\n\nData Classes as introduced in PEP557, can be passed into the DataFrame\nconstructor. Passing a list of dataclasses is equivalent to passing a list of\ndictionaries.\n\nPlease be aware, that all values in the list should be dataclasses, mixing\ntypes in the list would result in a TypeError.\n\nMissing data\n\nMuch more will be said on this topic in the Missing data section. To construct\na DataFrame with missing data, we use `np.nan` to represent missing values.\nAlternatively, you may pass a `numpy.MaskedArray` as the data argument to the\nDataFrame constructor, and its masked entries will be considered missing.\n\nDataFrame.from_dict\n\n`DataFrame.from_dict` takes a dict of dicts or a dict of array-like sequences\nand returns a DataFrame. It operates like the `DataFrame` constructor except\nfor the `orient` parameter which is `'columns'` by default, but which can be\nset to `'index'` in order to use the dict keys as row labels.\n\nIf you pass `orient='index'`, the keys will be the row labels. In this case,\nyou can also pass the desired column names:\n\nDataFrame.from_records\n\n`DataFrame.from_records` takes a list of tuples or an ndarray with structured\ndtype. It works analogously to the normal `DataFrame` constructor, except that\nthe resulting DataFrame index may be a specific field of the structured dtype.\nFor example:\n\nYou can treat a DataFrame semantically like a dict of like-indexed Series\nobjects. Getting, setting, and deleting columns works with the same syntax as\nthe analogous dict operations:\n\nColumns can be deleted or popped like with a dict:\n\nWhen inserting a scalar value, it will naturally be propagated to fill the\ncolumn:\n\nWhen inserting a Series that does not have the same index as the DataFrame, it\nwill be conformed to the DataFrame\u2019s index:\n\nYou can insert raw ndarrays but their length must match the length of the\nDataFrame\u2019s index.\n\nBy default, columns get inserted at the end. The `insert` function is\navailable to insert at a particular location in the columns:\n\nInspired by dplyr\u2019s `mutate` verb, DataFrame has an `assign()` method that\nallows you to easily create new columns that are potentially derived from\nexisting columns.\n\nIn the example above, we inserted a precomputed value. We can also pass in a\nfunction of one argument to be evaluated on the DataFrame being assigned to.\n\n`assign` always returns a copy of the data, leaving the original DataFrame\nuntouched.\n\nPassing a callable, as opposed to an actual value to be inserted, is useful\nwhen you don\u2019t have a reference to the DataFrame at hand. This is common when\nusing `assign` in a chain of operations. For example, we can limit the\nDataFrame to just those observations with a Sepal Length greater than 5,\ncalculate the ratio, and plot:\n\nSince a function is passed in, the function is computed on the DataFrame being\nassigned to. Importantly, this is the DataFrame that\u2019s been filtered to those\nrows with sepal length greater than 5. The filtering happens first, and then\nthe ratio calculations. This is an example where we didn\u2019t have a reference to\nthe filtered DataFrame available.\n\nThe function signature for `assign` is simply `**kwargs`. The keys are the\ncolumn names for the new fields, and the values are either a value to be\ninserted (for example, a `Series` or NumPy array), or a function of one\nargument to be called on the `DataFrame`. A copy of the original DataFrame is\nreturned, with the new values inserted.\n\nStarting with Python 3.6 the order of `**kwargs` is preserved. This allows for\ndependent assignment, where an expression later in `**kwargs` can refer to a\ncolumn created earlier in the same `assign()`.\n\nIn the second expression, `x['C']` will refer to the newly created column,\nthat\u2019s equal to `dfa['A'] + dfa['B']`.\n\nThe basics of indexing are as follows:\n\nOperation\n\nSyntax\n\nResult\n\nSelect column\n\n`df[col]`\n\nSeries\n\nSelect row by label\n\n`df.loc[label]`\n\nSeries\n\nSelect row by integer location\n\n`df.iloc[loc]`\n\nSeries\n\nSlice rows\n\n`df[5:10]`\n\nDataFrame\n\nSelect rows by boolean vector\n\n`df[bool_vec]`\n\nDataFrame\n\nRow selection, for example, returns a Series whose index is the columns of the\nDataFrame:\n\nFor a more exhaustive treatment of sophisticated label-based indexing and\nslicing, see the section on indexing. We will address the fundamentals of\nreindexing / conforming to new sets of labels in the section on reindexing.\n\nData alignment between DataFrame objects automatically align on both the\ncolumns and the index (row labels). Again, the resulting object will have the\nunion of the column and row labels.\n\nWhen doing an operation between DataFrame and Series, the default behavior is\nto align the Series index on the DataFrame columns, thus broadcasting row-\nwise. For example:\n\nFor explicit control over the matching and broadcasting behavior, see the\nsection on flexible binary operations.\n\nOperations with scalars are just as you would expect:\n\nBoolean operators work as well:\n\nTo transpose, access the `T` attribute (also the `transpose` function),\nsimilar to an ndarray:\n\nElementwise NumPy ufuncs (log, exp, sqrt, \u2026) and various other NumPy functions\ncan be used with no issues on Series and DataFrame, assuming the data within\nare numeric:\n\nDataFrame is not intended to be a drop-in replacement for ndarray as its\nindexing semantics and data model are quite different in places from an\nn-dimensional array.\n\n`Series` implements `__array_ufunc__`, which allows it to work with NumPy\u2019s\nuniversal functions.\n\nThe ufunc is applied to the underlying array in a Series.\n\nChanged in version 0.25.0: When multiple `Series` are passed to a ufunc, they\nare aligned before performing the operation.\n\nLike other parts of the library, pandas will automatically align labeled\ninputs as part of a ufunc with multiple inputs. For example, using\n`numpy.remainder()` on two `Series` with differently ordered labels will align\nbefore the operation.\n\nAs usual, the union of the two indices is taken, and non-overlapping values\nare filled with missing values.\n\nWhen a binary ufunc is applied to a `Series` and `Index`, the Series\nimplementation takes precedence and a Series is returned.\n\nNumPy ufuncs are safe to apply to `Series` backed by non-ndarray arrays, for\nexample `arrays.SparseArray` (see Sparse calculation). If possible, the ufunc\nis applied without converting the underlying data to an ndarray.\n\nVery large DataFrames will be truncated to display them in the console. You\ncan also get a summary using `info()`. (Here I am reading a CSV version of the\nbaseball dataset from the plyr R package):\n\nHowever, using `to_string` will return a string representation of the\nDataFrame in tabular form, though it won\u2019t always fit the console width:\n\nWide DataFrames will be printed across multiple rows by default:\n\nYou can change how much to print on a single row by setting the\n`display.width` option:\n\nYou can adjust the max width of the individual columns by setting\n`display.max_colwidth`\n\nYou can also disable this feature via the `expand_frame_repr` option. This\nwill print the table in one block.\n\nIf a DataFrame column label is a valid Python variable name, the column can be\naccessed like an attribute:\n\nThe columns are also connected to the IPython completion mechanism so they can\nbe tab-completed:\n\n"}, {"name": "IO tools (text, CSV, HDF5, \u2026)", "path": "user_guide/io", "type": "Manual", "text": "\nThe pandas I/O API is a set of top level `reader` functions accessed like\n`pandas.read_csv()` that generally return a pandas object. The corresponding\n`writer` functions are object methods that are accessed like\n`DataFrame.to_csv()`. Below is a table containing available `readers` and\n`writers`.\n\nFormat Type\n\nData Description\n\nReader\n\nWriter\n\ntext\n\nCSV\n\nread_csv\n\nto_csv\n\ntext\n\nFixed-Width Text File\n\nread_fwf\n\ntext\n\nJSON\n\nread_json\n\nto_json\n\ntext\n\nHTML\n\nread_html\n\nto_html\n\ntext\n\nLaTeX\n\nStyler.to_latex\n\ntext\n\nXML\n\nread_xml\n\nto_xml\n\ntext\n\nLocal clipboard\n\nread_clipboard\n\nto_clipboard\n\nbinary\n\nMS Excel\n\nread_excel\n\nto_excel\n\nbinary\n\nOpenDocument\n\nread_excel\n\nbinary\n\nHDF5 Format\n\nread_hdf\n\nto_hdf\n\nbinary\n\nFeather Format\n\nread_feather\n\nto_feather\n\nbinary\n\nParquet Format\n\nread_parquet\n\nto_parquet\n\nbinary\n\nORC Format\n\nread_orc\n\nbinary\n\nStata\n\nread_stata\n\nto_stata\n\nbinary\n\nSAS\n\nread_sas\n\nbinary\n\nSPSS\n\nread_spss\n\nbinary\n\nPython Pickle Format\n\nread_pickle\n\nto_pickle\n\nSQL\n\nSQL\n\nread_sql\n\nto_sql\n\nSQL\n\nGoogle BigQuery\n\nread_gbq\n\nto_gbq\n\nHere is an informal performance comparison for some of these IO methods.\n\nNote\n\nFor examples that use the `StringIO` class, make sure you import it with `from\nio import StringIO` for Python 3.\n\nThe workhorse function for reading text files (a.k.a. flat files) is\n`read_csv()`. See the cookbook for some advanced strategies.\n\n`read_csv()` accepts the following common arguments:\n\nEither a path to a file (a `str`, `pathlib.Path`, or\n`py:py._path.local.LocalPath`), URL (including http, ftp, and S3 locations),\nor any object with a `read()` method (such as an open file or `StringIO`).\n\nDelimiter to use. If sep is `None`, the C engine cannot automatically detect\nthe separator, but the Python parsing engine can, meaning the latter will be\nused and automatically detect the separator by Python\u2019s builtin sniffer tool,\n`csv.Sniffer`. In addition, separators longer than 1 character and different\nfrom `'\\s+'` will be interpreted as regular expressions and will also force\nthe use of the Python parsing engine. Note that regex delimiters are prone to\nignoring quoted data. Regex example: `'\\\\r\\\\t'`.\n\nAlternative argument name for sep.\n\nSpecifies whether or not whitespace (e.g. `' '` or `'\\t'`) will be used as the\ndelimiter. Equivalent to setting `sep='\\s+'`. If this option is set to `True`,\nnothing should be passed in for the `delimiter` parameter.\n\nRow number(s) to use as the column names, and the start of the data. Default\nbehavior is to infer the column names: if no names are passed the behavior is\nidentical to `header=0` and column names are inferred from the first line of\nthe file, if column names are passed explicitly then the behavior is identical\nto `header=None`. Explicitly pass `header=0` to be able to replace existing\nnames.\n\nThe header can be a list of ints that specify row locations for a MultiIndex\non the columns e.g. `[0,1,3]`. Intervening rows that are not specified will be\nskipped (e.g. 2 in this example is skipped). Note that this parameter ignores\ncommented lines and empty lines if `skip_blank_lines=True`, so header=0\ndenotes the first line of data rather than the first line of the file.\n\nList of column names to use. If file contains no header row, then you should\nexplicitly pass `header=None`. Duplicates in this list are not allowed.\n\nColumn(s) to use as the row labels of the `DataFrame`, either given as string\nname or column index. If a sequence of int / str is given, a MultiIndex is\nused.\n\nNote: `index_col=False` can be used to force pandas to not use the first\ncolumn as the index, e.g. when you have a malformed file with delimiters at\nthe end of each line.\n\nThe default value of `None` instructs pandas to guess. If the number of fields\nin the column header row is equal to the number of fields in the body of the\ndata file, then a default index is used. If it is larger, then the first\ncolumns are used as index so that the remaining number of fields in the body\nare equal to the number of fields in the header.\n\nThe first row after the header is used to determine the number of columns,\nwhich will go into the index. If the subsequent rows contain less columns than\nthe first row, they are filled with `NaN`.\n\nThis can be avoided through `usecols`. This ensures that the columns are taken\nas is and the trailing data are ignored.\n\nReturn a subset of the columns. If list-like, all elements must either be\npositional (i.e. integer indices into the document columns) or strings that\ncorrespond to column names provided either by the user in `names` or inferred\nfrom the document header row(s). If `names` are given, the document header\nrow(s) are not taken into account. For example, a valid list-like `usecols`\nparameter would be `[0, 1, 2]` or `['foo', 'bar', 'baz']`.\n\nElement order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`. To\ninstantiate a DataFrame from `data` with element order preserved use\n`pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]` for columns in\n`['foo', 'bar']` order or `pd.read_csv(data, usecols=['foo', 'bar'])[['bar',\n'foo']]` for `['bar', 'foo']` order.\n\nIf callable, the callable function will be evaluated against the column names,\nreturning names where the callable function evaluates to True:\n\nUsing this parameter results in much faster parsing time and lower memory\nusage when using the c engine. The Python engine loads the data first before\ndeciding which columns to drop.\n\nIf the parsed data only contains one column then return a `Series`.\n\nDeprecated since version 1.4.0: Append `.squeeze(\"columns\")` to the call to\n`{func_name}` to squeeze the data.\n\nPrefix to add to column numbers when no header, e.g. \u2018X\u2019 for X0, X1, \u2026\n\nDeprecated since version 1.4.0: Use a list comprehension on the DataFrame\u2019s\ncolumns after calling `read_csv`.\n\nDuplicate columns will be specified as \u2018X\u2019, \u2018X.1\u2019\u2026\u2019X.N\u2019, rather than \u2018X\u2019\u2026\u2019X\u2019.\nPassing in `False` will cause data to be overwritten if there are duplicate\nnames in the columns.\n\nData type for data or columns. E.g. `{'a': np.float64, 'b': np.int32}`\n(unsupported with `engine='python'`). Use `str` or `object` together with\nsuitable `na_values` settings to preserve and not interpret dtype.\n\nParser engine to use. The C and pyarrow engines are faster, while the python\nengine is currently more feature-complete. Multithreading is currently only\nsupported by the pyarrow engine.\n\nNew in version 1.4.0: The \u201cpyarrow\u201d engine was added as an experimental\nengine, and some features are unsupported, or may not work correctly, with\nthis engine.\n\nDict of functions for converting values in certain columns. Keys can either be\nintegers or column labels.\n\nValues to consider as `True`.\n\nValues to consider as `False`.\n\nSkip spaces after delimiter.\n\nLine numbers to skip (0-indexed) or number of lines to skip (int) at the start\nof the file.\n\nIf callable, the callable function will be evaluated against the row indices,\nreturning True if the row should be skipped and False otherwise:\n\nNumber of lines at bottom of file to skip (unsupported with engine=\u2019c\u2019).\n\nNumber of rows of file to read. Useful for reading pieces of large files.\n\nInternally process the file in chunks, resulting in lower memory use while\nparsing, but possibly mixed type inference. To ensure no mixed types either\nset `False`, or specify the type with the `dtype` parameter. Note that the\nentire file is read into a single `DataFrame` regardless, use the `chunksize`\nor `iterator` parameter to return the data in chunks. (Only valid with C\nparser)\n\nIf a filepath is provided for `filepath_or_buffer`, map the file object\ndirectly onto memory and access the data directly from there. Using this\noption can improve performance because there is no longer any I/O overhead.\n\nAdditional strings to recognize as NA/NaN. If dict passed, specific per-column\nNA values. See na values const below for a list of the values interpreted as\nNaN by default.\n\nWhether or not to include the default NaN values when parsing the data.\nDepending on whether `na_values` is passed in, the behavior is as follows:\n\nIf `keep_default_na` is `True`, and `na_values` are specified, `na_values` is\nappended to the default NaN values used for parsing.\n\nIf `keep_default_na` is `True`, and `na_values` are not specified, only the\ndefault NaN values are used for parsing.\n\nIf `keep_default_na` is `False`, and `na_values` are specified, only the NaN\nvalues specified `na_values` are used for parsing.\n\nIf `keep_default_na` is `False`, and `na_values` are not specified, no strings\nwill be parsed as NaN.\n\nNote that if `na_filter` is passed in as `False`, the `keep_default_na` and\n`na_values` parameters will be ignored.\n\nDetect missing value markers (empty strings and the value of na_values). In\ndata without any NAs, passing `na_filter=False` can improve the performance of\nreading a large file.\n\nIndicate number of NA values placed in non-numeric columns.\n\nIf `True`, skip over blank lines rather than interpreting as NaN values.\n\nIf `True` -> try parsing the index.\n\nIf `[1, 2, 3]` -> try parsing columns 1, 2, 3 each as a separate date column.\n\nIf `[[1, 3]]` -> combine columns 1 and 3 and parse as a single date column.\n\nIf `{'foo': [1, 3]}` -> parse columns 1, 3 as date and call result \u2018foo\u2019. A\nfast-path exists for iso8601-formatted dates.\n\nIf `True` and parse_dates is enabled for a column, attempt to infer the\ndatetime format to speed up the processing.\n\nIf `True` and parse_dates specifies combining multiple columns then keep the\noriginal columns.\n\nFunction to use for converting a sequence of string columns to an array of\ndatetime instances. The default uses `dateutil.parser.parser` to do the\nconversion. pandas will try to call date_parser in three different ways,\nadvancing to the next if an exception occurs: 1) Pass one or more arrays (as\ndefined by parse_dates) as arguments; 2) concatenate (row-wise) the string\nvalues from the columns defined by parse_dates into a single array and pass\nthat; and 3) call date_parser once for each row using one or more strings\n(corresponding to the columns defined by parse_dates) as arguments.\n\nDD/MM format dates, international and European format.\n\nIf True, use a cache of unique, converted dates to apply the datetime\nconversion. May produce significant speed-up when parsing duplicate date\nstrings, especially ones with timezone offsets.\n\nNew in version 0.25.0.\n\nReturn `TextFileReader` object for iteration or getting chunks with\n`get_chunk()`.\n\nReturn `TextFileReader` object for iteration. See iterating and chunking\nbelow.\n\nFor on-the-fly decompression of on-disk data. If \u2018infer\u2019, then use gzip, bz2,\nzip, xz, or zstandard if `filepath_or_buffer` is path-like ending in \u2018.gz\u2019,\n\u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, \u2018.zst\u2019, respectively, and no decompression otherwise.\nIf using \u2018zip\u2019, the ZIP file must contain only one data file to be read in.\nSet to `None` for no decompression. Can also be a dict with key `'method'` set\nto one of {`'zip'`, `'gzip'`, `'bz2'`, `'zstd'`} and other key-value pairs are\nforwarded to `zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or\n`zstandard.ZstdDecompressor`. As an example, the following could be passed for\nfaster compression and to create a reproducible gzip archive:\n`compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}`.\n\nChanged in version 1.1.0: dict option extended to support `gzip` and `bz2`.\n\nChanged in version 1.2.0: Previous versions forwarded dict entries for \u2018gzip\u2019\nto `gzip.open`.\n\nThousands separator.\n\nCharacter to recognize as decimal point. E.g. use `','` for European data.\n\nSpecifies which converter the C engine should use for floating-point values.\nThe options are `None` for the ordinary converter, `high` for the high-\nprecision converter, and `round_trip` for the round-trip converter.\n\nCharacter to break file into lines. Only valid with C parser.\n\nThe character used to denote the start and end of a quoted item. Quoted items\ncan include the delimiter and it will be ignored.\n\nControl field quoting behavior per `csv.QUOTE_*` constants. Use one of\n`QUOTE_MINIMAL` (0), `QUOTE_ALL` (1), `QUOTE_NONNUMERIC` (2) or `QUOTE_NONE`\n(3).\n\nWhen `quotechar` is specified and `quoting` is not `QUOTE_NONE`, indicate\nwhether or not to interpret two consecutive `quotechar` elements inside a\nfield as a single `quotechar` element.\n\nOne-character string used to escape delimiter when quoting is `QUOTE_NONE`.\n\nIndicates remainder of line should not be parsed. If found at the beginning of\na line, the line will be ignored altogether. This parameter must be a single\ncharacter. Like empty lines (as long as `skip_blank_lines=True`), fully\ncommented lines are ignored by the parameter `header` but not by `skiprows`.\nFor example, if `comment='#'`, parsing \u2018#empty\\na,b,c\\n1,2,3\u2019 with `header=0`\nwill result in \u2018a,b,c\u2019 being treated as the header.\n\nEncoding to use for UTF when reading/writing (e.g. `'utf-8'`). List of Python\nstandard encodings.\n\nIf provided, this parameter will override values (default or not) for the\nfollowing parameters: `delimiter`, `doublequote`, `escapechar`,\n`skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to override\nvalues, a ParserWarning will be issued. See `csv.Dialect` documentation for\nmore details.\n\nLines with too many fields (e.g. a csv line with too many commas) will by\ndefault cause an exception to be raised, and no `DataFrame` will be returned.\nIf `False`, then these \u201cbad lines\u201d will dropped from the `DataFrame` that is\nreturned. See bad lines below.\n\nDeprecated since version 1.3.0: The `on_bad_lines` parameter should be used\ninstead to specify behavior upon encountering a bad line instead.\n\nIf error_bad_lines is `False`, and warn_bad_lines is `True`, a warning for\neach \u201cbad line\u201d will be output.\n\nDeprecated since version 1.3.0: The `on_bad_lines` parameter should be used\ninstead to specify behavior upon encountering a bad line instead.\n\nSpecifies what to do upon encountering a bad line (a line with too many\nfields). Allowed values are :\n\n\u2018error\u2019, raise an ParserError when a bad line is encountered.\n\n\u2018warn\u2019, print a warning when a bad line is encountered and skip that line.\n\n\u2018skip\u2019, skip bad lines without raising or warning when they are encountered.\n\nNew in version 1.3.0.\n\nYou can indicate the data type for the whole `DataFrame` or individual\ncolumns:\n\nFortunately, pandas offers more than one way to ensure that your column(s)\ncontain only one `dtype`. If you\u2019re unfamiliar with these concepts, you can\nsee here to learn more about dtypes, and here to learn more about `object`\nconversion in pandas.\n\nFor instance, you can use the `converters` argument of `read_csv()`:\n\nOr you can use the `to_numeric()` function to coerce the dtypes after reading\nin the data,\n\nwhich will convert all valid parsing to floats, leaving the invalid parsing as\n`NaN`.\n\nUltimately, how you deal with reading in columns containing mixed dtypes\ndepends on your specific needs. In the case above, if you wanted to `NaN` out\nthe data anomalies, then `to_numeric()` is probably your best option. However,\nif you wanted for all the data to be coerced, no matter the type, then using\nthe `converters` argument of `read_csv()` would certainly be worth trying.\n\nNote\n\nIn some cases, reading in abnormal data with columns containing mixed dtypes\nwill result in an inconsistent dataset. If you rely on pandas to infer the\ndtypes of your columns, the parsing engine will go and infer the dtypes for\ndifferent chunks of the data, rather than the whole dataset at once.\nConsequently, you can end up with column(s) with mixed dtypes. For example,\n\nwill result with `mixed_df` containing an `int` dtype for certain chunks of\nthe column, and `str` for others due to the mixed dtypes from the data that\nwas read in. It is important to note that the overall column will be marked\nwith a `dtype` of `object`, which is used for columns with mixed dtypes.\n\n`Categorical` columns can be parsed directly by specifying `dtype='category'`\nor `dtype=CategoricalDtype(categories, ordered)`.\n\nIndividual columns can be parsed as a `Categorical` using a dict\nspecification:\n\nSpecifying `dtype='category'` will result in an unordered `Categorical` whose\n`categories` are the unique values observed in the data. For more control on\nthe categories and order, create a `CategoricalDtype` ahead of time, and pass\nthat for that column\u2019s `dtype`.\n\nWhen using `dtype=CategoricalDtype`, \u201cunexpected\u201d values outside of\n`dtype.categories` are treated as missing values.\n\nThis matches the behavior of `Categorical.set_categories()`.\n\nNote\n\nWith `dtype='category'`, the resulting categories will always be parsed as\nstrings (object dtype). If the categories are numeric they can be converted\nusing the `to_numeric()` function, or as appropriate, another converter such\nas `to_datetime()`.\n\nWhen `dtype` is a `CategoricalDtype` with homogeneous `categories` ( all\nnumeric, all datetimes, etc.), the conversion is done automatically.\n\nA file may or may not have a header row. pandas assumes the first row should\nbe used as the column names:\n\nBy specifying the `names` argument in conjunction with `header` you can\nindicate other names to use and whether or not to throw away the header row\n(if any):\n\nIf the header is in a row other than the first, pass the row number to\n`header`. This will skip the preceding rows:\n\nNote\n\nDefault behavior is to infer the column names: if no names are passed the\nbehavior is identical to `header=0` and column names are inferred from the\nfirst non-blank line of the file, if column names are passed explicitly then\nthe behavior is identical to `header=None`.\n\nIf the file or header contains duplicate names, pandas will by default\ndistinguish between them so as to prevent overwriting data:\n\nThere is no more duplicate data because `mangle_dupe_cols=True` by default,\nwhich modifies a series of duplicate columns \u2018X\u2019, \u2026, \u2018X\u2019 to become \u2018X\u2019, \u2018X.1\u2019,\n\u2026, \u2018X.N\u2019. If `mangle_dupe_cols=False`, duplicate data can arise:\n\nTo prevent users from encountering this problem with duplicate data, a\n`ValueError` exception is raised if `mangle_dupe_cols != True`:\n\nThe `usecols` argument allows you to select any subset of the columns in a\nfile, either using the column names, position numbers or a callable:\n\nThe `usecols` argument can also be used to specify which columns not to use in\nthe final result:\n\nIn this case, the callable is specifying that we exclude the \u201ca\u201d and \u201cc\u201d\ncolumns from the output.\n\nIf the `comment` parameter is specified, then completely commented lines will\nbe ignored. By default, completely blank lines will be ignored as well.\n\nIf `skip_blank_lines=False`, then `read_csv` will not ignore blank lines:\n\nWarning\n\nThe presence of ignored lines might create ambiguities involving line numbers;\nthe parameter `header` uses row numbers (ignoring commented/empty lines),\nwhile `skiprows` uses line numbers (including commented/empty lines):\n\nIf both `header` and `skiprows` are specified, `header` will be relative to\nthe end of `skiprows`. For example:\n\nSometimes comments or meta data may be included in a file:\n\nBy default, the parser includes the comments in the output:\n\nWe can suppress the comments using the `comment` keyword:\n\nThe `encoding` argument should be used for encoded unicode data, which will\nresult in byte strings being decoded to unicode in the result:\n\nSome formats which encode all characters as multiple bytes, like UTF-16, won\u2019t\nparse correctly at all without specifying the encoding. Full list of Python\nstandard encodings.\n\nIf a file has one more column of data than the number of column names, the\nfirst column will be used as the `DataFrame`\u2019s row names:\n\nOrdinarily, you can achieve this behavior using the `index_col` option.\n\nThere are some exception cases when a file has been prepared with delimiters\nat the end of each data line, confusing the parser. To explicitly disable the\nindex column inference and discard the last column, pass `index_col=False`:\n\nIf a subset of data is being parsed using the `usecols` option, the\n`index_col` specification is based on that subset, not the original data.\n\nTo better facilitate working with datetime data, `read_csv()` uses the keyword\narguments `parse_dates` and `date_parser` to allow users to specify a variety\nof columns and date/time formats to turn the input text data into `datetime`\nobjects.\n\nThe simplest case is to just pass in `parse_dates=True`:\n\nIt is often the case that we may want to store date and time data separately,\nor store various date fields separately. the `parse_dates` keyword can be used\nto specify a combination of columns to parse the dates and/or times from.\n\nYou can specify a list of column lists to `parse_dates`, the resulting date\ncolumns will be prepended to the output (so as to not affect the existing\ncolumn order) and the new column names will be the concatenation of the\ncomponent column names:\n\nBy default the parser removes the component date columns, but you can choose\nto retain them via the `keep_date_col` keyword:\n\nNote that if you wish to combine multiple columns into a single date column, a\nnested list must be used. In other words, `parse_dates=[1, 2]` indicates that\nthe second and third columns should each be parsed as separate date columns\nwhile `parse_dates=[[1, 2]]` means the two columns should be parsed into a\nsingle column.\n\nYou can also use a dict to specify custom name columns:\n\nIt is important to remember that if multiple text columns are to be parsed\ninto a single date column, then a new column is prepended to the data. The\n`index_col` specification is based off of this new set of columns rather than\nthe original data columns:\n\nNote\n\nIf a column or index contains an unparsable date, the entire column or index\nwill be returned unaltered as an object data type. For non-standard datetime\nparsing, use `to_datetime()` after `pd.read_csv`.\n\nNote\n\nread_csv has a fast_path for parsing datetime strings in iso8601 format, e.g\n\u201c2000-01-01T00:01:02+00:00\u201d and similar variations. If you can arrange for\nyour data to store datetimes in this format, load times will be significantly\nfaster, ~20x has been observed.\n\nFinally, the parser allows you to specify a custom `date_parser` function to\ntake full advantage of the flexibility of the date parsing API:\n\npandas will try to call the `date_parser` function in three different ways. If\nan exception is raised, the next one is tried:\n\n`date_parser` is first called with one or more arrays as arguments, as defined\nusing `parse_dates` (e.g., `date_parser(['2013', '2013'], ['1', '2'])`).\n\nIf #1 fails, `date_parser` is called with all the columns concatenated row-\nwise into a single array (e.g., `date_parser(['2013 1', '2013 2'])`).\n\nNote that performance-wise, you should try these methods of parsing dates in\norder:\n\nTry to infer the format using `infer_datetime_format=True` (see section\nbelow).\n\nIf you know the format, use `pd.to_datetime()`: `date_parser=lambda x:\npd.to_datetime(x, format=...)`.\n\nIf you have a really non-standard format, use a custom `date_parser` function.\nFor optimal performance, this should be vectorized, i.e., it should accept\narrays as arguments.\n\npandas cannot natively represent a column or index with mixed timezones. If\nyour CSV file contains columns with a mixture of timezones, the default result\nwill be an object-dtype column with strings, even with `parse_dates`.\n\nTo parse the mixed-timezone values as a datetime column, pass a partially-\napplied `to_datetime()` with `utc=True` as the `date_parser`.\n\nIf you have `parse_dates` enabled for some or all of your columns, and your\ndatetime strings are all formatted the same way, you may get a large speed up\nby setting `infer_datetime_format=True`. If set, pandas will attempt to guess\nthe format of your datetime strings, and then use a faster means of parsing\nthe strings. 5-10x parsing speeds have been observed. pandas will fallback to\nthe usual parsing if either the format cannot be guessed or the format that\nwas guessed cannot properly parse the entire column of strings. So in general,\n`infer_datetime_format` should not have any negative consequences if enabled.\n\nHere are some examples of datetime strings that can be guessed (All\nrepresenting December 30th, 2011 at 00:00:00):\n\n\u201c20111230\u201d\n\n\u201c2011/12/30\u201d\n\n\u201c20111230 00:00:00\u201d\n\n\u201c12/30/2011 00:00:00\u201d\n\n\u201c30/Dec/2011 00:00:00\u201d\n\n\u201c30/December/2011 00:00:00\u201d\n\nNote that `infer_datetime_format` is sensitive to `dayfirst`. With\n`dayfirst=True`, it will guess \u201c01/12/2011\u201d to be December 1st. With\n`dayfirst=False` (default) it will guess \u201c01/12/2011\u201d to be January 12th.\n\nWhile US date formats tend to be MM/DD/YYYY, many international formats use\nDD/MM/YYYY instead. For convenience, a `dayfirst` keyword is provided:\n\nNew in version 1.2.0.\n\n`df.to_csv(..., mode=\"wb\")` allows writing a CSV to a file object opened\nbinary mode. In most cases, it is not necessary to specify `mode` as Pandas\nwill auto-detect whether the file object is opened in text or binary mode.\n\nThe parameter `float_precision` can be specified in order to use a specific\nfloating-point converter during parsing with the C engine. The options are the\nordinary converter, the high-precision converter, and the round-trip converter\n(which is guaranteed to round-trip values after writing to a file). For\nexample:\n\nFor large numbers that have been written with a thousands separator, you can\nset the `thousands` keyword to a string of length 1 so that integers will be\nparsed correctly:\n\nBy default, numbers with a thousands separator will be parsed as strings:\n\nThe `thousands` keyword allows integers to be parsed correctly:\n\nTo control which values are parsed as missing values (which are signified by\n`NaN`), specify a string in `na_values`. If you specify a list of strings,\nthen all values in it are considered to be missing values. If you specify a\nnumber (a `float`, like `5.0` or an `integer` like `5`), the corresponding\nequivalent values will also imply a missing value (in this case effectively\n`[5.0, 5]` are recognized as `NaN`).\n\nTo completely override the default values that are recognized as missing,\nspecify `keep_default_na=False`.\n\nThe default `NaN` recognized values are `['-1.#IND', '1.#QNAN', '1.#IND',\n'-1.#QNAN', '#N/A N/A', '#N/A', 'N/A', 'n/a', 'NA', '<NA>', '#NA', 'NULL',\n'null', 'NaN', '-NaN', 'nan', '-nan', '']`.\n\nLet us consider some examples:\n\nIn the example above `5` and `5.0` will be recognized as `NaN`, in addition to\nthe defaults. A string will first be interpreted as a numerical `5`, then as a\n`NaN`.\n\nAbove, only an empty field will be recognized as `NaN`.\n\nAbove, both `NA` and `0` as strings are `NaN`.\n\nThe default values, in addition to the string `\"Nope\"` are recognized as\n`NaN`.\n\n`inf` like values will be parsed as `np.inf` (positive infinity), and `-inf`\nas `-np.inf` (negative infinity). These will ignore the case of the value,\nmeaning `Inf`, will also be parsed as `np.inf`.\n\nUsing the `squeeze` keyword, the parser will return output with a single\ncolumn as a `Series`:\n\nDeprecated since version 1.4.0: Users should append `.squeeze(\"columns\")` to\nthe DataFrame returned by `read_csv` instead.\n\nThe common values `True`, `False`, `TRUE`, and `FALSE` are all recognized as\nboolean. Occasionally you might want to recognize other values as being\nboolean. To do this, use the `true_values` and `false_values` options as\nfollows:\n\nSome files may have malformed lines with too few fields or too many. Lines\nwith too few fields will have NA values filled in the trailing fields. Lines\nwith too many fields will raise an error by default:\n\nYou can elect to skip bad lines:\n\nOr pass a callable function to handle the bad line if `engine=\"python\"`. The\nbad line will be a list of strings that was split by the `sep`:\n\nYou can also use the `usecols` parameter to eliminate extraneous column data\nthat appear in some lines but not others:\n\nIn case you want to keep all data including the lines with too many fields,\nyou can specify a sufficient number of `names`. This ensures that lines with\nnot enough fields are filled with `NaN`.\n\nThe `dialect` keyword gives greater flexibility in specifying the file format.\nBy default it uses the Excel dialect but you can specify either the dialect\nname or a `csv.Dialect` instance.\n\nSuppose you had data with unenclosed quotes:\n\nBy default, `read_csv` uses the Excel dialect and treats the double quote as\nthe quote character, which causes it to fail when it finds a newline before it\nfinds the closing double quote.\n\nWe can get around this using `dialect`:\n\nAll of the dialect options can be specified separately by keyword arguments:\n\nAnother common dialect option is `skipinitialspace`, to skip any whitespace\nafter a delimiter:\n\nThe parsers make every attempt to \u201cdo the right thing\u201d and not be fragile.\nType inference is a pretty big deal. If a column can be coerced to integer\ndtype without altering the contents, the parser will do so. Any non-numeric\ncolumns will come through as object dtype as with the rest of pandas objects.\n\nQuotes (and other escape characters) in embedded fields can be handled in any\nnumber of ways. One way is to use backslashes; to properly parse this data,\nyou should pass the `escapechar` option:\n\nWhile `read_csv()` reads delimited data, the `read_fwf()` function works with\ndata files that have known and fixed column widths. The function parameters to\n`read_fwf` are largely the same as `read_csv` with two extra parameters, and a\ndifferent usage of the `delimiter` parameter:\n\n`colspecs`: A list of pairs (tuples) giving the extents of the fixed-width\nfields of each line as half-open intervals (i.e., [from, to[ ). String value\n\u2018infer\u2019 can be used to instruct the parser to try detecting the column\nspecifications from the first 100 rows of the data. Default behavior, if not\nspecified, is to infer.\n\n`widths`: A list of field widths which can be used instead of \u2018colspecs\u2019 if\nthe intervals are contiguous.\n\n`delimiter`: Characters to consider as filler characters in the fixed-width\nfile. Can be used to specify the filler character of the fields if it is not\nspaces (e.g., \u2018~\u2019).\n\nConsider a typical fixed-width data file:\n\nIn order to parse this file into a `DataFrame`, we simply need to supply the\ncolumn specifications to the `read_fwf` function along with the file name:\n\nNote how the parser automatically picks column names X.<column number> when\n`header=None` argument is specified. Alternatively, you can supply just the\ncolumn widths for contiguous columns:\n\nThe parser will take care of extra white spaces around the columns so it\u2019s ok\nto have extra separation between the columns in the file.\n\nBy default, `read_fwf` will try to infer the file\u2019s `colspecs` by using the\nfirst 100 rows of the file. It can do it only in cases when the columns are\naligned and correctly separated by the provided `delimiter` (default delimiter\nis whitespace).\n\n`read_fwf` supports the `dtype` parameter for specifying the types of parsed\ncolumns to be different from the inferred type.\n\nConsider a file with one less entry in the header than the number of data\ncolumn:\n\nIn this special case, `read_csv` assumes that the first column is to be used\nas the index of the `DataFrame`:\n\nNote that the dates weren\u2019t automatically parsed. In that case you would need\nto do as before:\n\nSuppose you have data indexed by two columns:\n\nThe `index_col` argument to `read_csv` can take a list of column numbers to\nturn multiple columns into a `MultiIndex` for the index of the returned\nobject:\n\nBy specifying list of row locations for the `header` argument, you can read in\na `MultiIndex` for the columns. Specifying non-consecutive rows will skip the\nintervening rows.\n\n`read_csv` is also able to interpret a more common format of multi-columns\nindices.\n\nNote: If an `index_col` is not specified (e.g. you don\u2019t have an index, or\nwrote it with `df.to_csv(..., index=False)`, then any `names` on the columns\nindex will be lost.\n\n`read_csv` is capable of inferring delimited (not necessarily comma-separated)\nfiles, as pandas uses the `csv.Sniffer` class of the csv module. For this, you\nhave to specify `sep=None`.\n\nIt\u2019s best to use `concat()` to combine multiple files. See the cookbook for an\nexample.\n\nSuppose you wish to iterate through a (potentially very large) file lazily\nrather than reading the entire file into memory, such as the following:\n\nBy specifying a `chunksize` to `read_csv`, the return value will be an\niterable object of type `TextFileReader`:\n\nChanged in version 1.2: `read_csv/json/sas` return a context-manager when\niterating through a file.\n\nSpecifying `iterator=True` will also return the `TextFileReader` object:\n\nPandas currently supports three engines, the C engine, the python engine, and\nan experimental pyarrow engine (requires the `pyarrow` package). In general,\nthe pyarrow engine is fastest on larger workloads and is equivalent in speed\nto the C engine on most other workloads. The python engine tends to be slower\nthan the pyarrow and C engines on most workloads. However, the pyarrow engine\nis much less robust than the C engine, which lacks a few features compared to\nthe Python engine.\n\nWhere possible, pandas uses the C parser (specified as `engine='c'`), but it\nmay fall back to Python if C-unsupported options are specified.\n\nCurrently, options unsupported by the C and pyarrow engines include:\n\n`sep` other than a single character (e.g. regex separators)\n\n`skipfooter`\n\n`sep=None` with `delim_whitespace=False`\n\nSpecifying any of the above options will produce a `ParserWarning` unless the\npython engine is selected explicitly using `engine='python'`.\n\nOptions that are unsupported by the pyarrow engine which are not covered by\nthe list above include:\n\n`float_precision`\n\n`chunksize`\n\n`comment`\n\n`nrows`\n\n`thousands`\n\n`memory_map`\n\n`dialect`\n\n`warn_bad_lines`\n\n`error_bad_lines`\n\n`on_bad_lines`\n\n`delim_whitespace`\n\n`quoting`\n\n`lineterminator`\n\n`converters`\n\n`decimal`\n\n`iterator`\n\n`dayfirst`\n\n`infer_datetime_format`\n\n`verbose`\n\n`skipinitialspace`\n\n`low_memory`\n\nSpecifying these options with `engine='pyarrow'` will raise a `ValueError`.\n\nYou can pass in a URL to read or write remote files to many of pandas\u2019 IO\nfunctions - the following example shows reading a CSV file:\n\nNew in version 1.3.0.\n\nA custom header can be sent alongside HTTP(s) requests by passing a dictionary\nof header key value mappings to the `storage_options` keyword argument as\nshown below:\n\nAll URLs which are not local files or HTTP(s) are handled by fsspec, if\ninstalled, and its various filesystem implementations (including Amazon S3,\nGoogle Cloud, SSH, FTP, webHDFS\u2026). Some of these implementations will require\nadditional packages to be installed, for example S3 URLs require the s3fs\nlibrary:\n\nWhen dealing with remote storage systems, you might need extra configuration\nwith environment variables or config files in special locations. For example,\nto access data in your S3 bucket, you will need to define credentials in one\nof the several ways listed in the S3Fs documentation. The same is true for\nseveral of the storage backends, and you should follow the links at fsimpl1\nfor implementations built into `fsspec` and fsimpl2 for those not included in\nthe main `fsspec` distribution.\n\nYou can also pass parameters directly to the backend driver. For example, if\nyou do not have S3 credentials, you can still access public data by specifying\nan anonymous connection, such as\n\nNew in version 1.2.0.\n\n`fsspec` also allows complex URLs, for accessing data in compressed archives,\nlocal caching of files, and more. To locally cache the above example, you\nwould modify the call to\n\nwhere we specify that the \u201canon\u201d parameter is meant for the \u201cs3\u201d part of the\nimplementation, not to the caching implementation. Note that this caches to a\ntemporary directory for the duration of the session only, but you can also\nspecify a permanent store.\n\nThe `Series` and `DataFrame` objects have an instance method `to_csv` which\nallows storing the contents of the object as a comma-separated-values file.\nThe function takes a number of arguments. Only the first is required.\n\n`path_or_buf`: A string path to the file to write or a file object. If a file\nobject it must be opened with `newline=''`\n\n`sep` : Field delimiter for the output file (default \u201c,\u201d)\n\n`na_rep`: A string representation of a missing value (default \u2018\u2019)\n\n`float_format`: Format string for floating point numbers\n\n`columns`: Columns to write (default None)\n\n`header`: Whether to write out the column names (default True)\n\n`index`: whether to write row (index) names (default True)\n\n`index_label`: Column label(s) for index column(s) if desired. If None\n(default), and `header` and `index` are True, then the index names are used.\n(A sequence should be given if the `DataFrame` uses MultiIndex).\n\n`mode` : Python write mode, default \u2018w\u2019\n\n`encoding`: a string representing the encoding to use if the contents are non-\nASCII, for Python versions prior to 3\n\n`line_terminator`: Character sequence denoting line end (default `os.linesep`)\n\n`quoting`: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL).\nNote that if you have set a `float_format` then floats are converted to\nstrings and csv.QUOTE_NONNUMERIC will treat them as non-numeric\n\n`quotechar`: Character used to quote fields (default \u2018\u201d\u2019)\n\n`doublequote`: Control quoting of `quotechar` in fields (default True)\n\n`escapechar`: Character used to escape `sep` and `quotechar` when appropriate\n(default None)\n\n`chunksize`: Number of rows to write at a time\n\n`date_format`: Format string for datetime objects\n\nThe `DataFrame` object has an instance method `to_string` which allows control\nover the string representation of the object. All arguments are optional:\n\n`buf` default None, for example a StringIO object\n\n`columns` default None, which columns to write\n\n`col_space` default None, minimum width of each column.\n\n`na_rep` default `NaN`, representation of NA value\n\n`formatters` default None, a dictionary (by column) of functions each of which\ntakes a single argument and returns a formatted string\n\n`float_format` default None, a function which takes a single (float) argument\nand returns a formatted string; to be applied to floats in the `DataFrame`.\n\n`sparsify` default True, set to False for a `DataFrame` with a hierarchical\nindex to print every MultiIndex key at each row.\n\n`index_names` default True, will print the names of the indices\n\n`index` default True, will print the index (ie, row labels)\n\n`header` default True, will print the column labels\n\n`justify` default `left`, will print column headers left- or right-justified\n\nThe `Series` object also has a `to_string` method, but with only the `buf`,\n`na_rep`, `float_format` arguments. There is also a `length` argument which,\nif set to `True`, will additionally output the length of the Series.\n\nRead and write `JSON` format files and strings.\n\nA `Series` or `DataFrame` can be converted to a valid JSON string. Use\n`to_json` with optional parameters:\n\n`path_or_buf` : the pathname or buffer to write the output This can be `None`\nin which case a JSON string is returned\n\n`orient` :\n\ndefault is `index`\n\nallowed values are {`split`, `records`, `index`}\n\ndefault is `columns`\n\nallowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}\n\nThe format of the JSON string\n\n`split`\n\ndict like {index -> [index], columns -> [columns], data -> [values]}\n\n`records`\n\nlist like [{column -> value}, \u2026 , {column -> value}]\n\n`index`\n\ndict like {index -> {column -> value}}\n\n`columns`\n\ndict like {column -> {index -> value}}\n\n`values`\n\njust the values array\n\n`table`\n\nadhering to the JSON Table Schema\n\n`date_format` : string, type of date conversion, \u2018epoch\u2019 for timestamp, \u2018iso\u2019\nfor ISO8601.\n\n`double_precision` : The number of decimal places to use when encoding\nfloating point values, default 10.\n\n`force_ascii` : force encoded string to be ASCII, default True.\n\n`date_unit` : The time unit to encode to, governs timestamp and ISO8601\nprecision. One of \u2018s\u2019, \u2018ms\u2019, \u2018us\u2019 or \u2018ns\u2019 for seconds, milliseconds,\nmicroseconds and nanoseconds respectively. Default \u2018ms\u2019.\n\n`default_handler` : The handler to call if an object cannot otherwise be\nconverted to a suitable format for JSON. Takes a single argument, which is the\nobject to convert, and returns a serializable object.\n\n`lines` : If `records` orient, then will write each record per line as json.\n\nNote `NaN`\u2019s, `NaT`\u2019s and `None` will be converted to `null` and `datetime`\nobjects will be converted based on the `date_format` and `date_unit`\nparameters.\n\nThere are a number of different options for the format of the resulting JSON\nfile / string. Consider the following `DataFrame` and `Series`:\n\nColumn oriented (the default for `DataFrame`) serializes the data as nested\nJSON objects with column labels acting as the primary index:\n\nIndex oriented (the default for `Series`) similar to column oriented but the\nindex labels are now primary:\n\nRecord oriented serializes the data to a JSON array of column -> value\nrecords, index labels are not included. This is useful for passing `DataFrame`\ndata to plotting libraries, for example the JavaScript library `d3.js`:\n\nValue oriented is a bare-bones option which serializes to nested JSON arrays\nof values only, column and index labels are not included:\n\nSplit oriented serializes to a JSON object containing separate entries for\nvalues, index and columns. Name is also included for `Series`:\n\nTable oriented serializes to the JSON Table Schema, allowing for the\npreservation of metadata including but not limited to dtypes and index names.\n\nNote\n\nAny orient option that encodes to a JSON object will not preserve the ordering\nof index and column labels during round-trip serialization. If you wish to\npreserve label ordering use the `split` option as it uses ordered containers.\n\nWriting in ISO date format:\n\nWriting in ISO date format, with microseconds:\n\nEpoch timestamps, in seconds:\n\nWriting to a file, with a date index and a date column:\n\nIf the JSON serializer cannot handle the container contents directly it will\nfall back in the following manner:\n\nif the dtype is unsupported (e.g. `np.complex_`) then the `default_handler`,\nif provided, will be called for each value, otherwise an exception is raised.\n\nif an object is unsupported it will attempt the following:\n\ncheck if the object has defined a `toDict` method and call it. A `toDict`\nmethod should return a `dict` which will then be JSON serialized.\n\ninvoke the `default_handler` if one was provided.\n\nconvert the object to a `dict` by traversing its contents. However this will\noften fail with an `OverflowError` or give unexpected results.\n\nIn general the best approach for unsupported objects or dtypes is to provide a\n`default_handler`. For example:\n\ncan be dealt with by specifying a simple `default_handler`:\n\nReading a JSON string to pandas object can take a number of parameters. The\nparser will try to parse a `DataFrame` if `typ` is not supplied or is `None`.\nTo explicitly force `Series` parsing, pass `typ=series`\n\n`filepath_or_buffer` : a VALID JSON string or file handle / StringIO. The\nstring could be a URL. Valid URL schemes include http, ftp, S3, and file. For\nfile URLs, a host is expected. For instance, a local file could be file\n://localhost/path/to/table.json\n\n`typ` : type of object to recover (series or frame), default \u2018frame\u2019\n\n`orient` :\n\ndefault is `index`\n\nallowed values are {`split`, `records`, `index`}\n\ndefault is `columns`\n\nallowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}\n\nThe format of the JSON string\n\n`split`\n\ndict like {index -> [index], columns -> [columns], data -> [values]}\n\n`records`\n\nlist like [{column -> value}, \u2026 , {column -> value}]\n\n`index`\n\ndict like {index -> {column -> value}}\n\n`columns`\n\ndict like {column -> {index -> value}}\n\n`values`\n\njust the values array\n\n`table`\n\nadhering to the JSON Table Schema\n\n`dtype` : if True, infer dtypes, if a dict of column to dtype, then use those,\nif `False`, then don\u2019t infer dtypes at all, default is True, apply only to the\ndata.\n\n`convert_axes` : boolean, try to convert the axes to the proper dtypes,\ndefault is `True`\n\n`convert_dates` : a list of columns to parse for dates; If `True`, then try to\nparse date-like columns, default is `True`.\n\n`keep_default_dates` : boolean, default `True`. If parsing dates, then parse\nthe default date-like columns.\n\n`numpy` : direct decoding to NumPy arrays. default is `False`; Supports\nnumeric data only, although labels may be non-numeric. Also note that the JSON\nordering MUST be the same for each term if `numpy=True`.\n\n`precise_float` : boolean, default `False`. Set to enable usage of higher\nprecision (strtod) function when decoding string to double values. Default\n(`False`) is to use fast but less precise builtin functionality.\n\n`date_unit` : string, the timestamp unit to detect if converting dates.\nDefault None. By default the timestamp precision will be detected, if this is\nnot desired then pass one of \u2018s\u2019, \u2018ms\u2019, \u2018us\u2019 or \u2018ns\u2019 to force timestamp\nprecision to seconds, milliseconds, microseconds or nanoseconds respectively.\n\n`lines` : reads file as one json object per line.\n\n`encoding` : The encoding to use to decode py3 bytes.\n\n`chunksize` : when used in combination with `lines=True`, return a JsonReader\nwhich reads in `chunksize` lines per iteration.\n\nThe parser will raise one of `ValueError/TypeError/AssertionError` if the JSON\nis not parseable.\n\nIf a non-default `orient` was used when encoding to JSON be sure to pass the\nsame option here so that decoding produces sensible results, see Orient\nOptions for an overview.\n\nThe default of `convert_axes=True`, `dtype=True`, and `convert_dates=True`\nwill try to parse the axes, and all of the data into appropriate types,\nincluding dates. If you need to override specific dtypes, pass a dict to\n`dtype`. `convert_axes` should only be set to `False` if you need to preserve\nstring-like numbers (e.g. \u20181\u2019, \u20182\u2019) in an axes.\n\nNote\n\nLarge integer values may be converted to dates if `convert_dates=True` and the\ndata and / or column labels appear \u2018date-like\u2019. The exact threshold depends on\nthe `date_unit` specified. \u2018date-like\u2019 means that the column label meets one\nof the following criteria:\n\nit ends with `'_at'`\n\nit ends with `'_time'`\n\nit begins with `'timestamp'`\n\nit is `'modified'`\n\nit is `'date'`\n\nWarning\n\nWhen reading JSON data, automatic coercing into dtypes has some quirks:\n\nan index can be reconstructed in a different order from serialization, that\nis, the returned order is not guaranteed to be the same as before\nserialization\n\na column that was `float` data will be converted to `integer` if it can be\ndone safely, e.g. a column of `1.`\n\nbool columns will be converted to `integer` on reconstruction\n\nThus there are times where you may want to specify specific dtypes via the\n`dtype` keyword argument.\n\nReading from a JSON string:\n\nReading from a file:\n\nDon\u2019t convert any data (but still convert axes and dates):\n\nSpecify dtypes for conversion:\n\nPreserve string indices:\n\nDates written in nanoseconds need to be read back in nanoseconds:\n\nNote\n\nThis param has been deprecated as of version 1.0.0 and will raise a\n`FutureWarning`.\n\nThis supports numeric data only. Index and columns labels may be non-numeric,\ne.g. strings, dates etc.\n\nIf `numpy=True` is passed to `read_json` an attempt will be made to sniff an\nappropriate dtype during deserialization and to subsequently decode directly\nto NumPy arrays, bypassing the need for intermediate Python objects.\n\nThis can provide speedups if you are deserialising a large amount of numeric\ndata:\n\nThe speedup is less noticeable for smaller datasets:\n\nWarning\n\nDirect NumPy decoding makes a number of assumptions and may fail or produce\nunexpected output if these assumptions are not satisfied:\n\ndata is numeric.\n\ndata is uniform. The dtype is sniffed from the first value decoded. A\n`ValueError` may be raised, or incorrect output may be produced if this\ncondition is not satisfied.\n\nlabels are ordered. Labels are only read from the first container, it is\nassumed that each subsequent row / column has been encoded in the same order.\nThis should be satisfied if the data was encoded using `to_json` but may not\nbe the case if the JSON is from another source.\n\npandas provides a utility function to take a dict or list of dicts and\nnormalize this semi-structured data into a flat table.\n\nThe max_level parameter provides more control over which level to end\nnormalization. With max_level=1 the following snippet normalizes until 1st\nnesting level of the provided dict.\n\npandas is able to read and write line-delimited json files that are common in\ndata processing pipelines using Hadoop or Spark.\n\nFor line-delimited json files, pandas can also return an iterator which reads\nin `chunksize` lines at a time. This can be useful for large files or to read\nfrom a stream.\n\nTable Schema is a spec for describing tabular datasets as a JSON object. The\nJSON includes information on the field names, types, and other attributes. You\ncan use the orient `table` to build a JSON string with two fields, `schema`\nand `data`.\n\nThe `schema` field contains the `fields` key, which itself contains a list of\ncolumn name to type pairs, including the `Index` or `MultiIndex` (see below\nfor a list of types). The `schema` field also contains a `primaryKey` field if\nthe (Multi)index is unique.\n\nThe second field, `data`, contains the serialized data with the `records`\norient. The index is included, and any datetimes are ISO 8601 formatted, as\nrequired by the Table Schema spec.\n\nThe full list of types supported are described in the Table Schema spec. This\ntable shows the mapping from pandas types:\n\npandas type\n\nTable Schema type\n\nint64\n\ninteger\n\nfloat64\n\nnumber\n\nbool\n\nboolean\n\ndatetime64[ns]\n\ndatetime\n\ntimedelta64[ns]\n\nduration\n\ncategorical\n\nany\n\nobject\n\nstr\n\nA few notes on the generated table schema:\n\nThe `schema` object contains a `pandas_version` field. This contains the\nversion of pandas\u2019 dialect of the schema, and will be incremented with each\nrevision.\n\nAll dates are converted to UTC when serializing. Even timezone naive values,\nwhich are treated as UTC with an offset of 0.\n\ndatetimes with a timezone (before serializing), include an additional field\n`tz` with the time zone name (e.g. `'US/Central'`).\n\nPeriods are converted to timestamps before serialization, and so have the same\nbehavior of being converted to UTC. In addition, periods will contain and\nadditional field `freq` with the period\u2019s frequency, e.g. `'A-DEC'`.\n\nCategoricals use the `any` type and an `enum` constraint listing the set of\npossible values. Additionally, an `ordered` field is included:\n\nA `primaryKey` field, containing an array of labels, is included if the index\nis unique:\n\nThe `primaryKey` behavior is the same with MultiIndexes, but in this case the\n`primaryKey` is an array:\n\nThe default naming roughly follows these rules:\n\nFor series, the `object.name` is used. If that\u2019s none, then the name is\n`values`\n\nFor `DataFrames`, the stringified version of the column name is used\n\nFor `Index` (not `MultiIndex`), `index.name` is used, with a fallback to\n`index` if that is None.\n\nFor `MultiIndex`, `mi.names` is used. If any level has no name, then\n`level_<i>` is used.\n\n`read_json` also accepts `orient='table'` as an argument. This allows for the\npreservation of metadata such as dtypes and index names in a round-trippable\nmanner.\n\nPlease note that the literal string \u2018index\u2019 as the name of an `Index` is not\nround-trippable, nor are any names beginning with `'level_'` within a\n`MultiIndex`. These are used by default in `DataFrame.to_json()` to indicate\nmissing values and the subsequent read cannot distinguish the intent.\n\nWhen using `orient='table'` along with user-defined `ExtensionArray`, the\ngenerated schema will contain an additional `extDtype` key in the respective\n`fields` element. This extra key is not standard but does enable JSON\nroundtrips for extension types (e.g. `read_json(df.to_json(orient=\"table\"),\norient=\"table\")`).\n\nThe `extDtype` key carries the name of the extension, if you have properly\nregistered the `ExtensionDtype`, pandas will use said name to perform a lookup\ninto the registry and re-convert the serialized data into your custom dtype.\n\nWarning\n\nWe highly encourage you to read the HTML Table Parsing gotchas below regarding\nthe issues surrounding the BeautifulSoup4/html5lib/lxml parsers.\n\nThe top-level `read_html()` function can accept an HTML string/file/URL and\nwill parse HTML tables into list of pandas `DataFrames`. Let\u2019s look at a few\nexamples.\n\nNote\n\n`read_html` returns a `list` of `DataFrame` objects, even if there is only a\nsingle table contained in the HTML content.\n\nRead a URL with no options:\n\nNote\n\nThe data from the above URL changes every Monday so the resulting data above\nand the data below may be slightly different.\n\nRead in the content of the file from the above URL and pass it to `read_html`\nas a string:\n\nYou can even pass in an instance of `StringIO` if you so desire:\n\nNote\n\nThe following examples are not run by the IPython evaluator due to the fact\nthat having so many network-accessing functions slows down the documentation\nbuild. If you spot an error or an example that doesn\u2019t run, please do not\nhesitate to report it over on pandas GitHub issues page.\n\nRead a URL and match a table that contains specific text:\n\nSpecify a header row (by default `<th>` or `<td>` elements located within a\n`<thead>` are used to form the column index, if multiple rows are contained\nwithin `<thead>` then a MultiIndex is created); if specified, the header row\nis taken from the data minus the parsed header elements (`<th>` elements).\n\nSpecify an index column:\n\nSpecify a number of rows to skip:\n\nSpecify a number of rows to skip using a list (`range` works as well):\n\nSpecify an HTML attribute:\n\nSpecify values that should be converted to NaN:\n\nSpecify whether to keep the default set of NaN values:\n\nSpecify converters for columns. This is useful for numerical text data that\nhas leading zeros. By default columns that are numerical are cast to numeric\ntypes and the leading zeros are lost. To avoid this, we can convert these\ncolumns to strings.\n\nUse some combination of the above:\n\nRead in pandas `to_html` output (with some loss of floating point precision):\n\nThe `lxml` backend will raise an error on a failed parse if that is the only\nparser you provide. If you only have a single parser you can provide just a\nstring, but it is considered good practice to pass a list with one string if,\nfor example, the function expects a sequence of strings. You may use:\n\nOr you could pass `flavor='lxml'` without a list:\n\nHowever, if you have bs4 and html5lib installed and pass `None` or `['lxml',\n'bs4']` then the parse will most likely succeed. Note that as soon as a parse\nsucceeds, the function will return.\n\n`DataFrame` objects have an instance method `to_html` which renders the\ncontents of the `DataFrame` as an HTML table. The function arguments are as in\nthe method `to_string` described above.\n\nNote\n\nNot all of the possible options for `DataFrame.to_html` are shown here for\nbrevity\u2019s sake. See `to_html()` for the full set of options.\n\nHTML:\n\nThe `columns` argument will limit the columns shown:\n\nHTML:\n\n`float_format` takes a Python callable to control the precision of floating\npoint values:\n\nHTML:\n\n`bold_rows` will make the row labels bold by default, but you can turn that\noff:\n\nThe `classes` argument provides the ability to give the resulting HTML table\nCSS classes. Note that these classes are appended to the existing\n`'dataframe'` class.\n\nThe `render_links` argument provides the ability to add hyperlinks to cells\nthat contain URLs.\n\nHTML:\n\nFinally, the `escape` argument allows you to control whether the \u201c<\u201d, \u201c>\u201d and\n\u201c&\u201d characters escaped in the resulting HTML (by default it is `True`). So to\nget the HTML without escaped characters pass `escape=False`\n\nEscaped:\n\nNot escaped:\n\nNote\n\nSome browsers may not show a difference in the rendering of the previous two\nHTML tables.\n\nThere are some versioning issues surrounding the libraries that are used to\nparse HTML tables in the top-level pandas io function `read_html`.\n\nIssues with lxml\n\nBenefits\n\nlxml is very fast.\n\nlxml requires Cython to install correctly.\n\nDrawbacks\n\nlxml does not make any guarantees about the results of its parse unless it is\ngiven strictly valid markup.\n\nIn light of the above, we have chosen to allow you, the user, to use the lxml\nbackend, but this backend will use html5lib if lxml fails to parse\n\nIt is therefore highly recommended that you install both BeautifulSoup4 and\nhtml5lib, so that you will still get a valid result (provided everything else\nis valid) even if lxml fails.\n\nIssues with BeautifulSoup4 using lxml as a backend\n\nThe above issues hold here as well since BeautifulSoup4 is essentially just a\nwrapper around a parser backend.\n\nIssues with BeautifulSoup4 using html5lib as a backend\n\nBenefits\n\nhtml5lib is far more lenient than lxml and consequently deals with real-life\nmarkup in a much saner way rather than just, e.g., dropping an element without\nnotifying you.\n\nhtml5lib generates valid HTML5 markup from invalid markup automatically. This\nis extremely important for parsing HTML tables, since it guarantees a valid\ndocument. However, that does NOT mean that it is \u201ccorrect\u201d, since the process\nof fixing markup does not have a single definition.\n\nhtml5lib is pure Python and requires no additional build steps beyond its own\ninstallation.\n\nDrawbacks\n\nThe biggest drawback to using html5lib is that it is slow as molasses. However\nconsider the fact that many tables on the web are not big enough for the\nparsing algorithm runtime to matter. It is more likely that the bottleneck\nwill be in the process of reading the raw text from the URL over the web,\ni.e., IO (input-output). For very large tables, this might not be true.\n\nNew in version 1.3.0.\n\nCurrently there are no methods to read from LaTeX, only output methods.\n\nNote\n\nDataFrame and Styler objects currently have a `to_latex` method. We recommend\nusing the Styler.to_latex() method over DataFrame.to_latex() due to the\nformer\u2019s greater flexibility with conditional styling, and the latter\u2019s\npossible future deprecation.\n\nReview the documentation for Styler.to_latex, which gives examples of\nconditional styling and explains the operation of its keyword arguments.\n\nFor simple application the following pattern is sufficient.\n\nTo format values before output, chain the Styler.format method.\n\nNew in version 1.3.0.\n\nThe top-level `read_xml()` function can accept an XML string/file/URL and will\nparse nodes and attributes into a pandas `DataFrame`.\n\nNote\n\nSince there is no standard XML structure where design types can vary in many\nways, `read_xml` works best with flatter, shallow versions. If an XML document\nis deeply nested, use the `stylesheet` feature to transform XML into a flatter\nversion.\n\nLet\u2019s look at a few examples.\n\nRead an XML string:\n\nRead a URL with no options:\n\nRead in the content of the \u201cbooks.xml\u201d file and pass it to `read_xml` as a\nstring:\n\nRead in the content of the \u201cbooks.xml\u201d as instance of `StringIO` or `BytesIO`\nand pass it to `read_xml`:\n\nEven read XML from AWS S3 buckets such as Python Software Foundation\u2019s IRS 990\nForm:\n\nWith lxml as default `parser`, you access the full-featured XML library that\nextends Python\u2019s ElementTree API. One powerful tool is ability to query nodes\nselectively or conditionally with more expressive XPath:\n\nSpecify only elements or only attributes to parse:\n\nXML documents can have namespaces with prefixes and default namespaces without\nprefixes both of which are denoted with a special attribute `xmlns`. In order\nto parse by node under a namespace context, `xpath` must reference a prefix.\n\nFor example, below XML contains a namespace with prefix, `doc`, and URI at\n`https://example.com`. In order to parse `doc:row` nodes, `namespaces` must be\nused.\n\nSimilarly, an XML document can have a default namespace without prefix.\nFailing to assign a temporary prefix will return no nodes and raise a\n`ValueError`. But assigning any temporary name to correct URI allows parsing\nby nodes.\n\nHowever, if XPath does not reference node names such as default, `/*`, then\n`namespaces` is not required.\n\nWith lxml as parser, you can flatten nested XML documents with an XSLT script\nwhich also can be string/file/URL types. As background, XSLT is a special-\npurpose language written in a special XML file that can transform original XML\ndocuments into other XML, HTML, even text (CSV, JSON, etc.) using an XSLT\nprocessor.\n\nFor example, consider this somewhat nested structure of Chicago \u201cL\u201d Rides\nwhere station and rides elements encapsulate data in their own sections. With\nbelow XSLT, `lxml` can transform original nested document into a flatter\noutput (as shown below for demonstration) for easier parse into `DataFrame`:\n\nNew in version 1.3.0.\n\n`DataFrame` objects have an instance method `to_xml` which renders the\ncontents of the `DataFrame` as an XML document.\n\nNote\n\nThis method does not support special properties of XML including DTD, CData,\nXSD schemas, processing instructions, comments, and others. Only namespaces at\nthe root level is supported. However, `stylesheet` allows design changes after\ninitial output.\n\nLet\u2019s look at a few examples.\n\nWrite an XML without options:\n\nWrite an XML with new root and row name:\n\nWrite an attribute-centric XML:\n\nWrite a mix of elements and attributes:\n\nAny `DataFrames` with hierarchical columns will be flattened for XML element\nnames with levels delimited by underscores:\n\nWrite an XML with default namespace:\n\nWrite an XML with namespace prefix:\n\nWrite an XML without declaration or pretty print:\n\nWrite an XML and transform with stylesheet:\n\nAll XML documents adhere to W3C specifications. Both `etree` and `lxml`\nparsers will fail to parse any markup document that is not well-formed or\nfollows XML syntax rules. Do be aware HTML is not an XML document unless it\nfollows XHTML specs. However, other popular markup types including KML, XAML,\nRSS, MusicML, MathML are compliant XML schemas.\n\nFor above reason, if your application builds XML prior to pandas operations,\nuse appropriate DOM libraries like `etree` and `lxml` to build the necessary\ndocument and not by string concatenation or regex adjustments. Always remember\nXML is a special text file with markup rules.\n\nWith very large XML files (several hundred MBs to GBs), XPath and XSLT can\nbecome memory-intensive operations. Be sure to have enough available RAM for\nreading and writing to large XML files (roughly about 5 times the size of\ntext).\n\nBecause XSLT is a programming language, use it with caution since such scripts\ncan pose a security risk in your environment and can run large or infinite\nrecursive operations. Always test scripts on small fragments before full run.\n\nThe etree parser supports all functionality of both `read_xml` and `to_xml`\nexcept for complex XPath and any XSLT. Though limited in features, `etree` is\nstill a reliable and capable parser and tree builder. Its performance may\ntrail `lxml` to a certain degree for larger files but relatively unnoticeable\non small to medium size files.\n\nThe `read_excel()` method can read Excel 2007+ (`.xlsx`) files using the\n`openpyxl` Python module. Excel 2003 (`.xls`) files can be read using `xlrd`.\nBinary Excel (`.xlsb`) files can be read using `pyxlsb`. The `to_excel()`\ninstance method is used for saving a `DataFrame` to Excel. Generally the\nsemantics are similar to working with csv data. See the cookbook for some\nadvanced strategies.\n\nWarning\n\nThe xlwt package for writing old-style `.xls` excel files is no longer\nmaintained. The xlrd package is now only for reading old-style `.xls` files.\n\nBefore pandas 1.3.0, the default argument `engine=None` to `read_excel()`\nwould result in using the `xlrd` engine in many cases, including new Excel\n2007+ (`.xlsx`) files. pandas will now default to using the openpyxl engine.\n\nIt is strongly encouraged to install `openpyxl` to read Excel 2007+ (`.xlsx`)\nfiles. Please do not report issues when using ``xlrd`` to read ``.xlsx``\nfiles. This is no longer supported, switch to using `openpyxl` instead.\n\nAttempting to use the the `xlwt` engine will raise a `FutureWarning` unless\nthe option `io.excel.xls.writer` is set to `\"xlwt\"`. While this option is now\ndeprecated and will also raise a `FutureWarning`, it can be globally set and\nthe warning suppressed. Users are recommended to write `.xlsx` files using the\n`openpyxl` engine instead.\n\nIn the most basic use-case, `read_excel` takes a path to an Excel file, and\nthe `sheet_name` indicating which sheet to parse.\n\nTo facilitate working with multiple sheets from the same file, the `ExcelFile`\nclass can be used to wrap the file and can be passed into `read_excel` There\nwill be a performance benefit for reading multiple sheets as the file is read\ninto memory only once.\n\nThe `ExcelFile` class can also be used as a context manager.\n\nThe `sheet_names` property will generate a list of the sheet names in the\nfile.\n\nThe primary use-case for an `ExcelFile` is parsing multiple sheets with\ndifferent parameters:\n\nNote that if the same parsing parameters are used for all sheets, a list of\nsheet names can simply be passed to `read_excel` with no loss in performance.\n\n`ExcelFile` can also be called with a `xlrd.book.Book` object as a parameter.\nThis allows the user to control how the excel file is read. For example,\nsheets can be loaded on demand by calling `xlrd.open_workbook()` with\n`on_demand=True`.\n\nNote\n\nThe second argument is `sheet_name`, not to be confused with\n`ExcelFile.sheet_names`.\n\nNote\n\nAn ExcelFile\u2019s attribute `sheet_names` provides access to a list of sheets.\n\nThe arguments `sheet_name` allows specifying the sheet or sheets to read.\n\nThe default value for `sheet_name` is 0, indicating to read the first sheet\n\nPass a string to refer to the name of a particular sheet in the workbook.\n\nPass an integer to refer to the index of a sheet. Indices follow Python\nconvention, beginning at 0.\n\nPass a list of either strings or integers, to return a dictionary of specified\nsheets.\n\nPass a `None` to return a dictionary of all available sheets.\n\nUsing the sheet index:\n\nUsing all default values:\n\nUsing None to get all sheets:\n\nUsing a list to get multiple sheets:\n\n`read_excel` can read more than one sheet, by setting `sheet_name` to either a\nlist of sheet names, a list of sheet positions, or `None` to read all sheets.\nSheets can be specified by sheet index or sheet name, using an integer or\nstring, respectively.\n\n`read_excel` can read a `MultiIndex` index, by passing a list of columns to\n`index_col` and a `MultiIndex` column by passing a list of rows to `header`.\nIf either the `index` or `columns` have serialized level names those will be\nread in as well by specifying the rows/columns that make up the levels.\n\nFor example, to read in a `MultiIndex` index without names:\n\nIf the index has level names, they will parsed as well, using the same\nparameters.\n\nIf the source file has both `MultiIndex` index and columns, lists specifying\neach should be passed to `index_col` and `header`:\n\nIt is often the case that users will insert columns to do temporary\ncomputations in Excel and you may not want to read in those columns.\n`read_excel` takes a `usecols` keyword to allow you to specify a subset of\ncolumns to parse.\n\nChanged in version 1.0.0.\n\nPassing in an integer for `usecols` will no longer work. Please pass in a list\nof ints from 0 to `usecols` inclusive instead.\n\nYou can specify a comma-delimited set of Excel columns and ranges as a string:\n\nIf `usecols` is a list of integers, then it is assumed to be the file column\nindices to be parsed.\n\nElement order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`.\n\nIf `usecols` is a list of strings, it is assumed that each string corresponds\nto a column name provided either by the user in `names` or inferred from the\ndocument header row(s). Those strings define which columns will be parsed:\n\nElement order is ignored, so `usecols=['baz', 'joe']` is the same as `['joe',\n'baz']`.\n\nIf `usecols` is callable, the callable function will be evaluated against the\ncolumn names, returning names where the callable function evaluates to `True`.\n\nDatetime-like values are normally automatically converted to the appropriate\ndtype when reading the excel file. But if you have a column of strings that\nlook like dates (but are not actually formatted as dates in excel), you can\nuse the `parse_dates` keyword to parse those strings to datetimes:\n\nIt is possible to transform the contents of Excel cells via the `converters`\noption. For instance, to convert a column to boolean:\n\nThis options handles missing values and treats exceptions in the converters as\nmissing data. Transformations are applied cell by cell rather than to the\ncolumn as a whole, so the array dtype is not guaranteed. For instance, a\ncolumn of integers with missing values cannot be transformed to an array with\ninteger dtype, because NaN is strictly a float. You can manually mask missing\ndata to recover integer dtype:\n\nAs an alternative to converters, the type for an entire column can be\nspecified using the `dtype` keyword, which takes a dictionary mapping column\nnames to types. To interpret data with no type inference, use the type `str`\nor `object`.\n\nTo write a `DataFrame` object to a sheet of an Excel file, you can use the\n`to_excel` instance method. The arguments are largely the same as `to_csv`\ndescribed above, the first argument being the name of the excel file, and the\noptional second argument the name of the sheet to which the `DataFrame` should\nbe written. For example:\n\nFiles with a `.xls` extension will be written using `xlwt` and those with a\n`.xlsx` extension will be written using `xlsxwriter` (if available) or\n`openpyxl`.\n\nThe `DataFrame` will be written in a way that tries to mimic the REPL output.\nThe `index_label` will be placed in the second row instead of the first. You\ncan place it in the first row by setting the `merge_cells` option in\n`to_excel()` to `False`:\n\nIn order to write separate `DataFrames` to separate sheets in a single Excel\nfile, one can pass an `ExcelWriter`.\n\npandas supports writing Excel files to buffer-like objects such as `StringIO`\nor `BytesIO` using `ExcelWriter`.\n\nNote\n\n`engine` is optional but recommended. Setting the engine determines the\nversion of workbook produced. Setting `engine='xlrd'` will produce an Excel\n2003-format workbook (xls). Using either `'openpyxl'` or `'xlsxwriter'` will\nproduce an Excel 2007-format workbook (xlsx). If omitted, an Excel\n2007-formatted workbook is produced.\n\nDeprecated since version 1.2.0: As the xlwt package is no longer maintained,\nthe `xlwt` engine will be removed from a future version of pandas. This is the\nonly engine in pandas that supports writing to `.xls` files.\n\npandas chooses an Excel writer via two methods:\n\nthe `engine` keyword argument\n\nthe filename extension (via the default specified in config options)\n\nBy default, pandas uses the XlsxWriter for `.xlsx`, openpyxl for `.xlsm`, and\nxlwt for `.xls` files. If you have multiple engines installed, you can set the\ndefault engine through setting the config options `io.excel.xlsx.writer` and\n`io.excel.xls.writer`. pandas will fall back on openpyxl for `.xlsx` files if\nXlsxwriter is not available.\n\nTo specify which writer you want to use, you can pass an engine keyword\nargument to `to_excel` and to `ExcelWriter`. The built-in engines are:\n\n`openpyxl`: version 2.4 or higher is required\n\n`xlsxwriter`\n\n`xlwt`\n\nThe look and feel of Excel worksheets created from pandas can be modified\nusing the following parameters on the `DataFrame`\u2019s `to_excel` method.\n\n`float_format` : Format string for floating point numbers (default `None`).\n\n`freeze_panes` : A tuple of two integers representing the bottommost row and\nrightmost column to freeze. Each of these parameters is one-based, so (1, 1)\nwill freeze the first row and first column (default `None`).\n\nUsing the Xlsxwriter engine provides many options for controlling the format\nof an Excel worksheet created with the `to_excel` method. Excellent examples\ncan be found in the Xlsxwriter documentation here:\nhttps://xlsxwriter.readthedocs.io/working_with_pandas.html\n\nNew in version 0.25.\n\nThe `read_excel()` method can also read OpenDocument spreadsheets using the\n`odfpy` module. The semantics and features for reading OpenDocument\nspreadsheets match what can be done for Excel files using `engine='odf'`.\n\nNote\n\nCurrently pandas only supports reading OpenDocument spreadsheets. Writing is\nnot implemented.\n\nNew in version 1.0.0.\n\nThe `read_excel()` method can also read binary Excel files using the `pyxlsb`\nmodule. The semantics and features for reading binary Excel files mostly match\nwhat can be done for Excel files using `engine='pyxlsb'`. `pyxlsb` does not\nrecognize datetime types in files and will return floats instead.\n\nNote\n\nCurrently pandas only supports reading binary Excel files. Writing is not\nimplemented.\n\nA handy way to grab data is to use the `read_clipboard()` method, which takes\nthe contents of the clipboard buffer and passes them to the `read_csv` method.\nFor instance, you can copy the following text to the clipboard (CTRL-C on many\noperating systems):\n\nAnd then import the data directly to a `DataFrame` by calling:\n\nThe `to_clipboard` method can be used to write the contents of a `DataFrame`\nto the clipboard. Following which you can paste the clipboard contents into\nother applications (CTRL-V on many operating systems). Here we illustrate\nwriting a `DataFrame` into clipboard and reading it back.\n\nWe can see that we got the same content back, which we had earlier written to\nthe clipboard.\n\nNote\n\nYou may need to install xclip or xsel (with PyQt5, PyQt4 or qtpy) on Linux to\nuse these methods.\n\nAll pandas objects are equipped with `to_pickle` methods which use Python\u2019s\n`cPickle` module to save data structures to disk using the pickle format.\n\nThe `read_pickle` function in the `pandas` namespace can be used to load any\npickled pandas object (or any other pickled object) from file:\n\nWarning\n\nLoading pickled data received from untrusted sources can be unsafe.\n\nSee: https://docs.python.org/3/library/pickle.html\n\nWarning\n\n`read_pickle()` is only guaranteed backwards compatible back to pandas version\n0.20.3\n\n`read_pickle()`, `DataFrame.to_pickle()` and `Series.to_pickle()` can read and\nwrite compressed pickle files. The compression types of `gzip`, `bz2`, `xz`,\n`zstd` are supported for reading and writing. The `zip` file format only\nsupports reading and must contain only one data file to be read.\n\nThe compression type can be an explicit parameter or be inferred from the file\nextension. If \u2018infer\u2019, then use `gzip`, `bz2`, `zip`, `xz`, `zstd` if filename\nends in `'.gz'`, `'.bz2'`, `'.zip'`, `'.xz'`, or `'.zst'`, respectively.\n\nThe compression parameter can also be a `dict` in order to pass options to the\ncompression protocol. It must have a `'method'` key set to the name of the\ncompression protocol, which must be one of {`'zip'`, `'gzip'`, `'bz2'`,\n`'xz'`, `'zstd'`}. All other key-value pairs are passed to the underlying\ncompression library.\n\nUsing an explicit compression type:\n\nInferring compression type from the extension:\n\nThe default is to \u2018infer\u2019:\n\nPassing options to the compression protocol in order to speed up compression:\n\npandas support for `msgpack` has been removed in version 1.0.0. It is\nrecommended to use pickle instead.\n\nAlternatively, you can also the Arrow IPC serialization format for on-the-wire\ntransmission of pandas objects. For documentation on pyarrow, see here.\n\n`HDFStore` is a dict-like object which reads and writes pandas using the high\nperformance HDF5 format using the excellent PyTables library. See the cookbook\nfor some advanced strategies\n\nWarning\n\npandas uses PyTables for reading and writing HDF5 files, which allows\nserializing object-dtype data with pickle. Loading pickled data received from\nuntrusted sources can be unsafe.\n\nSee: https://docs.python.org/3/library/pickle.html for more.\n\nObjects can be written to the file just like adding key-value pairs to a dict:\n\nIn a current or later Python session, you can retrieve stored objects:\n\nDeletion of the object specified by the key:\n\nClosing a Store and using a context manager:\n\n`HDFStore` supports a top-level API using `read_hdf` for reading and `to_hdf`\nfor writing, similar to how `read_csv` and `to_csv` work.\n\nHDFStore will by default not drop rows that are all missing. This behavior can\nbe changed by setting `dropna=True`.\n\nThe examples above show storing using `put`, which write the HDF5 to\n`PyTables` in a fixed array format, called the `fixed` format. These types of\nstores are not appendable once written (though you can simply remove them and\nrewrite). Nor are they queryable; they must be retrieved in their entirety.\nThey also do not support dataframes with non-unique column names. The `fixed`\nformat stores offer very fast writing and slightly faster reading than `table`\nstores. This format is specified by default when using `put` or `to_hdf` or by\n`format='fixed'` or `format='f'`.\n\nWarning\n\nA `fixed` format will raise a `TypeError` if you try to retrieve using a\n`where`:\n\n`HDFStore` supports another `PyTables` format on disk, the `table` format.\nConceptually a `table` is shaped very much like a DataFrame, with rows and\ncolumns. A `table` may be appended to in the same or other sessions. In\naddition, delete and query type operations are supported. This format is\nspecified by `format='table'` or `format='t'` to `append` or `put` or\n`to_hdf`.\n\nThis format can be set as an option as well\n`pd.set_option('io.hdf.default_format','table')` to enable `put/append/to_hdf`\nto by default store in the `table` format.\n\nNote\n\nYou can also create a `table` by passing `format='table'` or `format='t'` to a\n`put` operation.\n\nKeys to a store can be specified as a string. These can be in a hierarchical\npath-name like format (e.g. `foo/bar/bah`), which will generate a hierarchy of\nsub-stores (or `Groups` in PyTables parlance). Keys can be specified without\nthe leading \u2018/\u2019 and are always absolute (e.g. \u2018foo\u2019 refers to \u2018/foo\u2019). Removal\noperations can remove everything in the sub-store and below, so be careful.\n\nYou can walk through the group hierarchy using the `walk` method which will\nyield a tuple for each group key along with the relative keys of its contents.\n\nWarning\n\nHierarchical keys cannot be retrieved as dotted (attribute) access as\ndescribed above for items stored under the root node.\n\nInstead, use explicit string based keys:\n\nStoring mixed-dtype data is supported. Strings are stored as a fixed-width\nusing the maximum size of the appended column. Subsequent attempts at\nappending longer strings will raise a `ValueError`.\n\nPassing `min_itemsize={`values`: size}` as a parameter to append will set a\nlarger minimum for the string columns. Storing `floats, strings, ints, bools,\ndatetime64` are currently supported. For string columns, passing `nan_rep =\n'nan'` to append will change the default nan representation on disk (which\nconverts to/from `np.nan`), this defaults to `nan`.\n\nStoring MultiIndex `DataFrames` as tables is very similar to storing/selecting\nfrom homogeneous index `DataFrames`.\n\nNote\n\nThe `index` keyword is reserved and cannot be use as a level name.\n\n`select` and `delete` operations have an optional criterion that can be\nspecified to select/delete only a subset of the data. This allows one to have\na very large on-disk table and retrieve only a portion of the data.\n\nA query is specified using the `Term` class under the hood, as a boolean\nexpression.\n\n`index` and `columns` are supported indexers of `DataFrames`.\n\nif `data_columns` are specified, these can be used as additional indexers.\n\nlevel name in a MultiIndex, with default name `level_0`, `level_1`, \u2026 if not\nprovided.\n\nValid comparison operators are:\n\n`=, ==, !=, >, >=, <, <=`\n\nValid boolean expressions are combined with:\n\n`|` : or\n\n`&` : and\n\n`(` and `)` : for grouping\n\nThese rules are similar to how boolean expressions are used in pandas for\nindexing.\n\nNote\n\n`=` will be automatically expanded to the comparison operator `==`\n\n`~` is the not operator, but can only be used in very limited circumstances\n\nIf a list/tuple of expressions is passed they will be combined via `&`\n\nThe following are valid expressions:\n\n`'index >= date'`\n\n`\"columns = ['A', 'D']\"`\n\n`\"columns in ['A', 'D']\"`\n\n`'columns = A'`\n\n`'columns == A'`\n\n`\"~(columns = ['A', 'B'])\"`\n\n`'index > df.index[3] & string = \"bar\"'`\n\n`'(index > df.index[3] & index <= df.index[6]) | string = \"bar\"'`\n\n`\"ts >= Timestamp('2012-02-01')\"`\n\n`\"major_axis>=20130101\"`\n\nThe `indexers` are on the left-hand side of the sub-expression:\n\n`columns`, `major_axis`, `ts`\n\nThe right-hand side of the sub-expression (after a comparison operator) can\nbe:\n\nfunctions that will be evaluated, e.g. `Timestamp('2012-02-01')`\n\nstrings, e.g. `\"bar\"`\n\ndate-like, e.g. `20130101`, or `\"20130101\"`\n\nlists, e.g. `\"['A', 'B']\"`\n\nvariables that are defined in the local names space, e.g. `date`\n\nNote\n\nPassing a string to a query by interpolating it into the query expression is\nnot recommended. Simply assign the string of interest to a variable and use\nthat variable in an expression. For example, do this\n\ninstead of this\n\nThe latter will not work and will raise a `SyntaxError`.Note that there\u2019s a\nsingle quote followed by a double quote in the `string` variable.\n\nIf you must interpolate, use the `'%r'` format specifier\n\nwhich will quote `string`.\n\nHere are some examples:\n\nUse boolean expressions, with in-line function evaluation.\n\nUse inline column reference.\n\nThe `columns` keyword can be supplied to select a list of columns to be\nreturned, this is equivalent to passing a\n`'columns=list_of_columns_to_filter'`:\n\n`start` and `stop` parameters can be specified to limit the total search\nspace. These are in terms of the total number of rows in a table.\n\nNote\n\n`select` will raise a `ValueError` if the query expression has an unknown\nvariable reference. Usually this means that you are trying to select on a\ncolumn that is not a data_column.\n\n`select` will raise a `SyntaxError` if the query expression is not valid.\n\nYou can store and query using the `timedelta64[ns]` type. Terms can be\nspecified in the format: `<float>(<unit>)`, where float may be signed (and\nfractional), and unit can be `D,s,ms,us,ns` for the timedelta. Here\u2019s an\nexample:\n\nSelecting from a `MultiIndex` can be achieved by using the name of the level.\n\nIf the `MultiIndex` levels names are `None`, the levels are automatically made\navailable via the `level_n` keyword with `n` the level of the `MultiIndex` you\nwant to select from.\n\nYou can create/modify an index for a table with `create_table_index` after\ndata is already in the table (after and `append/put` operation). Creating a\ntable index is highly encouraged. This will speed your queries a great deal\nwhen you use a `select` with the indexed dimension as the `where`.\n\nNote\n\nIndexes are automagically created on the indexables and any data columns you\nspecify. This behavior can be turned off by passing `index=False` to `append`.\n\nOftentimes when appending large amounts of data to a store, it is useful to\nturn off index creation for each append, then recreate at the end.\n\nThen create the index when finished appending.\n\nSee here for how to create a completely-sorted-index (CSI) on an existing\nstore.\n\nYou can designate (and index) certain columns that you want to be able to\nperform queries (other than the `indexable` columns, which you can always\nquery). For instance say you want to perform this common operation, on-disk,\nand return just the frame that matches this query. You can specify\n`data_columns = True` to force all columns to be `data_columns`.\n\nThere is some performance degradation by making lots of columns into `data\ncolumns`, so it is up to the user to designate these. In addition, you cannot\nchange data columns (nor indexables) after the first append/put operation (Of\ncourse you can simply read in the data and create a new table!).\n\nYou can pass `iterator=True` or `chunksize=number_in_a_chunk` to `select` and\n`select_as_multiple` to return an iterator on the results. The default is\n50,000 rows returned in a chunk.\n\nNote\n\nYou can also use the iterator with `read_hdf` which will open, then\nautomatically close the store when finished iterating.\n\nNote, that the chunksize keyword applies to the source rows. So if you are\ndoing a query, then the chunksize will subdivide the total rows in the table\nand the query applied, returning an iterator on potentially unequal sized\nchunks.\n\nHere is a recipe for generating a query and using it to create equal sized\nreturn chunks.\n\nTo retrieve a single indexable or data column, use the method `select_column`.\nThis will, for example, enable you to get the index very quickly. These return\na `Series` of the result, indexed by the row number. These do not currently\naccept the `where` selector.\n\nSometimes you want to get the coordinates (a.k.a the index locations) of your\nquery. This returns an `Int64Index` of the resulting locations. These\ncoordinates can also be passed to subsequent `where` operations.\n\nSometime your query can involve creating a list of rows to select. Usually\nthis `mask` would be a resulting `index` from an indexing operation. This\nexample selects the months of a datetimeindex which are 5.\n\nIf you want to inspect the stored object, retrieve via `get_storer`. You could\nuse this programmatically to say get the number of rows in an object.\n\nThe methods `append_to_multiple` and `select_as_multiple` can perform\nappending/selecting from multiple tables at once. The idea is to have one\ntable (call it the selector table) that you index most/all of the columns, and\nperform your queries. The other table(s) are data tables with an index\nmatching the selector table\u2019s index. You can then perform a very fast query on\nthe selector table, yet get lots of data back. This method is similar to\nhaving a very wide table, but enables more efficient queries.\n\nThe `append_to_multiple` method splits a given single DataFrame into multiple\ntables according to `d`, a dictionary that maps the table names to a list of\n\u2018columns\u2019 you want in that table. If `None` is used in place of a list, that\ntable will have the remaining unspecified columns of the given DataFrame. The\nargument `selector` defines which table is the selector table (which you can\nmake queries from). The argument `dropna` will drop rows from the input\n`DataFrame` to ensure tables are synchronized. This means that if a row for\none of the tables being written to is entirely `np.NaN`, that row will be\ndropped from all tables.\n\nIf `dropna` is False, THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES.\nRemember that entirely `np.Nan` rows are not written to the HDFStore, so if\nyou choose to call `dropna=False`, some tables may have more rows than others,\nand therefore `select_as_multiple` may not work or it may return unexpected\nresults.\n\nYou can delete from a table selectively by specifying a `where`. In deleting\nrows, it is important to understand the `PyTables` deletes rows by erasing the\nrows, then moving the following data. Thus deleting can potentially be a very\nexpensive operation depending on the orientation of your data. To get optimal\nperformance, it\u2019s worthwhile to have the dimension you are deleting be the\nfirst of the `indexables`.\n\nData is ordered (on the disk) in terms of the `indexables`. Here\u2019s a simple\nuse case. You store panel-type data, with dates in the `major_axis` and ids in\nthe `minor_axis`. The data is then interleaved like this:\n\nid_1\n\nid_2\n\n.\n\nid_n\n\nid_1\n\n.\n\nid_n\n\nIt should be clear that a delete operation on the `major_axis` will be fairly\nquick, as one chunk is removed, then the following data moved. On the other\nhand a delete operation on the `minor_axis` will be very expensive. In this\ncase it would almost certainly be faster to rewrite the table using a `where`\nthat selects all but the missing data.\n\nWarning\n\nPlease note that HDF5 DOES NOT RECLAIM SPACE in the h5 files automatically.\nThus, repeatedly deleting (or removing nodes) and adding again, WILL TEND TO\nINCREASE THE FILE SIZE.\n\nTo repack and clean the file, use ptrepack.\n\n`PyTables` allows the stored data to be compressed. This applies to all kinds\nof stores, not just tables. Two parameters are used to control compression:\n`complevel` and `complib`.\n\n`complevel` specifies if and how hard data is to be compressed. `complevel=0`\nand `complevel=None` disables compression and `0<complevel<10` enables\ncompression.\n\n`complib` specifies which compression library to use. If nothing is specified\nthe default library `zlib` is used. A compression library usually optimizes\nfor either good compression rates or speed and the results will depend on the\ntype of data. Which type of compression to choose depends on your specific\nneeds and data. The list of supported compression libraries:\n\nzlib: The default compression library. A classic in terms of compression,\nachieves good compression rates but is somewhat slow.\n\nlzo: Fast compression and decompression.\n\nbzip2: Good compression rates.\n\nblosc: Fast compression and decompression.\n\nSupport for alternative blosc compressors:\n\nblosc:blosclz This is the default compressor for `blosc`\n\nblosc:lz4: A compact, very popular and fast compressor.\n\nblosc:lz4hc: A tweaked version of LZ4, produces better compression ratios at\nthe expense of speed.\n\nblosc:snappy: A popular compressor used in many places.\n\nblosc:zlib: A classic; somewhat slower than the previous ones, but achieving\nbetter compression ratios.\n\nblosc:zstd: An extremely well balanced codec; it provides the best compression\nratios among the others above, and at reasonably fast speed.\n\nIf `complib` is defined as something other than the listed libraries a\n`ValueError` exception is issued.\n\nNote\n\nIf the library specified with the `complib` option is missing on your\nplatform, compression defaults to `zlib` without further ado.\n\nEnable compression for all objects within the file:\n\nOr on-the-fly compression (this only applies to tables) in stores where\ncompression is not enabled:\n\n`PyTables` offers better write performance when tables are compressed after\nthey are written, as opposed to turning on compression at the very beginning.\nYou can use the supplied `PyTables` utility `ptrepack`. In addition,\n`ptrepack` can change compression levels after the fact.\n\nFurthermore `ptrepack in.h5 out.h5` will repack the file to allow you to reuse\npreviously deleted space. Alternatively, one can simply remove the file and\nwrite again, or use the `copy` method.\n\nWarning\n\n`HDFStore` is not-threadsafe for writing. The underlying `PyTables` only\nsupports concurrent reads (via threading or processes). If you need reading\nand writing at the same time, you need to serialize these operations in a\nsingle thread in a single process. You will corrupt your data otherwise. See\nthe (GH2397) for more information.\n\nIf you use locks to manage write access between multiple processes, you may\nwant to use `fsync()` before releasing write locks. For convenience you can\nuse `store.flush(fsync=True)` to do this for you.\n\nOnce a `table` is created columns (DataFrame) are fixed; only exactly the same\ncolumns can be appended\n\nBe aware that timezones (e.g., `pytz.timezone('US/Eastern')`) are not\nnecessarily equal across timezone versions. So if data is localized to a\nspecific timezone in the HDFStore using one version of a timezone library and\nthat data is updated with another version, the data will be converted to UTC\nsince these timezones are not considered equal. Either use the same version of\ntimezone library or use `tz_convert` with the updated timezone definition.\n\nWarning\n\n`PyTables` will show a `NaturalNameWarning` if a column name cannot be used as\nan attribute selector. Natural identifiers contain only letters, numbers, and\nunderscores, and may not begin with a number. Other identifiers cannot be used\nin a `where` clause and are generally a bad idea.\n\n`HDFStore` will map an object dtype to the `PyTables` underlying dtype. This\nmeans the following types are known to work:\n\nType\n\nRepresents missing values\n\nfloating : `float64, float32, float16`\n\n`np.nan`\n\ninteger : `int64, int32, int8, uint64,uint32, uint8`\n\nboolean\n\n`datetime64[ns]`\n\n`NaT`\n\n`timedelta64[ns]`\n\n`NaT`\n\ncategorical : see the section below\n\nobject : `strings`\n\n`np.nan`\n\n`unicode` columns are not supported, and WILL FAIL.\n\nYou can write data that contains `category` dtypes to a `HDFStore`. Queries\nwork the same as if it was an object array. However, the `category` dtyped\ndata is stored in a more efficient manner.\n\nmin_itemsize\n\nThe underlying implementation of `HDFStore` uses a fixed column width\n(itemsize) for string columns. A string column itemsize is calculated as the\nmaximum of the length of data (for that column) that is passed to the\n`HDFStore`, in the first append. Subsequent appends, may introduce a string\nfor a column larger than the column can hold, an Exception will be raised\n(otherwise you could have a silent truncation of these columns, leading to\nloss of information). In the future we may relax this and allow a user-\nspecified truncation to occur.\n\nPass `min_itemsize` on the first table creation to a-priori specify the\nminimum length of a particular string column. `min_itemsize` can be an\ninteger, or a dict mapping a column name to an integer. You can pass `values`\nas a key to allow all indexables or data_columns to have this min_itemsize.\n\nPassing a `min_itemsize` dict will cause all passed columns to be created as\ndata_columns automatically.\n\nNote\n\nIf you are not passing any `data_columns`, then the `min_itemsize` will be the\nmaximum of the length of any string passed\n\nnan_rep\n\nString columns will serialize a `np.nan` (a missing value) with the `nan_rep`\nstring representation. This defaults to the string value `nan`. You could\ninadvertently turn an actual `nan` value into a missing value.\n\n`HDFStore` writes `table` format objects in specific formats suitable for\nproducing loss-less round trips to pandas objects. For external compatibility,\n`HDFStore` can read native `PyTables` format tables.\n\nIt is possible to write an `HDFStore` object that can easily be imported into\n`R` using the `rhdf5` library (Package website). Create a table format store\nlike this:\n\nIn R this file can be read into a `data.frame` object using the `rhdf5`\nlibrary. The following example function reads the corresponding column names\nand data values from the values and assembles them into a `data.frame`:\n\nNow you can import the `DataFrame` into R:\n\nNote\n\nThe R function lists the entire HDF5 file\u2019s contents and assembles the\n`data.frame` object from all matching nodes, so use this only as a starting\npoint if you have stored multiple `DataFrame` objects to a single HDF5 file.\n\n`tables` format come with a writing performance penalty as compared to `fixed`\nstores. The benefit is the ability to append/delete and query (potentially\nvery large amounts of data). Write times are generally longer as compared with\nregular stores. Query times can be quite fast, especially on an indexed axis.\n\nYou can pass `chunksize=<int>` to `append`, specifying the write chunksize\n(default is 50000). This will significantly lower your memory usage on\nwriting.\n\nYou can pass `expectedrows=<int>` to the first `append`, to set the TOTAL\nnumber of rows that `PyTables` will expect. This will optimize read/write\nperformance.\n\nDuplicate rows can be written to tables, but are filtered out in selection\n(with the last items being selected; thus a table is unique on major, minor\npairs)\n\nA `PerformanceWarning` will be raised if you are attempting to store types\nthat will be pickled by PyTables (rather than stored as endemic types). See\nHere for more information and some solutions.\n\nFeather provides binary columnar serialization for data frames. It is designed\nto make reading and writing data frames efficient, and to make sharing data\nacross data analysis languages easy.\n\nFeather is designed to faithfully serialize and de-serialize DataFrames,\nsupporting all of the pandas dtypes, including extension dtypes such as\ncategorical and datetime with tz.\n\nSeveral caveats:\n\nThe format will NOT write an `Index`, or `MultiIndex` for the `DataFrame` and\nwill raise an error if a non-default one is provided. You can `.reset_index()`\nto store the index or `.reset_index(drop=True)` to ignore it.\n\nDuplicate column names and non-string columns names are not supported\n\nActual Python objects in object dtype columns are not supported. These will\nraise a helpful error message on an attempt at serialization.\n\nSee the Full Documentation.\n\nWrite to a feather file.\n\nRead from a feather file.\n\nApache Parquet provides a partitioned binary columnar serialization for data\nframes. It is designed to make reading and writing data frames efficient, and\nto make sharing data across data analysis languages easy. Parquet can use a\nvariety of compression techniques to shrink the file size as much as possible\nwhile still maintaining good read performance.\n\nParquet is designed to faithfully serialize and de-serialize `DataFrame` s,\nsupporting all of the pandas dtypes, including extension dtypes such as\ndatetime with tz.\n\nSeveral caveats.\n\nDuplicate column names and non-string columns names are not supported.\n\nThe `pyarrow` engine always writes the index to the output, but `fastparquet`\nonly writes non-default indexes. This extra column can cause problems for non-\npandas consumers that are not expecting it. You can force including or\nomitting indexes with the `index` argument, regardless of the underlying\nengine.\n\nIndex level names, if specified, must be strings.\n\nIn the `pyarrow` engine, categorical dtypes for non-string types can be\nserialized to parquet, but will de-serialize as their primitive dtype.\n\nThe `pyarrow` engine preserves the `ordered` flag of categorical dtypes with\nstring types. `fastparquet` does not preserve the `ordered` flag.\n\nNon supported types include `Interval` and actual Python object types. These\nwill raise a helpful error message on an attempt at serialization. `Period`\ntype is supported with pyarrow >= 0.16.0.\n\nThe `pyarrow` engine preserves extension data types such as the nullable\ninteger and string data type (requiring pyarrow >= 0.16.0, and requiring the\nextension type to implement the needed protocols, see the extension types\ndocumentation).\n\nYou can specify an `engine` to direct the serialization. This can be one of\n`pyarrow`, or `fastparquet`, or `auto`. If the engine is NOT specified, then\nthe `pd.options.io.parquet.engine` option is checked; if this is also `auto`,\nthen `pyarrow` is tried, and falling back to `fastparquet`.\n\nSee the documentation for pyarrow and fastparquet.\n\nNote\n\nThese engines are very similar and should read/write nearly identical parquet\nformat files. Currently `pyarrow` does not support timedelta data,\n`fastparquet>=0.1.4` supports timezone aware datetimes. These libraries differ\nby having different underlying dependencies (`fastparquet` by using `numba`,\nwhile `pyarrow` uses a c-library).\n\nWrite to a parquet file.\n\nRead from a parquet file.\n\nRead only certain columns of a parquet file.\n\nSerializing a `DataFrame` to parquet may include the implicit index as one or\nmore columns in the output file. Thus, this code:\n\ncreates a parquet file with three columns if you use `pyarrow` for\nserialization: `a`, `b`, and `__index_level_0__`. If you\u2019re using\n`fastparquet`, the index may or may not be written to the file.\n\nThis unexpected extra column causes some databases like Amazon Redshift to\nreject the file, because that column doesn\u2019t exist in the target table.\n\nIf you want to omit a dataframe\u2019s indexes when writing, pass `index=False` to\n`to_parquet()`:\n\nThis creates a parquet file with just the two expected columns, `a` and `b`.\nIf your `DataFrame` has a custom index, you won\u2019t get it back when you load\nthis file into a `DataFrame`.\n\nPassing `index=True` will always write the index, even if that\u2019s not the\nunderlying engine\u2019s default behavior.\n\nParquet supports partitioning of data based on the values of one or more\ncolumns.\n\nThe `path` specifies the parent directory to which data will be saved. The\n`partition_cols` are the column names by which the dataset will be\npartitioned. Columns are partitioned in the order they are given. The\npartition splits are determined by the unique values in the partition columns.\nThe above example creates a partitioned dataset that may look like:\n\nNew in version 1.0.0.\n\nSimilar to the parquet format, the ORC Format is a binary columnar\nserialization for data frames. It is designed to make reading data frames\nefficient. pandas provides only a reader for the ORC format, `read_orc()`.\nThis requires the pyarrow library.\n\nWarning\n\nIt is highly recommended to install pyarrow using conda due to some issues\noccurred by pyarrow.\n\n`read_orc()` is not supported on Windows yet, you can find valid environments\non install optional dependencies.\n\nThe `pandas.io.sql` module provides a collection of query wrappers to both\nfacilitate data retrieval and to reduce dependency on DB-specific API.\nDatabase abstraction is provided by SQLAlchemy if installed. In addition you\nwill need a driver library for your database. Examples of such drivers are\npsycopg2 for PostgreSQL or pymysql for MySQL. For SQLite this is included in\nPython\u2019s standard library by default. You can find an overview of supported\ndrivers for each SQL dialect in the SQLAlchemy docs.\n\nIf SQLAlchemy is not installed, a fallback is only provided for sqlite (and\nfor mysql for backwards compatibility, but this is deprecated and will be\nremoved in a future version). This mode requires a Python database adapter\nwhich respect the Python DB-API.\n\nSee also some cookbook examples for some advanced strategies.\n\nThe key functions are:\n\n`read_sql_table`(table_name, con[, schema, ...])\n\nRead SQL database table into a DataFrame.\n\n`read_sql_query`(sql, con[, index_col, ...])\n\nRead SQL query into a DataFrame.\n\n`read_sql`(sql, con[, index_col, ...])\n\nRead SQL query or database table into a DataFrame.\n\n`DataFrame.to_sql`(name, con[, schema, ...])\n\nWrite records stored in a DataFrame to a SQL database.\n\nNote\n\nThe function `read_sql()` is a convenience wrapper around `read_sql_table()`\nand `read_sql_query()` (and for backward compatibility) and will delegate to\nspecific function depending on the provided input (database table name or sql\nquery). Table names do not need to be quoted if they have special characters.\n\nIn the following example, we use the SQlite SQL database engine. You can use a\ntemporary SQLite database where data are stored in \u201cmemory\u201d.\n\nTo connect with SQLAlchemy you use the `create_engine()` function to create an\nengine object from database URI. You only need to create the engine once per\ndatabase you are connecting to. For more information on `create_engine()` and\nthe URI formatting, see the examples below and the SQLAlchemy documentation\n\nIf you want to manage your own connections you can pass one of those instead.\nThe example below opens a connection to the database using a Python context\nmanager that automatically closes the connection after the block has\ncompleted. See the SQLAlchemy docs for an explanation of how the database\nconnection is handled.\n\nWarning\n\nWhen you open a connection to a database you are also responsible for closing\nit. Side effects of leaving a connection open may include locking the database\nor other breaking behaviour.\n\nAssuming the following data is in a `DataFrame` `data`, we can insert it into\nthe database using `to_sql()`.\n\nid\n\nDate\n\nCol_1\n\nCol_2\n\nCol_3\n\n26\n\n2012-10-18\n\nX\n\n25.7\n\nTrue\n\n42\n\n2012-10-19\n\nY\n\n-12.4\nFalse\n\n63\n\n2012-10-20\n\nZ\n\n5.73\n\nTrue\n\nWith some databases, writing large DataFrames can result in errors due to\npacket size limitations being exceeded. This can be avoided by setting the\n`chunksize` parameter when calling `to_sql`. For example, the following writes\n`data` to the database in batches of 1000 rows at a time:\n\n`to_sql()` will try to map your data to an appropriate SQL data type based on\nthe dtype of the data. When you have columns of dtype `object`, pandas will\ntry to infer the data type.\n\nYou can always override the default type by specifying the desired SQL type of\nany of the columns by using the `dtype` argument. This argument needs a\ndictionary mapping column names to SQLAlchemy types (or strings for the\nsqlite3 fallback mode). For example, specifying to use the sqlalchemy `String`\ntype instead of the default `Text` type for string columns:\n\nNote\n\nDue to the limited support for timedelta\u2019s in the different database flavors,\ncolumns with type `timedelta64` will be written as integer values as\nnanoseconds to the database and a warning will be raised.\n\nNote\n\nColumns of `category` dtype will be converted to the dense representation as\nyou would get with `np.asarray(categorical)` (e.g. for string categories this\ngives an array of strings). Because of this, reading the database table back\nin does not generate a categorical.\n\nUsing SQLAlchemy, `to_sql()` is capable of writing datetime data that is\ntimezone naive or timezone aware. However, the resulting data stored in the\ndatabase ultimately depends on the supported data type for datetime data of\nthe database system being used.\n\nThe following table lists supported data types for datetime data for some\ncommon databases. Other database dialects may have different data types for\ndatetime data.\n\nDatabase\n\nSQL Datetime Types\n\nTimezone Support\n\nSQLite\n\n`TEXT`\n\nNo\n\nMySQL\n\n`TIMESTAMP` or `DATETIME`\n\nNo\n\nPostgreSQL\n\n`TIMESTAMP` or `TIMESTAMP WITH TIME ZONE`\n\nYes\n\nWhen writing timezone aware data to databases that do not support timezones,\nthe data will be written as timezone naive timestamps that are in local time\nwith respect to the timezone.\n\n`read_sql_table()` is also capable of reading datetime data that is timezone\naware or naive. When reading `TIMESTAMP WITH TIME ZONE` types, pandas will\nconvert the data to UTC.\n\nThe parameter `method` controls the SQL insertion clause used. Possible values\nare:\n\n`None`: Uses standard SQL `INSERT` clause (one per row).\n\n`'multi'`: Pass multiple values in a single `INSERT` clause. It uses a special\nSQL syntax not supported by all backends. This usually provides better\nperformance for analytic databases like Presto and Redshift, but has worse\nperformance for traditional SQL backend if the table contains many columns.\nFor more information check the SQLAlchemy documentation.\n\ncallable with signature `(pd_table, conn, keys, data_iter)`: This can be used\nto implement a more performant insertion method based on specific backend\ndialect features.\n\nExample of a callable using PostgreSQL COPY clause:\n\n`read_sql_table()` will read a database table given the table name and\noptionally a subset of columns to read.\n\nNote\n\nIn order to use `read_sql_table()`, you must have the SQLAlchemy optional\ndependency installed.\n\nNote\n\nNote that pandas infers column dtypes from query outputs, and not by looking\nup data types in the physical database schema. For example, assume `userid` is\nan integer column in a table. Then, intuitively, `select userid ...` will\nreturn integer-valued series, while `select cast(userid as text) ...` will\nreturn object-valued (str) series. Accordingly, if the query output is empty,\nthen all resulting columns will be returned as object-valued (since they are\nmost general). If you foresee that your query will sometimes generate an empty\nresult, you may want to explicitly typecast afterwards to ensure dtype\nintegrity.\n\nYou can also specify the name of the column as the `DataFrame` index, and\nspecify a subset of columns to be read.\n\nAnd you can explicitly force columns to be parsed as dates:\n\nIf needed you can explicitly specify a format string, or a dict of arguments\nto pass to `pandas.to_datetime()`:\n\nYou can check if a table exists using `has_table()`\n\nReading from and writing to different schema\u2019s is supported through the\n`schema` keyword in the `read_sql_table()` and `to_sql()` functions. Note\nhowever that this depends on the database flavor (sqlite does not have\nschema\u2019s). For example:\n\nYou can query using raw SQL in the `read_sql_query()` function. In this case\nyou must use the SQL variant appropriate for your database. When using\nSQLAlchemy, you can also pass SQLAlchemy Expression language constructs, which\nare database-agnostic.\n\nOf course, you can specify a more \u201ccomplex\u201d query.\n\nThe `read_sql_query()` function supports a `chunksize` argument. Specifying\nthis will return an iterator through chunks of the query result:\n\nYou can also run a plain query without creating a `DataFrame` with\n`execute()`. This is useful for queries that don\u2019t return values, such as\nINSERT. This is functionally equivalent to calling `execute` on the SQLAlchemy\nengine or db connection object. Again, you must use the SQL syntax variant\nappropriate for your database.\n\nTo connect with SQLAlchemy you use the `create_engine()` function to create an\nengine object from database URI. You only need to create the engine once per\ndatabase you are connecting to.\n\nFor more information see the examples the SQLAlchemy documentation\n\nYou can use SQLAlchemy constructs to describe your query.\n\nUse `sqlalchemy.text()` to specify query parameters in a backend-neutral way\n\nIf you have an SQLAlchemy description of your database you can express where\nconditions using SQLAlchemy expressions\n\nYou can combine SQLAlchemy expressions with parameters passed to `read_sql()`\nusing `sqlalchemy.bindparam()`\n\nThe use of sqlite is supported without using SQLAlchemy. This mode requires a\nPython database adapter which respect the Python DB-API.\n\nYou can create connections like so:\n\nAnd then issue the following queries:\n\nWarning\n\nStarting in 0.20.0, pandas has split off Google BigQuery support into the\nseparate package `pandas-gbq`. You can `pip install pandas-gbq` to get it.\n\nThe `pandas-gbq` package provides functionality to read/write from Google\nBigQuery.\n\npandas integrates with this external package. if `pandas-gbq` is installed,\nyou can use the pandas methods `pd.read_gbq` and `DataFrame.to_gbq`, which\nwill call the respective functions from `pandas-gbq`.\n\nFull documentation can be found here.\n\nThe method `to_stata()` will write a DataFrame into a .dta file. The format\nversion of this file is always 115 (Stata 12).\n\nStata data files have limited data type support; only strings with 244 or\nfewer characters, `int8`, `int16`, `int32`, `float32` and `float64` can be\nstored in `.dta` files. Additionally, Stata reserves certain values to\nrepresent missing data. Exporting a non-missing value that is outside of the\npermitted range in Stata for a particular data type will retype the variable\nto the next larger size. For example, `int8` values are restricted to lie\nbetween -127 and 100 in Stata, and so variables with values above 100 will\ntrigger a conversion to `int16`. `nan` values in floating points data types\nare stored as the basic missing data type (`.` in Stata).\n\nNote\n\nIt is not possible to export missing data values for integer data types.\n\nThe Stata writer gracefully handles other data types including `int64`,\n`bool`, `uint8`, `uint16`, `uint32` by casting to the smallest supported type\nthat can represent the data. For example, data with a type of `uint8` will be\ncast to `int8` if all values are less than 100 (the upper bound for non-\nmissing `int8` data in Stata), or, if values are outside of this range, the\nvariable is cast to `int16`.\n\nWarning\n\nConversion from `int64` to `float64` may result in a loss of precision if\n`int64` values are larger than 2**53.\n\nWarning\n\n`StataWriter` and `to_stata()` only support fixed width strings containing up\nto 244 characters, a limitation imposed by the version 115 dta file format.\nAttempting to write Stata dta files with strings longer than 244 characters\nraises a `ValueError`.\n\nThe top-level function `read_stata` will read a dta file and return either a\n`DataFrame` or a `StataReader` that can be used to read the file\nincrementally.\n\nSpecifying a `chunksize` yields a `StataReader` instance that can be used to\nread `chunksize` lines from the file at a time. The `StataReader` object can\nbe used as an iterator.\n\nFor more fine-grained control, use `iterator=True` and specify `chunksize`\nwith each call to `read()`.\n\nCurrently the `index` is retrieved as a column.\n\nThe parameter `convert_categoricals` indicates whether value labels should be\nread and used to create a `Categorical` variable from them. Value labels can\nalso be retrieved by the function `value_labels`, which requires `read()` to\nbe called before use.\n\nThe parameter `convert_missing` indicates whether missing value\nrepresentations in Stata should be preserved. If `False` (the default),\nmissing values are represented as `np.nan`. If `True`, missing values are\nrepresented using `StataMissingValue` objects, and columns containing missing\nvalues will have `object` data type.\n\nNote\n\n`read_stata()` and `StataReader` support .dta formats 113-115 (Stata 10-12),\n117 (Stata 13), and 118 (Stata 14).\n\nNote\n\nSetting `preserve_dtypes=False` will upcast to the standard pandas data types:\n`int64` for all integer types and `float64` for floating point data. By\ndefault, the Stata data types are preserved when importing.\n\n`Categorical` data can be exported to Stata data files as value labeled data.\nThe exported data consists of the underlying category codes as integer data\nvalues and the categories as value labels. Stata does not have an explicit\nequivalent to a `Categorical` and information about whether the variable is\nordered is lost when exporting.\n\nWarning\n\nStata only supports string value labels, and so `str` is called on the\ncategories when exporting data. Exporting `Categorical` variables with non-\nstring categories produces a warning, and can result a loss of information if\nthe `str` representations of the categories are not unique.\n\nLabeled data can similarly be imported from Stata data files as `Categorical`\nvariables using the keyword argument `convert_categoricals` (`True` by\ndefault). The keyword argument `order_categoricals` (`True` by default)\ndetermines whether imported `Categorical` variables are ordered.\n\nNote\n\nWhen importing categorical data, the values of the variables in the Stata data\nfile are not preserved since `Categorical` variables always use integer data\ntypes between `-1` and `n-1` where `n` is the number of categories. If the\noriginal values in the Stata data file are required, these can be imported by\nsetting `convert_categoricals=False`, which will import original data (but not\nthe variable labels). The original values can be matched to the imported\ncategorical data since there is a simple mapping between the original Stata\ndata values and the category codes of imported Categorical variables: missing\nvalues are assigned code `-1`, and the smallest original value is assigned\n`0`, the second smallest is assigned `1` and so on until the largest original\nvalue is assigned the code `n-1`.\n\nNote\n\nStata supports partially labeled series. These series have value labels for\nsome but not all data values. Importing a partially labeled series will\nproduce a `Categorical` with string categories for the values that are labeled\nand numeric categories for values with no label.\n\nThe top-level function `read_sas()` can read (but not write) SAS XPORT (.xpt)\nand (since v0.18.0) SAS7BDAT (.sas7bdat) format files.\n\nSAS files only contain two value types: ASCII text and floating point values\n(usually 8 bytes but sometimes truncated). For xport files, there is no\nautomatic type conversion to integers, dates, or categoricals. For SAS7BDAT\nfiles, the format codes may allow date variables to be automatically converted\nto dates. By default the whole file is read and returned as a `DataFrame`.\n\nSpecify a `chunksize` or use `iterator=True` to obtain reader objects\n(`XportReader` or `SAS7BDATReader`) for incrementally reading the file. The\nreader objects also have attributes that contain additional information about\nthe file and its variables.\n\nRead a SAS7BDAT file:\n\nObtain an iterator and read an XPORT file 100,000 lines at a time:\n\nThe specification for the xport file format is available from the SAS web\nsite.\n\nNo official documentation is available for the SAS7BDAT format.\n\nNew in version 0.25.0.\n\nThe top-level function `read_spss()` can read (but not write) SPSS SAV (.sav)\nand ZSAV (.zsav) format files.\n\nSPSS files contain column names. By default the whole file is read,\ncategorical columns are converted into `pd.Categorical`, and a `DataFrame`\nwith all columns is returned.\n\nSpecify the `usecols` parameter to obtain a subset of columns. Specify\n`convert_categoricals=False` to avoid converting categorical columns into\n`pd.Categorical`.\n\nRead an SPSS file:\n\nExtract a subset of columns contained in `usecols` from an SPSS file and avoid\nconverting categorical columns into `pd.Categorical`:\n\nMore information about the SAV and ZSAV file formats is available here.\n\npandas itself only supports IO with a limited set of file formats that map\ncleanly to its tabular data model. For reading and writing other file formats\ninto and from pandas, we recommend these packages from the broader community.\n\nxarray provides data structures inspired by the pandas `DataFrame` for working\nwith multi-dimensional datasets, with a focus on the netCDF file format and\neasy conversion to and from pandas.\n\nThis is an informal comparison of various IO methods, using pandas 0.24.2.\nTimings are machine dependent and small differences should be ignored.\n\nThe following test functions will be used below to compare the performance of\nseveral IO methods:\n\nWhen writing, the top three functions in terms of speed are\n`test_feather_write`, `test_hdf_fixed_write` and\n`test_hdf_fixed_write_compress`.\n\nWhen reading, the top three functions in terms of speed are\n`test_feather_read`, `test_pickle_read` and `test_hdf_fixed_read`.\n\nThe files `test.pkl.compress`, `test.parquet` and `test.feather` took the\nleast space on disk (in bytes).\n\n"}, {"name": "Merge, join, concatenate and compare", "path": "user_guide/merging", "type": "Manual", "text": "\npandas provides various facilities for easily combining together Series or\nDataFrame with various kinds of set logic for the indexes and relational\nalgebra functionality in the case of join / merge-type operations.\n\nIn addition, pandas also provides utilities to compare two Series or DataFrame\nand summarize their differences.\n\nThe `concat()` function (in the main pandas namespace) does all of the heavy\nlifting of performing concatenation operations along an axis while performing\noptional set logic (union or intersection) of the indexes (if any) on the\nother axes. Note that I say \u201cif any\u201d because there is only a single possible\naxis of concatenation for Series.\n\nBefore diving into all of the details of `concat` and what it can do, here is\na simple example:\n\nLike its sibling function on ndarrays, `numpy.concatenate`, `pandas.concat`\ntakes a list or dict of homogeneously-typed objects and concatenates them with\nsome configurable handling of \u201cwhat to do with the other axes\u201d:\n\n`objs` : a sequence or mapping of Series or DataFrame objects. If a dict is\npassed, the sorted keys will be used as the `keys` argument, unless it is\npassed, in which case the values will be selected (see below). Any None\nobjects will be dropped silently unless they are all None in which case a\nValueError will be raised.\n\n`axis` : {0, 1, \u2026}, default 0. The axis to concatenate along.\n\n`join` : {\u2018inner\u2019, \u2018outer\u2019}, default \u2018outer\u2019. How to handle indexes on other\naxis(es). Outer for union and inner for intersection.\n\n`ignore_index` : boolean, default False. If True, do not use the index values\non the concatenation axis. The resulting axis will be labeled 0, \u2026, n - 1.\nThis is useful if you are concatenating objects where the concatenation axis\ndoes not have meaningful indexing information. Note the index values on the\nother axes are still respected in the join.\n\n`keys` : sequence, default None. Construct hierarchical index using the passed\nkeys as the outermost level. If multiple levels passed, should contain tuples.\n\n`levels` : list of sequences, default None. Specific levels (unique values) to\nuse for constructing a MultiIndex. Otherwise they will be inferred from the\nkeys.\n\n`names` : list, default None. Names for the levels in the resulting\nhierarchical index.\n\n`verify_integrity` : boolean, default False. Check whether the new\nconcatenated axis contains duplicates. This can be very expensive relative to\nthe actual data concatenation.\n\n`copy` : boolean, default True. If False, do not copy data unnecessarily.\n\nWithout a little bit of context many of these arguments don\u2019t make much sense.\nLet\u2019s revisit the above example. Suppose we wanted to associate specific keys\nwith each of the pieces of the chopped up DataFrame. We can do this using the\n`keys` argument:\n\nAs you can see (if you\u2019ve read the rest of the documentation), the resulting\nobject\u2019s index has a hierarchical index. This means that we can now select out\neach chunk by key:\n\nIt\u2019s not a stretch to see how this can be very useful. More detail on this\nfunctionality below.\n\nNote\n\nIt is worth noting that `concat()` (and therefore `append()`) makes a full\ncopy of the data, and that constantly reusing this function can create a\nsignificant performance hit. If you need to use the operation over several\ndatasets, use a list comprehension.\n\nNote\n\nWhen concatenating DataFrames with named axes, pandas will attempt to preserve\nthese index/column names whenever possible. In the case where all inputs share\na common name, this name will be assigned to the result. When the input names\ndo not all agree, the result will be unnamed. The same is true for\n`MultiIndex`, but the logic is applied separately on a level-by-level basis.\n\nWhen gluing together multiple DataFrames, you have a choice of how to handle\nthe other axes (other than the one being concatenated). This can be done in\nthe following two ways:\n\nTake the union of them all, `join='outer'`. This is the default option as it\nresults in zero information loss.\n\nTake the intersection, `join='inner'`.\n\nHere is an example of each of these methods. First, the default `join='outer'`\nbehavior:\n\nHere is the same thing with `join='inner'`:\n\nLastly, suppose we just wanted to reuse the exact index from the original\nDataFrame:\n\nSimilarly, we could index before the concatenation:\n\nFor `DataFrame` objects which don\u2019t have a meaningful index, you may wish to\nappend them and ignore the fact that they may have overlapping indexes. To do\nthis, use the `ignore_index` argument:\n\nYou can concatenate a mix of `Series` and `DataFrame` objects. The `Series`\nwill be transformed to `DataFrame` with the column name as the name of the\n`Series`.\n\nNote\n\nSince we\u2019re concatenating a `Series` to a `DataFrame`, we could have achieved\nthe same result with `DataFrame.assign()`. To concatenate an arbitrary number\nof pandas objects (`DataFrame` or `Series`), use `concat`.\n\nIf unnamed `Series` are passed they will be numbered consecutively.\n\nPassing `ignore_index=True` will drop all name references.\n\nA fairly common use of the `keys` argument is to override the column names\nwhen creating a new `DataFrame` based on existing `Series`. Notice how the\ndefault behaviour consists on letting the resulting `DataFrame` inherit the\nparent `Series`\u2019 name, when these existed.\n\nThrough the `keys` argument we can override the existing column names.\n\nLet\u2019s consider a variation of the very first example presented:\n\nYou can also pass a dict to `concat` in which case the dict keys will be used\nfor the `keys` argument (unless other keys are specified):\n\nThe MultiIndex created has levels that are constructed from the passed keys\nand the index of the `DataFrame` pieces:\n\nIf you wish to specify other levels (as will occasionally be the case), you\ncan do so using the `levels` argument:\n\nThis is fairly esoteric, but it is actually necessary for implementing things\nlike GroupBy where the order of a categorical variable is meaningful.\n\nIf you have a series that you want to append as a single row to a `DataFrame`,\nyou can convert the row into a `DataFrame` and use `concat`\n\nYou should use `ignore_index` with this method to instruct DataFrame to\ndiscard its index. If you wish to preserve the index, you should construct an\nappropriately-indexed DataFrame and append or concatenate those objects.\n\npandas has full-featured, high performance in-memory join operations\nidiomatically very similar to relational databases like SQL. These methods\nperform significantly better (in some cases well over an order of magnitude\nbetter) than other open source implementations (like `base::merge.data.frame`\nin R). The reason for this is careful algorithmic design and the internal\nlayout of the data in `DataFrame`.\n\nSee the cookbook for some advanced strategies.\n\nUsers who are familiar with SQL but new to pandas might be interested in a\ncomparison with SQL.\n\npandas provides a single function, `merge()`, as the entry point for all\nstandard database join operations between `DataFrame` or named `Series`\nobjects:\n\n`left`: A DataFrame or named Series object.\n\n`right`: Another DataFrame or named Series object.\n\n`on`: Column or index level names to join on. Must be found in both the left\nand right DataFrame and/or Series objects. If not passed and `left_index` and\n`right_index` are `False`, the intersection of the columns in the DataFrames\nand/or Series will be inferred to be the join keys.\n\n`left_on`: Columns or index levels from the left DataFrame or Series to use as\nkeys. Can either be column names, index level names, or arrays with length\nequal to the length of the DataFrame or Series.\n\n`right_on`: Columns or index levels from the right DataFrame or Series to use\nas keys. Can either be column names, index level names, or arrays with length\nequal to the length of the DataFrame or Series.\n\n`left_index`: If `True`, use the index (row labels) from the left DataFrame or\nSeries as its join key(s). In the case of a DataFrame or Series with a\nMultiIndex (hierarchical), the number of levels must match the number of join\nkeys from the right DataFrame or Series.\n\n`right_index`: Same usage as `left_index` for the right DataFrame or Series\n\n`how`: One of `'left'`, `'right'`, `'outer'`, `'inner'`, `'cross'`. Defaults\nto `inner`. See below for more detailed description of each method.\n\n`sort`: Sort the result DataFrame by the join keys in lexicographical order.\nDefaults to `True`, setting to `False` will improve performance substantially\nin many cases.\n\n`suffixes`: A tuple of string suffixes to apply to overlapping columns.\nDefaults to `('_x', '_y')`.\n\n`copy`: Always copy data (default `True`) from the passed DataFrame or named\nSeries objects, even when reindexing is not necessary. Cannot be avoided in\nmany cases but may improve performance / memory usage. The cases where copying\ncan be avoided are somewhat pathological but this option is provided\nnonetheless.\n\n`indicator`: Add a column to the output DataFrame called `_merge` with\ninformation on the source of each row. `_merge` is Categorical-type and takes\non a value of `left_only` for observations whose merge key only appears in\n`'left'` DataFrame or Series, `right_only` for observations whose merge key\nonly appears in `'right'` DataFrame or Series, and `both` if the observation\u2019s\nmerge key is found in both.\n\n`validate` : string, default None. If specified, checks if merge is of\nspecified type.\n\n\u201cone_to_one\u201d or \u201c1:1\u201d: checks if merge keys are unique in both left and right\ndatasets.\n\n\u201cone_to_many\u201d or \u201c1:m\u201d: checks if merge keys are unique in left dataset.\n\n\u201cmany_to_one\u201d or \u201cm:1\u201d: checks if merge keys are unique in right dataset.\n\n\u201cmany_to_many\u201d or \u201cm:m\u201d: allowed, but does not result in checks.\n\nNote\n\nSupport for specifying index levels as the `on`, `left_on`, and `right_on`\nparameters was added in version 0.23.0. Support for merging named `Series`\nobjects was added in version 0.24.0.\n\nThe return type will be the same as `left`. If `left` is a `DataFrame` or\nnamed `Series` and `right` is a subclass of `DataFrame`, the return type will\nstill be `DataFrame`.\n\n`merge` is a function in the pandas namespace, and it is also available as a\n`DataFrame` instance method `merge()`, with the calling `DataFrame` being\nimplicitly considered the left object in the join.\n\nThe related `join()` method, uses `merge` internally for the index-on-index\n(by default) and column(s)-on-index join. If you are joining on index only,\nyou may wish to use `DataFrame.join` to save yourself some typing.\n\nExperienced users of relational databases like SQL will be familiar with the\nterminology used to describe join operations between two SQL-table like\nstructures (`DataFrame` objects). There are several cases to consider which\nare very important to understand:\n\none-to-one joins: for example when joining two `DataFrame` objects on their\nindexes (which must contain unique values).\n\nmany-to-one joins: for example when joining an index (unique) to one or more\ncolumns in a different `DataFrame`.\n\nmany-to-many joins: joining columns on columns.\n\nNote\n\nWhen joining columns on columns (potentially a many-to-many join), any indexes\non the passed `DataFrame` objects will be discarded.\n\nIt is worth spending some time understanding the result of the many-to-many\njoin case. In SQL / standard relational algebra, if a key combination appears\nmore than once in both tables, the resulting table will have the Cartesian\nproduct of the associated data. Here is a very basic example with one unique\nkey combination:\n\nHere is a more complicated example with multiple join keys. Only the keys\nappearing in `left` and `right` are present (the intersection), since\n`how='inner'` by default.\n\nThe `how` argument to `merge` specifies how to determine which keys are to be\nincluded in the resulting table. If a key combination does not appear in\neither the left or right tables, the values in the joined table will be `NA`.\nHere is a summary of the `how` options and their SQL equivalent names:\n\nMerge method\n\nSQL Join Name\n\nDescription\n\n`left`\n\n`LEFT OUTER JOIN`\n\nUse keys from left frame only\n\n`right`\n\n`RIGHT OUTER JOIN`\n\nUse keys from right frame only\n\n`outer`\n\n`FULL OUTER JOIN`\n\nUse union of keys from both frames\n\n`inner`\n\n`INNER JOIN`\n\nUse intersection of keys from both frames\n\n`cross`\n\n`CROSS JOIN`\n\nCreate the cartesian product of rows of both frames\n\nYou can merge a mult-indexed Series and a DataFrame, if the names of the\nMultiIndex correspond to the columns from the DataFrame. Transform the Series\nto a DataFrame using `Series.reset_index()` before merging, as shown in the\nfollowing example.\n\nHere is another example with duplicate join keys in DataFrames:\n\nWarning\n\nJoining / merging on duplicate keys can cause a returned frame that is the\nmultiplication of the row dimensions, which may result in memory overflow. It\nis the user\u2019 s responsibility to manage duplicate values in keys before\njoining large DataFrames.\n\nUsers can use the `validate` argument to automatically check whether there are\nunexpected duplicates in their merge keys. Key uniqueness is checked before\nmerge operations and so should protect against memory overflows. Checking key\nuniqueness is also a good way to ensure user data structures are as expected.\n\nIn the following example, there are duplicate values of `B` in the right\n`DataFrame`. As this is not a one-to-one merge \u2013 as specified in the\n`validate` argument \u2013 an exception will be raised.\n\nIf the user is aware of the duplicates in the right `DataFrame` but wants to\nensure there are no duplicates in the left DataFrame, one can use the\n`validate='one_to_many'` argument instead, which will not raise an exception.\n\n`merge()` accepts the argument `indicator`. If `True`, a Categorical-type\ncolumn called `_merge` will be added to the output object that takes on\nvalues:\n\nObservation Origin\n\n`_merge` value\n\nMerge key only in `'left'` frame\n\n`left_only`\n\nMerge key only in `'right'` frame\n\n`right_only`\n\nMerge key in both frames\n\n`both`\n\nThe `indicator` argument will also accept string arguments, in which case the\nindicator function will use the value of the passed string as the name for the\nindicator column.\n\nMerging will preserve the dtype of the join keys.\n\nWe are able to preserve the join keys:\n\nOf course if you have missing values that are introduced, then the resulting\ndtype will be upcast.\n\nMerging will preserve `category` dtypes of the mergands. See also the section\non categoricals.\n\nThe left frame.\n\nThe right frame.\n\nThe merged result:\n\nNote\n\nThe category dtypes must be exactly the same, meaning the same categories and\nthe ordered attribute. Otherwise the result will coerce to the categories\u2019\ndtype.\n\nNote\n\nMerging on `category` dtypes that are the same can be quite performant\ncompared to `object` dtype merging.\n\n`DataFrame.join()` is a convenient method for combining the columns of two\npotentially differently-indexed `DataFrames` into a single result `DataFrame`.\nHere is a very basic example:\n\nThe same as above, but with `how='inner'`.\n\nThe data alignment here is on the indexes (row labels). This same behavior can\nbe achieved using `merge` plus additional arguments instructing it to use the\nindexes:\n\n`join()` takes an optional `on` argument which may be a column or multiple\ncolumn names, which specifies that the passed `DataFrame` is to be aligned on\nthat column in the `DataFrame`. These two function calls are completely\nequivalent:\n\nObviously you can choose whichever form you find more convenient. For many-to-\none joins (where one of the `DataFrame`\u2019s is already indexed by the join key),\nusing `join` may be more convenient. Here is a simple example:\n\nTo join on multiple keys, the passed DataFrame must have a `MultiIndex`:\n\nNow this can be joined by passing the two key column names:\n\nThe default for `DataFrame.join` is to perform a left join (essentially a\n\u201cVLOOKUP\u201d operation, for Excel users), which uses only the keys found in the\ncalling DataFrame. Other join types, for example inner join, can be just as\neasily performed:\n\nAs you can see, this drops any rows where there was no match.\n\nYou can join a singly-indexed `DataFrame` with a level of a MultiIndexed\n`DataFrame`. The level will match on the name of the index of the singly-\nindexed frame against a level name of the MultiIndexed frame.\n\nThis is equivalent but less verbose and more memory efficient / faster than\nthis.\n\nThis is supported in a limited way, provided that the index for the right\nargument is completely used in the join, and is a subset of the indices in the\nleft argument, as in this example:\n\nIf that condition is not satisfied, a join with two multi-indexes can be done\nusing the following code.\n\nStrings passed as the `on`, `left_on`, and `right_on` parameters may refer to\neither column names or index level names. This enables merging `DataFrame`\ninstances on a combination of index levels and columns without resetting\nindexes.\n\nNote\n\nWhen DataFrames are merged on a string that matches an index level in both\nframes, the index level is preserved as an index level in the resulting\nDataFrame.\n\nNote\n\nWhen DataFrames are merged using only some of the levels of a `MultiIndex`,\nthe extra levels will be dropped from the resulting merge. In order to\npreserve those levels, use `reset_index` on those level names to move those\nlevels to columns prior to doing the merge.\n\nNote\n\nIf a string matches both a column name and an index level name, then a warning\nis issued and the column takes precedence. This will result in an ambiguity\nerror in a future version.\n\nThe merge `suffixes` argument takes a tuple of list of strings to append to\noverlapping column names in the input `DataFrame`s to disambiguate the result\ncolumns:\n\n`DataFrame.join()` has `lsuffix` and `rsuffix` arguments which behave\nsimilarly.\n\nA list or tuple of `DataFrames` can also be passed to `join()` to join them\ntogether on their indexes.\n\nAnother fairly common situation is to have two like-indexed (or similarly\nindexed) `Series` or `DataFrame` objects and wanting to \u201cpatch\u201d values in one\nobject from values for matching indices in the other. Here is an example:\n\nFor this, use the `combine_first()` method:\n\nNote that this method only takes values from the right `DataFrame` if they are\nmissing in the left `DataFrame`. A related method, `update()`, alters non-NA\nvalues in place:\n\nA `merge_ordered()` function allows combining time series and other ordered\ndata. In particular it has an optional `fill_method` keyword to\nfill/interpolate missing data:\n\nA `merge_asof()` is similar to an ordered left-join except that we match on\nnearest key rather than equal keys. For each row in the `left` `DataFrame`, we\nselect the last row in the `right` `DataFrame` whose `on` key is less than the\nleft\u2019s key. Both DataFrames must be sorted by the key.\n\nOptionally an asof merge can perform a group-wise merge. This matches the `by`\nkey equally, in addition to the nearest match on the `on` key.\n\nFor example; we might have `trades` and `quotes` and we want to `asof` merge\nthem.\n\nBy default we are taking the asof of the quotes.\n\nWe only asof within `2ms` between the quote time and the trade time.\n\nWe only asof within `10ms` between the quote time and the trade time and we\nexclude exact matches on time. Note that though we exclude the exact matches\n(of the quotes), prior quotes do propagate to that point in time.\n\nThe `compare()` and `compare()` methods allow you to compare two DataFrame or\nSeries, respectively, and summarize their differences.\n\nThis feature was added in V1.1.0.\n\nFor example, you might want to compare two `DataFrame` and stack their\ndifferences side by side.\n\nBy default, if two corresponding values are equal, they will be shown as\n`NaN`. Furthermore, if all values in an entire row / column, the row / column\nwill be omitted from the result. The remaining differences will be aligned on\ncolumns.\n\nIf you wish, you may choose to stack the differences on rows.\n\nIf you wish to keep all original rows and columns, set `keep_shape` argument\nto `True`.\n\nYou may also keep all the original values even if they are equal.\n\n"}, {"name": "MultiIndex / advanced indexing", "path": "user_guide/advanced", "type": "Manual", "text": "\nThis section covers indexing with a MultiIndex and other advanced indexing\nfeatures.\n\nSee the Indexing and Selecting Data for general indexing documentation.\n\nWarning\n\nWhether a copy or a reference is returned for a setting operation may depend\non the context. This is sometimes called `chained assignment` and should be\navoided. See Returning a View versus Copy.\n\nSee the cookbook for some advanced strategies.\n\nHierarchical / Multi-level indexing is very exciting as it opens the door to\nsome quite sophisticated data analysis and manipulation, especially for\nworking with higher dimensional data. In essence, it enables you to store and\nmanipulate data with an arbitrary number of dimensions in lower dimensional\ndata structures like `Series` (1d) and `DataFrame` (2d).\n\nIn this section, we will show what exactly we mean by \u201chierarchical\u201d indexing\nand how it integrates with all of the pandas indexing functionality described\nabove and in prior sections. Later, when discussing group by and pivoting and\nreshaping data, we\u2019ll show non-trivial applications to illustrate how it aids\nin structuring data for analysis.\n\nSee the cookbook for some advanced strategies.\n\nThe `MultiIndex` object is the hierarchical analogue of the standard `Index`\nobject which typically stores the axis labels in pandas objects. You can think\nof `MultiIndex` as an array of tuples where each tuple is unique. A\n`MultiIndex` can be created from a list of arrays (using\n`MultiIndex.from_arrays()`), an array of tuples (using\n`MultiIndex.from_tuples()`), a crossed set of iterables (using\n`MultiIndex.from_product()`), or a `DataFrame` (using\n`MultiIndex.from_frame()`). The `Index` constructor will attempt to return a\n`MultiIndex` when it is passed a list of tuples. The following examples\ndemonstrate different ways to initialize MultiIndexes.\n\nWhen you want every pairing of the elements in two iterables, it can be easier\nto use the `MultiIndex.from_product()` method:\n\nYou can also construct a `MultiIndex` from a `DataFrame` directly, using the\nmethod `MultiIndex.from_frame()`. This is a complementary method to\n`MultiIndex.to_frame()`.\n\nAs a convenience, you can pass a list of arrays directly into `Series` or\n`DataFrame` to construct a `MultiIndex` automatically:\n\nAll of the `MultiIndex` constructors accept a `names` argument which stores\nstring names for the levels themselves. If no names are provided, `None` will\nbe assigned:\n\nThis index can back any axis of a pandas object, and the number of levels of\nthe index is up to you:\n\nWe\u2019ve \u201csparsified\u201d the higher levels of the indexes to make the console output\na bit easier on the eyes. Note that how the index is displayed can be\ncontrolled using the `multi_sparse` option in `pandas.set_options()`:\n\nIt\u2019s worth keeping in mind that there\u2019s nothing preventing you from using\ntuples as atomic labels on an axis:\n\nThe reason that the `MultiIndex` matters is that it can allow you to do\ngrouping, selection, and reshaping operations as we will describe below and in\nsubsequent areas of the documentation. As you will see in later sections, you\ncan find yourself working with hierarchically-indexed data without creating a\n`MultiIndex` explicitly yourself. However, when loading data from a file, you\nmay wish to generate your own `MultiIndex` when preparing the data set.\n\nThe method `get_level_values()` will return a vector of the labels for each\nlocation at a particular level:\n\nOne of the important features of hierarchical indexing is that you can select\ndata by a \u201cpartial\u201d label identifying a subgroup in the data. Partial\nselection \u201cdrops\u201d levels of the hierarchical index in the result in a\ncompletely analogous way to selecting a column in a regular DataFrame:\n\nSee Cross-section with hierarchical index for how to select on a deeper level.\n\nThe `MultiIndex` keeps all the defined levels of an index, even if they are\nnot actually used. When slicing an index, you may notice this. For example:\n\nThis is done to avoid a recomputation of the levels in order to make slicing\nhighly performant. If you want to see only the used levels, you can use the\n`get_level_values()` method.\n\nTo reconstruct the `MultiIndex` with only the used levels, the\n`remove_unused_levels()` method may be used.\n\nOperations between differently-indexed objects having `MultiIndex` on the axes\nwill work as you expect; data alignment will work the same as an Index of\ntuples:\n\nThe `reindex()` method of `Series`/`DataFrames` can be called with another\n`MultiIndex`, or even a list or array of tuples:\n\nSyntactically integrating `MultiIndex` in advanced indexing with `.loc` is a\nbit challenging, but we\u2019ve made every effort to do so. In general, MultiIndex\nkeys take the form of tuples. For example, the following works as you would\nexpect:\n\nNote that `df.loc['bar', 'two']` would also work in this example, but this\nshorthand notation can lead to ambiguity in general.\n\nIf you also want to index a specific column with `.loc`, you must use a tuple\nlike this:\n\nYou don\u2019t have to specify all levels of the `MultiIndex` by passing only the\nfirst elements of the tuple. For example, you can use \u201cpartial\u201d indexing to\nget all elements with `bar` in the first level as follows:\n\nThis is a shortcut for the slightly more verbose notation `df.loc[('bar',),]`\n(equivalent to `df.loc['bar',]` in this example).\n\n\u201cPartial\u201d slicing also works quite nicely.\n\nYou can slice with a \u2018range\u2019 of values, by providing a slice of tuples.\n\nPassing a list of labels or tuples works similar to reindexing:\n\nNote\n\nIt is important to note that tuples and lists are not treated identically in\npandas when it comes to indexing. Whereas a tuple is interpreted as one multi-\nlevel key, a list is used to specify several keys. Or in other words, tuples\ngo horizontally (traversing levels), lists go vertically (scanning levels).\n\nImportantly, a list of tuples indexes several complete `MultiIndex` keys,\nwhereas a tuple of lists refer to several values within a level:\n\nYou can slice a `MultiIndex` by providing multiple indexers.\n\nYou can provide any of the selectors as if you are indexing by label, see\nSelection by Label, including slices, lists of labels, labels, and boolean\nindexers.\n\nYou can use `slice(None)` to select all the contents of that level. You do not\nneed to specify all the deeper levels, they will be implied as `slice(None)`.\n\nAs usual, both sides of the slicers are included as this is label indexing.\n\nWarning\n\nYou should specify all axes in the `.loc` specifier, meaning the indexer for\nthe index and for the columns. There are some ambiguous cases where the passed\nindexer could be mis-interpreted as indexing both axes, rather than into say\nthe `MultiIndex` for the rows.\n\nYou should do this:\n\nYou should not do this:\n\nBasic MultiIndex slicing using slices, lists, and labels.\n\nYou can use `pandas.IndexSlice` to facilitate a more natural syntax using `:`,\nrather than using `slice(None)`.\n\nIt is possible to perform quite complicated selections using this method on\nmultiple axes at the same time.\n\nUsing a boolean indexer you can provide selection related to the values.\n\nYou can also specify the `axis` argument to `.loc` to interpret the passed\nslicers on a single axis.\n\nFurthermore, you can set the values using the following methods.\n\nYou can use a right-hand-side of an alignable object as well.\n\nThe `xs()` method of `DataFrame` additionally takes a level argument to make\nselecting data at a particular level of a `MultiIndex` easier.\n\nYou can also select on the columns with `xs`, by providing the axis argument.\n\n`xs` also allows selection with multiple keys.\n\nYou can pass `drop_level=False` to `xs` to retain the level that was selected.\n\nCompare the above with the result using `drop_level=True` (the default value).\n\nUsing the parameter `level` in the `reindex()` and `align()` methods of pandas\nobjects is useful to broadcast values across a level. For instance:\n\nThe `swaplevel()` method can switch the order of two levels:\n\nThe `reorder_levels()` method generalizes the `swaplevel` method, allowing you\nto permute the hierarchical index levels in one step:\n\nThe `rename()` method is used to rename the labels of a `MultiIndex`, and is\ntypically used to rename the columns of a `DataFrame`. The `columns` argument\nof `rename` allows a dictionary to be specified that includes only the columns\nyou wish to rename.\n\nThis method can also be used to rename specific labels of the main index of\nthe `DataFrame`.\n\nThe `rename_axis()` method is used to rename the name of a `Index` or\n`MultiIndex`. In particular, the names of the levels of a `MultiIndex` can be\nspecified, which is useful if `reset_index()` is later used to move the values\nfrom the `MultiIndex` to a column.\n\nNote that the columns of a `DataFrame` are an index, so that using\n`rename_axis` with the `columns` argument will change the name of that index.\n\nBoth `rename` and `rename_axis` support specifying a dictionary, `Series` or a\nmapping function to map labels/names to new values.\n\nWhen working with an `Index` object directly, rather than via a `DataFrame`,\n`Index.set_names()` can be used to change the names.\n\nYou cannot set the names of the MultiIndex via a level.\n\nUse `Index.set_names()` instead.\n\nFor `MultiIndex`-ed objects to be indexed and sliced effectively, they need to\nbe sorted. As with any index, you can use `sort_index()`.\n\nYou may also pass a level name to `sort_index` if the `MultiIndex` levels are\nnamed.\n\nOn higher dimensional objects, you can sort any of the other axes by level if\nthey have a `MultiIndex`:\n\nIndexing will work even if the data are not sorted, but will be rather\ninefficient (and show a `PerformanceWarning`). It will also return a copy of\nthe data rather than a view:\n\nFurthermore, if you try to index something that is not fully lexsorted, this\ncan raise:\n\nThe `is_monotonic_increasing()` method on a `MultiIndex` shows if the index is\nsorted:\n\nAnd now selection works as expected.\n\nSimilar to NumPy ndarrays, pandas `Index`, `Series`, and `DataFrame` also\nprovides the `take()` method that retrieves elements along a given axis at the\ngiven indices. The given indices must be either a list or an ndarray of\ninteger index positions. `take` will also accept negative integers as relative\npositions to the end of the object.\n\nFor DataFrames, the given indices should be a 1d list or ndarray that\nspecifies row or column positions.\n\nIt is important to note that the `take` method on pandas objects are not\nintended to work on boolean indices and may return unexpected results.\n\nFinally, as a small note on performance, because the `take` method handles a\nnarrower range of inputs, it can offer performance that is a good deal faster\nthan fancy indexing.\n\nWe have discussed `MultiIndex` in the previous sections pretty extensively.\nDocumentation about `DatetimeIndex` and `PeriodIndex` are shown here, and\ndocumentation about `TimedeltaIndex` is found here.\n\nIn the following sub-sections we will highlight some other index types.\n\n`CategoricalIndex` is a type of index that is useful for supporting indexing\nwith duplicates. This is a container around a `Categorical` and allows\nefficient indexing and storage of an index with a large number of duplicated\nelements.\n\nSetting the index will create a `CategoricalIndex`.\n\nIndexing with `__getitem__/.iloc/.loc` works similarly to an `Index` with\nduplicates. The indexers must be in the category or the operation will raise a\n`KeyError`.\n\nThe `CategoricalIndex` is preserved after indexing:\n\nSorting the index will sort by the order of the categories (recall that we\ncreated the index with `CategoricalDtype(list('cab'))`, so the sorted order is\n`cab`).\n\nGroupby operations on the index will preserve the index nature as well.\n\nReindexing operations will return a resulting index based on the type of the\npassed indexer. Passing a list will return a plain-old `Index`; indexing with\na `Categorical` will return a `CategoricalIndex`, indexed according to the\ncategories of the passed `Categorical` dtype. This allows one to arbitrarily\nindex these even with values not in the categories, similarly to how you can\nreindex any pandas index.\n\nWarning\n\nReshaping and Comparison operations on a `CategoricalIndex` must have the same\ncategories or a `TypeError` will be raised.\n\nDeprecated since version 1.4.0: In pandas 2.0, `Index` will become the default\nindex type for numeric types instead of `Int64Index`, `Float64Index` and\n`UInt64Index` and those index types are therefore deprecated and will be\nremoved in a futire version. `RangeIndex` will not be removed, as it\nrepresents an optimized version of an integer index.\n\n`Int64Index` is a fundamental basic index in pandas. This is an immutable\narray implementing an ordered, sliceable set.\n\n`RangeIndex` is a sub-class of `Int64Index` that provides the default index\nfor all `NDFrame` objects. `RangeIndex` is an optimized version of\n`Int64Index` that can represent a monotonic ordered set. These are analogous\nto Python range types.\n\nDeprecated since version 1.4.0: `Index` will become the default index type for\nnumeric types in the future instead of `Int64Index`, `Float64Index` and\n`UInt64Index` and those index types are therefore deprecated and will be\nremoved in a future version of Pandas. `RangeIndex` will not be removed as it\nrepresents an optimized version of an integer index.\n\nBy default a `Float64Index` will be automatically created when passing\nfloating, or mixed-integer-floating values in index creation. This enables a\npure label-based slicing paradigm that makes `[],ix,loc` for scalar indexing\nand slicing work exactly the same.\n\nScalar selection for `[],.loc` will always be label based. An integer will\nmatch an equal float index (e.g. `3` is equivalent to `3.0`).\n\nThe only positional indexing is via `iloc`.\n\nA scalar index that is not found will raise a `KeyError`. Slicing is primarily\non the values of the index when using `[],ix,loc`, and always positional when\nusing `iloc`. The exception is when the slice is boolean, in which case it\nwill always be positional.\n\nIn float indexes, slicing using floats is allowed.\n\nIn non-float indexes, slicing using floats will raise a `TypeError`.\n\nHere is a typical use-case for using this type of indexing. Imagine that you\nhave a somewhat irregular timedelta-like indexing scheme, but the data is\nrecorded as floats. This could, for example, be millisecond offsets.\n\nSelection operations then will always work on a value basis, for all selection\noperators.\n\nYou could retrieve the first 1 second (1000 ms) of data as such:\n\nIf you need integer based selection, you should use `iloc`:\n\n`IntervalIndex` together with its own dtype, `IntervalDtype` as well as the\n`Interval` scalar type, allow first-class support in pandas for interval\nnotation.\n\nThe `IntervalIndex` allows some unique indexing and is also used as a return\ntype for the categories in `cut()` and `qcut()`.\n\nAn `IntervalIndex` can be used in `Series` and in `DataFrame` as the index.\n\nLabel based indexing via `.loc` along the edges of an interval works as you\nwould expect, selecting that particular interval.\n\nIf you select a label contained within an interval, this will also select the\ninterval.\n\nSelecting using an `Interval` will only return exact matches (starting from\npandas 0.25.0).\n\nTrying to select an `Interval` that is not exactly contained in the\n`IntervalIndex` will raise a `KeyError`.\n\nSelecting all `Intervals` that overlap a given `Interval` can be performed\nusing the `overlaps()` method to create a boolean indexer.\n\n`cut()` and `qcut()` both return a `Categorical` object, and the bins they\ncreate are stored as an `IntervalIndex` in its `.categories` attribute.\n\n`cut()` also accepts an `IntervalIndex` for its `bins` argument, which enables\na useful pandas idiom. First, We call `cut()` with some data and `bins` set to\na fixed number, to generate the bins. Then, we pass the values of\n`.categories` as the `bins` argument in subsequent calls to `cut()`, supplying\nnew data which will be binned into the same bins.\n\nAny value which falls outside all bins will be assigned a `NaN` value.\n\nIf we need intervals on a regular frequency, we can use the `interval_range()`\nfunction to create an `IntervalIndex` using various combinations of `start`,\n`end`, and `periods`. The default frequency for `interval_range` is a 1 for\nnumeric intervals, and calendar day for datetime-like intervals:\n\nThe `freq` parameter can used to specify non-default frequencies, and can\nutilize a variety of frequency aliases with datetime-like intervals:\n\nAdditionally, the `closed` parameter can be used to specify which side(s) the\nintervals are closed on. Intervals are closed on the right side by default.\n\nSpecifying `start`, `end`, and `periods` will generate a range of evenly\nspaced intervals from `start` to `end` inclusively, with `periods` number of\nelements in the resulting `IntervalIndex`:\n\nLabel-based indexing with integer axis labels is a thorny topic. It has been\ndiscussed heavily on mailing lists and among various members of the scientific\nPython community. In pandas, our general viewpoint is that labels matter more\nthan integer locations. Therefore, with an integer axis index only label-based\nindexing is possible with the standard tools like `.loc`. The following code\nwill generate exceptions:\n\nThis deliberate decision was made to prevent ambiguities and subtle bugs (many\nusers reported finding bugs when the API change was made to stop \u201cfalling\nback\u201d on position-based indexing).\n\nIf the index of a `Series` or `DataFrame` is monotonically increasing or\ndecreasing, then the bounds of a label-based slice can be outside the range of\nthe index, much like slice indexing a normal Python `list`. Monotonicity of an\nindex can be tested with the `is_monotonic_increasing()` and\n`is_monotonic_decreasing()` attributes.\n\nOn the other hand, if the index is not monotonic, then both slice bounds must\nbe unique members of the index.\n\n`Index.is_monotonic_increasing` and `Index.is_monotonic_decreasing` only check\nthat an index is weakly monotonic. To check for strict monotonicity, you can\ncombine one of those with the `is_unique()` attribute.\n\nCompared with standard Python sequence slicing in which the slice endpoint is\nnot inclusive, label-based slicing in pandas is inclusive. The primary reason\nfor this is that it is often not possible to easily determine the \u201csuccessor\u201d\nor next element after a particular label in an index. For example, consider\nthe following `Series`:\n\nSuppose we wished to slice from `c` to `e`, using integers this would be\naccomplished as such:\n\nHowever, if you only had `c` and `e`, determining the next element in the\nindex can be somewhat complicated. For example, the following does not work:\n\nA very common use case is to limit a time series to start and end at two\nspecific dates. To enable this, we made the design choice to make label-based\nslicing include both endpoints:\n\nThis is most definitely a \u201cpracticality beats purity\u201d sort of thing, but it is\nsomething to watch out for if you expect label-based slicing to behave exactly\nin the way that standard Python integer slicing works.\n\nThe different indexing operation can potentially change the dtype of a\n`Series`.\n\nThis is because the (re)indexing operations above silently inserts `NaNs` and\nthe `dtype` changes accordingly. This can cause some issues when using `numpy`\n`ufuncs` such as `numpy.logical_and`.\n\nSee the GH2388 for a more detailed discussion.\n\n"}, {"name": "Nullable Boolean data type", "path": "user_guide/boolean", "type": "Manual", "text": "\nNote\n\nBooleanArray is currently experimental. Its API or implementation may change\nwithout warning.\n\nNew in version 1.0.0.\n\npandas allows indexing with `NA` values in a boolean array, which are treated\nas `False`.\n\nChanged in version 1.0.2.\n\nIf you would prefer to keep the `NA` values you can manually fill them with\n`fillna(True)`.\n\n`arrays.BooleanArray` implements Kleene Logic (sometimes called three-value\nlogic) for logical operations like `&` (and), `|` (or) and `^` (exclusive-or).\n\nThis table demonstrates the results for every combination. These operations\nare symmetrical, so flipping the left- and right-hand side makes no difference\nin the result.\n\nExpression\n\nResult\n\n`True & True`\n\n`True`\n\n`True & False`\n\n`False`\n\n`True & NA`\n\n`NA`\n\n`False & False`\n\n`False`\n\n`False & NA`\n\n`False`\n\n`NA & NA`\n\n`NA`\n\n`True | True`\n\n`True`\n\n`True | False`\n\n`True`\n\n`True | NA`\n\n`True`\n\n`False | False`\n\n`False`\n\n`False | NA`\n\n`NA`\n\n`NA | NA`\n\n`NA`\n\n`True ^ True`\n\n`False`\n\n`True ^ False`\n\n`True`\n\n`True ^ NA`\n\n`NA`\n\n`False ^ False`\n\n`False`\n\n`False ^ NA`\n\n`NA`\n\n`NA ^ NA`\n\n`NA`\n\nWhen an `NA` is present in an operation, the output value is `NA` only if the\nresult cannot be determined solely based on the other input. For example,\n`True | NA` is `True`, because both `True | True` and `True | False` are\n`True`. In that case, we don\u2019t actually need to consider the value of the\n`NA`.\n\nOn the other hand, `True & NA` is `NA`. The result depends on whether the `NA`\nreally is `True` or `False`, since `True & True` is `True`, but `True & False`\nis `False`, so we can\u2019t determine the output.\n\nThis differs from how `np.nan` behaves in logical operations. pandas treated\n`np.nan` is always false in the output.\n\nIn `or`\n\nIn `and`\n\n"}, {"name": "Nullable integer data type", "path": "user_guide/integer_na", "type": "Manual", "text": "\nNote\n\nIntegerArray is currently experimental. Its API or implementation may change\nwithout warning.\n\nChanged in version 1.0.0: Now uses `pandas.NA` as the missing value rather\nthan `numpy.nan`.\n\nIn Working with missing data, we saw that pandas primarily uses `NaN` to\nrepresent missing data. Because `NaN` is a float, this forces an array of\nintegers with any missing values to become floating point. In some cases, this\nmay not matter much. But if your integer column is, say, an identifier,\ncasting to float can be problematic. Some integers cannot even be represented\nas floating point numbers.\n\npandas can represent integer data with possibly missing values using\n`arrays.IntegerArray`. This is an extension types implemented within pandas.\n\nOr the string alias `\"Int64\"` (note the capital `\"I\"`, to differentiate from\nNumPy\u2019s `'int64'` dtype:\n\nAll NA-like values are replaced with `pandas.NA`.\n\nThis array can be stored in a `DataFrame` or `Series` like any NumPy array.\n\nYou can also pass the list-like object to the `Series` constructor with the\ndtype.\n\nWarning\n\nCurrently `pandas.array()` and `pandas.Series()` use different rules for dtype\ninference. `pandas.array()` will infer a nullable- integer dtype\n\nFor backwards-compatibility, `Series` infers these as either integer or float\ndtype\n\nWe recommend explicitly providing the dtype to avoid confusion.\n\nIn the future, we may provide an option for `Series` to infer a nullable-\ninteger dtype.\n\nOperations involving an integer array will behave similar to NumPy arrays.\nMissing values will be propagated, and the data will be coerced to another\ndtype if needed.\n\nThese dtypes can operate as part of `DataFrame`.\n\nThese dtypes can be merged & reshaped & casted.\n\nReduction and groupby operations such as \u2018sum\u2019 work as well.\n\n`arrays.IntegerArray` uses `pandas.NA` as its scalar missing value. Slicing a\nsingle element that\u2019s missing will return `pandas.NA`\n\n"}, {"name": "Options and settings", "path": "user_guide/options", "type": "Manual", "text": "\npandas has an options system that lets you customize some aspects of its\nbehaviour, display-related options being those the user is most likely to\nadjust.\n\nOptions have a full \u201cdotted-style\u201d, case-insensitive name (e.g.\n`display.max_rows`). You can get/set options directly as attributes of the\ntop-level `options` attribute:\n\nThe API is composed of 5 relevant functions, available directly from the\n`pandas` namespace:\n\n`get_option()` / `set_option()` \\- get/set the value of a single option.\n\n`reset_option()` \\- reset one or more options to their default value.\n\n`describe_option()` \\- print the descriptions of one or more options.\n\n`option_context()` \\- execute a codeblock with a set of options that revert to\nprior settings after execution.\n\nNote: Developers can check out pandas/core/config_init.py for more\ninformation.\n\nAll of the functions above accept a regexp pattern (`re.search` style) as an\nargument, and so passing in a substring will work - as long as it is\nunambiguous:\n\nThe following will not work because it matches multiple option names, e.g.\n`display.max_colwidth`, `display.max_rows`, `display.max_columns`:\n\nNote: Using this form of shorthand may cause your code to break if new options\nwith similar names are added in future versions.\n\nYou can get a list of available options and their descriptions with\n`describe_option`. When called with no argument `describe_option` will print\nout the descriptions for all available options.\n\nAs described above, `get_option()` and `set_option()` are available from the\npandas namespace. To change an option, call `set_option('option regex',\nnew_value)`.\n\nNote: The option \u2018mode.sim_interactive\u2019 is mostly used for debugging purposes.\n\nAll options also have a default value, and you can use `reset_option` to do\njust that:\n\nIt\u2019s also possible to reset multiple options at once (using a regex):\n\n`option_context` context manager has been exposed through the top-level API,\nallowing you to execute code with given option values. Option values are\nrestored automatically when you exit the `with` block:\n\nUsing startup scripts for the Python/IPython environment to import pandas and\nset options makes working with pandas more efficient. To do this, create a .py\nor .ipy script in the startup directory of the desired profile. An example\nwhere the startup folder is in a default IPython profile can be found at:\n\nMore information can be found in the IPython documentation. An example startup\nscript for pandas is displayed below:\n\nThe following is a walk-through of the more frequently used display options.\n\n`display.max_rows` and `display.max_columns` sets the maximum number of rows\nand columns displayed when a frame is pretty-printed. Truncated lines are\nreplaced by an ellipsis.\n\nOnce the `display.max_rows` is exceeded, the `display.min_rows` options\ndetermines how many rows are shown in the truncated repr.\n\n`display.expand_frame_repr` allows for the representation of dataframes to\nstretch across pages, wrapped over the full column vs row-wise.\n\n`display.large_repr` lets you select whether to display dataframes that exceed\n`max_columns` or `max_rows` as a truncated frame, or as a summary.\n\n`display.max_colwidth` sets the maximum width of columns. Cells of this length\nor longer will be truncated with an ellipsis.\n\n`display.max_info_columns` sets a threshold for when by-column info will be\ngiven.\n\n`display.max_info_rows`: `df.info()` will usually show null-counts for each\ncolumn. For large frames this can be quite slow. `max_info_rows` and\n`max_info_cols` limit this null check only to frames with smaller dimensions\nthen specified. Note that you can specify the option\n`df.info(null_counts=True)` to override on showing a particular frame.\n\n`display.precision` sets the output display precision in terms of decimal\nplaces. This is only a suggestion.\n\n`display.chop_threshold` sets at what level pandas rounds to zero when it\ndisplays a Series of DataFrame. This setting does not change the precision at\nwhich the number is stored.\n\n`display.colheader_justify` controls the justification of the headers. The\noptions are \u2018right\u2019, and \u2018left\u2019.\n\nOption\n\nDefault\n\nFunction\n\ndisplay.chop_threshold\n\nNone\n\nIf set to a float value, all float values smaller then the given threshold\nwill be displayed as exactly 0 by repr and friends.\n\ndisplay.colheader_justify\n\nright\n\nControls the justification of column headers. used by DataFrameFormatter.\n\ndisplay.column_space\n\n12\n\nNo description available.\n\ndisplay.date_dayfirst\n\nFalse\n\nWhen True, prints and parses dates with the day first, eg 20/01/2005\n\ndisplay.date_yearfirst\n\nFalse\n\nWhen True, prints and parses dates with the year first, eg 2005/01/20\n\ndisplay.encoding\n\nUTF-8\n\nDefaults to the detected encoding of the console. Specifies the encoding to be\nused for strings returned by to_string, these are generally strings meant to\nbe displayed on the console.\n\ndisplay.expand_frame_repr\n\nTrue\n\nWhether to print out the full DataFrame repr for wide DataFrames across\nmultiple lines, `max_columns` is still respected, but the output will wrap-\naround across multiple \u201cpages\u201d if its width exceeds `display.width`.\n\ndisplay.float_format\n\nNone\n\nThe callable should accept a floating point number and return a string with\nthe desired format of the number. This is used in some places like\nSeriesFormatter. See core.format.EngFormatter for an example.\n\ndisplay.large_repr\n\ntruncate\n\nFor DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show\na truncated table (the default), or switch to the view from df.info() (the\nbehaviour in earlier versions of pandas). allowable settings, [\u2018truncate\u2019,\n\u2018info\u2019]\n\ndisplay.latex.repr\n\nFalse\n\nWhether to produce a latex DataFrame representation for Jupyter frontends that\nsupport it.\n\ndisplay.latex.escape\n\nTrue\n\nEscapes special characters in DataFrames, when using the to_latex method.\n\ndisplay.latex.longtable\n\nFalse\n\nSpecifies if the to_latex method of a DataFrame uses the longtable format.\n\ndisplay.latex.multicolumn\n\nTrue\n\nCombines columns when using a MultiIndex\n\ndisplay.latex.multicolumn_format\n\n\u2018l\u2019\n\nAlignment of multicolumn labels\n\ndisplay.latex.multirow\n\nFalse\n\nCombines rows when using a MultiIndex. Centered instead of top-aligned,\nseparated by clines.\n\ndisplay.max_columns\n\n0 or 20\n\nmax_rows and max_columns are used in __repr__() methods to decide if\nto_string() or info() is used to render an object to a string. In case\nPython/IPython is running in a terminal this is set to 0 by default and pandas\nwill correctly auto-detect the width of the terminal and switch to a smaller\nformat in case all columns would not fit vertically. The IPython notebook,\nIPython qtconsole, or IDLE do not run in a terminal and hence it is not\npossible to do correct auto-detection, in which case the default is set to 20.\n\u2018None\u2019 value means unlimited.\n\ndisplay.max_colwidth\n\n50\n\nThe maximum width in characters of a column in the repr of a pandas data\nstructure. When the column overflows, a \u201c\u2026\u201d placeholder is embedded in the\noutput. \u2018None\u2019 value means unlimited.\n\ndisplay.max_info_columns\n\n100\n\nmax_info_columns is used in DataFrame.info method to decide if per column\ninformation will be printed.\n\ndisplay.max_info_rows\n\n1690785\n\ndf.info() will usually show null-counts for each column. For large frames this\ncan be quite slow. max_info_rows and max_info_cols limit this null check only\nto frames with smaller dimensions then specified.\n\ndisplay.max_rows\n\n60\n\nThis sets the maximum number of rows pandas should output when printing out\nvarious output. For example, this value determines whether the repr() for a\ndataframe prints out fully or just a truncated or summary repr. \u2018None\u2019 value\nmeans unlimited.\n\ndisplay.min_rows\n\n10\n\nThe numbers of rows to show in a truncated repr (when `max_rows` is exceeded).\nIgnored when `max_rows` is set to None or 0. When set to None, follows the\nvalue of `max_rows`.\n\ndisplay.max_seq_items\n\n100\n\nwhen pretty-printing a long sequence, no more then `max_seq_items` will be\nprinted. If items are omitted, they will be denoted by the addition of \u201c\u2026\u201d to\nthe resulting string. If set to None, the number of items to be printed is\nunlimited.\n\ndisplay.memory_usage\n\nTrue\n\nThis specifies if the memory usage of a DataFrame should be displayed when the\ndf.info() method is invoked.\n\ndisplay.multi_sparse\n\nTrue\n\n\u201cSparsify\u201d MultiIndex display (don\u2019t display repeated elements in outer levels\nwithin groups)\n\ndisplay.notebook_repr_html\n\nTrue\n\nWhen True, IPython notebook will use html representation for pandas objects\n(if it is available).\n\ndisplay.pprint_nest_depth\n\n3\n\nControls the number of nested levels to process when pretty-printing\n\ndisplay.precision\n\n6\n\nFloating point output precision in terms of number of places after the\ndecimal, for regular formatting as well as scientific notation. Similar to\nnumpy\u2019s `precision` print option\n\ndisplay.show_dimensions\n\ntruncate\n\nWhether to print out dimensions at the end of DataFrame repr. If \u2018truncate\u2019 is\nspecified, only print out the dimensions if the frame is truncated (e.g. not\ndisplay all rows and/or columns)\n\ndisplay.width\n\n80\n\nWidth of the display in characters. In case Python/IPython is running in a\nterminal this can be set to None and pandas will correctly auto-detect the\nwidth. Note that the IPython notebook, IPython qtconsole, or IDLE do not run\nin a terminal and hence it is not possible to correctly detect the width.\n\ndisplay.html.table_schema\n\nFalse\n\nWhether to publish a Table Schema representation for frontends that support\nit.\n\ndisplay.html.border\n\n1\n\nA `border=value` attribute is inserted in the `<table>` tag for the DataFrame\nHTML repr.\n\ndisplay.html.use_mathjax\n\nTrue\n\nWhen True, Jupyter notebook will process table contents using MathJax,\nrendering mathematical expressions enclosed by the dollar symbol.\n\ndisplay.max_dir_items\n\n100\n\nThe number of columns from a dataframe that are added to dir. These columns\ncan then be suggested by tab completion. \u2018None\u2019 value means unlimited.\n\nio.excel.xls.writer\n\nxlwt\n\nThe default Excel writer engine for \u2018xls\u2019 files.\n\nDeprecated since version 1.2.0: As xlwt package is no longer maintained, the\n`xlwt` engine will be removed in a future version of pandas. Since this is the\nonly engine in pandas that supports writing to `.xls` files, this option will\nalso be removed.\n\nio.excel.xlsm.writer\n\nopenpyxl\n\nThe default Excel writer engine for \u2018xlsm\u2019 files. Available options:\n\u2018openpyxl\u2019 (the default).\n\nio.excel.xlsx.writer\n\nopenpyxl\n\nThe default Excel writer engine for \u2018xlsx\u2019 files.\n\nio.hdf.default_format\n\nNone\n\ndefault format writing format, if None, then put will default to \u2018fixed\u2019 and\nappend will default to \u2018table\u2019\n\nio.hdf.dropna_table\n\nTrue\n\ndrop ALL nan rows when appending to a table\n\nio.parquet.engine\n\nNone\n\nThe engine to use as a default for parquet reading and writing. If None then\ntry \u2018pyarrow\u2019 and \u2018fastparquet\u2019\n\nio.sql.engine\n\nNone\n\nThe engine to use as a default for sql reading and writing, with SQLAlchemy as\na higher level interface. If None then try \u2018sqlalchemy\u2019\n\nmode.chained_assignment\n\nwarn\n\nControls `SettingWithCopyWarning`: \u2018raise\u2019, \u2018warn\u2019, or None. Raise an\nexception, warn, or no action if trying to use chained assignment.\n\nmode.sim_interactive\n\nFalse\n\nWhether to simulate interactive mode for purposes of testing.\n\nmode.use_inf_as_na\n\nFalse\n\nTrue means treat None, NaN, -INF, INF as NA (old way), False means None and\nNaN are null, but INF, -INF are not NA (new way).\n\ncompute.use_bottleneck\n\nTrue\n\nUse the bottleneck library to accelerate computation if it is installed.\n\ncompute.use_numexpr\n\nTrue\n\nUse the numexpr library to accelerate computation if it is installed.\n\nplotting.backend\n\nmatplotlib\n\nChange the plotting backend to a different backend than the current matplotlib\none. Backends can be implemented as third-party libraries implementing the\npandas plotting API. They can use other plotting libraries like Bokeh, Altair,\netc.\n\nplotting.matplotlib.register_converters\n\nTrue\n\nRegister custom converters with matplotlib. Set to False to de-register.\n\nstyler.sparse.index\n\nTrue\n\n\u201cSparsify\u201d MultiIndex display for rows in Styler output (don\u2019t display\nrepeated elements in outer levels within groups).\n\nstyler.sparse.columns\n\nTrue\n\n\u201cSparsify\u201d MultiIndex display for columns in Styler output.\n\nstyler.render.repr\n\nhtml\n\nStandard output format for Styler rendered in Jupyter Notebook. Should be one\nof \u201chtml\u201d or \u201clatex\u201d.\n\nstyler.render.max_elements\n\n262144\n\nMaximum number of datapoints that Styler will render trimming either rows,\ncolumns or both to fit.\n\nstyler.render.max_rows\n\nNone\n\nMaximum number of rows that Styler will render. By default this is dynamic\nbased on `max_elements`.\n\nstyler.render.max_columns\n\nNone\n\nMaximum number of columns that Styler will render. By default this is dynamic\nbased on `max_elements`.\n\nstyler.render.encoding\n\nutf-8\n\nDefault encoding for output HTML or LaTeX files.\n\nstyler.format.formatter\n\nNone\n\nObject to specify formatting functions to `Styler.format`.\n\nstyler.format.na_rep\n\nNone\n\nString representation for missing data.\n\nstyler.format.precision\n\n6\n\nPrecision to display floating point and complex numbers.\n\nstyler.format.decimal\n\n.\n\nString representation for decimal point separator for floating point and\ncomplex numbers.\n\nstyler.format.thousands\n\nNone\n\nString representation for thousands separator for integers, and floating point\nand complex numbers.\n\nstyler.format.escape\n\nNone\n\nWhether to escape \u201chtml\u201d or \u201clatex\u201d special characters in the display\nrepresentation.\n\nstyler.html.mathjax\n\nTrue\n\nIf set to False will render specific CSS classes to table attributes that will\nprevent Mathjax from rendering in Jupyter Notebook.\n\nstyler.latex.multicol_align\n\nr\n\nAlignment of headers in a merged column due to sparsification. Can be in {\u201cr\u201d,\n\u201cc\u201d, \u201cl\u201d}.\n\nstyler.latex.multirow_align\n\nc\n\nAlignment of index labels in a merged row due to sparsification. Can be in\n{\u201cc\u201d, \u201ct\u201d, \u201cb\u201d}.\n\nstyler.latex.environment\n\nNone\n\nIf given will replace the default `\\\\begin{table}` environment. If \u201clongtable\u201d\nis specified this will render with a specific \u201clongtable\u201d template with\nlongtable features.\n\nstyler.latex.hrules\n\nFalse\n\nIf set to True will render `\\\\toprule`, `\\\\midrule`, and `\\bottomrule` by\ndefault.\n\npandas also allows you to set how numbers are displayed in the console. This\noption is not set through the `set_options` API.\n\nUse the `set_eng_float_format` function to alter the floating-point formatting\nof pandas objects to produce a particular format.\n\nFor instance:\n\nTo round floats on a case-by-case basis, you can also use `round()` and\n`round()`.\n\nWarning\n\nEnabling this option will affect the performance for printing of DataFrame and\nSeries (about 2 times slower). Use only when it is actually required.\n\nSome East Asian countries use Unicode characters whose width corresponds to\ntwo Latin characters. If a DataFrame or Series contains these characters, the\ndefault output mode may not align them properly.\n\nNote\n\nScreen captures are attached for each output to show the actual results.\n\nEnabling `display.unicode.east_asian_width` allows pandas to check each\ncharacter\u2019s \u201cEast Asian Width\u201d property. These characters can be aligned\nproperly by setting this option to `True`. However, this will result in longer\nrender times than the standard `len` function.\n\nIn addition, Unicode characters whose width is \u201cAmbiguous\u201d can either be 1 or\n2 characters wide depending on the terminal setting or encoding. The option\n`display.unicode.ambiguous_as_wide` can be used to handle the ambiguity.\n\nBy default, an \u201cAmbiguous\u201d character\u2019s width, such as \u201c\u00a1\u201d (inverted\nexclamation) in the example below, is taken to be 1.\n\nEnabling `display.unicode.ambiguous_as_wide` makes pandas interpret these\ncharacters\u2019 widths to be 2. (Note that this option will only be effective when\n`display.unicode.east_asian_width` is enabled.)\n\nHowever, setting this option incorrectly for your terminal will cause these\ncharacters to be aligned incorrectly:\n\n`DataFrame` and `Series` will publish a Table Schema representation by\ndefault. False by default, this can be enabled globally with the\n`display.html.table_schema` option:\n\nOnly `'display.max_rows'` are serialized and published.\n\n"}, {"name": "pandas arrays, scalars, and data types", "path": "reference/arrays", "type": "Pandas arrays", "text": "\nFor most data types, pandas uses NumPy arrays as the concrete objects\ncontained with a `Index`, `Series`, or `DataFrame`.\n\nFor some data types, pandas extends NumPy\u2019s type system. String aliases for\nthese types can be found at dtypes.\n\nKind of Data\n\npandas Data Type\n\nScalar\n\nArray\n\nTZ-aware datetime\n\n`DatetimeTZDtype`\n\n`Timestamp`\n\nDatetime data\n\nTimedeltas\n\n(none)\n\n`Timedelta`\n\nTimedelta data\n\nPeriod (time spans)\n\n`PeriodDtype`\n\n`Period`\n\nTimespan data\n\nIntervals\n\n`IntervalDtype`\n\n`Interval`\n\nInterval data\n\nNullable Integer\n\n`Int64Dtype`, \u2026\n\n(none)\n\nNullable integer\n\nCategorical\n\n`CategoricalDtype`\n\n(none)\n\nCategorical data\n\nSparse\n\n`SparseDtype`\n\n(none)\n\nSparse data\n\nStrings\n\n`StringDtype`\n\n`str`\n\nText data\n\nBoolean (with NA)\n\n`BooleanDtype`\n\n`bool`\n\nBoolean data with missing values\n\npandas and third-party libraries can extend NumPy\u2019s type system (see Extension\ntypes). The top-level `array()` method can be used to create a new array,\nwhich may be stored in a `Series`, `Index`, or as a column in a `DataFrame`.\n\n`array`(data[, dtype, copy])\n\nCreate an array.\n\nNumPy cannot natively represent timezone-aware datetimes. pandas supports this\nwith the `arrays.DatetimeArray` extension array, which can hold timezone-naive\nor timezone-aware values.\n\n`Timestamp`, a subclass of `datetime.datetime`, is pandas\u2019 scalar type for\ntimezone-naive or timezone-aware datetime data.\n\n`Timestamp`([ts_input, freq, tz, unit, year, ...])\n\nPandas replacement for python datetime.datetime object.\n\n`Timestamp.asm8`\n\nReturn numpy datetime64 format in nanoseconds.\n\n`Timestamp.day`\n\n`Timestamp.dayofweek`\n\nReturn day of the week.\n\n`Timestamp.day_of_week`\n\nReturn day of the week.\n\n`Timestamp.dayofyear`\n\nReturn the day of the year.\n\n`Timestamp.day_of_year`\n\nReturn the day of the year.\n\n`Timestamp.days_in_month`\n\nReturn the number of days in the month.\n\n`Timestamp.daysinmonth`\n\nReturn the number of days in the month.\n\n`Timestamp.fold`\n\n`Timestamp.hour`\n\n`Timestamp.is_leap_year`\n\nReturn True if year is a leap year.\n\n`Timestamp.is_month_end`\n\nReturn True if date is last day of month.\n\n`Timestamp.is_month_start`\n\nReturn True if date is first day of month.\n\n`Timestamp.is_quarter_end`\n\nReturn True if date is last day of the quarter.\n\n`Timestamp.is_quarter_start`\n\nReturn True if date is first day of the quarter.\n\n`Timestamp.is_year_end`\n\nReturn True if date is last day of the year.\n\n`Timestamp.is_year_start`\n\nReturn True if date is first day of the year.\n\n`Timestamp.max`\n\n`Timestamp.microsecond`\n\n`Timestamp.min`\n\n`Timestamp.minute`\n\n`Timestamp.month`\n\n`Timestamp.nanosecond`\n\n`Timestamp.quarter`\n\nReturn the quarter of the year.\n\n`Timestamp.resolution`\n\n`Timestamp.second`\n\n`Timestamp.tz`\n\nAlias for tzinfo.\n\n`Timestamp.tzinfo`\n\n`Timestamp.value`\n\n`Timestamp.week`\n\nReturn the week number of the year.\n\n`Timestamp.weekofyear`\n\nReturn the week number of the year.\n\n`Timestamp.year`\n\n`Timestamp.astimezone`(tz)\n\nConvert timezone-aware Timestamp to another time zone.\n\n`Timestamp.ceil`(freq[, ambiguous, nonexistent])\n\nReturn a new Timestamp ceiled to this resolution.\n\n`Timestamp.combine`(date, time)\n\nCombine date, time into datetime with same date and time fields.\n\n`Timestamp.ctime`\n\nReturn ctime() style string.\n\n`Timestamp.date`\n\nReturn date object with same year, month and day.\n\n`Timestamp.day_name`\n\nReturn the day name of the Timestamp with specified locale.\n\n`Timestamp.dst`\n\nReturn self.tzinfo.dst(self).\n\n`Timestamp.floor`(freq[, ambiguous, nonexistent])\n\nReturn a new Timestamp floored to this resolution.\n\n`Timestamp.freq`\n\n`Timestamp.freqstr`\n\nReturn the total number of days in the month.\n\n`Timestamp.fromordinal`(ordinal[, freq, tz])\n\nPassed an ordinal, translate and convert to a ts.\n\n`Timestamp.fromtimestamp`(ts)\n\nTransform timestamp[, tz] to tz's local time from POSIX timestamp.\n\n`Timestamp.isocalendar`\n\nReturn a 3-tuple containing ISO year, week number, and weekday.\n\n`Timestamp.isoformat`\n\nReturn the time formatted according to ISO 8610.\n\n`Timestamp.isoweekday`()\n\nReturn the day of the week represented by the date.\n\n`Timestamp.month_name`\n\nReturn the month name of the Timestamp with specified locale.\n\n`Timestamp.normalize`\n\nNormalize Timestamp to midnight, preserving tz information.\n\n`Timestamp.now`([tz])\n\nReturn new Timestamp object representing current time local to tz.\n\n`Timestamp.replace`([year, month, day, hour, ...])\n\nImplements datetime.replace, handles nanoseconds.\n\n`Timestamp.round`(freq[, ambiguous, nonexistent])\n\nRound the Timestamp to the specified resolution.\n\n`Timestamp.strftime`(format)\n\nReturn a string representing the given POSIX timestamp controlled by an\nexplicit format string.\n\n`Timestamp.strptime`(string, format)\n\nFunction is not implemented.\n\n`Timestamp.time`\n\nReturn time object with same time but with tzinfo=None.\n\n`Timestamp.timestamp`\n\nReturn POSIX timestamp as float.\n\n`Timestamp.timetuple`\n\nReturn time tuple, compatible with time.localtime().\n\n`Timestamp.timetz`\n\nReturn time object with same time and tzinfo.\n\n`Timestamp.to_datetime64`\n\nReturn a numpy.datetime64 object with 'ns' precision.\n\n`Timestamp.to_numpy`\n\nConvert the Timestamp to a NumPy datetime64.\n\n`Timestamp.to_julian_date`()\n\nConvert TimeStamp to a Julian Date.\n\n`Timestamp.to_period`\n\nReturn an period of which this timestamp is an observation.\n\n`Timestamp.to_pydatetime`\n\nConvert a Timestamp object to a native Python datetime object.\n\n`Timestamp.today`(cls[, tz])\n\nReturn the current time in the local timezone.\n\n`Timestamp.toordinal`\n\nReturn proleptic Gregorian ordinal.\n\n`Timestamp.tz_convert`(tz)\n\nConvert timezone-aware Timestamp to another time zone.\n\n`Timestamp.tz_localize`(tz[, ambiguous, ...])\n\nConvert naive Timestamp to local time zone, or remove timezone from timezone-\naware Timestamp.\n\n`Timestamp.tzname`\n\nReturn self.tzinfo.tzname(self).\n\n`Timestamp.utcfromtimestamp`(ts)\n\nConstruct a naive UTC datetime from a POSIX timestamp.\n\n`Timestamp.utcnow`()\n\nReturn a new Timestamp representing UTC day and time.\n\n`Timestamp.utcoffset`\n\nReturn self.tzinfo.utcoffset(self).\n\n`Timestamp.utctimetuple`\n\nReturn UTC time tuple, compatible with time.localtime().\n\n`Timestamp.weekday`()\n\nReturn the day of the week represented by the date.\n\nA collection of timestamps may be stored in a `arrays.DatetimeArray`. For\ntimezone-aware data, the `.dtype` of a `arrays.DatetimeArray` is a\n`DatetimeTZDtype`. For timezone-naive data, `np.dtype(\"datetime64[ns]\")` is\nused.\n\nIf the data are timezone-aware, then every value in the array must have the\nsame timezone.\n\n`arrays.DatetimeArray`(values[, dtype, freq, copy])\n\nPandas ExtensionArray for tz-naive or tz-aware datetime data.\n\n`DatetimeTZDtype`([unit, tz])\n\nAn ExtensionDtype for timezone-aware datetime data.\n\nNumPy can natively represent timedeltas. pandas provides `Timedelta` for\nsymmetry with `Timestamp`.\n\n`Timedelta`([value, unit])\n\nRepresents a duration, the difference between two dates or times.\n\n`Timedelta.asm8`\n\nReturn a numpy timedelta64 array scalar view.\n\n`Timedelta.components`\n\nReturn a components namedtuple-like.\n\n`Timedelta.days`\n\nNumber of days.\n\n`Timedelta.delta`\n\nReturn the timedelta in nanoseconds (ns), for internal compatibility.\n\n`Timedelta.freq`\n\n`Timedelta.is_populated`\n\n`Timedelta.max`\n\n`Timedelta.microseconds`\n\nNumber of microseconds (>= 0 and less than 1 second).\n\n`Timedelta.min`\n\n`Timedelta.nanoseconds`\n\nReturn the number of nanoseconds (n), where 0 <= n < 1 microsecond.\n\n`Timedelta.resolution`\n\n`Timedelta.seconds`\n\nNumber of seconds (>= 0 and less than 1 day).\n\n`Timedelta.value`\n\n`Timedelta.view`\n\nArray view compatibility.\n\n`Timedelta.ceil`(freq)\n\nReturn a new Timedelta ceiled to this resolution.\n\n`Timedelta.floor`(freq)\n\nReturn a new Timedelta floored to this resolution.\n\n`Timedelta.isoformat`\n\nFormat Timedelta as ISO 8601 Duration like `P[n]Y[n]M[n]DT[n]H[n]M[n]S`, where\nthe `[n]` s are replaced by the values.\n\n`Timedelta.round`(freq)\n\nRound the Timedelta to the specified resolution.\n\n`Timedelta.to_pytimedelta`\n\nConvert a pandas Timedelta object into a python `datetime.timedelta` object.\n\n`Timedelta.to_timedelta64`\n\nReturn a numpy.timedelta64 object with 'ns' precision.\n\n`Timedelta.to_numpy`\n\nConvert the Timedelta to a NumPy timedelta64.\n\n`Timedelta.total_seconds`\n\nTotal seconds in the duration.\n\nA collection of `Timedelta` may be stored in a `TimedeltaArray`.\n\n`arrays.TimedeltaArray`(values[, dtype, freq, ...])\n\nPandas ExtensionArray for timedelta data.\n\npandas represents spans of times as `Period` objects.\n\n`Period`([value, freq, ordinal, year, month, ...])\n\nRepresents a period of time.\n\n`Period.day`\n\nGet day of the month that a Period falls on.\n\n`Period.dayofweek`\n\nDay of the week the period lies in, with Monday=0 and Sunday=6.\n\n`Period.day_of_week`\n\nDay of the week the period lies in, with Monday=0 and Sunday=6.\n\n`Period.dayofyear`\n\nReturn the day of the year.\n\n`Period.day_of_year`\n\nReturn the day of the year.\n\n`Period.days_in_month`\n\nGet the total number of days in the month that this period falls on.\n\n`Period.daysinmonth`\n\nGet the total number of days of the month that the Period falls in.\n\n`Period.end_time`\n\nGet the Timestamp for the end of the period.\n\n`Period.freq`\n\n`Period.freqstr`\n\nReturn a string representation of the frequency.\n\n`Period.hour`\n\nGet the hour of the day component of the Period.\n\n`Period.is_leap_year`\n\nReturn True if the period's year is in a leap year.\n\n`Period.minute`\n\nGet minute of the hour component of the Period.\n\n`Period.month`\n\nReturn the month this Period falls on.\n\n`Period.ordinal`\n\n`Period.quarter`\n\nReturn the quarter this Period falls on.\n\n`Period.qyear`\n\nFiscal year the Period lies in according to its starting-quarter.\n\n`Period.second`\n\nGet the second component of the Period.\n\n`Period.start_time`\n\nGet the Timestamp for the start of the period.\n\n`Period.week`\n\nGet the week of the year on the given Period.\n\n`Period.weekday`\n\nDay of the week the period lies in, with Monday=0 and Sunday=6.\n\n`Period.weekofyear`\n\nGet the week of the year on the given Period.\n\n`Period.year`\n\nReturn the year this Period falls on.\n\n`Period.asfreq`\n\nConvert Period to desired frequency, at the start or end of the interval.\n\n`Period.now`\n\nReturn the period of now's date.\n\n`Period.strftime`\n\nReturns the string representation of the `Period`, depending on the selected\n`fmt`.\n\n`Period.to_timestamp`\n\nReturn the Timestamp representation of the Period.\n\nA collection of `Period` may be stored in a `arrays.PeriodArray`. Every period\nin a `arrays.PeriodArray` must have the same `freq`.\n\n`arrays.PeriodArray`(values[, dtype, freq, copy])\n\nPandas ExtensionArray for storing Period data.\n\n`PeriodDtype`([freq])\n\nAn ExtensionDtype for Period data.\n\nArbitrary intervals can be represented as `Interval` objects.\n\n`Interval`\n\nImmutable object implementing an Interval, a bounded slice-like interval.\n\n`Interval.closed`\n\nWhether the interval is closed on the left-side, right-side, both or neither.\n\n`Interval.closed_left`\n\nCheck if the interval is closed on the left side.\n\n`Interval.closed_right`\n\nCheck if the interval is closed on the right side.\n\n`Interval.is_empty`\n\nIndicates if an interval is empty, meaning it contains no points.\n\n`Interval.left`\n\nLeft bound for the interval.\n\n`Interval.length`\n\nReturn the length of the Interval.\n\n`Interval.mid`\n\nReturn the midpoint of the Interval.\n\n`Interval.open_left`\n\nCheck if the interval is open on the left side.\n\n`Interval.open_right`\n\nCheck if the interval is open on the right side.\n\n`Interval.overlaps`\n\nCheck whether two Interval objects overlap.\n\n`Interval.right`\n\nRight bound for the interval.\n\nA collection of intervals may be stored in an `arrays.IntervalArray`.\n\n`arrays.IntervalArray`(data[, closed, dtype, ...])\n\nPandas array for interval data that are closed on the same side.\n\n`IntervalDtype`([subtype, closed])\n\nAn ExtensionDtype for Interval data.\n\n`numpy.ndarray` cannot natively represent integer-data with missing values.\npandas provides this through `arrays.IntegerArray`.\n\n`arrays.IntegerArray`(values, mask[, copy])\n\nArray of integer (optional missing) values.\n\n`Int8Dtype`()\n\nAn ExtensionDtype for int8 integer data.\n\n`Int16Dtype`()\n\nAn ExtensionDtype for int16 integer data.\n\n`Int32Dtype`()\n\nAn ExtensionDtype for int32 integer data.\n\n`Int64Dtype`()\n\nAn ExtensionDtype for int64 integer data.\n\n`UInt8Dtype`()\n\nAn ExtensionDtype for uint8 integer data.\n\n`UInt16Dtype`()\n\nAn ExtensionDtype for uint16 integer data.\n\n`UInt32Dtype`()\n\nAn ExtensionDtype for uint32 integer data.\n\n`UInt64Dtype`()\n\nAn ExtensionDtype for uint64 integer data.\n\npandas defines a custom data type for representing data that can take only a\nlimited, fixed set of values. The dtype of a `Categorical` can be described by\na `CategoricalDtype`.\n\n`CategoricalDtype`([categories, ordered])\n\nType for categorical data with the categories and orderedness.\n\n`CategoricalDtype.categories`\n\nAn `Index` containing the unique categories allowed.\n\n`CategoricalDtype.ordered`\n\nWhether the categories have an ordered relationship.\n\nCategorical data can be stored in a `pandas.Categorical`\n\n`Categorical`(values[, categories, ordered, ...])\n\nRepresent a categorical variable in classic R / S-plus fashion.\n\nThe alternative `Categorical.from_codes()` constructor can be used when you\nhave the categories and integer codes already:\n\n`Categorical.from_codes`(codes[, categories, ...])\n\nMake a Categorical type from codes and categories or dtype.\n\nThe dtype information is available on the `Categorical`\n\n`Categorical.dtype`\n\nThe `CategoricalDtype` for this instance.\n\n`Categorical.categories`\n\nThe categories of this categorical.\n\n`Categorical.ordered`\n\nWhether the categories have an ordered relationship.\n\n`Categorical.codes`\n\nThe category codes of this categorical.\n\n`np.asarray(categorical)` works by implementing the array interface. Be aware,\nthat this converts the `Categorical` back to a NumPy array, so categories and\norder information is not preserved!\n\n`Categorical.__array__`([dtype])\n\nThe numpy array interface.\n\nA `Categorical` can be stored in a `Series` or `DataFrame`. To create a Series\nof dtype `category`, use `cat = s.astype(dtype)` or `Series(..., dtype=dtype)`\nwhere `dtype` is either\n\nthe string `'category'`\n\nan instance of `CategoricalDtype`.\n\nIf the `Series` is of dtype `CategoricalDtype`, `Series.cat` can be used to\nchange the categorical data. See Categorical accessor for more.\n\nData where a single value is repeated many times (e.g. `0` or `NaN`) may be\nstored efficiently as a `arrays.SparseArray`.\n\n`arrays.SparseArray`(data[, sparse_index, ...])\n\nAn ExtensionArray for storing sparse data.\n\n`SparseDtype`([dtype, fill_value])\n\nDtype for data stored in `SparseArray`.\n\nThe `Series.sparse` accessor may be used to access sparse-specific attributes\nand methods if the `Series` contains sparse values. See Sparse accessor for\nmore.\n\nWhen working with text data, where each valid element is a string or missing,\nwe recommend using `StringDtype` (with the alias `\"string\"`).\n\n`arrays.StringArray`(values[, copy])\n\nExtension array for string data.\n\n`arrays.ArrowStringArray`(values)\n\nExtension array for string data in a `pyarrow.ChunkedArray`.\n\n`StringDtype`([storage])\n\nExtension dtype for string data.\n\nThe `Series.str` accessor is available for `Series` backed by a\n`arrays.StringArray`. See String handling for more.\n\nThe boolean dtype (with the alias `\"boolean\"`) provides support for storing\nboolean data (`True`, `False`) with missing values, which is not possible with\na bool `numpy.ndarray`.\n\n`arrays.BooleanArray`(values, mask[, copy])\n\nArray of boolean (True/False) data with missing values.\n\n`BooleanDtype`()\n\nExtension dtype for boolean data.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray", "path": "reference/api/pandas.api.extensions.extensionarray", "type": "Extensions", "text": "\nAbstract base class for custom 1-D array types.\n\npandas will recognize instances of this class as proper arrays with a custom\ntype and will not attempt to coerce them to objects. They may be stored\ndirectly inside a `DataFrame` or `Series`.\n\nNotes\n\nThe interface includes the following abstract methods that must be implemented\nby subclasses:\n\n_from_sequence\n\n_from_factorized\n\n__getitem__\n\n__len__\n\n__eq__\n\ndtype\n\nnbytes\n\nisna\n\ntake\n\ncopy\n\n_concat_same_type\n\nA default repr displaying the type, (truncated) data, length, and dtype is\nprovided. It can be customized or replaced by by overriding:\n\n__repr__ : A default repr for the ExtensionArray.\n\n_formatter : Print scalars inside a Series or DataFrame.\n\nSome methods require casting the ExtensionArray to an ndarray of Python\nobjects with `self.astype(object)`, which may be expensive. When performance\nis a concern, we highly recommend overriding the following methods:\n\nfillna\n\ndropna\n\nunique\n\nfactorize / _values_for_factorize\n\nargsort / _values_for_argsort\n\nsearchsorted\n\nThe remaining methods implemented on this class should be performant, as they\nonly compose abstract methods. Still, a more efficient implementation may be\navailable, and these methods can be overridden.\n\nOne can implement methods to handle array reductions.\n\n_reduce\n\nOne can implement methods to handle parsing from strings that will be used in\nmethods such as `pandas.io.parsers.read_csv`.\n\n_from_sequence_of_strings\n\nThis class does not inherit from \u2018abc.ABCMeta\u2019 for performance reasons.\nMethods and properties required by the interface raise\n`pandas.errors.AbstractMethodError` and no `register` method is provided for\nregistering virtual subclasses.\n\nExtensionArrays are limited to 1 dimension.\n\nThey may be backed by none, one, or many NumPy arrays. For example,\n`pandas.Categorical` is an extension array backed by two arrays, one for codes\nand one for categories. An array of IPv6 address may be backed by a NumPy\nstructured array with two fields, one for the lower 64 bits and one for the\nupper 64 bits. Or they may be backed by some other storage type, like Python\nlists. Pandas makes no assumptions on how the data are stored, just that it\ncan be converted to a NumPy array. The ExtensionArray interface does not\nimpose any rules on how this data is stored. However, currently, the backing\ndata cannot be stored in attributes called `.values` or `._values` to ensure\nfull compatibility with pandas internals. But other names as `.data`,\n`._data`, `._items`, \u2026 can be freely used.\n\nIf implementing NumPy\u2019s `__array_ufunc__` interface, pandas expects that\n\nYou defer by returning `NotImplemented` when any Series are present in inputs.\nPandas will extract the arrays and call the ufunc again.\n\nYou define a `_HANDLED_TYPES` tuple as an attribute on the class. Pandas\ninspect this to determine whether the ufunc is valid for the types present.\n\nSee NumPy universal functions for more.\n\nBy default, ExtensionArrays are not hashable. Immutable subclasses may\noverride this behavior.\n\nAttributes\n\n`dtype`\n\nAn instance of 'ExtensionDtype'.\n\n`nbytes`\n\nThe number of bytes needed to store this object in memory.\n\n`ndim`\n\nExtension Arrays are only allowed to be 1-dimensional.\n\n`shape`\n\nReturn a tuple of the array dimensions.\n\nMethods\n\n`argsort`([ascending, kind, na_position])\n\nReturn the indices that would sort this array.\n\n`astype`(dtype[, copy])\n\nCast to a NumPy array or ExtensionArray with 'dtype'.\n\n`copy`()\n\nReturn a copy of the array.\n\n`dropna`()\n\nReturn ExtensionArray without NA values.\n\n`factorize`([na_sentinel])\n\nEncode the extension array as an enumerated type.\n\n`fillna`([value, method, limit])\n\nFill NA/NaN values using the specified method.\n\n`equals`(other)\n\nReturn if another array is equivalent to this array.\n\n`insert`(loc, item)\n\nInsert an item at the given position.\n\n`isin`(values)\n\nPointwise comparison for set containment in the given values.\n\n`isna`()\n\nA 1-D array indicating if each value is missing.\n\n`ravel`([order])\n\nReturn a flattened view on this array.\n\n`repeat`(repeats[, axis])\n\nRepeat elements of a ExtensionArray.\n\n`searchsorted`(value[, side, sorter])\n\nFind indices where elements should be inserted to maintain order.\n\n`shift`([periods, fill_value])\n\nShift values by desired number.\n\n`take`(indices, *[, allow_fill, fill_value])\n\nTake elements from an array.\n\n`tolist`()\n\nReturn a list of the values.\n\n`unique`()\n\nCompute the ExtensionArray of unique values.\n\n`view`([dtype])\n\nReturn a view on the array.\n\n`_concat_same_type`(to_concat)\n\nConcatenate multiple array of this dtype.\n\n`_formatter`([boxed])\n\nFormatting function for scalar values.\n\n`_from_factorized`(values, original)\n\nReconstruct an ExtensionArray after factorization.\n\n`_from_sequence`(scalars, *[, dtype, copy])\n\nConstruct a new ExtensionArray from a sequence of scalars.\n\n`_from_sequence_of_strings`(strings, *[, ...])\n\nConstruct a new ExtensionArray from a sequence of strings.\n\n`_reduce`(name, *[, skipna])\n\nReturn a scalar result of performing the reduction operation.\n\n`_values_for_argsort`()\n\nReturn values for sorting.\n\n`_values_for_factorize`()\n\nReturn an array and missing value suitable for factorization.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray._concat_same_type", "path": "reference/api/pandas.api.extensions.extensionarray._concat_same_type", "type": "Extensions", "text": "\nConcatenate multiple array of this dtype.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray._formatter", "path": "reference/api/pandas.api.extensions.extensionarray._formatter", "type": "Extensions", "text": "\nFormatting function for scalar values.\n\nThis is used in the default \u2018__repr__\u2019. The returned formatting function\nreceives instances of your scalar type.\n\nAn indicated for whether or not your array is being printed within a Series,\nDataFrame, or Index (True), or just by itself (False). This may be useful if\nyou want scalar values to appear differently within a Series versus on its own\n(e.g. quoted or not).\n\nA callable that gets instances of the scalar type and returns a string. By\ndefault, `repr()` is used when `boxed=False` and `str()` is used when\n`boxed=True`.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray._from_factorized", "path": "reference/api/pandas.api.extensions.extensionarray._from_factorized", "type": "Extensions", "text": "\nReconstruct an ExtensionArray after factorization.\n\nAn integer ndarray with the factorized values.\n\nThe original ExtensionArray that factorize was called on.\n\nSee also\n\nTop-level factorize method that dispatches here.\n\nEncode the extension array as an enumerated type.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray._from_sequence", "path": "reference/api/pandas.api.extensions.extensionarray._from_sequence", "type": "Extensions", "text": "\nConstruct a new ExtensionArray from a sequence of scalars.\n\nEach element will be an instance of the scalar type for this array,\n`cls.dtype.type` or be converted into this type in this method.\n\nConstruct for this particular dtype. This should be a Dtype compatible with\nthe ExtensionArray.\n\nIf True, copy the underlying data.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray._from_sequence_of_strings", "path": "reference/api/pandas.api.extensions.extensionarray._from_sequence_of_strings", "type": "Extensions", "text": "\nConstruct a new ExtensionArray from a sequence of strings.\n\nEach element will be an instance of the scalar type for this array,\n`cls.dtype.type`.\n\nConstruct for this particular dtype. This should be a Dtype compatible with\nthe ExtensionArray.\n\nIf True, copy the underlying data.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray._reduce", "path": "reference/api/pandas.api.extensions.extensionarray._reduce", "type": "Extensions", "text": "\nReturn a scalar result of performing the reduction operation.\n\nName of the function, supported values are: { any, all, min, max, sum, mean,\nmedian, prod, std, var, sem, kurt, skew }.\n\nIf True, skip NaN values.\n\nAdditional keyword arguments passed to the reduction function. Currently, ddof\nis the only supported kwarg.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray._values_for_argsort", "path": "reference/api/pandas.api.extensions.extensionarray._values_for_argsort", "type": "Extensions", "text": "\nReturn values for sorting.\n\nThe transformed values should maintain the ordering between values within the\narray.\n\nSee also\n\nReturn the indices that would sort this array.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray._values_for_factorize", "path": "reference/api/pandas.api.extensions.extensionarray._values_for_factorize", "type": "Extensions", "text": "\nReturn an array and missing value suitable for factorization.\n\nAn array suitable for factorization. This should maintain order and be a\nsupported dtype (Float64, Int64, UInt64, String, Object). By default, the\nextension array is cast to object dtype.\n\nThe value in values to consider missing. This will be treated as NA in the\nfactorization routines, so it will be coded as na_sentinel and not included in\nuniques. By default, `np.nan` is used.\n\nNotes\n\nThe values returned by this method are also used in\n`pandas.util.hash_pandas_object()`.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.argsort", "path": "reference/api/pandas.api.extensions.extensionarray.argsort", "type": "Extensions", "text": "\nReturn the indices that would sort this array.\n\nWhether the indices should result in an ascending or descending sort.\n\nSorting algorithm.\n\nPassed through to `numpy.argsort()`.\n\nArray of indices that sort `self`. If NaN values are contained, NaN values are\nplaced at the end.\n\nSee also\n\nSorting implementation used internally.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.astype", "path": "reference/api/pandas.api.extensions.extensionarray.astype", "type": "Extensions", "text": "\nCast to a NumPy array or ExtensionArray with \u2018dtype\u2019.\n\nTypecode or data-type to which the array is cast.\n\nWhether to copy the data, even if not necessary. If False, a copy is made only\nif the old dtype does not match the new dtype.\n\nAn ExtensionArray if dtype is ExtensionDtype, Otherwise a NumPy ndarray with\n\u2018dtype\u2019 for its dtype.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.copy", "path": "reference/api/pandas.api.extensions.extensionarray.copy", "type": "Extensions", "text": "\nReturn a copy of the array.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.dropna", "path": "reference/api/pandas.api.extensions.extensionarray.dropna", "type": "Extensions", "text": "\nReturn ExtensionArray without NA values.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.dtype", "path": "reference/api/pandas.api.extensions.extensionarray.dtype", "type": "Extensions", "text": "\nAn instance of \u2018ExtensionDtype\u2019.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.equals", "path": "reference/api/pandas.api.extensions.extensionarray.equals", "type": "Extensions", "text": "\nReturn if another array is equivalent to this array.\n\nEquivalent means that both arrays have the same shape and dtype, and all\nvalues compare equal. Missing values in the same location are considered equal\n(in contrast with normal equality).\n\nArray to compare to this Array.\n\nWhether the arrays are equivalent.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.factorize", "path": "reference/api/pandas.api.extensions.extensionarray.factorize", "type": "Extensions", "text": "\nEncode the extension array as an enumerated type.\n\nValue to use in the codes array to indicate missing values.\n\nAn integer NumPy array that\u2019s an indexer into the original ExtensionArray.\n\nAn ExtensionArray containing the unique values of self.\n\nNote\n\nuniques will not contain an entry for the NA value of the ExtensionArray if\nthere are any missing values present in self.\n\nSee also\n\nTop-level factorize method that dispatches here.\n\nNotes\n\n`pandas.factorize()` offers a sort keyword as well.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.fillna", "path": "reference/api/pandas.api.extensions.extensionarray.fillna", "type": "Extensions", "text": "\nFill NA/NaN values using the specified method.\n\nIf a scalar value is passed it is used to fill all missing values.\nAlternatively, an array-like \u2018value\u2019 can be given. It\u2019s expected that the\narray-like have the same length as \u2018self\u2019.\n\nMethod to use for filling holes in reindexed Series pad / ffill: propagate\nlast valid observation forward to next valid backfill / bfill: use NEXT valid\nobservation to fill gap.\n\nIf method is specified, this is the maximum number of consecutive NaN values\nto forward/backward fill. In other words, if there is a gap with more than\nthis number of consecutive NaNs, it will only be partially filled. If method\nis not specified, this is the maximum number of entries along the entire axis\nwhere NaNs will be filled.\n\nWith NA/NaN filled.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.insert", "path": "reference/api/pandas.api.extensions.extensionarray.insert", "type": "Extensions", "text": "\nInsert an item at the given position.\n\nNotes\n\nThis method should be both type and dtype-preserving. If the item cannot be\nheld in an array of this type/dtype, either ValueError or TypeError should be\nraised.\n\nThe default implementation relies on _from_sequence to raise on invalid items.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.isin", "path": "reference/api/pandas.api.extensions.extensionarray.isin", "type": "Extensions", "text": "\nPointwise comparison for set containment in the given values.\n\nRoughly equivalent to np.array([x in values for x in self])\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.isna", "path": "reference/api/pandas.api.extensions.extensionarray.isna", "type": "Extensions", "text": "\nA 1-D array indicating if each value is missing.\n\nIn most cases, this should return a NumPy ndarray. For exceptional cases like\n`SparseArray`, where returning an ndarray would be expensive, an\nExtensionArray may be returned.\n\nNotes\n\nIf returning an ExtensionArray, then\n\n`na_values._is_boolean` should be True\n\nna_values should implement `ExtensionArray._reduce()`\n\n`na_values.any` and `na_values.all` should be implemented\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.nbytes", "path": "reference/api/pandas.api.extensions.extensionarray.nbytes", "type": "Extensions", "text": "\nThe number of bytes needed to store this object in memory.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.ndim", "path": "reference/api/pandas.api.extensions.extensionarray.ndim", "type": "Extensions", "text": "\nExtension Arrays are only allowed to be 1-dimensional.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.ravel", "path": "reference/api/pandas.api.extensions.extensionarray.ravel", "type": "Extensions", "text": "\nReturn a flattened view on this array.\n\nNotes\n\nBecause ExtensionArrays are 1D-only, this is a no-op.\n\nThe \u201corder\u201d argument is ignored, is for compatibility with NumPy.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.repeat", "path": "reference/api/pandas.api.extensions.extensionarray.repeat", "type": "Extensions", "text": "\nRepeat elements of a ExtensionArray.\n\nReturns a new ExtensionArray where each element of the current ExtensionArray\nis repeated consecutively a given number of times.\n\nThe number of repetitions for each element. This should be a non-negative\ninteger. Repeating 0 times will return an empty ExtensionArray.\n\nMust be `None`. Has no effect but is accepted for compatibility with numpy.\n\nNewly created ExtensionArray with repeated elements.\n\nSee also\n\nEquivalent function for Series.\n\nEquivalent function for Index.\n\nSimilar method for `numpy.ndarray`.\n\nTake arbitrary positions.\n\nExamples\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.searchsorted", "path": "reference/api/pandas.api.extensions.extensionarray.searchsorted", "type": "Extensions", "text": "\nFind indices where elements should be inserted to maintain order.\n\nFind the indices into a sorted array self (a) such that, if the corresponding\nelements in value were inserted before the indices, the order of self would be\npreserved.\n\nAssuming that self is sorted:\n\nside\n\nreturned index i satisfies\n\nleft\n\n`self[i-1] < value <= self[i]`\n\nright\n\n`self[i-1] <= value < self[i]`\n\nValue(s) to insert into self.\n\nIf \u2018left\u2019, the index of the first suitable location found is given. If\n\u2018right\u2019, return the last such index. If there is no suitable index, return\neither 0 or N (where N is the length of self).\n\nOptional array of integer indices that sort array a into ascending order. They\nare typically the result of argsort.\n\nIf value is array-like, array of insertion points. If value is scalar, a\nsingle integer.\n\nSee also\n\nSimilar method from NumPy.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.shape", "path": "reference/api/pandas.api.extensions.extensionarray.shape", "type": "Extensions", "text": "\nReturn a tuple of the array dimensions.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.shift", "path": "reference/api/pandas.api.extensions.extensionarray.shift", "type": "Extensions", "text": "\nShift values by desired number.\n\nNewly introduced missing values are filled with `self.dtype.na_value`.\n\nThe number of periods to shift. Negative values are allowed for shifting\nbackwards.\n\nThe scalar value to use for newly introduced missing values. The default is\n`self.dtype.na_value`.\n\nShifted.\n\nNotes\n\nIf `self` is empty or `periods` is 0, a copy of `self` is returned.\n\nIf `periods > len(self)`, then an array of size len(self) is returned, with\nall values filled with `self.dtype.na_value`.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.take", "path": "reference/api/pandas.api.extensions.extensionarray.take", "type": "Extensions", "text": "\nTake elements from an array.\n\nIndices to be taken.\n\nHow to handle negative values in indices.\n\nFalse: negative values in indices indicate positional indices from the right\n(the default). This is similar to `numpy.take()`.\n\nTrue: negative values in indices indicate missing values. These values are set\nto fill_value. Any other other negative values raise a `ValueError`.\n\nFill value to use for NA-indices when allow_fill is True. This may be `None`,\nin which case the default NA value for the type, `self.dtype.na_value`, is\nused.\n\nFor many ExtensionArrays, there will be two representations of fill_value: a\nuser-facing \u201cboxed\u201d scalar, and a low-level physical NA value. fill_value\nshould be the user-facing version, and the implementation should handle\ntranslating that to the physical version for processing the take if necessary.\n\nWhen the indices are out of bounds for the array.\n\nWhen indices contains negative values other than `-1` and allow_fill is True.\n\nSee also\n\nTake elements from an array along an axis.\n\nTake elements from an array.\n\nNotes\n\nExtensionArray.take is called by `Series.__getitem__`, `.loc`, `iloc`, when\nindices is a sequence of values. Additionally, it\u2019s called by\n`Series.reindex()`, or any other method that causes realignment, with a\nfill_value.\n\nExamples\n\nHere\u2019s an example implementation, which relies on casting the extension array\nto object dtype. This uses the helper method `pandas.api.extensions.take()`.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.tolist", "path": "reference/api/pandas.api.extensions.extensionarray.tolist", "type": "Extensions", "text": "\nReturn a list of the values.\n\nThese are each a scalar type, which is a Python scalar (for str, int, float)\nor a pandas scalar (for Timestamp/Timedelta/Interval/Period)\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.unique", "path": "reference/api/pandas.api.extensions.extensionarray.unique", "type": "Extensions", "text": "\nCompute the ExtensionArray of unique values.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.view", "path": "reference/api/pandas.api.extensions.extensionarray.view", "type": "Extensions", "text": "\nReturn a view on the array.\n\nDefault None.\n\nA view on the `ExtensionArray`\u2019s data.\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype", "path": "reference/api/pandas.api.extensions.extensiondtype", "type": "Extensions", "text": "\nA custom data type, to be paired with an ExtensionArray.\n\nSee also\n\nRegister an ExtensionType with pandas as class decorator.\n\nAbstract base class for custom 1-D array types.\n\nNotes\n\nThe interface includes the following abstract methods that must be implemented\nby subclasses:\n\ntype\n\nname\n\nconstruct_array_type\n\nThe following attributes and methods influence the behavior of the dtype in\npandas operations\n\n_is_numeric\n\n_is_boolean\n\n_get_common_dtype\n\nThe na_value class attribute can be used to set the default NA value for this\ntype. `numpy.nan` is used by default.\n\nExtensionDtypes are required to be hashable. The base class provides a default\nimplementation, which relies on the `_metadata` class attribute. `_metadata`\nshould be a tuple containing the strings that define your data type. For\nexample, with `PeriodDtype` that\u2019s the `freq` attribute.\n\nIf you have a parametrized dtype you should set the ``_metadata`` class\nproperty.\n\nIdeally, the attributes in `_metadata` will match the parameters to your\n`ExtensionDtype.__init__` (if any). If any of the attributes in `_metadata`\ndon\u2019t implement the standard `__eq__` or `__hash__`, the default\nimplementations here will not work.\n\nFor interaction with Apache Arrow (pyarrow), a `__from_arrow__` method can be\nimplemented: this method receives a pyarrow Array or ChunkedArray as only\nargument and is expected to return the appropriate pandas ExtensionArray for\nthis dtype and the passed values:\n\nThis class does not inherit from \u2018abc.ABCMeta\u2019 for performance reasons.\nMethods and properties required by the interface raise\n`pandas.errors.AbstractMethodError` and no `register` method is provided for\nregistering virtual subclasses.\n\nAttributes\n\n`kind`\n\nA character code (one of 'biufcmMOSUV'), default 'O'\n\n`na_value`\n\nDefault NA value to use for this type.\n\n`name`\n\nA string identifying the data type.\n\n`names`\n\nOrdered list of field names, or None if there are no fields.\n\n`type`\n\nThe scalar type for the array, e.g.\n\nMethods\n\n`construct_array_type`()\n\nReturn the array type associated with this dtype.\n\n`construct_from_string`(string)\n\nConstruct this type from a string.\n\n`empty`(shape)\n\nConstruct an ExtensionArray of this dtype with the given shape.\n\n`is_dtype`(dtype)\n\nCheck if we match 'dtype'.\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.construct_array_type", "path": "reference/api/pandas.api.extensions.extensiondtype.construct_array_type", "type": "Extensions", "text": "\nReturn the array type associated with this dtype.\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.construct_from_string", "path": "reference/api/pandas.api.extensions.extensiondtype.construct_from_string", "type": "Extensions", "text": "\nConstruct this type from a string.\n\nThis is useful mainly for data types that accept parameters. For example, a\nperiod dtype accepts a frequency parameter that can be set as `period[H]`\n(where H means hourly frequency).\n\nBy default, in the abstract class, just the name of the type is expected. But\nsubclasses can overwrite this method to accept parameters.\n\nThe name of the type, for example `category`.\n\nInstance of the dtype.\n\nIf a class cannot be constructed from this \u2018string\u2019.\n\nExamples\n\nFor extension dtypes with arguments the following may be an adequate\nimplementation.\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.empty", "path": "reference/api/pandas.api.extensions.extensiondtype.empty", "type": "Extensions", "text": "\nConstruct an ExtensionArray of this dtype with the given shape.\n\nAnalogous to numpy.empty.\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.is_dtype", "path": "reference/api/pandas.api.extensions.extensiondtype.is_dtype", "type": "Extensions", "text": "\nCheck if we match \u2018dtype\u2019.\n\nThe object to check.\n\nNotes\n\nThe default implementation is True if\n\n`cls.construct_from_string(dtype)` is an instance of `cls`.\n\n`dtype` is an object and is an instance of `cls`\n\n`dtype` has a `dtype` attribute, and any of the above conditions is true for\n`dtype.dtype`.\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.kind", "path": "reference/api/pandas.api.extensions.extensiondtype.kind", "type": "Extensions", "text": "\nA character code (one of \u2018biufcmMOSUV\u2019), default \u2018O\u2019\n\nThis should match the NumPy dtype used when the array is converted to an\nndarray, which is probably \u2018O\u2019 for object if the extension type cannot be\nrepresented as a built-in NumPy type.\n\nSee also\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.na_value", "path": "reference/api/pandas.api.extensions.extensiondtype.na_value", "type": "Extensions", "text": "\nDefault NA value to use for this type.\n\nThis is used in e.g. ExtensionArray.take. This should be the user-facing\n\u201cboxed\u201d version of the NA value, not the physical NA value for storage. e.g.\nfor JSONArray, this is an empty dictionary.\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.name", "path": "reference/api/pandas.api.extensions.extensiondtype.name", "type": "Extensions", "text": "\nA string identifying the data type.\n\nWill be used for display in, e.g. `Series.dtype`\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.names", "path": "reference/api/pandas.api.extensions.extensiondtype.names", "type": "Extensions", "text": "\nOrdered list of field names, or None if there are no fields.\n\nThis is for compatibility with NumPy arrays, and may be removed in the future.\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.type", "path": "reference/api/pandas.api.extensions.extensiondtype.type", "type": "Extensions", "text": "\nThe scalar type for the array, e.g. `int`\n\nIt\u2019s expected `ExtensionArray[item]` returns an instance of\n`ExtensionDtype.type` for scalar `item`, assuming that value is valid (not\nNA). NA values do not need to be instances of type.\n\n"}, {"name": "pandas.api.extensions.register_dataframe_accessor", "path": "reference/api/pandas.api.extensions.register_dataframe_accessor", "type": "Extensions", "text": "\nRegister a custom accessor on DataFrame objects.\n\nName under which the accessor should be registered. A warning is issued if\nthis name conflicts with a preexisting attribute.\n\nA class decorator.\n\nSee also\n\nRegister a custom accessor on DataFrame objects.\n\nRegister a custom accessor on Series objects.\n\nRegister a custom accessor on Index objects.\n\nNotes\n\nWhen accessed, your accessor will be initialized with the pandas object the\nuser is interacting with. So the signature must be\n\nFor consistency with pandas methods, you should raise an `AttributeError` if\nthe data passed to your accessor has an incorrect dtype.\n\nExamples\n\nIn your library code:\n\nBack in an interactive IPython session:\n\n"}, {"name": "pandas.api.extensions.register_extension_dtype", "path": "reference/api/pandas.api.extensions.register_extension_dtype", "type": "Extensions", "text": "\nRegister an ExtensionType with pandas as class decorator.\n\nThis enables operations like `.astype(name)` for the name of the\nExtensionDtype.\n\nA class decorator.\n\nExamples\n\n"}, {"name": "pandas.api.extensions.register_index_accessor", "path": "reference/api/pandas.api.extensions.register_index_accessor", "type": "Extensions", "text": "\nRegister a custom accessor on Index objects.\n\nName under which the accessor should be registered. A warning is issued if\nthis name conflicts with a preexisting attribute.\n\nA class decorator.\n\nSee also\n\nRegister a custom accessor on DataFrame objects.\n\nRegister a custom accessor on Series objects.\n\nRegister a custom accessor on Index objects.\n\nNotes\n\nWhen accessed, your accessor will be initialized with the pandas object the\nuser is interacting with. So the signature must be\n\nFor consistency with pandas methods, you should raise an `AttributeError` if\nthe data passed to your accessor has an incorrect dtype.\n\nExamples\n\nIn your library code:\n\nBack in an interactive IPython session:\n\n"}, {"name": "pandas.api.extensions.register_series_accessor", "path": "reference/api/pandas.api.extensions.register_series_accessor", "type": "Extensions", "text": "\nRegister a custom accessor on Series objects.\n\nName under which the accessor should be registered. A warning is issued if\nthis name conflicts with a preexisting attribute.\n\nA class decorator.\n\nSee also\n\nRegister a custom accessor on DataFrame objects.\n\nRegister a custom accessor on Series objects.\n\nRegister a custom accessor on Index objects.\n\nNotes\n\nWhen accessed, your accessor will be initialized with the pandas object the\nuser is interacting with. So the signature must be\n\nFor consistency with pandas methods, you should raise an `AttributeError` if\nthe data passed to your accessor has an incorrect dtype.\n\nExamples\n\nIn your library code:\n\nBack in an interactive IPython session:\n\n"}, {"name": "pandas.api.indexers.BaseIndexer", "path": "reference/api/pandas.api.indexers.baseindexer", "type": "Window", "text": "\nBase class for window bounds calculations.\n\nMethods\n\n`get_window_bounds`([num_values, min_periods, ...])\n\nComputes the bounds of a window.\n\n"}, {"name": "pandas.api.indexers.BaseIndexer.get_window_bounds", "path": "reference/api/pandas.api.indexers.baseindexer.get_window_bounds", "type": "Window", "text": "\nComputes the bounds of a window.\n\nnumber of values that will be aggregated over\n\nthe number of rows in a window\n\nmin_periods passed from the top level rolling API\n\ncenter passed from the top level rolling API\n\nclosed passed from the top level rolling API\n\nwin_type passed from the top level rolling API\n\n"}, {"name": "pandas.api.indexers.check_array_indexer", "path": "reference/api/pandas.api.indexers.check_array_indexer", "type": "Extensions", "text": "\nCheck if indexer is a valid array indexer for array.\n\nFor a boolean mask, array and indexer are checked to have the same length. The\ndtype is validated, and if it is an integer or boolean ExtensionArray, it is\nchecked if there are missing values present, and it is converted to the\nappropriate numpy array. Other dtypes will raise an error.\n\nNon-array indexers (integer, slice, Ellipsis, tuples, ..) are passed through\nas is.\n\nNew in version 1.0.0.\n\nThe array that is being indexed (only used for the length).\n\nThe array-like that\u2019s used to index. List-like input that is not yet a numpy\narray or an ExtensionArray is converted to one. Other input types are passed\nthrough as is.\n\nThe validated indexer as a numpy array that can be used to index.\n\nWhen the lengths don\u2019t match.\n\nWhen indexer cannot be converted to a numpy ndarray to index (e.g. presence of\nmissing values).\n\nSee also\n\nCheck if key is of boolean dtype.\n\nExamples\n\nWhen checking a boolean mask, a boolean ndarray is returned when the arguments\nare all valid.\n\nAn IndexError is raised when the lengths don\u2019t match.\n\nNA values in a boolean array are treated as False.\n\nA numpy boolean mask will get passed through (if the length is correct):\n\nSimilarly for integer indexers, an integer ndarray is returned when it is a\nvalid indexer, otherwise an error is (for integer indexers, a matching length\nis not required):\n\nFor non-integer/boolean dtypes, an appropriate error is raised:\n\n"}, {"name": "pandas.api.indexers.FixedForwardWindowIndexer", "path": "reference/api/pandas.api.indexers.fixedforwardwindowindexer", "type": "Window", "text": "\nCreates window boundaries for fixed-length windows that include the current\nrow.\n\nExamples\n\nMethods\n\n`get_window_bounds`([num_values, min_periods, ...])\n\nComputes the bounds of a window.\n\n"}, {"name": "pandas.api.indexers.FixedForwardWindowIndexer.get_window_bounds", "path": "reference/api/pandas.api.indexers.fixedforwardwindowindexer.get_window_bounds", "type": "Window", "text": "\nComputes the bounds of a window.\n\nnumber of values that will be aggregated over\n\nthe number of rows in a window\n\nmin_periods passed from the top level rolling API\n\ncenter passed from the top level rolling API\n\nclosed passed from the top level rolling API\n\nwin_type passed from the top level rolling API\n\n"}, {"name": "pandas.api.indexers.VariableOffsetWindowIndexer", "path": "reference/api/pandas.api.indexers.variableoffsetwindowindexer", "type": "Window", "text": "\nCalculate window boundaries based on a non-fixed offset such as a BusinessDay.\n\nMethods\n\n`get_window_bounds`([num_values, min_periods, ...])\n\nComputes the bounds of a window.\n\n"}, {"name": "pandas.api.indexers.VariableOffsetWindowIndexer.get_window_bounds", "path": "reference/api/pandas.api.indexers.variableoffsetwindowindexer.get_window_bounds", "type": "Window", "text": "\nComputes the bounds of a window.\n\nnumber of values that will be aggregated over\n\nthe number of rows in a window\n\nmin_periods passed from the top level rolling API\n\ncenter passed from the top level rolling API\n\nclosed passed from the top level rolling API\n\nwin_type passed from the top level rolling API\n\n"}, {"name": "pandas.api.types.infer_dtype", "path": "reference/api/pandas.api.types.infer_dtype", "type": "General utility functions", "text": "\nEfficiently infer the type of a passed val, or list-like array of values.\nReturn a string describing the type.\n\nIgnore NaN values when inferring the type.\n\nDescribing the common type of the input data.\n\nIf ndarray-like but cannot infer the dtype\n\nNotes\n\n\u2018mixed\u2019 is the catchall for anything that is not otherwise specialized\n\n\u2018mixed-integer-float\u2019 are floats and integers\n\n\u2018mixed-integer\u2019 are integers mixed with non-integers\n\n\u2018unknown-array\u2019 is the catchall for something that is an array (has a dtype\nattribute), but has a dtype unknown to pandas (e.g. external extension array)\n\nExamples\n\n"}, {"name": "pandas.api.types.is_bool", "path": "reference/api/pandas.api.types.is_bool", "type": "General utility functions", "text": "\nReturn True if given object is boolean.\n\n"}, {"name": "pandas.api.types.is_bool_dtype", "path": "reference/api/pandas.api.types.is_bool_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of a boolean dtype.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of a boolean dtype.\n\nNotes\n\nAn ExtensionArray is considered boolean when the `_is_boolean` attribute is\nset to True.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_categorical", "path": "reference/api/pandas.api.types.is_categorical", "type": "General utility functions", "text": "\nCheck whether an array-like is a Categorical instance.\n\nThe array-like to check.\n\nWhether or not the array-like is of a Categorical instance.\n\nExamples\n\nCategoricals, Series Categoricals, and CategoricalIndex will return True.\n\n"}, {"name": "pandas.api.types.is_categorical_dtype", "path": "reference/api/pandas.api.types.is_categorical_dtype", "type": "General utility functions", "text": "\nCheck whether an array-like or dtype is of the Categorical dtype.\n\nThe array-like or dtype to check.\n\nWhether or not the array-like or dtype is of the Categorical dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_complex", "path": "reference/api/pandas.api.types.is_complex", "type": "General utility functions", "text": "\nReturn True if given object is complex.\n\n"}, {"name": "pandas.api.types.is_complex_dtype", "path": "reference/api/pandas.api.types.is_complex_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of a complex dtype.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of a complex dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_datetime64_any_dtype", "path": "reference/api/pandas.api.types.is_datetime64_any_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of the datetime64 dtype.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of the datetime64 dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_datetime64_dtype", "path": "reference/api/pandas.api.types.is_datetime64_dtype", "type": "General utility functions", "text": "\nCheck whether an array-like or dtype is of the datetime64 dtype.\n\nThe array-like or dtype to check.\n\nWhether or not the array-like or dtype is of the datetime64 dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_datetime64_ns_dtype", "path": "reference/api/pandas.api.types.is_datetime64_ns_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of the datetime64[ns] dtype.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of the datetime64[ns] dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_datetime64tz_dtype", "path": "reference/api/pandas.api.types.is_datetime64tz_dtype", "type": "General utility functions", "text": "\nCheck whether an array-like or dtype is of a DatetimeTZDtype dtype.\n\nThe array-like or dtype to check.\n\nWhether or not the array-like or dtype is of a DatetimeTZDtype dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_dict_like", "path": "reference/api/pandas.api.types.is_dict_like", "type": "General utility functions", "text": "\nCheck if the object is dict-like.\n\nWhether obj has dict-like properties.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_extension_array_dtype", "path": "reference/api/pandas.api.types.is_extension_array_dtype", "type": "General utility functions", "text": "\nCheck if an object is a pandas extension array type.\n\nSee the Use Guide for more.\n\nFor array-like input, the `.dtype` attribute will be extracted.\n\nWhether the arr_or_dtype is an extension array type.\n\nNotes\n\nThis checks whether an object implements the pandas extension array interface.\nIn pandas, this includes:\n\nCategorical\n\nSparse\n\nInterval\n\nPeriod\n\nDatetimeArray\n\nTimedeltaArray\n\nThird-party libraries may implement arrays or types satisfying this interface\nas well.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_extension_type", "path": "reference/api/pandas.api.types.is_extension_type", "type": "General utility functions", "text": "\nCheck whether an array-like is of a pandas extension class instance.\n\nDeprecated since version 1.0.0: Use `is_extension_array_dtype` instead.\n\nExtension classes include categoricals, pandas sparse objects (i.e. classes\nrepresented within the pandas library and not ones external to it like scipy\nsparse matrices), and datetime-like arrays.\n\nThe array-like to check.\n\nWhether or not the array-like is of a pandas extension class instance.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_file_like", "path": "reference/api/pandas.api.types.is_file_like", "type": "General utility functions", "text": "\nCheck if the object is a file-like object.\n\nFor objects to be considered file-like, they must be an iterator AND have\neither a read and/or write method as an attribute.\n\nNote: file-like objects must be iterable, but iterable objects need not be\nfile-like.\n\nWhether obj has file-like properties.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_float", "path": "reference/api/pandas.api.types.is_float", "type": "General utility functions", "text": "\nReturn True if given object is float.\n\n"}, {"name": "pandas.api.types.is_float_dtype", "path": "reference/api/pandas.api.types.is_float_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of a float dtype.\n\nThis function is internal and should not be exposed in the public API.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of a float dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_hashable", "path": "reference/api/pandas.api.types.is_hashable", "type": "General utility functions", "text": "\nReturn True if hash(obj) will succeed, False otherwise.\n\nSome types will pass a test against collections.abc.Hashable but fail when\nthey are actually hashed with hash().\n\nDistinguish between these and other types by trying the call to hash() and\nseeing if they raise TypeError.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_int64_dtype", "path": "reference/api/pandas.api.types.is_int64_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of the int64 dtype.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of the int64 dtype.\n\nNotes\n\nDepending on system architecture, the return value of is_int64_dtype( int)\nwill be True if the OS uses 64-bit integers and False if the OS uses 32-bit\nintegers.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_integer", "path": "reference/api/pandas.api.types.is_integer", "type": "General utility functions", "text": "\nReturn True if given object is integer.\n\n"}, {"name": "pandas.api.types.is_integer_dtype", "path": "reference/api/pandas.api.types.is_integer_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of an integer dtype.\n\nUnlike in is_any_int_dtype, timedelta64 instances will return False.\n\nThe nullable Integer dtypes (e.g. pandas.Int64Dtype) are also considered as\ninteger by this function.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of an integer dtype and not an instance\nof timedelta64.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_interval", "path": "reference/api/pandas.api.types.is_interval", "type": "General utility functions", "text": "\n\n"}, {"name": "pandas.api.types.is_interval_dtype", "path": "reference/api/pandas.api.types.is_interval_dtype", "type": "General utility functions", "text": "\nCheck whether an array-like or dtype is of the Interval dtype.\n\nThe array-like or dtype to check.\n\nWhether or not the array-like or dtype is of the Interval dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_iterator", "path": "reference/api/pandas.api.types.is_iterator", "type": "General utility functions", "text": "\nCheck if the object is an iterator.\n\nThis is intended for generators, not list-like objects.\n\nWhether obj is an iterator.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_list_like", "path": "reference/api/pandas.api.types.is_list_like", "type": "General utility functions", "text": "\nCheck if the object is list-like.\n\nObjects that are considered list-like are for example Python lists, tuples,\nsets, NumPy arrays, and Pandas Series.\n\nStrings and datetime objects, however, are not considered list-like.\n\nObject to check.\n\nIf this parameter is False, sets will not be considered list-like.\n\nWhether obj has list-like properties.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_named_tuple", "path": "reference/api/pandas.api.types.is_named_tuple", "type": "General utility functions", "text": "\nCheck if the object is a named tuple.\n\nWhether obj is a named tuple.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_number", "path": "reference/api/pandas.api.types.is_number", "type": "General utility functions", "text": "\nCheck if the object is a number.\n\nReturns True when the object is a number, and False if is not.\n\nThe object to check if is a number.\n\nWhether obj is a number or not.\n\nSee also\n\nChecks a subgroup of numbers.\n\nExamples\n\nBooleans are valid because they are int subclass.\n\n"}, {"name": "pandas.api.types.is_numeric_dtype", "path": "reference/api/pandas.api.types.is_numeric_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of a numeric dtype.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of a numeric dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_object_dtype", "path": "reference/api/pandas.api.types.is_object_dtype", "type": "General utility functions", "text": "\nCheck whether an array-like or dtype is of the object dtype.\n\nThe array-like or dtype to check.\n\nWhether or not the array-like or dtype is of the object dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_period_dtype", "path": "reference/api/pandas.api.types.is_period_dtype", "type": "General utility functions", "text": "\nCheck whether an array-like or dtype is of the Period dtype.\n\nThe array-like or dtype to check.\n\nWhether or not the array-like or dtype is of the Period dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_re", "path": "reference/api/pandas.api.types.is_re", "type": "General utility functions", "text": "\nCheck if the object is a regex pattern instance.\n\nWhether obj is a regex pattern.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_re_compilable", "path": "reference/api/pandas.api.types.is_re_compilable", "type": "General utility functions", "text": "\nCheck if the object can be compiled into a regex pattern instance.\n\nWhether obj can be compiled as a regex pattern.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_scalar", "path": "reference/api/pandas.api.types.is_scalar", "type": "General utility functions", "text": "\nReturn True if given object is scalar.\n\nThis includes:\n\nnumpy array scalar (e.g. np.int64)\n\nPython builtin numerics\n\nPython builtin byte arrays and strings\n\nNone\n\ndatetime.datetime\n\ndatetime.timedelta\n\nPeriod\n\ndecimal.Decimal\n\nInterval\n\nDateOffset\n\nFraction\n\nNumber.\n\nReturn True if given object is scalar.\n\nExamples\n\npandas supports PEP 3141 numbers:\n\n"}, {"name": "pandas.api.types.is_signed_integer_dtype", "path": "reference/api/pandas.api.types.is_signed_integer_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of a signed integer dtype.\n\nUnlike in is_any_int_dtype, timedelta64 instances will return False.\n\nThe nullable Integer dtypes (e.g. pandas.Int64Dtype) are also considered as\ninteger by this function.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of a signed integer dtype and not an\ninstance of timedelta64.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_sparse", "path": "reference/api/pandas.api.types.is_sparse", "type": "General utility functions", "text": "\nCheck whether an array-like is a 1-D pandas sparse array.\n\nCheck that the one-dimensional array-like is a pandas sparse array. Returns\nTrue if it is a pandas sparse array, not another type of sparse array.\n\nArray-like to check.\n\nWhether or not the array-like is a pandas sparse array.\n\nExamples\n\nReturns True if the parameter is a 1-D pandas sparse array.\n\nReturns False if the parameter is not sparse.\n\nReturns False if the parameter is not a pandas sparse array.\n\nReturns False if the parameter has more than one dimension.\n\n"}, {"name": "pandas.api.types.is_string_dtype", "path": "reference/api/pandas.api.types.is_string_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of the string dtype.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of the string dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_timedelta64_dtype", "path": "reference/api/pandas.api.types.is_timedelta64_dtype", "type": "General utility functions", "text": "\nCheck whether an array-like or dtype is of the timedelta64 dtype.\n\nThe array-like or dtype to check.\n\nWhether or not the array-like or dtype is of the timedelta64 dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_timedelta64_ns_dtype", "path": "reference/api/pandas.api.types.is_timedelta64_ns_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of the timedelta64[ns] dtype.\n\nThis is a very specific dtype, so generic ones like np.timedelta64 will return\nFalse if passed into this function.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of the timedelta64[ns] dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_unsigned_integer_dtype", "path": "reference/api/pandas.api.types.is_unsigned_integer_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of an unsigned integer dtype.\n\nThe nullable Integer dtypes (e.g. pandas.UInt64Dtype) are also considered as\ninteger by this function.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of an unsigned integer dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.pandas_dtype", "path": "reference/api/pandas.api.types.pandas_dtype", "type": "General utility functions", "text": "\nConvert input into a pandas only dtype object or a numpy dtype object.\n\n"}, {"name": "pandas.api.types.union_categoricals", "path": "reference/api/pandas.api.types.union_categoricals", "type": "General utility functions", "text": "\nCombine list-like of Categorical-like, unioning categories.\n\nAll categories must have the same dtype.\n\nCategorical, CategoricalIndex, or Series with dtype=\u2019category\u2019.\n\nIf true, resulting categories will be lexsorted, otherwise they will be\nordered as they appear in the data.\n\nIf true, the ordered attribute of the Categoricals will be ignored. Results in\nan unordered categorical.\n\nall inputs do not have the same dtype\n\nall inputs do not have the same ordered property\n\nall inputs are ordered and their categories are not identical\n\nsort_categories=True and Categoricals are ordered\n\nEmpty list of categoricals passed\n\nNotes\n\nTo learn more about categories, see link\n\nExamples\n\nIf you want to combine categoricals that do not necessarily have the same\ncategories, union_categoricals will combine a list-like of categoricals. The\nnew categories will be the union of the categories being combined.\n\nBy default, the resulting categories will be ordered as they appear in the\ncategories of the data. If you want the categories to be lexsorted, use\nsort_categories=True argument.\n\nunion_categoricals also works with the case of combining two categoricals of\nthe same categories and order information (e.g. what you could also append\nfor).\n\nRaises TypeError because the categories are ordered and not identical.\n\nNew in version 0.20.0\n\nOrdered categoricals with different categories or orderings can be combined by\nusing the ignore_ordered=True argument.\n\nunion_categoricals also works with a CategoricalIndex, or Series containing\ncategorical data, but note that the resulting array will always be a plain\nCategorical\n\n"}, {"name": "pandas.array", "path": "reference/api/pandas.array", "type": "Pandas arrays", "text": "\nCreate an array.\n\nThe scalars inside data should be instances of the scalar type for dtype. It\u2019s\nexpected that data represents a 1-dimensional array of data.\n\nWhen data is an Index or Series, the underlying array will be extracted from\ndata.\n\nThe dtype to use for the array. This may be a NumPy dtype or an extension type\nregistered with pandas using\n`pandas.api.extensions.register_extension_dtype()`.\n\nIf not specified, there are two possibilities:\n\nWhen data is a `Series`, `Index`, or `ExtensionArray`, the dtype will be taken\nfrom the data.\n\nOtherwise, pandas will attempt to infer the dtype from the data.\n\nNote that when data is a NumPy array, `data.dtype` is not used for inferring\nthe array type. This is because NumPy cannot represent all the types of data\nthat can be held in extension arrays.\n\nCurrently, pandas will infer an extension dtype for sequences of\n\nScalar Type\n\nArray Type\n\n`pandas.Interval`\n\n`pandas.arrays.IntervalArray`\n\n`pandas.Period`\n\n`pandas.arrays.PeriodArray`\n\n`datetime.datetime`\n\n`pandas.arrays.DatetimeArray`\n\n`datetime.timedelta`\n\n`pandas.arrays.TimedeltaArray`\n\n`int`\n\n`pandas.arrays.IntegerArray`\n\n`float`\n\n`pandas.arrays.FloatingArray`\n\n`str`\n\n`pandas.arrays.StringArray` or `pandas.arrays.ArrowStringArray`\n\n`bool`\n\n`pandas.arrays.BooleanArray`\n\nThe ExtensionArray created when the scalar type is `str` is determined by\n`pd.options.mode.string_storage` if the dtype is not explicitly given.\n\nFor all other cases, NumPy\u2019s usual inference rules will be used.\n\nChanged in version 1.0.0: Pandas infers nullable-integer dtype for integer\ndata, string dtype for string data, and nullable-boolean dtype for boolean\ndata.\n\nChanged in version 1.2.0: Pandas now also infers nullable-floating dtype for\nfloat-like input data\n\nWhether to copy the data, even if not necessary. Depending on the type of\ndata, creating the new array may require copying data, even if `copy=False`.\n\nThe newly created array.\n\nWhen data is not 1-dimensional.\n\nSee also\n\nConstruct a NumPy array.\n\nConstruct a pandas Series.\n\nConstruct a pandas Index.\n\nExtensionArray wrapping a NumPy array.\n\nExtract the array stored within a Series.\n\nNotes\n\nOmitting the dtype argument means pandas will attempt to infer the best array\ntype from the values in the data. As new array types are added by pandas and\n3rd party libraries, the \u201cbest\u201d array type may change. We recommend specifying\ndtype to ensure that\n\nthe correct array type for the data is returned\n\nthe returned array type doesn\u2019t change as new extension types are added by\npandas and third-party libraries\n\nAdditionally, if the underlying memory representation of the returned array\nmatters, we recommend specifying the dtype as a concrete object rather than a\nstring alias or allowing it to be inferred. For example, a future version of\npandas or a 3rd-party library may include a dedicated ExtensionArray for\nstring data. In this event, the following would no longer return a\n`arrays.PandasArray` backed by a NumPy array.\n\nThis would instead return the new ExtensionArray dedicated for string data. If\nyou really need the new array to be backed by a NumPy array, specify that in\nthe dtype.\n\nFinally, Pandas has arrays that mostly overlap with NumPy\n\n`arrays.DatetimeArray`\n\n`arrays.TimedeltaArray`\n\nWhen data with a `datetime64[ns]` or `timedelta64[ns]` dtype is passed, pandas\nwill always return a `DatetimeArray` or `TimedeltaArray` rather than a\n`PandasArray`. This is for symmetry with the case of timezone-aware data,\nwhich NumPy does not natively support.\n\nExamples\n\nIf a dtype is not specified, pandas will infer the best dtype from the values.\nSee the description of dtype for the types pandas infers for.\n\nYou can use the string alias for dtype\n\nOr specify the actual dtype\n\nIf pandas does not infer a dedicated extension type a `arrays.PandasArray` is\nreturned.\n\nAs mentioned in the \u201cNotes\u201d section, new extension types may be added in the\nfuture (by pandas or 3rd party libraries), causing the return value to no\nlonger be a `arrays.PandasArray`. Specify the dtype as a NumPy dtype if you\nneed to ensure there\u2019s no future change in behavior.\n\ndata must be 1-dimensional. A ValueError is raised when the input has the\nwrong dimensionality.\n\n"}, {"name": "pandas.arrays.ArrowStringArray", "path": "reference/api/pandas.arrays.arrowstringarray", "type": "Pandas arrays", "text": "\nExtension array for string data in a `pyarrow.ChunkedArray`.\n\nNew in version 1.2.0.\n\nWarning\n\nArrowStringArray is considered experimental. The implementation and parts of\nthe API may change without warning.\n\nThe array of data.\n\nSee also\n\nThe recommended function for creating a ArrowStringArray.\n\nThe string methods are available on Series backed by a ArrowStringArray.\n\nNotes\n\nArrowStringArray returns a BooleanArray for comparison methods.\n\nExamples\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.arrays.BooleanArray", "path": "reference/api/pandas.arrays.booleanarray", "type": "Pandas arrays", "text": "\nArray of boolean (True/False) data with missing values.\n\nThis is a pandas Extension array for boolean data, under the hood represented\nby 2 numpy arrays: a boolean array with the data and a boolean array with the\nmask (True indicating missing).\n\nBooleanArray implements Kleene logic (sometimes called three-value logic) for\nlogical operations. See Kleene logical operations for more.\n\nTo construct an BooleanArray from generic array-like input, use\n`pandas.array()` specifying `dtype=\"boolean\"` (see examples below).\n\nNew in version 1.0.0.\n\nWarning\n\nBooleanArray is considered experimental. The implementation and parts of the\nAPI may change without warning.\n\nA 1-d boolean-dtype array with the data.\n\nA 1-d boolean-dtype array indicating missing values (True indicates missing).\n\nWhether to copy the values and mask arrays.\n\nExamples\n\nCreate an BooleanArray with `pandas.array()`:\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.arrays.DatetimeArray", "path": "reference/api/pandas.arrays.datetimearray", "type": "Pandas arrays", "text": "\nPandas ExtensionArray for tz-naive or tz-aware datetime data.\n\nWarning\n\nDatetimeArray is currently experimental, and its API may change without\nwarning. In particular, `DatetimeArray.dtype` is expected to change to always\nbe an instance of an `ExtensionDtype` subclass.\n\nThe datetime data.\n\nFor DatetimeArray values (or a Series or Index boxing one), dtype and freq\nwill be extracted from values.\n\nNote that the only NumPy dtype allowed is \u2018datetime64[ns]\u2019.\n\nThe frequency.\n\nWhether to copy the underlying array of values.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.arrays.IntegerArray", "path": "reference/api/pandas.arrays.integerarray", "type": "Pandas arrays", "text": "\nArray of integer (optional missing) values.\n\nChanged in version 1.0.0: Now uses `pandas.NA` as the missing value rather\nthan `numpy.nan`.\n\nWarning\n\nIntegerArray is currently experimental, and its API or internal implementation\nmay change without warning.\n\nWe represent an IntegerArray with 2 numpy arrays:\n\ndata: contains a numpy integer array of the appropriate dtype\n\nmask: a boolean array holding a mask on the data, True is missing\n\nTo construct an IntegerArray from generic array-like input, use\n`pandas.array()` with one of the integer dtypes (see examples).\n\nSee Nullable integer data type for more.\n\nA 1-d integer-dtype array.\n\nA 1-d boolean-dtype array indicating missing values.\n\nWhether to copy the values and mask.\n\nExamples\n\nCreate an IntegerArray with `pandas.array()`.\n\nString aliases for the dtypes are also available. They are capitalized.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.arrays.IntervalArray", "path": "reference/api/pandas.arrays.intervalarray", "type": "Pandas arrays", "text": "\nPandas array for interval data that are closed on the same side.\n\nNew in version 0.24.0.\n\nArray-like containing Interval objects from which to build the IntervalArray.\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\nIf None, dtype will be inferred.\n\nCopy the input data.\n\nVerify that the IntervalArray is valid.\n\nSee also\n\nThe base pandas Index type.\n\nA bounded slice-like interval; the elements of an IntervalArray.\n\nFunction to create a fixed frequency IntervalIndex.\n\nBin values into discrete Intervals.\n\nBin values into equal-sized Intervals based on rank or sample quantiles.\n\nNotes\n\nSee the user guide for more.\n\nExamples\n\nA new `IntervalArray` can be constructed directly from an array-like of\n`Interval` objects:\n\nIt may also be constructed using one of the constructor methods:\n`IntervalArray.from_arrays()`, `IntervalArray.from_breaks()`, and\n`IntervalArray.from_tuples()`.\n\nAttributes\n\n`left`\n\nReturn the left endpoints of each Interval in the IntervalArray as an Index.\n\n`right`\n\nReturn the right endpoints of each Interval in the IntervalArray as an Index.\n\n`closed`\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\n`mid`\n\nReturn the midpoint of each Interval in the IntervalArray as an Index.\n\n`length`\n\nReturn an Index with entries denoting the length of each Interval in the\nIntervalArray.\n\n`is_empty`\n\nIndicates if an interval is empty, meaning it contains no points.\n\n`is_non_overlapping_monotonic`\n\nReturn True if the IntervalArray is non-overlapping (no Intervals share\npoints) and is either monotonic increasing or monotonic decreasing, else\nFalse.\n\nMethods\n\n`from_arrays`(left, right[, closed, copy, dtype])\n\nConstruct from two arrays defining the left and right bounds.\n\n`from_tuples`(data[, closed, copy, dtype])\n\nConstruct an IntervalArray from an array-like of tuples.\n\n`from_breaks`(breaks[, closed, copy, dtype])\n\nConstruct an IntervalArray from an array of splits.\n\n`contains`(other)\n\nCheck elementwise if the Intervals contain the value.\n\n`overlaps`(other)\n\nCheck elementwise if an Interval overlaps the values in the IntervalArray.\n\n`set_closed`(closed)\n\nReturn an IntervalArray identical to the current one, but closed on the\nspecified side.\n\n`to_tuples`([na_tuple])\n\nReturn an ndarray of tuples of the form (left, right).\n\n"}, {"name": "pandas.arrays.IntervalArray.closed", "path": "reference/api/pandas.arrays.intervalarray.closed", "type": "Pandas arrays", "text": "\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\n"}, {"name": "pandas.arrays.IntervalArray.contains", "path": "reference/api/pandas.arrays.intervalarray.contains", "type": "Pandas arrays", "text": "\nCheck elementwise if the Intervals contain the value.\n\nReturn a boolean mask whether the value is contained in the Intervals of the\nIntervalArray.\n\nNew in version 0.25.0.\n\nThe value to check whether it is contained in the Intervals.\n\nSee also\n\nCheck whether Interval object contains value.\n\nCheck if an Interval overlaps the values in the IntervalArray.\n\nExamples\n\n"}, {"name": "pandas.arrays.IntervalArray.from_arrays", "path": "reference/api/pandas.arrays.intervalarray.from_arrays", "type": "Pandas arrays", "text": "\nConstruct from two arrays defining the left and right bounds.\n\nLeft bounds for each interval.\n\nRight bounds for each interval.\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\nCopy the data.\n\nIf None, dtype will be inferred.\n\nWhen a value is missing in only one of left or right. When a value in left is\ngreater than the corresponding value in right.\n\nSee also\n\nFunction to create a fixed frequency IntervalIndex.\n\nConstruct an IntervalArray from an array of splits.\n\nConstruct an IntervalArray from an array-like of tuples.\n\nNotes\n\nEach element of left must be less than or equal to the right element at the\nsame position. If an element is missing, it must be missing in both left and\nright. A TypeError is raised when using an unsupported type for left or right.\nAt the moment, \u2018category\u2019, \u2018object\u2019, and \u2018string\u2019 subtypes are not supported.\n\n"}, {"name": "pandas.arrays.IntervalArray.from_breaks", "path": "reference/api/pandas.arrays.intervalarray.from_breaks", "type": "Pandas arrays", "text": "\nConstruct an IntervalArray from an array of splits.\n\nLeft and right bounds for each interval.\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\nCopy the data.\n\nIf None, dtype will be inferred.\n\nSee also\n\nFunction to create a fixed frequency IntervalIndex.\n\nConstruct from a left and right array.\n\nConstruct from a sequence of tuples.\n\nExamples\n\n"}, {"name": "pandas.arrays.IntervalArray.from_tuples", "path": "reference/api/pandas.arrays.intervalarray.from_tuples", "type": "Pandas arrays", "text": "\nConstruct an IntervalArray from an array-like of tuples.\n\nArray of tuples.\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\nBy-default copy the data, this is compat only and ignored.\n\nIf None, dtype will be inferred.\n\nSee also\n\nFunction to create a fixed frequency IntervalIndex.\n\nConstruct an IntervalArray from a left and right array.\n\nConstruct an IntervalArray from an array of splits.\n\nExamples\n\n"}, {"name": "pandas.arrays.IntervalArray.is_empty", "path": "reference/api/pandas.arrays.intervalarray.is_empty", "type": "Pandas arrays", "text": "\nIndicates if an interval is empty, meaning it contains no points.\n\nNew in version 0.25.0.\n\nA boolean indicating if a scalar `Interval` is empty, or a boolean `ndarray`\npositionally indicating if an `Interval` in an `IntervalArray` or\n`IntervalIndex` is empty.\n\nExamples\n\nAn `Interval` that contains points is not empty:\n\nAn `Interval` that does not contain any points is empty:\n\nAn `Interval` that contains a single point is not empty:\n\nAn `IntervalArray` or `IntervalIndex` returns a boolean `ndarray` positionally\nindicating if an `Interval` is empty:\n\nMissing values are not considered empty:\n\n"}, {"name": "pandas.arrays.IntervalArray.is_non_overlapping_monotonic", "path": "reference/api/pandas.arrays.intervalarray.is_non_overlapping_monotonic", "type": "Pandas arrays", "text": "\nReturn True if the IntervalArray is non-overlapping (no Intervals share\npoints) and is either monotonic increasing or monotonic decreasing, else\nFalse.\n\n"}, {"name": "pandas.arrays.IntervalArray.left", "path": "reference/api/pandas.arrays.intervalarray.left", "type": "Pandas arrays", "text": "\nReturn the left endpoints of each Interval in the IntervalArray as an Index.\n\n"}, {"name": "pandas.arrays.IntervalArray.length", "path": "reference/api/pandas.arrays.intervalarray.length", "type": "Pandas arrays", "text": "\nReturn an Index with entries denoting the length of each Interval in the\nIntervalArray.\n\n"}, {"name": "pandas.arrays.IntervalArray.mid", "path": "reference/api/pandas.arrays.intervalarray.mid", "type": "Pandas arrays", "text": "\nReturn the midpoint of each Interval in the IntervalArray as an Index.\n\n"}, {"name": "pandas.arrays.IntervalArray.overlaps", "path": "reference/api/pandas.arrays.intervalarray.overlaps", "type": "Pandas arrays", "text": "\nCheck elementwise if an Interval overlaps the values in the IntervalArray.\n\nTwo intervals overlap if they share a common point, including closed\nendpoints. Intervals that only have an open endpoint in common do not overlap.\n\nInterval to check against for an overlap.\n\nBoolean array positionally indicating where an overlap occurs.\n\nSee also\n\nCheck whether two Interval objects overlap.\n\nExamples\n\nIntervals that share closed endpoints overlap:\n\nIntervals that only have an open endpoint in common do not overlap:\n\n"}, {"name": "pandas.arrays.IntervalArray.right", "path": "reference/api/pandas.arrays.intervalarray.right", "type": "Pandas arrays", "text": "\nReturn the right endpoints of each Interval in the IntervalArray as an Index.\n\n"}, {"name": "pandas.arrays.IntervalArray.set_closed", "path": "reference/api/pandas.arrays.intervalarray.set_closed", "type": "Pandas arrays", "text": "\nReturn an IntervalArray identical to the current one, but closed on the\nspecified side.\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\nExamples\n\n"}, {"name": "pandas.arrays.IntervalArray.to_tuples", "path": "reference/api/pandas.arrays.intervalarray.to_tuples", "type": "Pandas arrays", "text": "\nReturn an ndarray of tuples of the form (left, right).\n\nReturns NA as a tuple if True, `(nan, nan)`, or just as the NA value itself if\nFalse, `nan`.\n\n"}, {"name": "pandas.arrays.PandasArray", "path": "reference/api/pandas.arrays.pandasarray", "type": "Pandas arrays", "text": "\nA pandas ExtensionArray for NumPy data.\n\nThis is mostly for internal compatibility, and is not especially useful on its\nown.\n\nThe NumPy ndarray to wrap. Must be 1-dimensional.\n\nWhether to copy values.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.arrays.PeriodArray", "path": "reference/api/pandas.arrays.periodarray", "type": "Input/output", "text": "\nPandas ExtensionArray for storing Period data.\n\nUsers should use `period_array()` to create new instances. Alternatively,\n`array()` can be used to create new instances from a sequence of Period\nscalars.\n\nThe data to store. These should be arrays that can be directly converted to\nordinals without inference or copy (PeriodArray, ndarray[int64]), or a box\naround such an array (Series[period], PeriodIndex).\n\nA PeriodDtype instance from which to extract a freq. If both freq and dtype\nare specified, then the frequencies must match.\n\nThe freq to use for the array. Mostly applicable when values is an ndarray of\nintegers, when freq is required. When values is a PeriodArray (or box around),\nit\u2019s checked that `values.freq` matches freq.\n\nWhether to copy the ordinals before storing.\n\nSee also\n\nRepresents a period of time.\n\nImmutable Index for period data.\n\nCreate a fixed-frequency PeriodArray.\n\nConstruct a pandas array.\n\nNotes\n\nThere are two components to a PeriodArray\n\nordinals : integer ndarray\n\nfreq : pd.tseries.offsets.Offset\n\nThe values are physically stored as a 1-D ndarray of integers. These are\ncalled \u201cordinals\u201d and represent some kind of offset from a base.\n\nThe freq indicates the span covered by each element of the array. All elements\nin the PeriodArray have the same freq.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.arrays.SparseArray", "path": "reference/api/pandas.arrays.sparsearray", "type": "Pandas arrays", "text": "\nAn ExtensionArray for storing sparse data.\n\nA dense array of values to store in the SparseArray. This may contain\nfill_value.\n\nDeprecated since version 1.4.0: Use a function like np.full to construct an\narray with the desired repeats of the scalar value instead.\n\nElements in data that are `fill_value` are not stored in the SparseArray. For\nmemory savings, this should be the most common value in data. By default,\nfill_value depends on the dtype of data:\n\ndata.dtype\n\nna_value\n\nfloat\n\n`np.nan`\n\nint\n\n`0`\n\nbool\n\nFalse\n\ndatetime64\n\n`pd.NaT`\n\ntimedelta64\n\n`pd.NaT`\n\nThe fill value is potentially specified in three ways. In order of precedence,\nthese are\n\nThe fill_value argument\n\n`dtype.fill_value` if fill_value is None and dtype is a `SparseDtype`\n\n`data.dtype.fill_value` if fill_value is None and dtype is not a `SparseDtype`\nand data is a `SparseArray`.\n\nCan be \u2018integer\u2019 or \u2018block\u2019, default is \u2018integer\u2019. The type of storage for\nsparse locations.\n\n\u2018block\u2019: Stores a block and block_length for each contiguous span of sparse\nvalues. This is best when sparse data tends to be clumped together, with large\nregions of `fill-value` values between sparse values.\n\n\u2018integer\u2019: uses an integer to store the location of each sparse value.\n\nThe dtype to use for the SparseArray. For numpy dtypes, this determines the\ndtype of `self.sp_values`. For SparseDtype, this determines `self.sp_values`\nand `self.fill_value`.\n\nWhether to explicitly copy the incoming data array.\n\nExamples\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.arrays.StringArray", "path": "reference/api/pandas.arrays.stringarray", "type": "Pandas arrays", "text": "\nExtension array for string data.\n\nNew in version 1.0.0.\n\nWarning\n\nStringArray is considered experimental. The implementation and parts of the\nAPI may change without warning.\n\nThe array of data.\n\nWarning\n\nCurrently, this expects an object-dtype ndarray where the elements are Python\nstrings or `pandas.NA`. This may change without warning in the future. Use\n`pandas.array()` with `dtype=\"string\"` for a stable way of creating a\nStringArray from any sequence.\n\nWhether to copy the array of data.\n\nSee also\n\nThe recommended function for creating a StringArray.\n\nThe string methods are available on Series backed by a StringArray.\n\nNotes\n\nStringArray returns a BooleanArray for comparison methods.\n\nExamples\n\nUnlike arrays instantiated with `dtype=\"object\"`, `StringArray` will convert\nthe values to strings.\n\nHowever, instantiating StringArrays directly with non-strings will raise an\nerror.\n\nFor comparison methods, StringArray returns a `pandas.BooleanArray`:\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.arrays.TimedeltaArray", "path": "reference/api/pandas.arrays.timedeltaarray", "type": "Pandas arrays", "text": "\nPandas ExtensionArray for timedelta data.\n\nWarning\n\nTimedeltaArray is currently experimental, and its API may change without\nwarning. In particular, `TimedeltaArray.dtype` is expected to change to be an\ninstance of an `ExtensionDtype` subclass.\n\nThe timedelta data.\n\nCurrently, only `numpy.dtype(\"timedelta64[ns]\")` is accepted.\n\nWhether to copy the underlying array of data.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.bdate_range", "path": "reference/api/pandas.bdate_range", "type": "General functions", "text": "\nReturn a fixed frequency DatetimeIndex, with business day as the default\nfrequency.\n\nLeft bound for generating dates.\n\nRight bound for generating dates.\n\nNumber of periods to generate.\n\nFrequency strings can have multiples, e.g. \u20185H\u2019.\n\nTime zone name for returning localized DatetimeIndex, for example\nAsia/Beijing.\n\nNormalize start/end dates to midnight before generating date range.\n\nName of the resulting DatetimeIndex.\n\nWeekmask of valid business days, passed to `numpy.busdaycalendar`, only used\nwhen custom frequency strings are passed. The default value None is equivalent\nto \u2018Mon Tue Wed Thu Fri\u2019.\n\nDates to exclude from the set of valid business days, passed to\n`numpy.busdaycalendar`, only used when custom frequency strings are passed.\n\nMake the interval closed with respect to the given frequency to the \u2018left\u2019,\n\u2018right\u2019, or both sides (None).\n\nDeprecated since version 1.4.0: Argument closed has been deprecated to\nstandardize boundary inputs. Use inclusive instead, to set each bound as\nclosed or open.\n\nInclude boundaries; Whether to set each bound as closed or open.\n\nNew in version 1.4.0.\n\nFor compatibility. Has no effect on the result.\n\nNotes\n\nOf the four parameters: `start`, `end`, `periods`, and `freq`, exactly three\nmust be specified. Specifying `freq` is a requirement for `bdate_range`. Use\n`date_range` if specifying `freq` is not desired.\n\nTo learn more about the frequency strings, please see this link.\n\nExamples\n\nNote how the two weekend days are skipped in the result.\n\n"}, {"name": "pandas.BooleanDtype", "path": "reference/api/pandas.booleandtype", "type": "Pandas arrays", "text": "\nExtension dtype for boolean data.\n\nNew in version 1.0.0.\n\nWarning\n\nBooleanDtype is considered experimental. The implementation and parts of the\nAPI may change without warning.\n\nExamples\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.Categorical", "path": "reference/api/pandas.categorical", "type": "Pandas arrays", "text": "\nRepresent a categorical variable in classic R / S-plus fashion.\n\nCategoricals can only take on only a limited, and usually fixed, number of\npossible values (categories). In contrast to statistical categorical\nvariables, a Categorical might have an order, but numerical operations\n(additions, divisions, \u2026) are not possible.\n\nAll values of the Categorical are either in categories or np.nan. Assigning\nvalues outside of categories will raise a ValueError. Order is defined by the\norder of the categories, not lexical order of the values.\n\nThe values of the categorical. If categories are given, values not in\ncategories will be replaced with NaN.\n\nThe unique categories for this categorical. If not given, the categories are\nassumed to be the unique values of values (sorted, if possible, otherwise in\nthe order in which they appear).\n\nWhether or not this categorical is treated as a ordered categorical. If True,\nthe resulting categorical will be ordered. An ordered categorical respects,\nwhen sorted, the order of its categories attribute (which in turn is the\ncategories argument, if provided).\n\nAn instance of `CategoricalDtype` to use for this categorical.\n\nIf the categories do not validate.\n\nIf an explicit `ordered=True` is given but no categories and the values are\nnot sortable.\n\nSee also\n\nType for categorical data.\n\nAn Index with an underlying `Categorical`.\n\nNotes\n\nSee the user guide for more.\n\nExamples\n\nMissing values are not included as a category.\n\nHowever, their presence is indicated in the codes attribute by code -1.\n\nOrdered Categoricals can be sorted according to the custom order of the\ncategories and can have a min and max value.\n\nAttributes\n\n`categories`\n\nThe categories of this categorical.\n\n`codes`\n\nThe category codes of this categorical.\n\n`ordered`\n\nWhether the categories have an ordered relationship.\n\n`dtype`\n\nThe `CategoricalDtype` for this instance.\n\nMethods\n\n`from_codes`(codes[, categories, ordered, dtype])\n\nMake a Categorical type from codes and categories or dtype.\n\n`__array__`([dtype])\n\nThe numpy array interface.\n\n"}, {"name": "pandas.Categorical.__array__", "path": "reference/api/pandas.categorical.__array__", "type": "Pandas arrays", "text": "\nThe numpy array interface.\n\nA numpy array of either the specified dtype or, if dtype==None (default), the\nsame dtype as categorical.categories.dtype.\n\n"}, {"name": "pandas.Categorical.categories", "path": "reference/api/pandas.categorical.categories", "type": "Pandas arrays", "text": "\nThe categories of this categorical.\n\nSetting assigns new values to each category (effectively a rename of each\nindividual category).\n\nThe assigned value has to be a list-like object. All items must be unique and\nthe number of items in the new categories must be the same as the number of\nitems in the old categories.\n\nAssigning to categories is a inplace operation!\n\nIf the new categories do not validate as categories or if the number of new\ncategories is unequal the number of old categories\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\n"}, {"name": "pandas.Categorical.codes", "path": "reference/api/pandas.categorical.codes", "type": "Pandas arrays", "text": "\nThe category codes of this categorical.\n\nCodes are an array of integers which are the positions of the actual values in\nthe categories array.\n\nThere is no setter, use the other categorical methods and the normal item\nsetter to change values in the categorical.\n\nA non-writable view of the codes array.\n\n"}, {"name": "pandas.Categorical.dtype", "path": "reference/api/pandas.categorical.dtype", "type": "Pandas arrays", "text": "\nThe `CategoricalDtype` for this instance.\n\n"}, {"name": "pandas.Categorical.from_codes", "path": "reference/api/pandas.categorical.from_codes", "type": "Pandas arrays", "text": "\nMake a Categorical type from codes and categories or dtype.\n\nThis constructor is useful if you already have codes and categories/dtype and\nso do not need the (computation intensive) factorization step, which is\nusually done on the constructor.\n\nIf your data does not follow this convention, please use the normal\nconstructor.\n\nAn integer array, where each integer points to a category in categories or\ndtype.categories, or else is -1 for NaN.\n\nThe categories for the categorical. Items need to be unique. If the categories\nare not given here, then they must be provided in dtype.\n\nWhether or not this categorical is treated as an ordered categorical. If not\ngiven here or in dtype, the resulting categorical will be unordered.\n\nIf `CategoricalDtype`, cannot be used together with categories or ordered.\n\nExamples\n\n"}, {"name": "pandas.Categorical.ordered", "path": "reference/api/pandas.categorical.ordered", "type": "Pandas arrays", "text": "\nWhether the categories have an ordered relationship.\n\n"}, {"name": "pandas.CategoricalDtype", "path": "reference/api/pandas.categoricaldtype", "type": "Pandas arrays", "text": "\nType for categorical data with the categories and orderedness.\n\nMust be unique, and must not contain any nulls. The categories are stored in\nan Index, and if an index is provided the dtype of that index will be used.\n\nWhether or not this categorical is treated as a ordered categorical. None can\nbe used to maintain the ordered value of existing categoricals when used in\noperations that combine categoricals, e.g. astype, and will resolve to False\nif there is no existing ordered to maintain.\n\nSee also\n\nRepresent a categorical variable in classic R / S-plus fashion.\n\nNotes\n\nThis class is useful for specifying the type of a `Categorical` independent of\nthe values. See CategoricalDtype for more.\n\nExamples\n\nAn empty CategoricalDtype with a specific dtype can be created by providing an\nempty index. As follows,\n\nAttributes\n\n`categories`\n\nAn `Index` containing the unique categories allowed.\n\n`ordered`\n\nWhether the categories have an ordered relationship.\n\nMethods\n\nNone\n\n"}, {"name": "pandas.CategoricalDtype.categories", "path": "reference/api/pandas.categoricaldtype.categories", "type": "Pandas arrays", "text": "\nAn `Index` containing the unique categories allowed.\n\n"}, {"name": "pandas.CategoricalDtype.ordered", "path": "reference/api/pandas.categoricaldtype.ordered", "type": "Pandas arrays", "text": "\nWhether the categories have an ordered relationship.\n\n"}, {"name": "pandas.CategoricalIndex", "path": "reference/api/pandas.categoricalindex", "type": "Index Objects", "text": "\nIndex based on an underlying `Categorical`.\n\nCategoricalIndex, like Categorical, can only take on a limited, and usually\nfixed, number of possible values (categories). Also, like Categorical, it\nmight have an order, but numerical operations (additions, divisions, \u2026) are\nnot possible.\n\nThe values of the categorical. If categories are given, values not in\ncategories will be replaced with NaN.\n\nThe categories for the categorical. Items need to be unique. If the categories\nare not given here (and also not in dtype), they will be inferred from the\ndata.\n\nWhether or not this categorical is treated as an ordered categorical. If not\ngiven here or in dtype, the resulting categorical will be unordered.\n\nIf `CategoricalDtype`, cannot be used together with categories or ordered.\n\nMake a copy of input ndarray.\n\nName to be stored in the index.\n\nIf the categories do not validate.\n\nIf an explicit `ordered=True` is given but no categories and the values are\nnot sortable.\n\nSee also\n\nThe base pandas Index type.\n\nA categorical array.\n\nType for categorical data.\n\nNotes\n\nSee the user guide for more.\n\nExamples\n\n`CategoricalIndex` can also be instantiated from a `Categorical`:\n\nOrdered `CategoricalIndex` can have a min and max value.\n\nAttributes\n\n`codes`\n\nThe category codes of this categorical.\n\n`categories`\n\nThe categories of this categorical.\n\n`ordered`\n\nWhether the categories have an ordered relationship.\n\nMethods\n\n`rename_categories`(*args, **kwargs)\n\nRename categories.\n\n`reorder_categories`(*args, **kwargs)\n\nReorder categories as specified in new_categories.\n\n`add_categories`(*args, **kwargs)\n\nAdd new categories.\n\n`remove_categories`(*args, **kwargs)\n\nRemove the specified categories.\n\n`remove_unused_categories`(*args, **kwargs)\n\nRemove categories which are not used.\n\n`set_categories`(*args, **kwargs)\n\nSet the categories to the specified new_categories.\n\n`as_ordered`(*args, **kwargs)\n\nSet the Categorical to be ordered.\n\n`as_unordered`(*args, **kwargs)\n\nSet the Categorical to be unordered.\n\n`map`(mapper)\n\nMap values using input an input mapping or function.\n\n"}, {"name": "pandas.CategoricalIndex.add_categories", "path": "reference/api/pandas.categoricalindex.add_categories", "type": "Index Objects", "text": "\nAdd new categories.\n\nnew_categories will be included at the last/highest place in the categories\nand will be unused directly after this call.\n\nThe new categories to be included.\n\nWhether or not to add the categories inplace or return a copy of this\ncategorical with added categories.\n\nDeprecated since version 1.3.0.\n\nCategorical with new categories added or None if `inplace=True`.\n\nIf the new categories include old categories or do not validate as categories\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\nExamples\n\n"}, {"name": "pandas.CategoricalIndex.as_ordered", "path": "reference/api/pandas.categoricalindex.as_ordered", "type": "Index Objects", "text": "\nSet the Categorical to be ordered.\n\nWhether or not to set the ordered attribute in-place or return a copy of this\ncategorical with ordered set to True.\n\nOrdered Categorical or None if `inplace=True`.\n\n"}, {"name": "pandas.CategoricalIndex.as_unordered", "path": "reference/api/pandas.categoricalindex.as_unordered", "type": "Index Objects", "text": "\nSet the Categorical to be unordered.\n\nWhether or not to set the ordered attribute in-place or return a copy of this\ncategorical with ordered set to False.\n\nUnordered Categorical or None if `inplace=True`.\n\n"}, {"name": "pandas.CategoricalIndex.categories", "path": "reference/api/pandas.categoricalindex.categories", "type": "Index Objects", "text": "\nThe categories of this categorical.\n\nSetting assigns new values to each category (effectively a rename of each\nindividual category).\n\nThe assigned value has to be a list-like object. All items must be unique and\nthe number of items in the new categories must be the same as the number of\nitems in the old categories.\n\nAssigning to categories is a inplace operation!\n\nIf the new categories do not validate as categories or if the number of new\ncategories is unequal the number of old categories\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\n"}, {"name": "pandas.CategoricalIndex.codes", "path": "reference/api/pandas.categoricalindex.codes", "type": "Index Objects", "text": "\nThe category codes of this categorical.\n\nCodes are an array of integers which are the positions of the actual values in\nthe categories array.\n\nThere is no setter, use the other categorical methods and the normal item\nsetter to change values in the categorical.\n\nA non-writable view of the codes array.\n\n"}, {"name": "pandas.CategoricalIndex.equals", "path": "reference/api/pandas.categoricalindex.equals", "type": "Index Objects", "text": "\nDetermine if two CategoricalIndex objects contain the same elements.\n\nIf two CategoricalIndex objects have equal elements True, otherwise False.\n\n"}, {"name": "pandas.CategoricalIndex.map", "path": "reference/api/pandas.categoricalindex.map", "type": "Index Objects", "text": "\nMap values using input an input mapping or function.\n\nMaps the values (their categories, not the codes) of the index to new\ncategories. If the mapping correspondence is one-to-one the result is a\n`CategoricalIndex` which has the same order property as the original,\notherwise an `Index` is returned.\n\nIf a dict or `Series` is used any unmapped category is mapped to NaN. Note\nthat if this happens an `Index` will be returned.\n\nMapping correspondence.\n\nMapped index.\n\nSee also\n\nApply a mapping correspondence on an `Index`.\n\nApply a mapping correspondence on a `Series`.\n\nApply more complex functions on a `Series`.\n\nExamples\n\nIf the mapping is one-to-one the ordering of the categories is preserved:\n\nIf the mapping is not one-to-one an `Index` is returned:\n\nIf a dict is used, all unmapped categories are mapped to NaN and the result is\nan `Index`:\n\n"}, {"name": "pandas.CategoricalIndex.ordered", "path": "reference/api/pandas.categoricalindex.ordered", "type": "Index Objects", "text": "\nWhether the categories have an ordered relationship.\n\n"}, {"name": "pandas.CategoricalIndex.remove_categories", "path": "reference/api/pandas.categoricalindex.remove_categories", "type": "Index Objects", "text": "\nRemove the specified categories.\n\nremovals must be included in the old categories. Values which were in the\nremoved categories will be set to NaN\n\nThe categories which should be removed.\n\nWhether or not to remove the categories inplace or return a copy of this\ncategorical with removed categories.\n\nDeprecated since version 1.3.0.\n\nCategorical with removed categories or None if `inplace=True`.\n\nIf the removals are not contained in the categories\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nAdd new categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\nExamples\n\n"}, {"name": "pandas.CategoricalIndex.remove_unused_categories", "path": "reference/api/pandas.categoricalindex.remove_unused_categories", "type": "Index Objects", "text": "\nRemove categories which are not used.\n\nWhether or not to drop unused categories inplace or return a copy of this\ncategorical with unused categories dropped.\n\nDeprecated since version 1.2.0.\n\nCategorical with unused categories dropped or None if `inplace=True`.\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nSet the categories to the specified ones.\n\nExamples\n\n"}, {"name": "pandas.CategoricalIndex.rename_categories", "path": "reference/api/pandas.categoricalindex.rename_categories", "type": "Index Objects", "text": "\nRename categories.\n\nNew categories which will replace old categories.\n\nlist-like: all items must be unique and the number of items in the new\ncategories must match the existing number of categories.\n\ndict-like: specifies a mapping from old categories to new. Categories not\ncontained in the mapping are passed through and extra categories in the\nmapping are ignored.\n\ncallable : a callable that is called on all items in the old categories and\nwhose return values comprise the new categories.\n\nWhether or not to rename the categories inplace or return a copy of this\ncategorical with renamed categories.\n\nDeprecated since version 1.3.0.\n\nCategorical with removed categories or None if `inplace=True`.\n\nIf new categories are list-like and do not have the same number of items than\nthe current categories or do not validate as categories\n\nSee also\n\nReorder categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\nExamples\n\nFor dict-like `new_categories`, extra keys are ignored and categories not in\nthe dictionary are passed through\n\nYou may also provide a callable to create the new categories\n\n"}, {"name": "pandas.CategoricalIndex.reorder_categories", "path": "reference/api/pandas.categoricalindex.reorder_categories", "type": "Index Objects", "text": "\nReorder categories as specified in new_categories.\n\nnew_categories need to include all old categories and no new category items.\n\nThe categories in new order.\n\nWhether or not the categorical is treated as a ordered categorical. If not\ngiven, do not change the ordered information.\n\nWhether or not to reorder the categories inplace or return a copy of this\ncategorical with reordered categories.\n\nDeprecated since version 1.3.0.\n\nCategorical with removed categories or None if `inplace=True`.\n\nIf the new categories do not contain all old category items or any new ones\n\nSee also\n\nRename categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\n"}, {"name": "pandas.CategoricalIndex.set_categories", "path": "reference/api/pandas.categoricalindex.set_categories", "type": "Index Objects", "text": "\nSet the categories to the specified new_categories.\n\nnew_categories can include new categories (which will result in unused\ncategories) or remove old categories (which results in values set to NaN). If\nrename==True, the categories will simple be renamed (less or more items than\nin old categories will result in values set to NaN or in unused categories\nrespectively).\n\nThis method can be used to perform more than one action of adding, removing,\nand reordering simultaneously and is therefore faster than performing the\nindividual steps via the more specialised methods.\n\nOn the other hand this methods does not do checks (e.g., whether the old\ncategories are included in the new categories on a reorder), which can result\nin surprising changes, for example when using special string dtypes, which\ndoes not considers a S1 string equal to a single char python string.\n\nThe categories in new order.\n\nWhether or not the categorical is treated as a ordered categorical. If not\ngiven, do not change the ordered information.\n\nWhether or not the new_categories should be considered as a rename of the old\ncategories or as reordered categories.\n\nWhether or not to reorder the categories in-place or return a copy of this\ncategorical with reordered categories.\n\nDeprecated since version 1.3.0.\n\nIf new_categories does not validate as categories\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\n"}, {"name": "pandas.concat", "path": "reference/api/pandas.concat", "type": "General functions", "text": "\nConcatenate pandas objects along a particular axis with optional set logic\nalong the other axes.\n\nCan also add a layer of hierarchical indexing on the concatenation axis, which\nmay be useful if the labels are the same (or overlapping) on the passed axis\nnumber.\n\nIf a mapping is passed, the sorted keys will be used as the keys argument,\nunless it is passed, in which case the values will be selected (see below).\nAny None objects will be dropped silently unless they are all None in which\ncase a ValueError will be raised.\n\nThe axis to concatenate along.\n\nHow to handle indexes on other axis (or axes).\n\nIf True, do not use the index values along the concatenation axis. The\nresulting axis will be labeled 0, \u2026, n - 1. This is useful if you are\nconcatenating objects where the concatenation axis does not have meaningful\nindexing information. Note the index values on the other axes are still\nrespected in the join.\n\nIf multiple levels passed, should contain tuples. Construct hierarchical index\nusing the passed keys as the outermost level.\n\nSpecific levels (unique values) to use for constructing a MultiIndex.\nOtherwise they will be inferred from the keys.\n\nNames for the levels in the resulting hierarchical index.\n\nCheck whether the new concatenated axis contains duplicates. This can be very\nexpensive relative to the actual data concatenation.\n\nSort non-concatenation axis if it is not already aligned when join is \u2018outer\u2019.\nThis has no effect when `join='inner'`, which already preserves the order of\nthe non-concatenation axis.\n\nChanged in version 1.0.0: Changed to not sort by default.\n\nIf False, do not copy data unnecessarily.\n\nWhen concatenating all `Series` along the index (axis=0), a `Series` is\nreturned. When `objs` contains at least one `DataFrame`, a `DataFrame` is\nreturned. When concatenating along the columns (axis=1), a `DataFrame` is\nreturned.\n\nSee also\n\nConcatenate Series.\n\nConcatenate DataFrames.\n\nJoin DataFrames using indexes.\n\nMerge DataFrames by indexes or columns.\n\nNotes\n\nThe keys, levels, and names arguments are all optional.\n\nA walkthrough of how this method fits in with other tools for combining pandas\nobjects can be found here.\n\nExamples\n\nCombine two `Series`.\n\nClear the existing index and reset it in the result by setting the\n`ignore_index` option to `True`.\n\nAdd a hierarchical index at the outermost level of the data with the `keys`\noption.\n\nLabel the index keys you create with the `names` option.\n\nCombine two `DataFrame` objects with identical columns.\n\nCombine `DataFrame` objects with overlapping columns and return everything.\nColumns outside the intersection will be filled with `NaN` values.\n\nCombine `DataFrame` objects with overlapping columns and return only those\nthat are shared by passing `inner` to the `join` keyword argument.\n\nCombine `DataFrame` objects horizontally along the x axis by passing in\n`axis=1`.\n\nPrevent the result from including duplicate index values with the\n`verify_integrity` option.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.aggregate", "path": "reference/api/pandas.core.groupby.dataframegroupby.aggregate", "type": "GroupBy", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a DataFrame or when passed to DataFrame.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nCan also accept a Numba JIT function with `engine='numba'` specified. Only\npassing a single function is supported with this engine.\n\nIf the `'numba'` engine is chosen, the function must be a user defined\nfunction with `values` and `index` as the first and second arguments\nrespectively in the function signature. Each group\u2019s index will be passed to\nthe user defined function and optionally available for use.\n\nChanged in version 1.1.0.\n\nPositional arguments to pass to func.\n\n`'cython'` : Runs the function through C-extensions from cython.\n\n`'numba'` : Runs the function through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.1.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}` and will be applied to the function\n\nNew in version 1.1.0.\n\nKeyword arguments to be passed into func.\n\nSee also\n\nApply function func group-wise and combine the results together.\n\nAggregate using one or more operations over the specified axis.\n\nTransforms the Series on each group based on the given function.\n\nNotes\n\nWhen using `engine='numba'`, there will be no \u201cfall back\u201d behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe passed `func`, see the examples below.\n\nExamples\n\nThe aggregation is for each column.\n\nMultiple aggregations\n\nSelect a column for aggregation\n\nDifferent aggregations per column\n\nTo control the output names with different aggregations per column, pandas\nsupports \u201cnamed aggregation\u201d\n\nThe keywords are the output column names\n\nThe values are tuples whose first element is the column to select and the\nsecond element is the aggregation to apply to that column. Pandas provides the\n`pandas.NamedAgg` namedtuple with the fields `['column', 'aggfunc']` to make\nit clearer what the arguments are. As usual, the aggregation can be a callable\nor a string alias.\n\nSee Named aggregation for more.\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe aggregating function.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.all", "path": "reference/api/pandas.core.groupby.dataframegroupby.all", "type": "GroupBy", "text": "\nReturn True if all values in the group are truthful, else False.\n\nFlag to ignore nan values during truth testing.\n\nDataFrame or Series of boolean values, where a value is True if all elements\nare True within its respective group, False otherwise.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.any", "path": "reference/api/pandas.core.groupby.dataframegroupby.any", "type": "GroupBy", "text": "\nReturn True if any value in the group is truthful, else False.\n\nFlag to ignore nan values during truth testing.\n\nDataFrame or Series of boolean values, where a value is True if any element is\nTrue within its respective group, False otherwise.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.backfill", "path": "reference/api/pandas.core.groupby.dataframegroupby.backfill", "type": "GroupBy", "text": "\nBackward fill the values.\n\nLimit of how many values to fill.\n\nObject with missing values filled.\n\nSee also\n\nBackward fill the missing values in the dataset.\n\nBackward fill the missing values in the dataset.\n\nFill NaN values of a Series.\n\nFill NaN values of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.bfill", "path": "reference/api/pandas.core.groupby.dataframegroupby.bfill", "type": "GroupBy", "text": "\nBackward fill the values.\n\nLimit of how many values to fill.\n\nObject with missing values filled.\n\nSee also\n\nBackward fill the missing values in the dataset.\n\nBackward fill the missing values in the dataset.\n\nFill NaN values of a Series.\n\nFill NaN values of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.boxplot", "path": "reference/api/pandas.core.groupby.dataframegroupby.boxplot", "type": "GroupBy", "text": "\nMake box plots from DataFrameGroupBy data.\n\n`False` \\- no subplots will be used\n\n`True` \\- create a subplot for each group.\n\nCan be any valid input to groupby.\n\nThe layout of the plot: (rows, columns).\n\nWhether x-axes will be shared among subplots.\n\nWhether y-axes will be shared among subplots.\n\nBackend to use instead of the backend specified in the option\n`plotting.backend`. For instance, \u2018matplotlib\u2019. Alternatively, to specify the\n`plotting.backend` for the whole session, set `pd.options.plotting.backend`.\n\nNew in version 1.0.0.\n\nAll other plotting keyword arguments to be passed to matplotlib\u2019s boxplot\nfunction.\n\nExamples\n\nYou can create boxplots for grouped data and show them as separate subplots:\n\nThe `subplots=False` option shows the boxplots in a single figure.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.corr", "path": "reference/api/pandas.core.groupby.dataframegroupby.corr", "type": "GroupBy", "text": "\nCompute pairwise correlation of columns, excluding NA/null values.\n\nMethod of correlation:\n\npearson : standard correlation coefficient\n\nkendall : Kendall Tau correlation coefficient\n\nspearman : Spearman rank correlation\n\nand returning a float. Note that the returned matrix from corr will have 1\nalong the diagonals and will be symmetric regardless of the callable\u2019s\nbehavior.\n\nMinimum number of observations required per pair of columns to have a valid\nresult. Currently only available for Pearson and Spearman correlation.\n\nCorrelation matrix.\n\nSee also\n\nCompute pairwise correlation with another DataFrame or Series.\n\nCompute the correlation between two Series.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.corrwith", "path": "reference/api/pandas.core.groupby.dataframegroupby.corrwith", "type": "GroupBy", "text": "\nCompute pairwise correlation.\n\nPairwise correlation is computed between rows or columns of DataFrame with\nrows or columns of Series or DataFrame. DataFrames are first aligned along\nboth axes before computing the correlations.\n\nObject with which to compute correlations.\n\nThe axis to use. 0 or \u2018index\u2019 to compute column-wise, 1 or \u2018columns\u2019 for row-\nwise.\n\nDrop missing indices from result.\n\nMethod of correlation:\n\npearson : standard correlation coefficient\n\nkendall : Kendall Tau correlation coefficient\n\nspearman : Spearman rank correlation\n\nand returning a float.\n\nPairwise correlations.\n\nSee also\n\nCompute pairwise correlation of columns.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.count", "path": "reference/api/pandas.core.groupby.dataframegroupby.count", "type": "GroupBy", "text": "\nCompute count of group, excluding missing values.\n\nCount of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.cov", "path": "reference/api/pandas.core.groupby.dataframegroupby.cov", "type": "GroupBy", "text": "\nCompute pairwise covariance of columns, excluding NA/null values.\n\nCompute the pairwise covariance among the series of a DataFrame. The returned\ndata frame is the covariance matrix of the columns of the DataFrame.\n\nBoth NA and null values are automatically excluded from the calculation. (See\nthe note below about bias from missing values.) A threshold can be set for the\nminimum number of observations for each value created. Comparisons with\nobservations below this threshold will be returned as `NaN`.\n\nThis method is generally used for the analysis of time series data to\nunderstand the relationship between different measures across time.\n\nMinimum number of observations required per pair of columns to have a valid\nresult.\n\nDelta degrees of freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nNew in version 1.1.0.\n\nThe covariance matrix of the series of the DataFrame.\n\nSee also\n\nCompute covariance with another Series.\n\nExponential weighted sample covariance.\n\nExpanding sample covariance.\n\nRolling sample covariance.\n\nNotes\n\nReturns the covariance matrix of the DataFrame\u2019s time series. The covariance\nis normalized by N-ddof.\n\nFor DataFrames that have Series that are missing data (assuming that data is\nmissing at random) the returned covariance matrix will be an unbiased estimate\nof the variance and covariance between the member Series.\n\nHowever, for many applications this estimate may not be acceptable because the\nestimate covariance matrix is not guaranteed to be positive semi-definite.\nThis could lead to estimate correlations having absolute values which are\ngreater than one, and/or a non-invertible covariance matrix. See Estimation of\ncovariance matrices for more details.\n\nExamples\n\nMinimum number of periods\n\nThis method also supports an optional `min_periods` keyword that specifies the\nrequired minimum number of non-NA observations for each column pair in order\nto have a valid result:\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.cumcount", "path": "reference/api/pandas.core.groupby.dataframegroupby.cumcount", "type": "GroupBy", "text": "\nNumber each item in each group from 0 to the length of that group - 1.\n\nEssentially this is equivalent to\n\nIf False, number in reverse, from length of group - 1 to 0.\n\nSequence number of each element within each group.\n\nSee also\n\nNumber the groups themselves.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.cummax", "path": "reference/api/pandas.core.groupby.dataframegroupby.cummax", "type": "GroupBy", "text": "\nCumulative max for each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.cummin", "path": "reference/api/pandas.core.groupby.dataframegroupby.cummin", "type": "GroupBy", "text": "\nCumulative min for each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.cumprod", "path": "reference/api/pandas.core.groupby.dataframegroupby.cumprod", "type": "GroupBy", "text": "\nCumulative product for each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.cumsum", "path": "reference/api/pandas.core.groupby.dataframegroupby.cumsum", "type": "GroupBy", "text": "\nCumulative sum for each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.describe", "path": "reference/api/pandas.core.groupby.dataframegroupby.describe", "type": "GroupBy", "text": "\nGenerate descriptive statistics.\n\nDescriptive statistics include those that summarize the central tendency,\ndispersion and shape of a dataset\u2019s distribution, excluding `NaN` values.\n\nAnalyzes both numeric and object series, as well as `DataFrame` column sets of\nmixed data types. The output will vary depending on what is provided. Refer to\nthe notes below for more detail.\n\nThe percentiles to include in the output. All should fall between 0 and 1. The\ndefault is `[.25, .5, .75]`, which returns the 25th, 50th, and 75th\npercentiles.\n\nA white list of data types to include in the result. Ignored for `Series`.\nHere are the options:\n\n\u2018all\u2019 : All columns of the input will be included in the output.\n\nA list-like of dtypes : Limits the results to the provided data types. To\nlimit the result to numeric types submit `numpy.number`. To limit it instead\nto object columns submit the `numpy.object` data type. Strings can also be\nused in the style of `select_dtypes` (e.g. `df.describe(include=['O'])`). To\nselect pandas categorical columns, use `'category'`\n\nNone (default) : The result will include all numeric columns.\n\nA black list of data types to omit from the result. Ignored for `Series`. Here\nare the options:\n\nA list-like of dtypes : Excludes the provided data types from the result. To\nexclude numeric types submit `numpy.number`. To exclude object columns submit\nthe data type `numpy.object`. Strings can also be used in the style of\n`select_dtypes` (e.g. `df.describe(exclude=['O'])`). To exclude pandas\ncategorical columns, use `'category'`\n\nNone (default) : The result will exclude nothing.\n\nWhether to treat datetime dtypes as numeric. This affects statistics\ncalculated for the column. For DataFrame input, this also controls whether\ndatetime columns are included by default.\n\nNew in version 1.1.0.\n\nSummary statistics of the Series or Dataframe provided.\n\nSee also\n\nCount number of non-NA/null observations.\n\nMaximum of the values in the object.\n\nMinimum of the values in the object.\n\nMean of the values.\n\nStandard deviation of the observations.\n\nSubset of a DataFrame including/excluding columns based on their dtype.\n\nNotes\n\nFor numeric data, the result\u2019s index will include `count`, `mean`, `std`,\n`min`, `max` as well as lower, `50` and upper percentiles. By default the\nlower percentile is `25` and the upper percentile is `75`. The `50` percentile\nis the same as the median.\n\nFor object data (e.g. strings or timestamps), the result\u2019s index will include\n`count`, `unique`, `top`, and `freq`. The `top` is the most common value. The\n`freq` is the most common value\u2019s frequency. Timestamps also include the\n`first` and `last` items.\n\nIf multiple object values have the highest count, then the `count` and `top`\nresults will be arbitrarily chosen from among those with the highest count.\n\nFor mixed data types provided via a `DataFrame`, the default is to return only\nan analysis of numeric columns. If the dataframe consists only of object and\ncategorical data without any numeric columns, the default is to return an\nanalysis of both the object and categorical columns. If `include='all'` is\nprovided as an option, the result will include a union of attributes of each\ntype.\n\nThe include and exclude parameters can be used to limit which columns in a\n`DataFrame` are analyzed for the output. The parameters are ignored when\nanalyzing a `Series`.\n\nExamples\n\nDescribing a numeric `Series`.\n\nDescribing a categorical `Series`.\n\nDescribing a timestamp `Series`.\n\nDescribing a `DataFrame`. By default only numeric fields are returned.\n\nDescribing all columns of a `DataFrame` regardless of data type.\n\nDescribing a column from a `DataFrame` by accessing it as an attribute.\n\nIncluding only numeric columns in a `DataFrame` description.\n\nIncluding only string columns in a `DataFrame` description.\n\nIncluding only categorical columns from a `DataFrame` description.\n\nExcluding numeric columns from a `DataFrame` description.\n\nExcluding object columns from a `DataFrame` description.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.diff", "path": "reference/api/pandas.core.groupby.dataframegroupby.diff", "type": "GroupBy", "text": "\nFirst discrete difference of element.\n\nCalculates the difference of a Dataframe element compared with another element\nin the Dataframe (default is element in previous row).\n\nPeriods to shift for calculating difference, accepts negative values.\n\nTake difference over rows (0) or columns (1).\n\nFirst differences of the Series.\n\nSee also\n\nPercent change over given number of periods.\n\nShift index by desired number of periods with an optional time freq.\n\nFirst discrete difference of object.\n\nNotes\n\nFor boolean dtypes, this uses `operator.xor()` rather than `operator.sub()`.\nThe result is calculated according to current dtype in Dataframe, however\ndtype of the result is always float64.\n\nExamples\n\nDifference with previous row\n\nDifference with previous column\n\nDifference with 3rd previous row\n\nDifference with following row\n\nOverflow in input dtype\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.ffill", "path": "reference/api/pandas.core.groupby.dataframegroupby.ffill", "type": "GroupBy", "text": "\nForward fill the values.\n\nLimit of how many values to fill.\n\nObject with missing values filled.\n\nSee also\n\nReturns Series with minimum number of char in object.\n\nObject with missing values filled or None if inplace=True.\n\nFill NaN values of a Series.\n\nFill NaN values of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.fillna", "path": "reference/api/pandas.core.groupby.dataframegroupby.fillna", "type": "GroupBy", "text": "\nFill NA/NaN values using the specified method.\n\nValue to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of\nvalues specifying which value to use for each index (for a Series) or column\n(for a DataFrame). Values not in the dict/Series/DataFrame will not be filled.\nThis value cannot be a list.\n\nMethod to use for filling holes in reindexed Series pad / ffill: propagate\nlast valid observation forward to next valid backfill / bfill: use next valid\nobservation to fill gap.\n\nAxis along which to fill missing values.\n\nIf True, fill in-place. Note: this will modify any other views on this object\n(e.g., a no-copy slice for a column in a DataFrame).\n\nIf method is specified, this is the maximum number of consecutive NaN values\nto forward/backward fill. In other words, if there is a gap with more than\nthis number of consecutive NaNs, it will only be partially filled. If method\nis not specified, this is the maximum number of entries along the entire axis\nwhere NaNs will be filled. Must be greater than 0 if not None.\n\nA dict of item->dtype of what to downcast if possible, or the string \u2018infer\u2019\nwhich will try to downcast to an appropriate equal type (e.g. float64 to int64\nif possible).\n\nObject with missing values filled or None if `inplace=True`.\n\nSee also\n\nFill NaN values using interpolation.\n\nConform object to new index.\n\nConvert TimeSeries to specified frequency.\n\nExamples\n\nReplace all NaN elements with 0s.\n\nWe can also propagate non-null values forward or backward.\n\nReplace all NaN elements in column \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019, with 0, 1, 2, and 3\nrespectively.\n\nOnly replace the first NaN element.\n\nWhen filling using a DataFrame, replacement happens along the same column\nnames and same indices\n\nNote that column D is not affected since it is not present in df2.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.filter", "path": "reference/api/pandas.core.groupby.dataframegroupby.filter", "type": "GroupBy", "text": "\nReturn a copy of a DataFrame excluding filtered elements.\n\nElements from groups are filtered if they do not satisfy the boolean criterion\nspecified by func.\n\nFunction to apply to each subframe. Should return True or False.\n\nIf False, groups that evaluate False are filled with NaNs.\n\nNotes\n\nEach subframe is endowed the attribute \u2018name\u2019 in case you need to know which\ngroup you are working on.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.hist", "path": "reference/api/pandas.core.groupby.dataframegroupby.hist", "type": "GroupBy", "text": "\nMake a histogram of the DataFrame\u2019s columns.\n\nA histogram is a representation of the distribution of data. This function\ncalls `matplotlib.pyplot.hist()`, on each series in the DataFrame, resulting\nin one histogram per column.\n\nThe pandas object holding the data.\n\nIf passed, will be used to limit data to a subset of columns.\n\nIf passed, then used to form histograms for separate groups.\n\nWhether to show axis grid lines.\n\nIf specified changes the x-axis label size.\n\nRotation of x axis labels. For example, a value of 90 displays the x labels\nrotated 90 degrees clockwise.\n\nIf specified changes the y-axis label size.\n\nRotation of y axis labels. For example, a value of 90 displays the y labels\nrotated 90 degrees clockwise.\n\nThe axes to plot the histogram on.\n\nIn case subplots=True, share x axis and set some x axis labels to invisible;\ndefaults to True if ax is None otherwise False if an ax is passed in. Note\nthat passing in both an ax and sharex=True will alter all x axis labels for\nall subplots in a figure.\n\nIn case subplots=True, share y axis and set some y axis labels to invisible.\n\nThe size in inches of the figure to create. Uses the value in\nmatplotlib.rcParams by default.\n\nTuple of (rows, columns) for the layout of the histograms.\n\nNumber of histogram bins to be used. If an integer is given, bins + 1 bin\nedges are calculated and returned. If bins is a sequence, gives bin edges,\nincluding left edge of first bin and right edge of last bin. In this case,\nbins is returned unmodified.\n\nBackend to use instead of the backend specified in the option\n`plotting.backend`. For instance, \u2018matplotlib\u2019. Alternatively, to specify the\n`plotting.backend` for the whole session, set `pd.options.plotting.backend`.\n\nNew in version 1.0.0.\n\nWhether to show the legend.\n\nNew in version 1.1.0.\n\nAll other plotting keyword arguments to be passed to\n`matplotlib.pyplot.hist()`.\n\nSee also\n\nPlot a histogram using matplotlib.\n\nExamples\n\nThis example draws a histogram based on the length and width of some animals,\ndisplayed in three bins\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.idxmax", "path": "reference/api/pandas.core.groupby.dataframegroupby.idxmax", "type": "GroupBy", "text": "\nReturn index of first occurrence of maximum over requested axis.\n\nNA/null values are excluded.\n\nThe axis to use. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nIndexes of maxima along the specified axis.\n\nIf the row/column is empty\n\nSee also\n\nReturn index of the maximum element.\n\nNotes\n\nThis method is the DataFrame version of `ndarray.argmax`.\n\nExamples\n\nConsider a dataset containing food consumption in Argentina.\n\nBy default, it returns the index for the maximum value in each column.\n\nTo return the index for the maximum value in each row, use `axis=\"columns\"`.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.idxmin", "path": "reference/api/pandas.core.groupby.dataframegroupby.idxmin", "type": "GroupBy", "text": "\nReturn index of first occurrence of minimum over requested axis.\n\nNA/null values are excluded.\n\nThe axis to use. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nIndexes of minima along the specified axis.\n\nIf the row/column is empty\n\nSee also\n\nReturn index of the minimum element.\n\nNotes\n\nThis method is the DataFrame version of `ndarray.argmin`.\n\nExamples\n\nConsider a dataset containing food consumption in Argentina.\n\nBy default, it returns the index for the minimum value in each column.\n\nTo return the index for the minimum value in each row, use `axis=\"columns\"`.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.mad", "path": "reference/api/pandas.core.groupby.dataframegroupby.mad", "type": "GroupBy", "text": "\nReturn the mean absolute deviation of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.nunique", "path": "reference/api/pandas.core.groupby.dataframegroupby.nunique", "type": "GroupBy", "text": "\nReturn DataFrame with counts of unique elements in each position.\n\nDon\u2019t include NaN in the counts.\n\nExamples\n\nCheck for rows with the same id but conflicting values:\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.pad", "path": "reference/api/pandas.core.groupby.dataframegroupby.pad", "type": "GroupBy", "text": "\nForward fill the values.\n\nLimit of how many values to fill.\n\nObject with missing values filled.\n\nSee also\n\nReturns Series with minimum number of char in object.\n\nObject with missing values filled or None if inplace=True.\n\nFill NaN values of a Series.\n\nFill NaN values of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.pct_change", "path": "reference/api/pandas.core.groupby.dataframegroupby.pct_change", "type": "GroupBy", "text": "\nCalculate pct_change of each value to previous entry in group.\n\nPercentage changes within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.plot", "path": "reference/api/pandas.core.groupby.dataframegroupby.plot", "type": "GroupBy", "text": "\nClass implementing the .plot attribute for groupby objects.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.quantile", "path": "reference/api/pandas.core.groupby.dataframegroupby.quantile", "type": "GroupBy", "text": "\nReturn group values at the given quantile, a la numpy.percentile.\n\nValue(s) between 0 and 1 providing the quantile(s) to compute.\n\nMethod to use when the desired quantile falls between two points.\n\nReturn type determined by caller of GroupBy object.\n\nSee also\n\nSimilar method for Series.\n\nSimilar method for DataFrame.\n\nNumPy method to compute qth percentile.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.rank", "path": "reference/api/pandas.core.groupby.dataframegroupby.rank", "type": "GroupBy", "text": "\nProvide the rank of values within each group.\n\naverage: average rank of group.\n\nmin: lowest rank in group.\n\nmax: highest rank in group.\n\nfirst: ranks assigned in order they appear in the array.\n\ndense: like \u2018min\u2019, but rank always increases by 1 between groups.\n\nFalse for ranks by high (1) to low (N).\n\nkeep: leave NA values where they are.\n\ntop: smallest rank if ascending.\n\nbottom: smallest rank if descending.\n\nCompute percentage rank of data within each group.\n\nThe axis of the object over which to compute the rank.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.resample", "path": "reference/api/pandas.core.groupby.dataframegroupby.resample", "type": "GroupBy", "text": "\nProvide resampling when using a TimeGrouper.\n\nGiven a grouper, the function resamples it according to a string \u201cstring\u201d ->\n\u201cfrequency\u201d.\n\nSee the frequency aliases documentation for more details.\n\nThe offset string or object representing target grouper conversion.\n\nPossible arguments are how, fill_method, limit, kind and on, and other\narguments of TimeGrouper.\n\nReturn a new grouper with our resampler appended.\n\nSee also\n\nSpecify a frequency to resample with when grouping by a key.\n\nFrequency conversion and resampling of time series.\n\nExamples\n\nDownsample the DataFrame into 3 minute bins and sum the values of the\ntimestamps falling into a bin.\n\nUpsample the series into 30 second bins.\n\nResample by month. Values are assigned to the month of the period.\n\nDownsample the series into 3 minute bins as above, but close the right side of\nthe bin interval.\n\nDownsample the series into 3 minute bins and close the right side of the bin\ninterval, but label each bin using the right edge instead of the left.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.sample", "path": "reference/api/pandas.core.groupby.dataframegroupby.sample", "type": "GroupBy", "text": "\nReturn a random sample of items from each group.\n\nYou can use random_state for reproducibility.\n\nNew in version 1.1.0.\n\nNumber of items to return for each group. Cannot be used with frac and must be\nno larger than the smallest group unless replace is True. Default is one if\nfrac is None.\n\nFraction of items to return. Cannot be used with n.\n\nAllow or disallow sampling of the same row more than once.\n\nDefault None results in equal probability weighting. If passed a list-like\nthen values must have the same length as the underlying DataFrame or Series\nobject and will be used as sampling probabilities after normalization within\neach group. Values must be non-negative with at least one positive element\nwithin each group.\n\nIf int, array-like, or BitGenerator, seed for random number generator. If\nnp.random.RandomState or np.random.Generator, use as given.\n\nChanged in version 1.4.0: np.random.Generator objects now accepted\n\nA new object of same type as caller containing items randomly sampled within\neach group from the caller object.\n\nSee also\n\nGenerate random samples from a DataFrame object.\n\nGenerate a random sample from a given 1-D numpy array.\n\nExamples\n\nSelect one row at random for each distinct value in column a. The random_state\nargument can be used to guarantee reproducibility:\n\nSet frac to sample fixed proportions rather than counts:\n\nControl sample probabilities within groups by setting weights:\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.shift", "path": "reference/api/pandas.core.groupby.dataframegroupby.shift", "type": "GroupBy", "text": "\nShift each group by periods observations.\n\nIf freq is passed, the index will be increased using the periods and the freq.\n\nNumber of periods to shift.\n\nFrequency string.\n\nShift direction.\n\nThe scalar value to use for newly introduced missing values.\n\nObject shifted within each group.\n\nSee also\n\nShift values of Index.\n\nShift the time index, using the index\u2019s frequency if available.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.size", "path": "reference/api/pandas.core.groupby.dataframegroupby.size", "type": "GroupBy", "text": "\nCompute group sizes.\n\nNumber of rows in each group as a Series if as_index is True or a DataFrame if\nas_index is False.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.skew", "path": "reference/api/pandas.core.groupby.dataframegroupby.skew", "type": "GroupBy", "text": "\nReturn unbiased skew over requested axis.\n\nNormalized by N-1.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.take", "path": "reference/api/pandas.core.groupby.dataframegroupby.take", "type": "GroupBy", "text": "\nReturn the elements in the given positional indices along an axis.\n\nThis means that we are not indexing according to actual values in the index\nattribute of the object. We are indexing according to the actual position of\nthe element in the object.\n\nAn array of ints indicating which positions to take.\n\nThe axis on which to select elements. `0` means that we are selecting rows,\n`1` means that we are selecting columns.\n\nBefore pandas 1.0, `is_copy=False` can be specified to ensure that the return\nvalue is an actual copy. Starting with pandas 1.0, `take` always returns a\ncopy, and the keyword is therefore deprecated.\n\nDeprecated since version 1.0.0.\n\nFor compatibility with `numpy.take()`. Has no effect on the output.\n\nAn array-like containing the elements taken from the object.\n\nSee also\n\nSelect a subset of a DataFrame by labels.\n\nSelect a subset of a DataFrame by positions.\n\nTake elements from an array along an axis.\n\nExamples\n\nTake elements at positions 0 and 3 along the axis 0 (default).\n\nNote how the actual indices selected (0 and 1) do not correspond to our\nselected indices 0 and 3. That\u2019s because we are selecting the 0th and 3rd\nrows, not rows whose indices equal 0 and 3.\n\nTake elements at indices 1 and 2 along the axis 1 (column selection).\n\nWe may take elements using negative integers for positive indices, starting\nfrom the end of the object, just like with Python lists.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.transform", "path": "reference/api/pandas.core.groupby.dataframegroupby.transform", "type": "GroupBy", "text": "\nCall function producing a like-indexed DataFrame on each group and return a\nDataFrame having the same indexes as the original object filled with the\ntransformed values.\n\nFunction to apply to each group.\n\nCan also accept a Numba JIT function with `engine='numba'` specified.\n\nIf the `'numba'` engine is chosen, the function must be a user defined\nfunction with `values` and `index` as the first and second arguments\nrespectively in the function signature. Each group\u2019s index will be passed to\nthe user defined function and optionally available for use.\n\nChanged in version 1.1.0.\n\nPositional arguments to pass to func.\n\n`'cython'` : Runs the function through C-extensions from cython.\n\n`'numba'` : Runs the function through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or the global setting `compute.use_numba`\n\nNew in version 1.1.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}` and will be applied to the function\n\nNew in version 1.1.0.\n\nKeyword arguments to be passed into func.\n\nSee also\n\nApply function `func` group-wise and combine the results together.\n\nAggregate using one or more operations over the specified axis.\n\nCall `func` on self producing a DataFrame with the same axis shape as self.\n\nNotes\n\nEach group is endowed the attribute \u2018name\u2019 in case you need to know which\ngroup you are working on.\n\nThe current implementation imposes three requirements on f:\n\nf must return a value that either has the same shape as the input subframe or\ncan be broadcast to the shape of the input subframe. For example, if f returns\na scalar it will be broadcast to have the same shape as the input subframe.\n\nif this is a DataFrame, f must support application column-by-column in the\nsubframe. If f also supports application to the entire subframe, then a fast\npath is used starting from the second chunk.\n\nf must not mutate groups. Mutation is not supported and may produce unexpected\nresults. See Mutating with User Defined Function (UDF) methods for more\ndetails.\n\nWhen using `engine='numba'`, there will be no \u201cfall back\u201d behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe passed `func`, see the examples below.\n\nExamples\n\nBroadcast result of the transformation\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe passed `func`, for example:\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.tshift", "path": "reference/api/pandas.core.groupby.dataframegroupby.tshift", "type": "GroupBy", "text": "\nShift the time index, using the index\u2019s frequency if available.\n\nDeprecated since version 1.1.0: Use shift instead.\n\nNumber of periods to move, can be positive or negative.\n\nIncrement to use from the tseries module or time rule expressed as a string\n(e.g. \u2018EOM\u2019).\n\nCorresponds to the axis that contains the Index.\n\nNotes\n\nIf freq is not specified then tries to use the freq or inferred_freq\nattributes of the index. If neither of those attributes exist, a ValueError is\nthrown\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.value_counts", "path": "reference/api/pandas.core.groupby.dataframegroupby.value_counts", "type": "GroupBy", "text": "\nReturn a Series or DataFrame containing counts of unique rows.\n\nNew in version 1.4.0.\n\nColumns to use when counting unique combinations.\n\nReturn proportions rather than frequencies.\n\nSort by frequencies.\n\nSort in ascending order.\n\nDon\u2019t include counts of rows that contain NA values.\n\nSeries if the groupby as_index is True, otherwise DataFrame.\n\nSee also\n\nEquivalent method on Series.\n\nEquivalent method on DataFrame.\n\nEquivalent method on SeriesGroupBy.\n\nNotes\n\nIf the groupby as_index is True then the returned Series will have a\nMultiIndex with one level per input column.\n\nIf the groupby as_index is False then the returned DataFrame will have an\nadditional column with the value_counts. The column is labelled \u2018count\u2019 or\n\u2018proportion\u2019, depending on the `normalize` parameter.\n\nBy default, rows that contain any NA values are omitted from the result.\n\nBy default, the result will be in descending order so that the first element\nof each group is the most frequently-occurring row.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.GroupBy.__iter__", "path": "reference/api/pandas.core.groupby.groupby.__iter__", "type": "GroupBy", "text": "\nGroupby iterator.\n\n"}, {"name": "pandas.core.groupby.GroupBy.agg", "path": "reference/api/pandas.core.groupby.groupby.agg", "type": "GroupBy", "text": "\n\n"}, {"name": "pandas.core.groupby.GroupBy.all", "path": "reference/api/pandas.core.groupby.groupby.all", "type": "GroupBy", "text": "\nReturn True if all values in the group are truthful, else False.\n\nFlag to ignore nan values during truth testing.\n\nDataFrame or Series of boolean values, where a value is True if all elements\nare True within its respective group, False otherwise.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.any", "path": "reference/api/pandas.core.groupby.groupby.any", "type": "GroupBy", "text": "\nReturn True if any value in the group is truthful, else False.\n\nFlag to ignore nan values during truth testing.\n\nDataFrame or Series of boolean values, where a value is True if any element is\nTrue within its respective group, False otherwise.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.apply", "path": "reference/api/pandas.core.groupby.groupby.apply", "type": "GroupBy", "text": "\nApply function `func` group-wise and combine the results together.\n\nThe function passed to `apply` must take a dataframe as its first argument and\nreturn a DataFrame, Series or scalar. `apply` will then take care of combining\nthe results back together into a single dataframe or series. `apply` is\ntherefore a highly flexible grouping method.\n\nWhile `apply` is a very flexible method, its downside is that using it can be\nquite a bit slower than using more specific methods like `agg` or `transform`.\nPandas offers a wide range of method that will be much faster than using\n`apply` for their specific purposes, so try to use them before reaching for\n`apply`.\n\nA callable that takes a dataframe as its first argument, and returns a\ndataframe, a series or a scalar. In addition the callable may take positional\nand keyword arguments.\n\nOptional positional and keyword arguments to pass to `func`.\n\nSee also\n\nApply function to the full GroupBy object instead of to each group.\n\nApply aggregate function to the GroupBy object.\n\nApply function column-by-column to the GroupBy object.\n\nApply a function to a Series.\n\nApply a function to each row or column of a DataFrame.\n\nNotes\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe passed `func`, see the examples below.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nExamples\n\nNotice that `g` has two groups, `a` and `b`. Calling apply in various ways, we\ncan get different grouping results:\n\nExample 1: below the function passed to apply takes a DataFrame as its\nargument and returns a DataFrame. apply combines the result for each group\ntogether into a new DataFrame:\n\nExample 2: The function passed to apply takes a DataFrame as its argument and\nreturns a Series. apply combines the result for each group together into a new\nDataFrame.\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe passed `func`.\n\nExample 3: The function passed to apply takes a DataFrame as its argument and\nreturns a scalar. apply combines the result for each group together into a\nSeries, including setting the index as appropriate:\n\n"}, {"name": "pandas.core.groupby.GroupBy.backfill", "path": "reference/api/pandas.core.groupby.groupby.backfill", "type": "GroupBy", "text": "\nBackward fill the values.\n\nLimit of how many values to fill.\n\nObject with missing values filled.\n\nSee also\n\nBackward fill the missing values in the dataset.\n\nBackward fill the missing values in the dataset.\n\nFill NaN values of a Series.\n\nFill NaN values of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.bfill", "path": "reference/api/pandas.core.groupby.groupby.bfill", "type": "GroupBy", "text": "\nBackward fill the values.\n\nLimit of how many values to fill.\n\nObject with missing values filled.\n\nSee also\n\nBackward fill the missing values in the dataset.\n\nBackward fill the missing values in the dataset.\n\nFill NaN values of a Series.\n\nFill NaN values of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.count", "path": "reference/api/pandas.core.groupby.groupby.count", "type": "GroupBy", "text": "\nCompute count of group, excluding missing values.\n\nCount of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.cumcount", "path": "reference/api/pandas.core.groupby.groupby.cumcount", "type": "GroupBy", "text": "\nNumber each item in each group from 0 to the length of that group - 1.\n\nEssentially this is equivalent to\n\nIf False, number in reverse, from length of group - 1 to 0.\n\nSequence number of each element within each group.\n\nSee also\n\nNumber the groups themselves.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.GroupBy.cummax", "path": "reference/api/pandas.core.groupby.groupby.cummax", "type": "GroupBy", "text": "\nCumulative max for each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.cummin", "path": "reference/api/pandas.core.groupby.groupby.cummin", "type": "GroupBy", "text": "\nCumulative min for each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.cumprod", "path": "reference/api/pandas.core.groupby.groupby.cumprod", "type": "GroupBy", "text": "\nCumulative product for each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.cumsum", "path": "reference/api/pandas.core.groupby.groupby.cumsum", "type": "GroupBy", "text": "\nCumulative sum for each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.ffill", "path": "reference/api/pandas.core.groupby.groupby.ffill", "type": "GroupBy", "text": "\nForward fill the values.\n\nLimit of how many values to fill.\n\nObject with missing values filled.\n\nSee also\n\nReturns Series with minimum number of char in object.\n\nObject with missing values filled or None if inplace=True.\n\nFill NaN values of a Series.\n\nFill NaN values of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.first", "path": "reference/api/pandas.core.groupby.groupby.first", "type": "GroupBy", "text": "\nCompute first of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed first of values within each group.\n\n"}, {"name": "pandas.core.groupby.GroupBy.get_group", "path": "reference/api/pandas.core.groupby.groupby.get_group", "type": "GroupBy", "text": "\nConstruct DataFrame from group with provided name.\n\nThe name of the group to get as a DataFrame.\n\nThe DataFrame to take the DataFrame out of. If it is None, the object groupby\nwas called on will be used.\n\n"}, {"name": "pandas.core.groupby.GroupBy.groups", "path": "reference/api/pandas.core.groupby.groupby.groups", "type": "GroupBy", "text": "\nDict {group name -> group labels}.\n\n"}, {"name": "pandas.core.groupby.GroupBy.head", "path": "reference/api/pandas.core.groupby.groupby.head", "type": "GroupBy", "text": "\nReturn first n rows of each group.\n\nSimilar to `.apply(lambda x: x.head(n))`, but it returns a subset of rows from\nthe original DataFrame with original index and order preserved (`as_index`\nflag is ignored).\n\nIf positive: number of entries to include from start of each group. If\nnegative: number of entries to exclude from end of each group.\n\nSubset of original Series or DataFrame as determined by n.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.GroupBy.indices", "path": "reference/api/pandas.core.groupby.groupby.indices", "type": "GroupBy", "text": "\nDict {group name -> group indices}.\n\n"}, {"name": "pandas.core.groupby.GroupBy.last", "path": "reference/api/pandas.core.groupby.groupby.last", "type": "GroupBy", "text": "\nCompute last of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed last of values within each group.\n\n"}, {"name": "pandas.core.groupby.GroupBy.max", "path": "reference/api/pandas.core.groupby.groupby.max", "type": "GroupBy", "text": "\nCompute max of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed max of values within each group.\n\n"}, {"name": "pandas.core.groupby.GroupBy.mean", "path": "reference/api/pandas.core.groupby.groupby.mean", "type": "GroupBy", "text": "\nCompute mean of groups, excluding missing values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.4.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{{'nopython': True, 'nogil':\nFalse, 'parallel': False}}`\n\nNew in version 1.4.0.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\nExamples\n\nGroupby one column and return the mean of the remaining columns in each group.\n\nGroupby two columns and return the mean of the remaining column.\n\nGroupby one column and return the mean of only particular column in the group.\n\n"}, {"name": "pandas.core.groupby.GroupBy.median", "path": "reference/api/pandas.core.groupby.groupby.median", "type": "GroupBy", "text": "\nCompute median of groups, excluding missing values.\n\nFor multiple groupings, the result index will be a MultiIndex\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nMedian of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.min", "path": "reference/api/pandas.core.groupby.groupby.min", "type": "GroupBy", "text": "\nCompute min of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed min of values within each group.\n\n"}, {"name": "pandas.core.groupby.GroupBy.ngroup", "path": "reference/api/pandas.core.groupby.groupby.ngroup", "type": "GroupBy", "text": "\nNumber each group from 0 to the number of groups - 1.\n\nThis is the enumerative complement of cumcount. Note that the numbers given to\nthe groups match the order in which the groups would be seen when iterating\nover the groupby object, not the order they are first observed.\n\nIf False, number in reverse, from number of group - 1 to 0.\n\nUnique numbers for each group.\n\nSee also\n\nNumber the rows in each group.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.GroupBy.nth", "path": "reference/api/pandas.core.groupby.groupby.nth", "type": "GroupBy", "text": "\nTake the nth row from each group if n is an int, otherwise a subset of rows.\n\nCan be either a call or an index. dropna is not available with index notation.\nIndex notation accepts a comma separated list of integers and slices.\n\nIf dropna, will take the nth non-null row, dropna is either \u2018all\u2019 or \u2018any\u2019;\nthis is equivalent to calling dropna(how=dropna) before the groupby.\n\nA single nth value for the row or a list of nth values or slices.\n\nChanged in version 1.4.0: Added slice and lists containiing slices. Added\nindex notation.\n\nApply the specified dropna operation before counting which row is the nth row.\nOnly supported if n is an int.\n\nN-th value within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\nExamples\n\nIndex notation may also be used\n\nSpecifying dropna allows count ignoring `NaN`\n\nNaNs denote group exhausted when using dropna\n\nSpecifying as_index=False in groupby keeps the original index.\n\n"}, {"name": "pandas.core.groupby.GroupBy.ohlc", "path": "reference/api/pandas.core.groupby.groupby.ohlc", "type": "GroupBy", "text": "\nCompute open, high, low and close values of a group, excluding missing values.\n\nFor multiple groupings, the result index will be a MultiIndex\n\nOpen, high, low and close values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.pad", "path": "reference/api/pandas.core.groupby.groupby.pad", "type": "GroupBy", "text": "\nForward fill the values.\n\nLimit of how many values to fill.\n\nObject with missing values filled.\n\nSee also\n\nReturns Series with minimum number of char in object.\n\nObject with missing values filled or None if inplace=True.\n\nFill NaN values of a Series.\n\nFill NaN values of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.pct_change", "path": "reference/api/pandas.core.groupby.groupby.pct_change", "type": "GroupBy", "text": "\nCalculate pct_change of each value to previous entry in group.\n\nPercentage changes within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.pipe", "path": "reference/api/pandas.core.groupby.groupby.pipe", "type": "GroupBy", "text": "\nApply a function func with arguments to this GroupBy object and return the\nfunction\u2019s result.\n\nUse .pipe when you want to improve readability by chaining together functions\nthat expect Series, DataFrames, GroupBy or Resampler objects. Instead of\nwriting\n\nYou can write\n\nwhich is much more readable.\n\nFunction to apply to this GroupBy object or, alternatively, a (callable,\ndata_keyword) tuple where data_keyword is a string indicating the keyword of\ncallable that expects the GroupBy object.\n\nPositional arguments passed into func.\n\nA dictionary of keyword arguments passed into func.\n\nSee also\n\nApply a function with arguments to a series.\n\nApply a function with arguments to a dataframe.\n\nApply function to each group instead of to the full GroupBy object.\n\nNotes\n\nSee more here\n\nExamples\n\nTo get the difference between each groups maximum and minimum value in one\npass, you can do\n\n"}, {"name": "pandas.core.groupby.GroupBy.prod", "path": "reference/api/pandas.core.groupby.groupby.prod", "type": "GroupBy", "text": "\nCompute prod of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed prod of values within each group.\n\n"}, {"name": "pandas.core.groupby.GroupBy.rank", "path": "reference/api/pandas.core.groupby.groupby.rank", "type": "GroupBy", "text": "\nProvide the rank of values within each group.\n\naverage: average rank of group.\n\nmin: lowest rank in group.\n\nmax: highest rank in group.\n\nfirst: ranks assigned in order they appear in the array.\n\ndense: like \u2018min\u2019, but rank always increases by 1 between groups.\n\nFalse for ranks by high (1) to low (N).\n\nkeep: leave NA values where they are.\n\ntop: smallest rank if ascending.\n\nbottom: smallest rank if descending.\n\nCompute percentage rank of data within each group.\n\nThe axis of the object over which to compute the rank.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.GroupBy.sem", "path": "reference/api/pandas.core.groupby.groupby.sem", "type": "GroupBy", "text": "\nCompute standard error of the mean of groups, excluding missing values.\n\nFor multiple groupings, the result index will be a MultiIndex.\n\nDegrees of freedom.\n\nStandard error of the mean of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.size", "path": "reference/api/pandas.core.groupby.groupby.size", "type": "GroupBy", "text": "\nCompute group sizes.\n\nNumber of rows in each group as a Series if as_index is True or a DataFrame if\nas_index is False.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.std", "path": "reference/api/pandas.core.groupby.groupby.std", "type": "GroupBy", "text": "\nCompute standard deviation of groups, excluding missing values.\n\nFor multiple groupings, the result index will be a MultiIndex.\n\nDegrees of freedom.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.4.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{{'nopython': True, 'nogil':\nFalse, 'parallel': False}}`\n\nNew in version 1.4.0.\n\nStandard deviation of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.sum", "path": "reference/api/pandas.core.groupby.groupby.sum", "type": "GroupBy", "text": "\nCompute sum of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed sum of values within each group.\n\n"}, {"name": "pandas.core.groupby.GroupBy.tail", "path": "reference/api/pandas.core.groupby.groupby.tail", "type": "GroupBy", "text": "\nReturn last n rows of each group.\n\nSimilar to `.apply(lambda x: x.tail(n))`, but it returns a subset of rows from\nthe original DataFrame with original index and order preserved (`as_index`\nflag is ignored).\n\nIf positive: number of entries to include from end of each group. If negative:\nnumber of entries to exclude from start of each group.\n\nSubset of original Series or DataFrame as determined by n.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.GroupBy.var", "path": "reference/api/pandas.core.groupby.groupby.var", "type": "GroupBy", "text": "\nCompute variance of groups, excluding missing values.\n\nFor multiple groupings, the result index will be a MultiIndex.\n\nDegrees of freedom.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.4.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{{'nopython': True, 'nogil':\nFalse, 'parallel': False}}`\n\nNew in version 1.4.0.\n\nVariance of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.aggregate", "path": "reference/api/pandas.core.groupby.seriesgroupby.aggregate", "type": "Series", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a Series or when passed to Series.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nCan also accept a Numba JIT function with `engine='numba'` specified. Only\npassing a single function is supported with this engine.\n\nIf the `'numba'` engine is chosen, the function must be a user defined\nfunction with `values` and `index` as the first and second arguments\nrespectively in the function signature. Each group\u2019s index will be passed to\nthe user defined function and optionally available for use.\n\nChanged in version 1.1.0.\n\nPositional arguments to pass to func.\n\n`'cython'` : Runs the function through C-extensions from cython.\n\n`'numba'` : Runs the function through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.1.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}` and will be applied to the function\n\nNew in version 1.1.0.\n\nKeyword arguments to be passed into func.\n\nSee also\n\nApply function func group-wise and combine the results together.\n\nAggregate using one or more operations over the specified axis.\n\nTransforms the Series on each group based on the given function.\n\nNotes\n\nWhen using `engine='numba'`, there will be no \u201cfall back\u201d behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe passed `func`, see the examples below.\n\nExamples\n\nThe output column names can be controlled by passing the desired column names\nand aggregations as keyword arguments.\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe aggregating function.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.hist", "path": "reference/api/pandas.core.groupby.seriesgroupby.hist", "type": "Series", "text": "\nDraw histogram of the input series using matplotlib.\n\nIf passed, then used to form histograms for separate groups.\n\nIf not passed, uses gca().\n\nWhether to show axis grid lines.\n\nIf specified changes the x-axis label size.\n\nRotation of x axis labels.\n\nIf specified changes the y-axis label size.\n\nRotation of y axis labels.\n\nFigure size in inches by default.\n\nNumber of histogram bins to be used. If an integer is given, bins + 1 bin\nedges are calculated and returned. If bins is a sequence, gives bin edges,\nincluding left edge of first bin and right edge of last bin. In this case,\nbins is returned unmodified.\n\nBackend to use instead of the backend specified in the option\n`plotting.backend`. For instance, \u2018matplotlib\u2019. Alternatively, to specify the\n`plotting.backend` for the whole session, set `pd.options.plotting.backend`.\n\nNew in version 1.0.0.\n\nWhether to show the legend.\n\nNew in version 1.1.0.\n\nTo be passed to the actual plotting function.\n\nA histogram plot.\n\nSee also\n\nPlot a histogram using matplotlib.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.is_monotonic_decreasing", "path": "reference/api/pandas.core.groupby.seriesgroupby.is_monotonic_decreasing", "type": "Series", "text": "\nReturn boolean if values in the object are monotonic_decreasing.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.is_monotonic_increasing", "path": "reference/api/pandas.core.groupby.seriesgroupby.is_monotonic_increasing", "type": "Series", "text": "\nAlias for is_monotonic.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.nlargest", "path": "reference/api/pandas.core.groupby.seriesgroupby.nlargest", "type": "Series", "text": "\nReturn the largest n elements.\n\nReturn this many descending sorted values.\n\nWhen there are duplicate values that cannot all fit in a Series of n elements:\n\n`first` : return the first n occurrences in order of appearance.\n\n`last` : return the last n occurrences in reverse order of appearance.\n\n`all` : keep all occurrences. This can result in a Series of size larger than\nn.\n\nThe n largest values in the Series, sorted in decreasing order.\n\nSee also\n\nGet the n smallest elements.\n\nSort Series by values.\n\nReturn the first n rows.\n\nNotes\n\nFaster than `.sort_values(ascending=False).head(n)` for small n relative to\nthe size of the `Series` object.\n\nExamples\n\nThe n largest elements where `n=5` by default.\n\nThe n largest elements where `n=3`. Default keep value is \u2018first\u2019 so Malta\nwill be kept.\n\nThe n largest elements where `n=3` and keeping the last duplicates. Brunei\nwill be kept since it is the last with value 434000 based on the index order.\n\nThe n largest elements where `n=3` with all duplicates kept. Note that the\nreturned Series has five elements due to the three duplicates.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.nsmallest", "path": "reference/api/pandas.core.groupby.seriesgroupby.nsmallest", "type": "Series", "text": "\nReturn the smallest n elements.\n\nReturn this many ascending sorted values.\n\nWhen there are duplicate values that cannot all fit in a Series of n elements:\n\n`first` : return the first n occurrences in order of appearance.\n\n`last` : return the last n occurrences in reverse order of appearance.\n\n`all` : keep all occurrences. This can result in a Series of size larger than\nn.\n\nThe n smallest values in the Series, sorted in increasing order.\n\nSee also\n\nGet the n largest elements.\n\nSort Series by values.\n\nReturn the first n rows.\n\nNotes\n\nFaster than `.sort_values().head(n)` for small n relative to the size of the\n`Series` object.\n\nExamples\n\nThe n smallest elements where `n=5` by default.\n\nThe n smallest elements where `n=3`. Default keep value is \u2018first\u2019 so Nauru\nand Tuvalu will be kept.\n\nThe n smallest elements where `n=3` and keeping the last duplicates. Anguilla\nand Tuvalu will be kept since they are the last with value 11300 based on the\nindex order.\n\nThe n smallest elements where `n=3` with all duplicates kept. Note that the\nreturned Series has four elements due to the three duplicates.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.nunique", "path": "reference/api/pandas.core.groupby.seriesgroupby.nunique", "type": "Series", "text": "\nReturn number of unique elements in the group.\n\nNumber of unique values within each group.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.transform", "path": "reference/api/pandas.core.groupby.seriesgroupby.transform", "type": "Series", "text": "\nCall function producing a like-indexed Series on each group and return a\nSeries having the same indexes as the original object filled with the\ntransformed values.\n\nFunction to apply to each group.\n\nCan also accept a Numba JIT function with `engine='numba'` specified.\n\nIf the `'numba'` engine is chosen, the function must be a user defined\nfunction with `values` and `index` as the first and second arguments\nrespectively in the function signature. Each group\u2019s index will be passed to\nthe user defined function and optionally available for use.\n\nChanged in version 1.1.0.\n\nPositional arguments to pass to func.\n\n`'cython'` : Runs the function through C-extensions from cython.\n\n`'numba'` : Runs the function through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or the global setting `compute.use_numba`\n\nNew in version 1.1.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}` and will be applied to the function\n\nNew in version 1.1.0.\n\nKeyword arguments to be passed into func.\n\nSee also\n\nApply function `func` group-wise and combine the results together.\n\nAggregate using one or more operations over the specified axis.\n\nCall `func` on self producing a Series with the same axis shape as self.\n\nNotes\n\nEach group is endowed the attribute \u2018name\u2019 in case you need to know which\ngroup you are working on.\n\nThe current implementation imposes three requirements on f:\n\nf must return a value that either has the same shape as the input subframe or\ncan be broadcast to the shape of the input subframe. For example, if f returns\na scalar it will be broadcast to have the same shape as the input subframe.\n\nif this is a DataFrame, f must support application column-by-column in the\nsubframe. If f also supports application to the entire subframe, then a fast\npath is used starting from the second chunk.\n\nf must not mutate groups. Mutation is not supported and may produce unexpected\nresults. See Mutating with User Defined Function (UDF) methods for more\ndetails.\n\nWhen using `engine='numba'`, there will be no \u201cfall back\u201d behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe passed `func`, see the examples below.\n\nExamples\n\nBroadcast result of the transformation\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe passed `func`, for example:\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.unique", "path": "reference/api/pandas.core.groupby.seriesgroupby.unique", "type": "Series", "text": "\nReturn unique values of Series object.\n\nUniques are returned in order of appearance. Hash table-based unique,\ntherefore does NOT sort.\n\nThe unique values returned as a NumPy array. See Notes.\n\nSee also\n\nTop-level unique method for any 1-d array-like object.\n\nReturn Index with unique values from an Index object.\n\nNotes\n\nReturns the unique values as a NumPy array. In case of an extension-array\nbacked Series, a new `ExtensionArray` of that type with just the unique values\nis returned. This includes\n\nCategorical\n\nPeriod\n\nDatetime with Timezone\n\nInterval\n\nSparse\n\nIntegerNA\n\nSee Examples section.\n\nExamples\n\nAn Categorical will return categories in the order of appearance and with the\nsame dtype.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.value_counts", "path": "reference/api/pandas.core.groupby.seriesgroupby.value_counts", "type": "Series", "text": "\n\n"}, {"name": "pandas.core.resample.Resampler.__iter__", "path": "reference/api/pandas.core.resample.resampler.__iter__", "type": "Resampling", "text": "\nGroupby iterator.\n\n"}, {"name": "pandas.core.resample.Resampler.aggregate", "path": "reference/api/pandas.core.resample.resampler.aggregate", "type": "Resampling", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a DataFrame or when passed to DataFrame.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nPositional arguments to pass to func.\n\nKeyword arguments to pass to func.\n\nThe return can be:\n\nscalar : when Series.agg is called with single function\n\nSeries : when DataFrame.agg is called with a single function\n\nDataFrame : when DataFrame.agg is called with several functions\n\nReturn scalar, Series or DataFrame.\n\nSee also\n\nAggregate using callable, string, dict, or list of string/callables.\n\nTransforms the Series on each group based on the given function.\n\nAggregate using one or more operations over the specified axis.\n\nNotes\n\nagg is an alias for aggregate. Use the alias.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nA passed user-defined-function will be passed a Series for evaluation.\n\nExamples\n\n"}, {"name": "pandas.core.resample.Resampler.apply", "path": "reference/api/pandas.core.resample.resampler.apply", "type": "Resampling", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a DataFrame or when passed to DataFrame.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nPositional arguments to pass to func.\n\nKeyword arguments to pass to func.\n\nThe return can be:\n\nscalar : when Series.agg is called with single function\n\nSeries : when DataFrame.agg is called with a single function\n\nDataFrame : when DataFrame.agg is called with several functions\n\nReturn scalar, Series or DataFrame.\n\nSee also\n\nAggregate using callable, string, dict, or list of string/callables.\n\nTransforms the Series on each group based on the given function.\n\nAggregate using one or more operations over the specified axis.\n\nNotes\n\nagg is an alias for aggregate. Use the alias.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nA passed user-defined-function will be passed a Series for evaluation.\n\nExamples\n\n"}, {"name": "pandas.core.resample.Resampler.asfreq", "path": "reference/api/pandas.core.resample.resampler.asfreq", "type": "Resampling", "text": "\nReturn the values at the new freq, essentially a reindex.\n\nValue to use for missing values, applied during upsampling (note this does not\nfill NaNs that already were present).\n\nValues at the specified freq.\n\nSee also\n\nConvert TimeSeries to specified frequency.\n\nConvert TimeSeries to specified frequency.\n\n"}, {"name": "pandas.core.resample.Resampler.backfill", "path": "reference/api/pandas.core.resample.resampler.backfill", "type": "Resampling", "text": "\nBackward fill the new missing values in the resampled data.\n\nIn statistics, imputation is the process of replacing missing data with\nsubstituted values [1]. When resampling data, missing values may appear (e.g.,\nwhen the resampling frequency is higher than the original frequency). The\nbackward fill will replace NaN values that appeared in the resampled data with\nthe next value in the original sequence. Missing values that existed in the\noriginal data will not be modified.\n\nLimit of how many values to fill.\n\nAn upsampled Series or DataFrame with backward filled NaN values.\n\nSee also\n\nAlias of backfill.\n\nFill NaN values using the specified method, which can be \u2018backfill\u2019.\n\nFill NaN values with nearest neighbor starting from center.\n\nForward fill NaN values.\n\nFill NaN values in the Series using the specified method, which can be\n\u2018backfill\u2019.\n\nFill NaN values in the DataFrame using the specified method, which can be\n\u2018backfill\u2019.\n\nReferences\n\nhttps://en.wikipedia.org/wiki/Imputation_(statistics)\n\nExamples\n\nResampling a Series:\n\nResampling a DataFrame that has missing values:\n\n"}, {"name": "pandas.core.resample.Resampler.bfill", "path": "reference/api/pandas.core.resample.resampler.bfill", "type": "Resampling", "text": "\nBackward fill the new missing values in the resampled data.\n\nIn statistics, imputation is the process of replacing missing data with\nsubstituted values [1]. When resampling data, missing values may appear (e.g.,\nwhen the resampling frequency is higher than the original frequency). The\nbackward fill will replace NaN values that appeared in the resampled data with\nthe next value in the original sequence. Missing values that existed in the\noriginal data will not be modified.\n\nLimit of how many values to fill.\n\nAn upsampled Series or DataFrame with backward filled NaN values.\n\nSee also\n\nAlias of backfill.\n\nFill NaN values using the specified method, which can be \u2018backfill\u2019.\n\nFill NaN values with nearest neighbor starting from center.\n\nForward fill NaN values.\n\nFill NaN values in the Series using the specified method, which can be\n\u2018backfill\u2019.\n\nFill NaN values in the DataFrame using the specified method, which can be\n\u2018backfill\u2019.\n\nReferences\n\nhttps://en.wikipedia.org/wiki/Imputation_(statistics)\n\nExamples\n\nResampling a Series:\n\nResampling a DataFrame that has missing values:\n\n"}, {"name": "pandas.core.resample.Resampler.count", "path": "reference/api/pandas.core.resample.resampler.count", "type": "Resampling", "text": "\nCompute count of group, excluding missing values.\n\nCount of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.resample.Resampler.ffill", "path": "reference/api/pandas.core.resample.resampler.ffill", "type": "Resampling", "text": "\nForward fill the values.\n\nLimit of how many values to fill.\n\nSee also\n\nFill NA/NaN values using the specified method.\n\nFill NA/NaN values using the specified method.\n\n"}, {"name": "pandas.core.resample.Resampler.fillna", "path": "reference/api/pandas.core.resample.resampler.fillna", "type": "Resampling", "text": "\nFill missing values introduced by upsampling.\n\nIn statistics, imputation is the process of replacing missing data with\nsubstituted values [1]. When resampling data, missing values may appear (e.g.,\nwhen the resampling frequency is higher than the original frequency).\n\nMissing values that existed in the original data will not be modified.\n\nMethod to use for filling holes in resampled data\n\n\u2018pad\u2019 or \u2018ffill\u2019: use previous valid observation to fill gap (forward fill).\n\n\u2018backfill\u2019 or \u2018bfill\u2019: use next valid observation to fill gap.\n\n\u2018nearest\u2019: use nearest valid observation to fill gap.\n\nLimit of how many consecutive missing values to fill.\n\nAn upsampled Series or DataFrame with missing values filled.\n\nSee also\n\nBackward fill NaN values in the resampled data.\n\nForward fill NaN values in the resampled data.\n\nFill NaN values in the resampled data with nearest neighbor starting from\ncenter.\n\nFill NaN values using interpolation.\n\nFill NaN values in the Series using the specified method, which can be \u2018bfill\u2019\nand \u2018ffill\u2019.\n\nFill NaN values in the DataFrame using the specified method, which can be\n\u2018bfill\u2019 and \u2018ffill\u2019.\n\nReferences\n\nhttps://en.wikipedia.org/wiki/Imputation_(statistics)\n\nExamples\n\nResampling a Series:\n\nWithout filling the missing values you get:\n\nMissing values present before the upsampling are not affected.\n\nDataFrame resampling is done column-wise. All the same options are available.\n\n"}, {"name": "pandas.core.resample.Resampler.first", "path": "reference/api/pandas.core.resample.resampler.first", "type": "Resampling", "text": "\nCompute first of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed first of values within each group.\n\n"}, {"name": "pandas.core.resample.Resampler.get_group", "path": "reference/api/pandas.core.resample.resampler.get_group", "type": "Resampling", "text": "\nConstruct DataFrame from group with provided name.\n\nThe name of the group to get as a DataFrame.\n\nThe DataFrame to take the DataFrame out of. If it is None, the object groupby\nwas called on will be used.\n\n"}, {"name": "pandas.core.resample.Resampler.groups", "path": "reference/api/pandas.core.resample.resampler.groups", "type": "Resampling", "text": "\nDict {group name -> group labels}.\n\n"}, {"name": "pandas.core.resample.Resampler.indices", "path": "reference/api/pandas.core.resample.resampler.indices", "type": "Resampling", "text": "\nDict {group name -> group indices}.\n\n"}, {"name": "pandas.core.resample.Resampler.interpolate", "path": "reference/api/pandas.core.resample.resampler.interpolate", "type": "Resampling", "text": "\nInterpolate values according to different methods.\n\nFill NaN values using an interpolation method.\n\nPlease note that only `method='linear'` is supported for DataFrame/Series with\na MultiIndex.\n\nInterpolation technique to use. One of:\n\n\u2018linear\u2019: Ignore the index and treat the values as equally spaced. This is the\nonly method supported on MultiIndexes.\n\n\u2018time\u2019: Works on daily and higher resolution data to interpolate given length\nof interval.\n\n\u2018index\u2019, \u2018values\u2019: use the actual numerical values of the index.\n\n\u2018pad\u2019: Fill in NaNs using existing values.\n\n\u2018nearest\u2019, \u2018zero\u2019, \u2018slinear\u2019, \u2018quadratic\u2019, \u2018cubic\u2019, \u2018spline\u2019, \u2018barycentric\u2019,\n\u2018polynomial\u2019: Passed to scipy.interpolate.interp1d. These methods use the\nnumerical values of the index. Both \u2018polynomial\u2019 and \u2018spline\u2019 require that you\nalso specify an order (int), e.g. `df.interpolate(method='polynomial',\norder=5)`.\n\n\u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019, \u2018akima\u2019, \u2018cubicspline\u2019:\nWrappers around the SciPy interpolation methods of similar names. See Notes.\n\n\u2018from_derivatives\u2019: Refers to scipy.interpolate.BPoly.from_derivatives which\nreplaces \u2018piecewise_polynomial\u2019 interpolation method in scipy 0.18.\n\nAxis to interpolate along.\n\nMaximum number of consecutive NaNs to fill. Must be greater than 0.\n\nUpdate the data in place if possible.\n\nConsecutive NaNs will be filled in this direction.\n\nIf \u2018method\u2019 is \u2018pad\u2019 or \u2018ffill\u2019, \u2018limit_direction\u2019 must be \u2018forward\u2019.\n\nIf \u2018method\u2019 is \u2018backfill\u2019 or \u2018bfill\u2019, \u2018limit_direction\u2019 must be \u2018backwards\u2019.\n\nIf \u2018method\u2019 is \u2018backfill\u2019 or \u2018bfill\u2019, the default is \u2018backward\u2019\n\nelse the default is \u2018forward\u2019\n\nChanged in version 1.1.0: raises ValueError if limit_direction is \u2018forward\u2019 or\n\u2018both\u2019 and method is \u2018backfill\u2019 or \u2018bfill\u2019. raises ValueError if\nlimit_direction is \u2018backward\u2019 or \u2018both\u2019 and method is \u2018pad\u2019 or \u2018ffill\u2019.\n\nIf limit is specified, consecutive NaNs will be filled with this restriction.\n\n`None`: No fill restriction.\n\n\u2018inside\u2019: Only fill NaNs surrounded by valid values (interpolate).\n\n\u2018outside\u2019: Only fill NaNs outside valid values (extrapolate).\n\nDowncast dtypes if possible.\n\nKeyword arguments to pass on to the interpolating function.\n\nReturns the same object type as the caller, interpolated at some or all `NaN`\nvalues or None if `inplace=True`.\n\nSee also\n\nFill missing values using different methods.\n\nPiecewise cubic polynomials (Akima interpolator).\n\nPiecewise polynomial in the Bernstein basis.\n\nInterpolate a 1-D function.\n\nInterpolate polynomial (Krogh interpolator).\n\nPCHIP 1-d monotonic cubic interpolation.\n\nCubic spline data interpolator.\n\nNotes\n\nThe \u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019 and \u2018akima\u2019 methods are\nwrappers around the respective SciPy implementations of similar names. These\nuse the actual numerical values of the index. For more information on their\nbehavior, see the SciPy documentation and SciPy tutorial.\n\nExamples\n\nFilling in `NaN` in a `Series` via linear interpolation.\n\nFilling in `NaN` in a Series by padding, but filling at most two consecutive\n`NaN` at a time.\n\nFilling in `NaN` in a Series via polynomial interpolation or splines: Both\n\u2018polynomial\u2019 and \u2018spline\u2019 methods require that you also specify an `order`\n(int).\n\nFill the DataFrame forward (that is, going down) along each column using\nlinear interpolation.\n\nNote how the last entry in column \u2018a\u2019 is interpolated differently, because\nthere is no entry after it to use for interpolation. Note how the first entry\nin column \u2018b\u2019 remains `NaN`, because there is no entry before it to use for\ninterpolation.\n\nUsing polynomial interpolation.\n\n"}, {"name": "pandas.core.resample.Resampler.last", "path": "reference/api/pandas.core.resample.resampler.last", "type": "Resampling", "text": "\nCompute last of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed last of values within each group.\n\n"}, {"name": "pandas.core.resample.Resampler.max", "path": "reference/api/pandas.core.resample.resampler.max", "type": "Resampling", "text": "\nCompute max of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed max of values within each group.\n\n"}, {"name": "pandas.core.resample.Resampler.mean", "path": "reference/api/pandas.core.resample.resampler.mean", "type": "Resampling", "text": "\nCompute mean of groups, excluding missing values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.4.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{{'nopython': True, 'nogil':\nFalse, 'parallel': False}}`\n\nNew in version 1.4.0.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\nExamples\n\nGroupby one column and return the mean of the remaining columns in each group.\n\nGroupby two columns and return the mean of the remaining column.\n\nGroupby one column and return the mean of only particular column in the group.\n\n"}, {"name": "pandas.core.resample.Resampler.median", "path": "reference/api/pandas.core.resample.resampler.median", "type": "Resampling", "text": "\nCompute median of groups, excluding missing values.\n\nFor multiple groupings, the result index will be a MultiIndex\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nMedian of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.resample.Resampler.min", "path": "reference/api/pandas.core.resample.resampler.min", "type": "Resampling", "text": "\nCompute min of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed min of values within each group.\n\n"}, {"name": "pandas.core.resample.Resampler.nearest", "path": "reference/api/pandas.core.resample.resampler.nearest", "type": "Resampling", "text": "\nResample by using the nearest value.\n\nWhen resampling data, missing values may appear (e.g., when the resampling\nfrequency is higher than the original frequency). The nearest method will\nreplace `NaN` values that appeared in the resampled data with the value from\nthe nearest member of the sequence, based on the index value. Missing values\nthat existed in the original data will not be modified. If limit is given,\nfill only this many values in each direction for each of the original values.\n\nLimit of how many values to fill.\n\nAn upsampled Series or DataFrame with `NaN` values filled with their nearest\nvalue.\n\nSee also\n\nBackward fill the new missing values in the resampled data.\n\nForward fill `NaN` values.\n\nExamples\n\nLimit the number of upsampled values imputed by the nearest:\n\n"}, {"name": "pandas.core.resample.Resampler.nunique", "path": "reference/api/pandas.core.resample.resampler.nunique", "type": "Resampling", "text": "\nReturn number of unique elements in the group.\n\nNumber of unique values within each group.\n\n"}, {"name": "pandas.core.resample.Resampler.ohlc", "path": "reference/api/pandas.core.resample.resampler.ohlc", "type": "Resampling", "text": "\nCompute open, high, low and close values of a group, excluding missing values.\n\nFor multiple groupings, the result index will be a MultiIndex\n\nOpen, high, low and close values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.resample.Resampler.pad", "path": "reference/api/pandas.core.resample.resampler.pad", "type": "Resampling", "text": "\nForward fill the values.\n\nLimit of how many values to fill.\n\nSee also\n\nFill NA/NaN values using the specified method.\n\nFill NA/NaN values using the specified method.\n\n"}, {"name": "pandas.core.resample.Resampler.pipe", "path": "reference/api/pandas.core.resample.resampler.pipe", "type": "Resampling", "text": "\nApply a function func with arguments to this Resampler object and return the\nfunction\u2019s result.\n\nUse .pipe when you want to improve readability by chaining together functions\nthat expect Series, DataFrames, GroupBy or Resampler objects. Instead of\nwriting\n\nYou can write\n\nwhich is much more readable.\n\nFunction to apply to this Resampler object or, alternatively, a (callable,\ndata_keyword) tuple where data_keyword is a string indicating the keyword of\ncallable that expects the Resampler object.\n\nPositional arguments passed into func.\n\nA dictionary of keyword arguments passed into func.\n\nSee also\n\nApply a function with arguments to a series.\n\nApply a function with arguments to a dataframe.\n\nApply function to each group instead of to the full Resampler object.\n\nNotes\n\nSee more here\n\nExamples\n\nTo get the difference between each 2-day period\u2019s maximum and minimum value in\none pass, you can do\n\n"}, {"name": "pandas.core.resample.Resampler.prod", "path": "reference/api/pandas.core.resample.resampler.prod", "type": "Resampling", "text": "\nCompute prod of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed prod of values within each group.\n\n"}, {"name": "pandas.core.resample.Resampler.quantile", "path": "reference/api/pandas.core.resample.resampler.quantile", "type": "Resampling", "text": "\nReturn value at the given quantile.\n\nQuantile of values within each group.\n\nSee also\n\nReturn a series, where the index is q and the values are the quantiles.\n\nReturn a DataFrame, where the columns are the columns of self, and the values\nare the quantiles.\n\nReturn a DataFrame, where the coulmns are groupby columns, and the values are\nits quantiles.\n\n"}, {"name": "pandas.core.resample.Resampler.sem", "path": "reference/api/pandas.core.resample.resampler.sem", "type": "Resampling", "text": "\nCompute standard error of the mean of groups, excluding missing values.\n\nFor multiple groupings, the result index will be a MultiIndex.\n\nDegrees of freedom.\n\nStandard error of the mean of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.resample.Resampler.size", "path": "reference/api/pandas.core.resample.resampler.size", "type": "Resampling", "text": "\nCompute group sizes.\n\nNumber of rows in each group as a Series if as_index is True or a DataFrame if\nas_index is False.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.resample.Resampler.std", "path": "reference/api/pandas.core.resample.resampler.std", "type": "Resampling", "text": "\nCompute standard deviation of groups, excluding missing values.\n\nDegrees of freedom.\n\nStandard deviation of values within each group.\n\n"}, {"name": "pandas.core.resample.Resampler.sum", "path": "reference/api/pandas.core.resample.resampler.sum", "type": "Resampling", "text": "\nCompute sum of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed sum of values within each group.\n\n"}, {"name": "pandas.core.resample.Resampler.transform", "path": "reference/api/pandas.core.resample.resampler.transform", "type": "Resampling", "text": "\nCall function producing a like-indexed Series on each group and return a\nSeries with the transformed values.\n\nTo apply to each group. Should return a Series with the same index.\n\nExamples\n\n"}, {"name": "pandas.core.resample.Resampler.var", "path": "reference/api/pandas.core.resample.resampler.var", "type": "Resampling", "text": "\nCompute variance of groups, excluding missing values.\n\nDegrees of freedom.\n\nVariance of values within each group.\n\n"}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.corr", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.corr", "type": "Window", "text": "\nCalculate the ewm (exponential weighted moment) sample correlation.\n\nIf not supplied then will default to self and produce pairwise output.\n\nIf False then only matching columns between self and other will be used and\nthe output will be a DataFrame. If True then all pairwise combinations will be\ncalculated and the output will be a MultiIndex DataFrame in the case of\nDataFrame inputs. In the case of missing elements, only complete pairwise\nobservations will be used.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling ewm with Series data.\n\nCalling ewm with DataFrames.\n\nAggregating corr for Series.\n\nAggregating corr for DataFrame.\n\n"}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.cov", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.cov", "type": "Window", "text": "\nCalculate the ewm (exponential weighted moment) sample covariance.\n\nIf not supplied then will default to self and produce pairwise output.\n\nIf False then only matching columns between self and other will be used and\nthe output will be a DataFrame. If True then all pairwise combinations will be\ncalculated and the output will be a MultiIndex DataFrame in the case of\nDataFrame inputs. In the case of missing elements, only complete pairwise\nobservations will be used.\n\nUse a standard estimation bias correction.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling ewm with Series data.\n\nCalling ewm with DataFrames.\n\nAggregating cov for Series.\n\nAggregating cov for DataFrame.\n\n"}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.mean", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.mean", "type": "Window", "text": "\nCalculate the ewm (exponential weighted moment) mean.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling ewm with Series data.\n\nCalling ewm with DataFrames.\n\nAggregating mean for Series.\n\nAggregating mean for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\n"}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.std", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.std", "type": "Window", "text": "\nCalculate the ewm (exponential weighted moment) standard deviation.\n\nUse a standard estimation bias correction.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling ewm with Series data.\n\nCalling ewm with DataFrames.\n\nAggregating std for Series.\n\nAggregating std for DataFrame.\n\n"}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.sum", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.sum", "type": "Window", "text": "\nCalculate the ewm (exponential weighted moment) sum.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling ewm with Series data.\n\nCalling ewm with DataFrames.\n\nAggregating sum for Series.\n\nAggregating sum for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\n"}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.var", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.var", "type": "Window", "text": "\nCalculate the ewm (exponential weighted moment) variance.\n\nUse a standard estimation bias correction.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling ewm with Series data.\n\nCalling ewm with DataFrames.\n\nAggregating var for Series.\n\nAggregating var for DataFrame.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.aggregate", "path": "reference/api/pandas.core.window.expanding.expanding.aggregate", "type": "Window", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a Series/Dataframe or when passed to Series/Dataframe.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nPositional arguments to pass to func.\n\nKeyword arguments to pass to func.\n\nThe return can be:\n\nscalar : when Series.agg is called with single function\n\nSeries : when DataFrame.agg is called with a single function\n\nDataFrame : when DataFrame.agg is called with several functions\n\nReturn scalar, Series or DataFrame.\n\nSee also\n\nSimilar DataFrame method.\n\nSimilar Series method.\n\nNotes\n\nagg is an alias for aggregate. Use the alias.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nA passed user-defined-function will be passed a Series for evaluation.\n\nExamples\n\n"}, {"name": "pandas.core.window.expanding.Expanding.apply", "path": "reference/api/pandas.core.window.expanding.expanding.apply", "type": "Window", "text": "\nCalculate the expanding custom aggregation function.\n\nMust produce a single value from an ndarray input if `raw=True` or a single\nvalue from a Series if `raw=False`. Can also accept a Numba JIT function with\n`engine='numba'` specified.\n\nChanged in version 1.0.0.\n\n`False` : passes each row or column as a Series to the function.\n\n`True` : the passed function will receive ndarray objects instead. If you are\njust applying a NumPy reduction function this will achieve much better\nperformance.\n\n`'cython'` : Runs rolling apply through C-extensions from cython.\n\n`'numba'` : Runs rolling apply through JIT compiled code from numba. Only\navailable when `raw` is set to `True`.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.0.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}` and will be applied to both the `func` and the\n`apply` rolling aggregation.\n\nNew in version 1.0.0.\n\nPositional arguments to be passed into func.\n\nKeyword arguments to be passed into func.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating apply for Series.\n\nAggregating apply for DataFrame.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.corr", "path": "reference/api/pandas.core.window.expanding.expanding.corr", "type": "Window", "text": "\nCalculate the expanding correlation.\n\nIf not supplied then will default to self and produce pairwise output.\n\nIf False then only matching columns between self and other will be used and\nthe output will be a DataFrame. If True then all pairwise combinations will be\ncalculated and the output will be a MultiIndexed DataFrame in the case of\nDataFrame inputs. In the case of missing elements, only complete pairwise\nobservations will be used.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nSimilar method to calculate covariance.\n\nNumPy Pearson\u2019s correlation calculation.\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating corr for Series.\n\nAggregating corr for DataFrame.\n\nNotes\n\nThis function uses Pearson\u2019s definition of correlation\n(https://en.wikipedia.org/wiki/Pearson_correlation_coefficient).\n\nWhen other is not specified, the output will be self correlation (e.g. all\n1\u2019s), except for `DataFrame` inputs with pairwise set to True.\n\nFunction will return `NaN` for correlations of equal valued sequences; this is\nthe result of a 0/0 division error.\n\nWhen pairwise is set to False, only matching columns between self and other\nwill be used.\n\nWhen pairwise is set to True, the output will be a MultiIndex DataFrame with\nthe original index on the first level, and the other DataFrame columns on the\nsecond level.\n\nIn the case of missing elements, only complete pairwise observations will be\nused.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.count", "path": "reference/api/pandas.core.window.expanding.expanding.count", "type": "Window", "text": "\nCalculate the expanding count of non NaN observations.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating count for Series.\n\nAggregating count for DataFrame.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.cov", "path": "reference/api/pandas.core.window.expanding.expanding.cov", "type": "Window", "text": "\nCalculate the expanding sample covariance.\n\nIf not supplied then will default to self and produce pairwise output.\n\nIf False then only matching columns between self and other will be used and\nthe output will be a DataFrame. If True then all pairwise combinations will be\ncalculated and the output will be a MultiIndexed DataFrame in the case of\nDataFrame inputs. In the case of missing elements, only complete pairwise\nobservations will be used.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating cov for Series.\n\nAggregating cov for DataFrame.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.kurt", "path": "reference/api/pandas.core.window.expanding.expanding.kurt", "type": "Window", "text": "\nCalculate the expanding Fisher\u2019s definition of kurtosis without bias.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nReference SciPy method.\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating kurt for Series.\n\nAggregating kurt for DataFrame.\n\nNotes\n\nA minimum of four periods is required for the calculation.\n\nExamples\n\nThe example below will show a rolling calculation with a window size of four\nmatching the equivalent function call using scipy.stats.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.max", "path": "reference/api/pandas.core.window.expanding.expanding.max", "type": "Window", "text": "\nCalculate the expanding maximum.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating max for Series.\n\nAggregating max for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.mean", "path": "reference/api/pandas.core.window.expanding.expanding.mean", "type": "Window", "text": "\nCalculate the expanding mean.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating mean for Series.\n\nAggregating mean for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.median", "path": "reference/api/pandas.core.window.expanding.expanding.median", "type": "Window", "text": "\nCalculate the expanding median.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating median for Series.\n\nAggregating median for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.min", "path": "reference/api/pandas.core.window.expanding.expanding.min", "type": "Window", "text": "\nCalculate the expanding minimum.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating min for Series.\n\nAggregating min for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.quantile", "path": "reference/api/pandas.core.window.expanding.expanding.quantile", "type": "Window", "text": "\nCalculate the expanding quantile.\n\nQuantile to compute. 0 <= quantile <= 1.\n\nThis optional parameter specifies the interpolation method to use, when the\ndesired quantile lies between two data points i and j:\n\nlinear: i + (j - i) * fraction, where fraction is the fractional part of the\nindex surrounded by i and j.\n\nlower: i.\n\nhigher: j.\n\nnearest: i or j whichever is nearest.\n\nmidpoint: (i \\+ j) / 2.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating quantile for Series.\n\nAggregating quantile for DataFrame.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.rank", "path": "reference/api/pandas.core.window.expanding.expanding.rank", "type": "Window", "text": "\nCalculate the expanding rank.\n\nNew in version 1.4.0.\n\nHow to rank the group of records that have the same value (i.e. ties):\n\naverage: average rank of the group\n\nmin: lowest rank in the group\n\nmax: highest rank in the group\n\nWhether or not the elements should be ranked in ascending order.\n\nWhether or not to display the returned rankings in percentile form.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating rank for Series.\n\nAggregating rank for DataFrame.\n\nExamples\n\n"}, {"name": "pandas.core.window.expanding.Expanding.sem", "path": "reference/api/pandas.core.window.expanding.expanding.sem", "type": "Window", "text": "\nCalculate the expanding standard error of mean.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating sem for Series.\n\nAggregating sem for DataFrame.\n\nNotes\n\nA minimum of one period is required for the calculation.\n\nExamples\n\n"}, {"name": "pandas.core.window.expanding.Expanding.skew", "path": "reference/api/pandas.core.window.expanding.expanding.skew", "type": "Window", "text": "\nCalculate the expanding unbiased skewness.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nThird moment of a probability density.\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating skew for Series.\n\nAggregating skew for DataFrame.\n\nNotes\n\nA minimum of three periods is required for the rolling calculation.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.std", "path": "reference/api/pandas.core.window.expanding.expanding.std", "type": "Window", "text": "\nCalculate the expanding standard deviation.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.4.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.4.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nEquivalent method for NumPy array.\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating std for Series.\n\nAggregating std for DataFrame.\n\nNotes\n\nThe default `ddof` of 1 used in `Series.std()` is different than the default\n`ddof` of 0 in `numpy.std()`.\n\nA minimum of one period is required for the rolling calculation.\n\nExamples\n\n"}, {"name": "pandas.core.window.expanding.Expanding.sum", "path": "reference/api/pandas.core.window.expanding.expanding.sum", "type": "Window", "text": "\nCalculate the expanding sum.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating sum for Series.\n\nAggregating sum for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.var", "path": "reference/api/pandas.core.window.expanding.expanding.var", "type": "Window", "text": "\nCalculate the expanding variance.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.4.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.4.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nEquivalent method for NumPy array.\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating var for Series.\n\nAggregating var for DataFrame.\n\nNotes\n\nThe default `ddof` of 1 used in `Series.var()` is different than the default\n`ddof` of 0 in `numpy.var()`.\n\nA minimum of one period is required for the rolling calculation.\n\nExamples\n\n"}, {"name": "pandas.core.window.rolling.Rolling.aggregate", "path": "reference/api/pandas.core.window.rolling.rolling.aggregate", "type": "Window", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a Series/Dataframe or when passed to Series/Dataframe.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nPositional arguments to pass to func.\n\nKeyword arguments to pass to func.\n\nThe return can be:\n\nscalar : when Series.agg is called with single function\n\nSeries : when DataFrame.agg is called with a single function\n\nDataFrame : when DataFrame.agg is called with several functions\n\nReturn scalar, Series or DataFrame.\n\nSee also\n\nCalling object with Series data.\n\nCalling object with DataFrame data.\n\nNotes\n\nagg is an alias for aggregate. Use the alias.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nA passed user-defined-function will be passed a Series for evaluation.\n\nExamples\n\n"}, {"name": "pandas.core.window.rolling.Rolling.apply", "path": "reference/api/pandas.core.window.rolling.rolling.apply", "type": "Window", "text": "\nCalculate the rolling custom aggregation function.\n\nMust produce a single value from an ndarray input if `raw=True` or a single\nvalue from a Series if `raw=False`. Can also accept a Numba JIT function with\n`engine='numba'` specified.\n\nChanged in version 1.0.0.\n\n`False` : passes each row or column as a Series to the function.\n\n`True` : the passed function will receive ndarray objects instead. If you are\njust applying a NumPy reduction function this will achieve much better\nperformance.\n\n`'cython'` : Runs rolling apply through C-extensions from cython.\n\n`'numba'` : Runs rolling apply through JIT compiled code from numba. Only\navailable when `raw` is set to `True`.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.0.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}` and will be applied to both the `func` and the\n`apply` rolling aggregation.\n\nNew in version 1.0.0.\n\nPositional arguments to be passed into func.\n\nKeyword arguments to be passed into func.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating apply for Series.\n\nAggregating apply for DataFrame.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.corr", "path": "reference/api/pandas.core.window.rolling.rolling.corr", "type": "Window", "text": "\nCalculate the rolling correlation.\n\nIf not supplied then will default to self and produce pairwise output.\n\nIf False then only matching columns between self and other will be used and\nthe output will be a DataFrame. If True then all pairwise combinations will be\ncalculated and the output will be a MultiIndexed DataFrame in the case of\nDataFrame inputs. In the case of missing elements, only complete pairwise\nobservations will be used.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nSimilar method to calculate covariance.\n\nNumPy Pearson\u2019s correlation calculation.\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating corr for Series.\n\nAggregating corr for DataFrame.\n\nNotes\n\nThis function uses Pearson\u2019s definition of correlation\n(https://en.wikipedia.org/wiki/Pearson_correlation_coefficient).\n\nWhen other is not specified, the output will be self correlation (e.g. all\n1\u2019s), except for `DataFrame` inputs with pairwise set to True.\n\nFunction will return `NaN` for correlations of equal valued sequences; this is\nthe result of a 0/0 division error.\n\nWhen pairwise is set to False, only matching columns between self and other\nwill be used.\n\nWhen pairwise is set to True, the output will be a MultiIndex DataFrame with\nthe original index on the first level, and the other DataFrame columns on the\nsecond level.\n\nIn the case of missing elements, only complete pairwise observations will be\nused.\n\nExamples\n\nThe below example shows a rolling calculation with a window size of four\nmatching the equivalent function call using `numpy.corrcoef()`.\n\nThe below example shows a similar rolling calculation on a DataFrame using the\npairwise option.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.count", "path": "reference/api/pandas.core.window.rolling.rolling.count", "type": "Window", "text": "\nCalculate the rolling count of non NaN observations.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating count for Series.\n\nAggregating count for DataFrame.\n\nExamples\n\n"}, {"name": "pandas.core.window.rolling.Rolling.cov", "path": "reference/api/pandas.core.window.rolling.rolling.cov", "type": "Window", "text": "\nCalculate the rolling sample covariance.\n\nIf not supplied then will default to self and produce pairwise output.\n\nIf False then only matching columns between self and other will be used and\nthe output will be a DataFrame. If True then all pairwise combinations will be\ncalculated and the output will be a MultiIndexed DataFrame in the case of\nDataFrame inputs. In the case of missing elements, only complete pairwise\nobservations will be used.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating cov for Series.\n\nAggregating cov for DataFrame.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.kurt", "path": "reference/api/pandas.core.window.rolling.rolling.kurt", "type": "Window", "text": "\nCalculate the rolling Fisher\u2019s definition of kurtosis without bias.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nReference SciPy method.\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating kurt for Series.\n\nAggregating kurt for DataFrame.\n\nNotes\n\nA minimum of four periods is required for the calculation.\n\nExamples\n\nThe example below will show a rolling calculation with a window size of four\nmatching the equivalent function call using scipy.stats.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.max", "path": "reference/api/pandas.core.window.rolling.rolling.max", "type": "Window", "text": "\nCalculate the rolling maximum.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating max for Series.\n\nAggregating max for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.mean", "path": "reference/api/pandas.core.window.rolling.rolling.mean", "type": "Window", "text": "\nCalculate the rolling mean.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating mean for Series.\n\nAggregating mean for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\nExamples\n\nThe below examples will show rolling mean calculations with window sizes of\ntwo and three, respectively.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.median", "path": "reference/api/pandas.core.window.rolling.rolling.median", "type": "Window", "text": "\nCalculate the rolling median.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating median for Series.\n\nAggregating median for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\nExamples\n\nCompute the rolling median of a series with a window size of 3.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.min", "path": "reference/api/pandas.core.window.rolling.rolling.min", "type": "Window", "text": "\nCalculate the rolling minimum.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating min for Series.\n\nAggregating min for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\nExamples\n\nPerforming a rolling minimum with a window size of 3.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.quantile", "path": "reference/api/pandas.core.window.rolling.rolling.quantile", "type": "Window", "text": "\nCalculate the rolling quantile.\n\nQuantile to compute. 0 <= quantile <= 1.\n\nThis optional parameter specifies the interpolation method to use, when the\ndesired quantile lies between two data points i and j:\n\nlinear: i + (j - i) * fraction, where fraction is the fractional part of the\nindex surrounded by i and j.\n\nlower: i.\n\nhigher: j.\n\nnearest: i or j whichever is nearest.\n\nmidpoint: (i \\+ j) / 2.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating quantile for Series.\n\nAggregating quantile for DataFrame.\n\nExamples\n\n"}, {"name": "pandas.core.window.rolling.Rolling.rank", "path": "reference/api/pandas.core.window.rolling.rolling.rank", "type": "Window", "text": "\nCalculate the rolling rank.\n\nNew in version 1.4.0.\n\nHow to rank the group of records that have the same value (i.e. ties):\n\naverage: average rank of the group\n\nmin: lowest rank in the group\n\nmax: highest rank in the group\n\nWhether or not the elements should be ranked in ascending order.\n\nWhether or not to display the returned rankings in percentile form.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating rank for Series.\n\nAggregating rank for DataFrame.\n\nExamples\n\n"}, {"name": "pandas.core.window.rolling.Rolling.sem", "path": "reference/api/pandas.core.window.rolling.rolling.sem", "type": "Window", "text": "\nCalculate the rolling standard error of mean.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating sem for Series.\n\nAggregating sem for DataFrame.\n\nNotes\n\nA minimum of one period is required for the calculation.\n\nExamples\n\n"}, {"name": "pandas.core.window.rolling.Rolling.skew", "path": "reference/api/pandas.core.window.rolling.rolling.skew", "type": "Window", "text": "\nCalculate the rolling unbiased skewness.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nThird moment of a probability density.\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating skew for Series.\n\nAggregating skew for DataFrame.\n\nNotes\n\nA minimum of three periods is required for the rolling calculation.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.std", "path": "reference/api/pandas.core.window.rolling.rolling.std", "type": "Window", "text": "\nCalculate the rolling standard deviation.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.4.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.4.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nEquivalent method for NumPy array.\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating std for Series.\n\nAggregating std for DataFrame.\n\nNotes\n\nThe default `ddof` of 1 used in `Series.std()` is different than the default\n`ddof` of 0 in `numpy.std()`.\n\nA minimum of one period is required for the rolling calculation.\n\nThe implementation is susceptible to floating point imprecision as shown in\nthe example below.\n\nExamples\n\n"}, {"name": "pandas.core.window.rolling.Rolling.sum", "path": "reference/api/pandas.core.window.rolling.rolling.sum", "type": "Window", "text": "\nCalculate the rolling sum.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating sum for Series.\n\nAggregating sum for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\nExamples\n\nFor DataFrame, each sum is computed column-wise.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.var", "path": "reference/api/pandas.core.window.rolling.rolling.var", "type": "Window", "text": "\nCalculate the rolling variance.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.4.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.4.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nEquivalent method for NumPy array.\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating var for Series.\n\nAggregating var for DataFrame.\n\nNotes\n\nThe default `ddof` of 1 used in `Series.var()` is different than the default\n`ddof` of 0 in `numpy.var()`.\n\nA minimum of one period is required for the rolling calculation.\n\nThe implementation is susceptible to floating point imprecision as shown in\nthe example below.\n\nExamples\n\n"}, {"name": "pandas.core.window.rolling.Window.mean", "path": "reference/api/pandas.core.window.rolling.window.mean", "type": "Window", "text": "\nCalculate the rolling weighted window mean.\n\nKeyword arguments to configure the `SciPy` weighted window type.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating mean for Series.\n\nAggregating mean for DataFrame.\n\n"}, {"name": "pandas.core.window.rolling.Window.std", "path": "reference/api/pandas.core.window.rolling.window.std", "type": "Window", "text": "\nCalculate the rolling weighted window standard deviation.\n\nNew in version 1.0.0.\n\nKeyword arguments to configure the `SciPy` weighted window type.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating std for Series.\n\nAggregating std for DataFrame.\n\n"}, {"name": "pandas.core.window.rolling.Window.sum", "path": "reference/api/pandas.core.window.rolling.window.sum", "type": "Window", "text": "\nCalculate the rolling weighted window sum.\n\nKeyword arguments to configure the `SciPy` weighted window type.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating sum for Series.\n\nAggregating sum for DataFrame.\n\n"}, {"name": "pandas.core.window.rolling.Window.var", "path": "reference/api/pandas.core.window.rolling.window.var", "type": "Window", "text": "\nCalculate the rolling weighted window variance.\n\nNew in version 1.0.0.\n\nKeyword arguments to configure the `SciPy` weighted window type.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating var for Series.\n\nAggregating var for DataFrame.\n\n"}, {"name": "pandas.crosstab", "path": "reference/api/pandas.crosstab", "type": "General functions", "text": "\nCompute a simple cross tabulation of two (or more) factors. By default\ncomputes a frequency table of the factors unless an array of values and an\naggregation function are passed.\n\nValues to group by in the rows.\n\nValues to group by in the columns.\n\nArray of values to aggregate according to the factors. Requires aggfunc be\nspecified.\n\nIf passed, must match number of row arrays passed.\n\nIf passed, must match number of column arrays passed.\n\nIf specified, requires values be specified as well.\n\nAdd row/column margins (subtotals).\n\nName of the row/column that will contain the totals when margins is True.\n\nDo not include columns whose entries are all NaN.\n\nNormalize by dividing all values by the sum of values.\n\nIf passed \u2018all\u2019 or True, will normalize over all values.\n\nIf passed \u2018index\u2019 will normalize over each row.\n\nIf passed \u2018columns\u2019 will normalize over each column.\n\nIf margins is True, will also normalize margin values.\n\nCross tabulation of the data.\n\nSee also\n\nReshape data based on column values.\n\nCreate a pivot table as a DataFrame.\n\nNotes\n\nAny Series passed will have their name attributes used unless row or column\nnames for the cross-tabulation are specified.\n\nAny input passed containing Categorical data will have all of its categories\nincluded in the cross-tabulation, even if the actual data does not contain any\ninstances of a particular category.\n\nIn the event that there aren\u2019t overlapping indexes an empty DataFrame will be\nreturned.\n\nExamples\n\nHere \u2018c\u2019 and \u2018f\u2019 are not represented in the data and will not be shown in the\noutput because dropna is True by default. Set dropna=False to preserve\ncategories with no data.\n\n"}, {"name": "pandas.cut", "path": "reference/api/pandas.cut", "type": "General functions", "text": "\nBin values into discrete intervals.\n\nUse cut when you need to segment and sort data values into bins. This function\nis also useful for going from a continuous variable to a categorical variable.\nFor example, cut could convert ages to groups of age ranges. Supports binning\ninto an equal number of bins, or a pre-specified array of bins.\n\nThe input array to be binned. Must be 1-dimensional.\n\nThe criteria to bin by.\n\nint : Defines the number of equal-width bins in the range of x. The range of x\nis extended by .1% on each side to include the minimum and maximum values of\nx.\n\nsequence of scalars : Defines the bin edges allowing for non-uniform width. No\nextension of the range of x is done.\n\nIntervalIndex : Defines the exact bins to be used. Note that IntervalIndex for\nbins must be non-overlapping.\n\nIndicates whether bins includes the rightmost edge or not. If `right == True`\n(the default), then the bins `[1, 2, 3, 4]` indicate (1,2], (2,3], (3,4]. This\nargument is ignored when bins is an IntervalIndex.\n\nSpecifies the labels for the returned bins. Must be the same length as the\nresulting bins. If False, returns only integer indicators of the bins. This\naffects the type of the output container (see below). This argument is ignored\nwhen bins is an IntervalIndex. If True, raises an error. When ordered=False,\nlabels must be provided.\n\nWhether to return the bins or not. Useful when bins is provided as a scalar.\n\nThe precision at which to store and display the bins labels.\n\nWhether the first interval should be left-inclusive or not.\n\nIf bin edges are not unique, raise ValueError or drop non-uniques.\n\nWhether the labels are ordered or not. Applies to returned types Categorical\nand Series (with Categorical dtype). If True, the resulting categorical will\nbe ordered. If False, the resulting categorical will be unordered (labels must\nbe provided).\n\nNew in version 1.1.0.\n\nAn array-like object representing the respective bin for each value of x. The\ntype depends on the value of labels.\n\nNone (default) : returns a Series for Series x or a Categorical for all other\ninputs. The values stored within are Interval dtype.\n\nsequence of scalars : returns a Series for Series x or a Categorical for all\nother inputs. The values stored within are whatever the type in the sequence\nis.\n\nFalse : returns an ndarray of integers.\n\nThe computed or specified bins. Only returned when retbins=True. For scalar or\nsequence bins, this is an ndarray with the computed bins. If set\nduplicates=drop, bins will drop non-unique bin. For an IntervalIndex bins,\nthis is equal to bins.\n\nSee also\n\nDiscretize variable into equal-sized buckets based on rank or based on sample\nquantiles.\n\nArray type for storing data that come from a fixed set of values.\n\nOne-dimensional array with axis labels (including time series).\n\nImmutable Index implementing an ordered, sliceable set.\n\nNotes\n\nAny NA values will be NA in the result. Out of bounds values will be NA in the\nresulting Series or Categorical object.\n\nExamples\n\nDiscretize into three equal-sized bins.\n\nDiscovers the same bins, but assign them specific labels. Notice that the\nreturned Categorical\u2019s categories are labels and is ordered.\n\n`ordered=False` will result in unordered categories when labels are passed.\nThis parameter can be used to allow non-unique labels:\n\n`labels=False` implies you just want the bins back.\n\nPassing a Series as an input returns a Series with categorical dtype:\n\nPassing a Series as an input returns a Series with mapping value. It is used\nto map numerically to intervals based on bins.\n\nUse drop optional when bins is not unique\n\nPassing an IntervalIndex for bins results in those categories exactly. Notice\nthat values not covered by the IntervalIndex are set to NaN. 0 is to the left\nof the first bin (which is closed on the right), and 1.5 falls between two\nbins.\n\n"}, {"name": "pandas.DataFrame", "path": "reference/api/pandas.dataframe", "type": "DataFrame", "text": "\nTwo-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns). Arithmetic\noperations align on both row and column labels. Can be thought of as a dict-\nlike container for Series objects. The primary pandas data structure.\n\nDict can contain Series, arrays, constants, dataclass or list-like objects. If\ndata is a dict, column order follows insertion-order. If a dict contains\nSeries which have an index defined, it is aligned by its index.\n\nChanged in version 0.25.0: If data is a list of dicts, column order follows\ninsertion-order.\n\nIndex to use for resulting frame. Will default to RangeIndex if no indexing\ninformation part of input data and no index provided.\n\nColumn labels to use for resulting frame when data does not have them,\ndefaulting to RangeIndex(0, 1, 2, \u2026, n). If data contains column labels, will\nperform column selection instead.\n\nData type to force. Only a single dtype is allowed. If None, infer.\n\nCopy data from inputs. For dict data, the default of None behaves like\n`copy=True`. For DataFrame or 2d ndarray input, the default of None behaves\nlike `copy=False`.\n\nChanged in version 1.3.0.\n\nSee also\n\nConstructor from tuples, also record arrays.\n\nFrom dicts of Series, arrays, or dicts.\n\nRead a comma-separated values (csv) file into DataFrame.\n\nRead general delimited file into DataFrame.\n\nRead text from clipboard into DataFrame.\n\nExamples\n\nConstructing DataFrame from a dictionary.\n\nNotice that the inferred dtype is int64.\n\nTo enforce a single dtype:\n\nConstructing DataFrame from a dictionary including Series:\n\nConstructing DataFrame from numpy ndarray:\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\nConstructing DataFrame from dataclass:\n\nAttributes\n\n`at`\n\nAccess a single value for a row/column label pair.\n\n`attrs`\n\nDictionary of global attributes of this dataset.\n\n`axes`\n\nReturn a list representing the axes of the DataFrame.\n\n`columns`\n\nThe column labels of the DataFrame.\n\n`dtypes`\n\nReturn the dtypes in the DataFrame.\n\n`empty`\n\nIndicator whether Series/DataFrame is empty.\n\n`flags`\n\nGet the properties associated with this pandas object.\n\n`iat`\n\nAccess a single value for a row/column pair by integer position.\n\n`iloc`\n\nPurely integer-location based indexing for selection by position.\n\n`index`\n\nThe index (row labels) of the DataFrame.\n\n`loc`\n\nAccess a group of rows and columns by label(s) or a boolean array.\n\n`ndim`\n\nReturn an int representing the number of axes / array dimensions.\n\n`shape`\n\nReturn a tuple representing the dimensionality of the DataFrame.\n\n`size`\n\nReturn an int representing the number of elements in this object.\n\n`style`\n\nReturns a Styler object.\n\n`values`\n\nReturn a Numpy representation of the DataFrame.\n\nT\n\nMethods\n\n`abs`()\n\nReturn a Series/DataFrame with absolute numeric value of each element.\n\n`add`(other[, axis, level, fill_value])\n\nGet Addition of dataframe and other, element-wise (binary operator add).\n\n`add_prefix`(prefix)\n\nPrefix labels with string prefix.\n\n`add_suffix`(suffix)\n\nSuffix labels with string suffix.\n\n`agg`([func, axis])\n\nAggregate using one or more operations over the specified axis.\n\n`aggregate`([func, axis])\n\nAggregate using one or more operations over the specified axis.\n\n`align`(other[, join, axis, level, copy, ...])\n\nAlign two objects on their axes with the specified join method.\n\n`all`([axis, bool_only, skipna, level])\n\nReturn whether all elements are True, potentially over an axis.\n\n`any`([axis, bool_only, skipna, level])\n\nReturn whether any element is True, potentially over an axis.\n\n`append`(other[, ignore_index, ...])\n\nAppend rows of other to the end of caller, returning a new object.\n\n`apply`(func[, axis, raw, result_type, args])\n\nApply a function along an axis of the DataFrame.\n\n`applymap`(func[, na_action])\n\nApply a function to a Dataframe elementwise.\n\n`asfreq`(freq[, method, how, normalize, ...])\n\nConvert time series to specified frequency.\n\n`asof`(where[, subset])\n\nReturn the last row(s) without any NaNs before where.\n\n`assign`(**kwargs)\n\nAssign new columns to a DataFrame.\n\n`astype`(dtype[, copy, errors])\n\nCast a pandas object to a specified dtype `dtype`.\n\n`at_time`(time[, asof, axis])\n\nSelect values at particular time of day (e.g., 9:30AM).\n\n`backfill`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\n`between_time`(start_time, end_time[, ...])\n\nSelect values between particular times of the day (e.g., 9:00-9:30 AM).\n\n`bfill`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\n`bool`()\n\nReturn the bool of a single element Series or DataFrame.\n\n`boxplot`([column, by, ax, fontsize, rot, ...])\n\nMake a box plot from DataFrame columns.\n\n`clip`([lower, upper, axis, inplace])\n\nTrim values at input threshold(s).\n\n`combine`(other, func[, fill_value, overwrite])\n\nPerform column-wise combine with another DataFrame.\n\n`combine_first`(other)\n\nUpdate null elements with value in the same location in other.\n\n`compare`(other[, align_axis, keep_shape, ...])\n\nCompare to another DataFrame and show the differences.\n\n`convert_dtypes`([infer_objects, ...])\n\nConvert columns to best possible dtypes using dtypes supporting `pd.NA`.\n\n`copy`([deep])\n\nMake a copy of this object's indices and data.\n\n`corr`([method, min_periods])\n\nCompute pairwise correlation of columns, excluding NA/null values.\n\n`corrwith`(other[, axis, drop, method])\n\nCompute pairwise correlation.\n\n`count`([axis, level, numeric_only])\n\nCount non-NA cells for each column or row.\n\n`cov`([min_periods, ddof])\n\nCompute pairwise covariance of columns, excluding NA/null values.\n\n`cummax`([axis, skipna])\n\nReturn cumulative maximum over a DataFrame or Series axis.\n\n`cummin`([axis, skipna])\n\nReturn cumulative minimum over a DataFrame or Series axis.\n\n`cumprod`([axis, skipna])\n\nReturn cumulative product over a DataFrame or Series axis.\n\n`cumsum`([axis, skipna])\n\nReturn cumulative sum over a DataFrame or Series axis.\n\n`describe`([percentiles, include, exclude, ...])\n\nGenerate descriptive statistics.\n\n`diff`([periods, axis])\n\nFirst discrete difference of element.\n\n`div`(other[, axis, level, fill_value])\n\nGet Floating division of dataframe and other, element-wise (binary operator\ntruediv).\n\n`divide`(other[, axis, level, fill_value])\n\nGet Floating division of dataframe and other, element-wise (binary operator\ntruediv).\n\n`dot`(other)\n\nCompute the matrix multiplication between the DataFrame and other.\n\n`drop`([labels, axis, index, columns, level, ...])\n\nDrop specified labels from rows or columns.\n\n`drop_duplicates`([subset, keep, inplace, ...])\n\nReturn DataFrame with duplicate rows removed.\n\n`droplevel`(level[, axis])\n\nReturn Series/DataFrame with requested index / column level(s) removed.\n\n`dropna`([axis, how, thresh, subset, inplace])\n\nRemove missing values.\n\n`duplicated`([subset, keep])\n\nReturn boolean Series denoting duplicate rows.\n\n`eq`(other[, axis, level])\n\nGet Equal to of dataframe and other, element-wise (binary operator eq).\n\n`equals`(other)\n\nTest whether two objects contain the same elements.\n\n`eval`(expr[, inplace])\n\nEvaluate a string describing operations on DataFrame columns.\n\n`ewm`([com, span, halflife, alpha, ...])\n\nProvide exponentially weighted (EW) calculations.\n\n`expanding`([min_periods, center, axis, method])\n\nProvide expanding window calculations.\n\n`explode`(column[, ignore_index])\n\nTransform each element of a list-like to a row, replicating index values.\n\n`ffill`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\n`fillna`([value, method, axis, inplace, ...])\n\nFill NA/NaN values using the specified method.\n\n`filter`([items, like, regex, axis])\n\nSubset the dataframe rows or columns according to the specified index labels.\n\n`first`(offset)\n\nSelect initial periods of time series data based on a date offset.\n\n`first_valid_index`()\n\nReturn index for first non-NA value or None, if no NA value is found.\n\n`floordiv`(other[, axis, level, fill_value])\n\nGet Integer division of dataframe and other, element-wise (binary operator\nfloordiv).\n\n`from_dict`(data[, orient, dtype, columns])\n\nConstruct DataFrame from dict of array-like or dicts.\n\n`from_records`(data[, index, exclude, ...])\n\nConvert structured or record ndarray to DataFrame.\n\n`ge`(other[, axis, level])\n\nGet Greater than or equal to of dataframe and other, element-wise (binary\noperator ge).\n\n`get`(key[, default])\n\nGet item from object for given key (ex: DataFrame column).\n\n`groupby`([by, axis, level, as_index, sort, ...])\n\nGroup DataFrame using a mapper or by a Series of columns.\n\n`gt`(other[, axis, level])\n\nGet Greater than of dataframe and other, element-wise (binary operator gt).\n\n`head`([n])\n\nReturn the first n rows.\n\n`hist`([column, by, grid, xlabelsize, xrot, ...])\n\nMake a histogram of the DataFrame's columns.\n\n`idxmax`([axis, skipna])\n\nReturn index of first occurrence of maximum over requested axis.\n\n`idxmin`([axis, skipna])\n\nReturn index of first occurrence of minimum over requested axis.\n\n`infer_objects`()\n\nAttempt to infer better dtypes for object columns.\n\n`info`([verbose, buf, max_cols, memory_usage, ...])\n\nPrint a concise summary of a DataFrame.\n\n`insert`(loc, column, value[, allow_duplicates])\n\nInsert column into DataFrame at specified location.\n\n`interpolate`([method, axis, limit, inplace, ...])\n\nFill NaN values using an interpolation method.\n\n`isin`(values)\n\nWhether each element in the DataFrame is contained in values.\n\n`isna`()\n\nDetect missing values.\n\n`isnull`()\n\nDataFrame.isnull is an alias for DataFrame.isna.\n\n`items`()\n\nIterate over (column name, Series) pairs.\n\n`iteritems`()\n\nIterate over (column name, Series) pairs.\n\n`iterrows`()\n\nIterate over DataFrame rows as (index, Series) pairs.\n\n`itertuples`([index, name])\n\nIterate over DataFrame rows as namedtuples.\n\n`join`(other[, on, how, lsuffix, rsuffix, sort])\n\nJoin columns of another DataFrame.\n\n`keys`()\n\nGet the 'info axis' (see Indexing for more).\n\n`kurt`([axis, skipna, level, numeric_only])\n\nReturn unbiased kurtosis over requested axis.\n\n`kurtosis`([axis, skipna, level, numeric_only])\n\nReturn unbiased kurtosis over requested axis.\n\n`last`(offset)\n\nSelect final periods of time series data based on a date offset.\n\n`last_valid_index`()\n\nReturn index for last non-NA value or None, if no NA value is found.\n\n`le`(other[, axis, level])\n\nGet Less than or equal to of dataframe and other, element-wise (binary\noperator le).\n\n`lookup`(row_labels, col_labels)\n\n(DEPRECATED) Label-based \"fancy indexing\" function for DataFrame.\n\n`lt`(other[, axis, level])\n\nGet Less than of dataframe and other, element-wise (binary operator lt).\n\n`mad`([axis, skipna, level])\n\nReturn the mean absolute deviation of the values over the requested axis.\n\n`mask`(cond[, other, inplace, axis, level, ...])\n\nReplace values where the condition is True.\n\n`max`([axis, skipna, level, numeric_only])\n\nReturn the maximum of the values over the requested axis.\n\n`mean`([axis, skipna, level, numeric_only])\n\nReturn the mean of the values over the requested axis.\n\n`median`([axis, skipna, level, numeric_only])\n\nReturn the median of the values over the requested axis.\n\n`melt`([id_vars, value_vars, var_name, ...])\n\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers\nset.\n\n`memory_usage`([index, deep])\n\nReturn the memory usage of each column in bytes.\n\n`merge`(right[, how, on, left_on, right_on, ...])\n\nMerge DataFrame or named Series objects with a database-style join.\n\n`min`([axis, skipna, level, numeric_only])\n\nReturn the minimum of the values over the requested axis.\n\n`mod`(other[, axis, level, fill_value])\n\nGet Modulo of dataframe and other, element-wise (binary operator mod).\n\n`mode`([axis, numeric_only, dropna])\n\nGet the mode(s) of each element along the selected axis.\n\n`mul`(other[, axis, level, fill_value])\n\nGet Multiplication of dataframe and other, element-wise (binary operator mul).\n\n`multiply`(other[, axis, level, fill_value])\n\nGet Multiplication of dataframe and other, element-wise (binary operator mul).\n\n`ne`(other[, axis, level])\n\nGet Not equal to of dataframe and other, element-wise (binary operator ne).\n\n`nlargest`(n, columns[, keep])\n\nReturn the first n rows ordered by columns in descending order.\n\n`notna`()\n\nDetect existing (non-missing) values.\n\n`notnull`()\n\nDataFrame.notnull is an alias for DataFrame.notna.\n\n`nsmallest`(n, columns[, keep])\n\nReturn the first n rows ordered by columns in ascending order.\n\n`nunique`([axis, dropna])\n\nCount number of distinct elements in specified axis.\n\n`pad`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\n`pct_change`([periods, fill_method, limit, freq])\n\nPercentage change between the current and a prior element.\n\n`pipe`(func, *args, **kwargs)\n\nApply chainable functions that expect Series or DataFrames.\n\n`pivot`([index, columns, values])\n\nReturn reshaped DataFrame organized by given index / column values.\n\n`pivot_table`([values, index, columns, ...])\n\nCreate a spreadsheet-style pivot table as a DataFrame.\n\n`plot`\n\nalias of `pandas.plotting._core.PlotAccessor`\n\n`pop`(item)\n\nReturn item and drop from frame.\n\n`pow`(other[, axis, level, fill_value])\n\nGet Exponential power of dataframe and other, element-wise (binary operator\npow).\n\n`prod`([axis, skipna, level, numeric_only, ...])\n\nReturn the product of the values over the requested axis.\n\n`product`([axis, skipna, level, numeric_only, ...])\n\nReturn the product of the values over the requested axis.\n\n`quantile`([q, axis, numeric_only, interpolation])\n\nReturn values at the given quantile over requested axis.\n\n`query`(expr[, inplace])\n\nQuery the columns of a DataFrame with a boolean expression.\n\n`radd`(other[, axis, level, fill_value])\n\nGet Addition of dataframe and other, element-wise (binary operator radd).\n\n`rank`([axis, method, numeric_only, ...])\n\nCompute numerical data ranks (1 through n) along axis.\n\n`rdiv`(other[, axis, level, fill_value])\n\nGet Floating division of dataframe and other, element-wise (binary operator\nrtruediv).\n\n`reindex`([labels, index, columns, axis, ...])\n\nConform Series/DataFrame to new index with optional filling logic.\n\n`reindex_like`(other[, method, copy, limit, ...])\n\nReturn an object with matching indices as other object.\n\n`rename`([mapper, index, columns, axis, copy, ...])\n\nAlter axes labels.\n\n`rename_axis`([mapper, index, columns, axis, ...])\n\nSet the name of the axis for the index or columns.\n\n`reorder_levels`(order[, axis])\n\nRearrange index levels using input order.\n\n`replace`([to_replace, value, inplace, limit, ...])\n\nReplace values given in to_replace with value.\n\n`resample`(rule[, axis, closed, label, ...])\n\nResample time-series data.\n\n`reset_index`([level, drop, inplace, ...])\n\nReset the index, or a level of it.\n\n`rfloordiv`(other[, axis, level, fill_value])\n\nGet Integer division of dataframe and other, element-wise (binary operator\nrfloordiv).\n\n`rmod`(other[, axis, level, fill_value])\n\nGet Modulo of dataframe and other, element-wise (binary operator rmod).\n\n`rmul`(other[, axis, level, fill_value])\n\nGet Multiplication of dataframe and other, element-wise (binary operator\nrmul).\n\n`rolling`(window[, min_periods, center, ...])\n\nProvide rolling window calculations.\n\n`round`([decimals])\n\nRound a DataFrame to a variable number of decimal places.\n\n`rpow`(other[, axis, level, fill_value])\n\nGet Exponential power of dataframe and other, element-wise (binary operator\nrpow).\n\n`rsub`(other[, axis, level, fill_value])\n\nGet Subtraction of dataframe and other, element-wise (binary operator rsub).\n\n`rtruediv`(other[, axis, level, fill_value])\n\nGet Floating division of dataframe and other, element-wise (binary operator\nrtruediv).\n\n`sample`([n, frac, replace, weights, ...])\n\nReturn a random sample of items from an axis of object.\n\n`select_dtypes`([include, exclude])\n\nReturn a subset of the DataFrame's columns based on the column dtypes.\n\n`sem`([axis, skipna, level, ddof, numeric_only])\n\nReturn unbiased standard error of the mean over requested axis.\n\n`set_axis`(labels[, axis, inplace])\n\nAssign desired index to given axis.\n\n`set_flags`(*[, copy, allows_duplicate_labels])\n\nReturn a new object with updated flags.\n\n`set_index`(keys[, drop, append, inplace, ...])\n\nSet the DataFrame index using existing columns.\n\n`shift`([periods, freq, axis, fill_value])\n\nShift index by desired number of periods with an optional time freq.\n\n`skew`([axis, skipna, level, numeric_only])\n\nReturn unbiased skew over requested axis.\n\n`slice_shift`([periods, axis])\n\n(DEPRECATED) Equivalent to shift without copying data.\n\n`sort_index`([axis, level, ascending, ...])\n\nSort object by labels (along an axis).\n\n`sort_values`(by[, axis, ascending, inplace, ...])\n\nSort by the values along either axis.\n\n`sparse`\n\nalias of `pandas.core.arrays.sparse.accessor.SparseFrameAccessor`\n\n`squeeze`([axis])\n\nSqueeze 1 dimensional axis objects into scalars.\n\n`stack`([level, dropna])\n\nStack the prescribed level(s) from columns to index.\n\n`std`([axis, skipna, level, ddof, numeric_only])\n\nReturn sample standard deviation over requested axis.\n\n`sub`(other[, axis, level, fill_value])\n\nGet Subtraction of dataframe and other, element-wise (binary operator sub).\n\n`subtract`(other[, axis, level, fill_value])\n\nGet Subtraction of dataframe and other, element-wise (binary operator sub).\n\n`sum`([axis, skipna, level, numeric_only, ...])\n\nReturn the sum of the values over the requested axis.\n\n`swapaxes`(axis1, axis2[, copy])\n\nInterchange axes and swap values axes appropriately.\n\n`swaplevel`([i, j, axis])\n\nSwap levels i and j in a `MultiIndex`.\n\n`tail`([n])\n\nReturn the last n rows.\n\n`take`(indices[, axis, is_copy])\n\nReturn the elements in the given positional indices along an axis.\n\n`to_clipboard`([excel, sep])\n\nCopy object to the system clipboard.\n\n`to_csv`([path_or_buf, sep, na_rep, ...])\n\nWrite object to a comma-separated values (csv) file.\n\n`to_dict`([orient, into])\n\nConvert the DataFrame to a dictionary.\n\n`to_excel`(excel_writer[, sheet_name, na_rep, ...])\n\nWrite object to an Excel sheet.\n\n`to_feather`(path, **kwargs)\n\nWrite a DataFrame to the binary Feather format.\n\n`to_gbq`(destination_table[, project_id, ...])\n\nWrite a DataFrame to a Google BigQuery table.\n\n`to_hdf`(path_or_buf, key[, mode, complevel, ...])\n\nWrite the contained data to an HDF5 file using HDFStore.\n\n`to_html`([buf, columns, col_space, header, ...])\n\nRender a DataFrame as an HTML table.\n\n`to_json`([path_or_buf, orient, date_format, ...])\n\nConvert the object to a JSON string.\n\n`to_latex`([buf, columns, col_space, header, ...])\n\nRender object to a LaTeX tabular, longtable, or nested table.\n\n`to_markdown`([buf, mode, index, storage_options])\n\nPrint DataFrame in Markdown-friendly format.\n\n`to_numpy`([dtype, copy, na_value])\n\nConvert the DataFrame to a NumPy array.\n\n`to_parquet`([path, engine, compression, ...])\n\nWrite a DataFrame to the binary parquet format.\n\n`to_period`([freq, axis, copy])\n\nConvert DataFrame from DatetimeIndex to PeriodIndex.\n\n`to_pickle`(path[, compression, protocol, ...])\n\nPickle (serialize) object to file.\n\n`to_records`([index, column_dtypes, index_dtypes])\n\nConvert DataFrame to a NumPy record array.\n\n`to_sql`(name, con[, schema, if_exists, ...])\n\nWrite records stored in a DataFrame to a SQL database.\n\n`to_stata`(path[, convert_dates, write_index, ...])\n\nExport DataFrame object to Stata dta format.\n\n`to_string`([buf, columns, col_space, header, ...])\n\nRender a DataFrame to a console-friendly tabular output.\n\n`to_timestamp`([freq, how, axis, copy])\n\nCast to DatetimeIndex of timestamps, at beginning of period.\n\n`to_xarray`()\n\nReturn an xarray object from the pandas object.\n\n`to_xml`([path_or_buffer, index, root_name, ...])\n\nRender a DataFrame to an XML document.\n\n`transform`(func[, axis])\n\nCall `func` on self producing a DataFrame with the same axis shape as self.\n\n`transpose`(*args[, copy])\n\nTranspose index and columns.\n\n`truediv`(other[, axis, level, fill_value])\n\nGet Floating division of dataframe and other, element-wise (binary operator\ntruediv).\n\n`truncate`([before, after, axis, copy])\n\nTruncate a Series or DataFrame before and after some index value.\n\n`tshift`([periods, freq, axis])\n\n(DEPRECATED) Shift the time index, using the index's frequency if available.\n\n`tz_convert`(tz[, axis, level, copy])\n\nConvert tz-aware axis to target time zone.\n\n`tz_localize`(tz[, axis, level, copy, ...])\n\nLocalize tz-naive index of a Series or DataFrame to target time zone.\n\n`unstack`([level, fill_value])\n\nPivot a level of the (necessarily hierarchical) index labels.\n\n`update`(other[, join, overwrite, ...])\n\nModify in place using non-NA values from another DataFrame.\n\n`value_counts`([subset, normalize, sort, ...])\n\nReturn a Series containing counts of unique rows in the DataFrame.\n\n`var`([axis, skipna, level, ddof, numeric_only])\n\nReturn unbiased variance over requested axis.\n\n`where`(cond[, other, inplace, axis, level, ...])\n\nReplace values where the condition is False.\n\n`xs`(key[, axis, level, drop_level])\n\nReturn cross-section from the Series/DataFrame.\n\n"}, {"name": "pandas.DataFrame.__iter__", "path": "reference/api/pandas.dataframe.__iter__", "type": "DataFrame", "text": "\nIterate over info axis.\n\nInfo axis as iterator.\n\n"}, {"name": "pandas.DataFrame.abs", "path": "reference/api/pandas.dataframe.abs", "type": "DataFrame", "text": "\nReturn a Series/DataFrame with absolute numeric value of each element.\n\nThis function only applies to elements that are all numeric.\n\nSeries/DataFrame containing the absolute value of each element.\n\nSee also\n\nCalculate the absolute value element-wise.\n\nNotes\n\nFor `complex` inputs, `1.2 + 1j`, the absolute value is \\\\(\\sqrt{ a^2 + b^2\n}\\\\).\n\nExamples\n\nAbsolute numeric values in a Series.\n\nAbsolute numeric values in a Series with complex numbers.\n\nAbsolute numeric values in a Series with a Timedelta element.\n\nSelect rows with data closest to certain value using argsort (from\nStackOverflow).\n\n"}, {"name": "pandas.DataFrame.add", "path": "reference/api/pandas.dataframe.add", "type": "DataFrame", "text": "\nGet Addition of dataframe and other, element-wise (binary operator add).\n\nEquivalent to `dataframe + other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, radd.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.add_prefix", "path": "reference/api/pandas.dataframe.add_prefix", "type": "DataFrame", "text": "\nPrefix labels with string prefix.\n\nFor Series, the row labels are prefixed. For DataFrame, the column labels are\nprefixed.\n\nThe string to add before each label.\n\nNew Series or DataFrame with updated labels.\n\nSee also\n\nSuffix row labels with string suffix.\n\nSuffix column labels with string suffix.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.add_suffix", "path": "reference/api/pandas.dataframe.add_suffix", "type": "DataFrame", "text": "\nSuffix labels with string suffix.\n\nFor Series, the row labels are suffixed. For DataFrame, the column labels are\nsuffixed.\n\nThe string to add after each label.\n\nNew Series or DataFrame with updated labels.\n\nSee also\n\nPrefix row labels with string prefix.\n\nPrefix column labels with string prefix.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.agg", "path": "reference/api/pandas.dataframe.agg", "type": "DataFrame", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a DataFrame or when passed to DataFrame.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nIf 0 or \u2018index\u2019: apply function to each column. If 1 or \u2018columns\u2019: apply\nfunction to each row.\n\nPositional arguments to pass to func.\n\nKeyword arguments to pass to func.\n\nThe return can be:\n\nscalar : when Series.agg is called with single function\n\nSeries : when DataFrame.agg is called with a single function\n\nDataFrame : when DataFrame.agg is called with several functions\n\nReturn scalar, Series or DataFrame.\n\nSee also\n\nPerform any type of operations.\n\nPerform transformation type operations.\n\nPerform operations over groups.\n\nPerform operations over resampled bins.\n\nPerform operations over rolling window.\n\nPerform operations over expanding window.\n\nPerform operation over exponential weighted window.\n\nNotes\n\nagg is an alias for aggregate. Use the alias.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nA passed user-defined-function will be passed a Series for evaluation.\n\nExamples\n\nAggregate these functions over the rows.\n\nDifferent aggregations per column.\n\nAggregate different functions over the columns and rename the index of the\nresulting DataFrame.\n\nAggregate over the columns.\n\n"}, {"name": "pandas.DataFrame.aggregate", "path": "reference/api/pandas.dataframe.aggregate", "type": "DataFrame", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a DataFrame or when passed to DataFrame.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nIf 0 or \u2018index\u2019: apply function to each column. If 1 or \u2018columns\u2019: apply\nfunction to each row.\n\nPositional arguments to pass to func.\n\nKeyword arguments to pass to func.\n\nThe return can be:\n\nscalar : when Series.agg is called with single function\n\nSeries : when DataFrame.agg is called with a single function\n\nDataFrame : when DataFrame.agg is called with several functions\n\nReturn scalar, Series or DataFrame.\n\nSee also\n\nPerform any type of operations.\n\nPerform transformation type operations.\n\nPerform operations over groups.\n\nPerform operations over resampled bins.\n\nPerform operations over rolling window.\n\nPerform operations over expanding window.\n\nPerform operation over exponential weighted window.\n\nNotes\n\nagg is an alias for aggregate. Use the alias.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nA passed user-defined-function will be passed a Series for evaluation.\n\nExamples\n\nAggregate these functions over the rows.\n\nDifferent aggregations per column.\n\nAggregate different functions over the columns and rename the index of the\nresulting DataFrame.\n\nAggregate over the columns.\n\n"}, {"name": "pandas.DataFrame.align", "path": "reference/api/pandas.dataframe.align", "type": "DataFrame", "text": "\nAlign two objects on their axes with the specified join method.\n\nJoin method is specified for each axis Index.\n\nAlign on index (0), columns (1), or both (None).\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nAlways returns new objects. If copy=False and no reindexing is required then\noriginal objects are returned.\n\nValue to use for missing values. Defaults to NaN, but can be any \u201ccompatible\u201d\nvalue.\n\nMethod to use for filling holes in reindexed Series:\n\npad / ffill: propagate last valid observation forward to next valid.\n\nbackfill / bfill: use NEXT valid observation to fill gap.\n\nIf method is specified, this is the maximum number of consecutive NaN values\nto forward/backward fill. In other words, if there is a gap with more than\nthis number of consecutive NaNs, it will only be partially filled. If method\nis not specified, this is the maximum number of entries along the entire axis\nwhere NaNs will be filled. Must be greater than 0 if not None.\n\nFilling axis, method and limit.\n\nBroadcast values along this axis, if aligning two objects of different\ndimensions.\n\nAligned objects.\n\nExamples\n\nAlign on columns:\n\nWe can also align on the index:\n\nFinally, the default axis=None will align on both index and columns:\n\n"}, {"name": "pandas.DataFrame.all", "path": "reference/api/pandas.dataframe.all", "type": "DataFrame", "text": "\nReturn whether all elements are True, potentially over an axis.\n\nReturns True unless there at least one element within a series or along a\nDataframe axis that is False or equivalent (e.g. zero or empty).\n\nIndicate which axis or axes should be reduced.\n\n0 / \u2018index\u2019 : reduce the index, return a Series whose index is the original\ncolumn labels.\n\n1 / \u2018columns\u2019 : reduce the columns, return a Series whose index is the\noriginal index.\n\nNone : reduce all axes, return a scalar.\n\nInclude only boolean columns. If None, will attempt to use everything, then\nuse only boolean data. Not implemented for Series.\n\nExclude NA/null values. If the entire row/column is NA and skipna is True,\nthen the result will be True, as for an empty row/column. If skipna is False,\nthen NA are treated as True, because these are not equal to zero.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nIf level is specified, then, DataFrame is returned; otherwise, Series is\nreturned.\n\nSee also\n\nReturn True if all elements are True.\n\nReturn True if one (or more) elements are True.\n\nExamples\n\nSeries\n\nDataFrames\n\nCreate a dataframe from a dictionary.\n\nDefault behaviour checks if column-wise values all return True.\n\nSpecify `axis='columns'` to check if row-wise values all return True.\n\nOr `axis=None` for whether every value is True.\n\n"}, {"name": "pandas.DataFrame.any", "path": "reference/api/pandas.dataframe.any", "type": "DataFrame", "text": "\nReturn whether any element is True, potentially over an axis.\n\nReturns False unless there is at least one element within a series or along a\nDataframe axis that is True or equivalent (e.g. non-zero or non-empty).\n\nIndicate which axis or axes should be reduced.\n\n0 / \u2018index\u2019 : reduce the index, return a Series whose index is the original\ncolumn labels.\n\n1 / \u2018columns\u2019 : reduce the columns, return a Series whose index is the\noriginal index.\n\nNone : reduce all axes, return a scalar.\n\nInclude only boolean columns. If None, will attempt to use everything, then\nuse only boolean data. Not implemented for Series.\n\nExclude NA/null values. If the entire row/column is NA and skipna is True,\nthen the result will be False, as for an empty row/column. If skipna is False,\nthen NA are treated as True, because these are not equal to zero.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nIf level is specified, then, DataFrame is returned; otherwise, Series is\nreturned.\n\nSee also\n\nNumpy version of this method.\n\nReturn whether any element is True.\n\nReturn whether all elements are True.\n\nReturn whether any element is True over requested axis.\n\nReturn whether all elements are True over requested axis.\n\nExamples\n\nSeries\n\nFor Series input, the output is a scalar indicating whether any element is\nTrue.\n\nDataFrame\n\nWhether each column contains at least one True element (the default).\n\nAggregating over the columns.\n\nAggregating over the entire DataFrame with `axis=None`.\n\nany for an empty DataFrame is an empty Series.\n\n"}, {"name": "pandas.DataFrame.append", "path": "reference/api/pandas.dataframe.append", "type": "DataFrame", "text": "\nAppend rows of other to the end of caller, returning a new object.\n\nColumns in other that are not in the caller are added as new columns.\n\nThe data to append.\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nIf True, raise ValueError on creating index with duplicates.\n\nSort columns if the columns of self and other are not aligned.\n\nChanged in version 1.0.0: Changed to not sort by default.\n\nA new DataFrame consisting of the rows of caller and the rows of other.\n\nSee also\n\nGeneral function to concatenate DataFrame or Series objects.\n\nNotes\n\nIf a list of dict/series is passed and the keys are all contained in the\nDataFrame\u2019s index, the order of the columns in the resulting DataFrame will be\nunchanged.\n\nIteratively appending rows to a DataFrame can be more computationally\nintensive than a single concatenate. A better solution is to append those rows\nto a list and then concatenate the list with the original DataFrame all at\nonce.\n\nExamples\n\nWith ignore_index set to True:\n\nThe following, while not recommended methods for generating DataFrames, show\ntwo ways to generate a DataFrame from multiple data sources.\n\nLess efficient:\n\nMore efficient:\n\n"}, {"name": "pandas.DataFrame.apply", "path": "reference/api/pandas.dataframe.apply", "type": "DataFrame", "text": "\nApply a function along an axis of the DataFrame.\n\nObjects passed to the function are Series objects whose index is either the\nDataFrame\u2019s index (`axis=0`) or the DataFrame\u2019s columns (`axis=1`). By default\n(`result_type=None`), the final return type is inferred from the return type\nof the applied function. Otherwise, it depends on the result_type argument.\n\nFunction to apply to each column or row.\n\nAxis along which the function is applied:\n\n0 or \u2018index\u2019: apply function to each column.\n\n1 or \u2018columns\u2019: apply function to each row.\n\nDetermines if row or column is passed as a Series or ndarray object:\n\n`False` : passes each row or column as a Series to the function.\n\n`True` : the passed function will receive ndarray objects instead. If you are\njust applying a NumPy reduction function this will achieve much better\nperformance.\n\nThese only act when `axis=1` (columns):\n\n\u2018expand\u2019 : list-like results will be turned into columns.\n\n\u2018reduce\u2019 : returns a Series if possible rather than expanding list-like\nresults. This is the opposite of \u2018expand\u2019.\n\n\u2018broadcast\u2019 : results will be broadcast to the original shape of the\nDataFrame, the original index and columns will be retained.\n\nThe default behaviour (None) depends on the return value of the applied\nfunction: list-like results will be returned as a Series of those. However if\nthe apply function returns a Series these are expanded to columns.\n\nPositional arguments to pass to func in addition to the array/series.\n\nAdditional keyword arguments to pass as keywords arguments to func.\n\nResult of applying `func` along the given axis of the DataFrame.\n\nSee also\n\nFor elementwise operations.\n\nOnly perform aggregating type operations.\n\nOnly perform transforming type operations.\n\nNotes\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nExamples\n\nUsing a numpy universal function (in this case the same as `np.sqrt(df)`):\n\nUsing a reducing function on either axis\n\nReturning a list-like will result in a Series\n\nPassing `result_type='expand'` will expand list-like results to columns of a\nDataframe\n\nReturning a Series inside the function is similar to passing\n`result_type='expand'`. The resulting column names will be the Series index.\n\nPassing `result_type='broadcast'` will ensure the same shape result, whether\nlist-like or scalar is returned by the function, and broadcast it along the\naxis. The resulting column names will be the originals.\n\n"}, {"name": "pandas.DataFrame.applymap", "path": "reference/api/pandas.dataframe.applymap", "type": "DataFrame", "text": "\nApply a function to a Dataframe elementwise.\n\nThis method applies a function that accepts and returns a scalar to every\nelement of a DataFrame.\n\nPython function, returns a single value from a single value.\n\nIf \u2018ignore\u2019, propagate NaN values, without passing them to func.\n\nNew in version 1.2.\n\nAdditional keyword arguments to pass as keywords arguments to func.\n\nNew in version 1.3.0.\n\nTransformed DataFrame.\n\nSee also\n\nApply a function along input axis of DataFrame.\n\nExamples\n\nLike Series.map, NA values can be ignored:\n\nNote that a vectorized version of func often exists, which will be much\nfaster. You could square each number elementwise.\n\nBut it\u2019s better to avoid applymap in that case.\n\n"}, {"name": "pandas.DataFrame.asfreq", "path": "reference/api/pandas.dataframe.asfreq", "type": "DataFrame", "text": "\nConvert time series to specified frequency.\n\nReturns the original data conformed to a new index with the specified\nfrequency.\n\nIf the index of this DataFrame is a `PeriodIndex`, the new index is the result\nof transforming the original index with `PeriodIndex.asfreq` (so the original\nindex will map one-to-one to the new index).\n\nOtherwise, the new index will be equivalent to `pd.date_range(start, end,\nfreq=freq)` where `start` and `end` are, respectively, the first and last\nentries in the original index (see `pandas.date_range()`). The values\ncorresponding to any timesteps in the new index which were not present in the\noriginal index will be null (`NaN`), unless a method for filling such unknowns\nis provided (see the `method` parameter below).\n\nThe `resample()` method is more appropriate if an operation on each group of\ntimesteps (such as an aggregate) is necessary to represent the data at the new\nfrequency.\n\nFrequency DateOffset or string.\n\nMethod to use for filling holes in reindexed Series (note this does not fill\nNaNs that already were present):\n\n\u2018pad\u2019 / \u2018ffill\u2019: propagate last valid observation forward to next valid\n\n\u2018backfill\u2019 / \u2018bfill\u2019: use NEXT valid observation to fill.\n\nFor PeriodIndex only (see PeriodIndex.asfreq).\n\nWhether to reset output index to midnight.\n\nValue to use for missing values, applied during upsampling (note this does not\nfill NaNs that already were present).\n\nDataFrame object reindexed to the specified frequency.\n\nSee also\n\nConform DataFrame to new index with optional filling logic.\n\nNotes\n\nTo learn more about the frequency strings, please see this link.\n\nExamples\n\nStart by creating a series with 4 one minute timestamps.\n\nUpsample the series into 30 second bins.\n\nUpsample again, providing a `fill value`.\n\nUpsample again, providing a `method`.\n\n"}, {"name": "pandas.DataFrame.asof", "path": "reference/api/pandas.dataframe.asof", "type": "DataFrame", "text": "\nReturn the last row(s) without any NaNs before where.\n\nThe last row (for each element in where, if list) without any NaN is taken. In\ncase of a `DataFrame`, the last row without NaN considering only the subset of\ncolumns (if not None)\n\nIf there is no good value, NaN is returned for a Series or a Series of NaN\nvalues for a DataFrame\n\nDate(s) before which the last row(s) are returned.\n\nFor DataFrame, if not None, only use these columns to check for NaNs.\n\nThe return can be:\n\nscalar : when self is a Series and where is a scalar\n\nSeries: when self is a Series and where is an array-like, or when self is a\nDataFrame and where is a scalar\n\nDataFrame : when self is a DataFrame and where is an array-like\n\nReturn scalar, Series, or DataFrame.\n\nSee also\n\nPerform an asof merge. Similar to left join.\n\nNotes\n\nDates are assumed to be sorted. Raises if this is not the case.\n\nExamples\n\nA Series and a scalar where.\n\nFor a sequence where, a Series is returned. The first value is NaN, because\nthe first element of where is before the first index value.\n\nMissing values are not considered. The following is `2.0`, not NaN, even\nthough NaN is at the index location for `30`.\n\nTake all columns into consideration\n\nTake a single column into consideration\n\n"}, {"name": "pandas.DataFrame.assign", "path": "reference/api/pandas.dataframe.assign", "type": "DataFrame", "text": "\nAssign new columns to a DataFrame.\n\nReturns a new object with all original columns in addition to new ones.\nExisting columns that are re-assigned will be overwritten.\n\nThe column names are keywords. If the values are callable, they are computed\non the DataFrame and assigned to the new columns. The callable must not change\ninput DataFrame (though pandas doesn\u2019t check it). If the values are not\ncallable, (e.g. a Series, scalar, or array), they are simply assigned.\n\nA new DataFrame with the new columns in addition to all the existing columns.\n\nNotes\n\nAssigning multiple columns within the same `assign` is possible. Later items\nin \u2018**kwargs\u2019 may refer to newly created or modified columns in \u2018df\u2019; items\nare computed and assigned into \u2018df\u2019 in order.\n\nExamples\n\nWhere the value is a callable, evaluated on df:\n\nAlternatively, the same behavior can be achieved by directly referencing an\nexisting Series or sequence:\n\nYou can create multiple columns within the same assign where one of the\ncolumns depends on another one defined within the same assign:\n\n"}, {"name": "pandas.DataFrame.astype", "path": "reference/api/pandas.dataframe.astype", "type": "DataFrame", "text": "\nCast a pandas object to a specified dtype `dtype`.\n\nUse a numpy.dtype or Python type to cast entire pandas object to the same\ntype. Alternatively, use {col: dtype, \u2026}, where col is a column label and\ndtype is a numpy.dtype or Python type to cast one or more of the DataFrame\u2019s\ncolumns to column-specific types.\n\nReturn a copy when `copy=True` (be very careful setting `copy=False` as\nchanges to values then may propagate to other pandas objects).\n\nControl raising of exceptions on invalid data for provided dtype.\n\n`raise` : allow exceptions to be raised\n\n`ignore` : suppress exceptions. On error return original object.\n\nSee also\n\nConvert argument to datetime.\n\nConvert argument to timedelta.\n\nConvert argument to a numeric type.\n\nCast a numpy array to a specified type.\n\nNotes\n\nDeprecated since version 1.3.0: Using `astype` to convert from timezone-naive\ndtype to timezone-aware dtype is deprecated and will raise in a future\nversion. Use `Series.dt.tz_localize()` instead.\n\nExamples\n\nCreate a DataFrame:\n\nCast all columns to int32:\n\nCast col1 to int32 using a dictionary:\n\nCreate a series:\n\nConvert to categorical type:\n\nConvert to ordered categorical type with custom ordering:\n\nNote that using `copy=False` and changing data on a new pandas object may\npropagate changes:\n\nCreate a series of dates:\n\n"}, {"name": "pandas.DataFrame.at", "path": "reference/api/pandas.dataframe.at", "type": "DataFrame", "text": "\nAccess a single value for a row/column label pair.\n\nSimilar to `loc`, in that both provide label-based lookups. Use `at` if you\nonly need to get or set a single value in a DataFrame or Series.\n\nIf \u2018label\u2019 does not exist in DataFrame.\n\nSee also\n\nAccess a single value for a row/column pair by integer position.\n\nAccess a group of rows and columns by label(s).\n\nAccess a single value using a label.\n\nExamples\n\nGet value at specified row/column pair\n\nSet value at specified row/column pair\n\nGet value within a Series\n\n"}, {"name": "pandas.DataFrame.at_time", "path": "reference/api/pandas.dataframe.at_time", "type": "DataFrame", "text": "\nSelect values at particular time of day (e.g., 9:30AM).\n\nIf the index is not a `DatetimeIndex`\n\nSee also\n\nSelect values between particular times of the day.\n\nSelect initial periods of time series based on a date offset.\n\nSelect final periods of time series based on a date offset.\n\nGet just the index locations for values at particular time of the day.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.attrs", "path": "reference/api/pandas.dataframe.attrs", "type": "DataFrame", "text": "\nDictionary of global attributes of this dataset.\n\nWarning\n\nattrs is experimental and may change without warning.\n\nSee also\n\nGlobal flags applying to this object.\n\n"}, {"name": "pandas.DataFrame.axes", "path": "reference/api/pandas.dataframe.axes", "type": "DataFrame", "text": "\nReturn a list representing the axes of the DataFrame.\n\nIt has the row axis labels and column axis labels as the only members. They\nare returned in that order.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.backfill", "path": "reference/api/pandas.dataframe.backfill", "type": "DataFrame", "text": "\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\nObject with missing values filled or None if `inplace=True`.\n\n"}, {"name": "pandas.DataFrame.between_time", "path": "reference/api/pandas.dataframe.between_time", "type": "DataFrame", "text": "\nSelect values between particular times of the day (e.g., 9:00-9:30 AM).\n\nBy setting `start_time` to be later than `end_time`, you can get the times\nthat are not between the two times.\n\nInitial time as a time filter limit.\n\nEnd time as a time filter limit.\n\nWhether the start time needs to be included in the result.\n\nDeprecated since version 1.4.0: Arguments include_start and include_end have\nbeen deprecated to standardize boundary inputs. Use inclusive instead, to set\neach bound as closed or open.\n\nWhether the end time needs to be included in the result.\n\nDeprecated since version 1.4.0: Arguments include_start and include_end have\nbeen deprecated to standardize boundary inputs. Use inclusive instead, to set\neach bound as closed or open.\n\nInclude boundaries; whether to set each bound as closed or open.\n\nDetermine range time on index or columns value.\n\nData from the original object filtered to the specified dates range.\n\nIf the index is not a `DatetimeIndex`\n\nSee also\n\nSelect values at a particular time of the day.\n\nSelect initial periods of time series based on a date offset.\n\nSelect final periods of time series based on a date offset.\n\nGet just the index locations for values between particular times of the day.\n\nExamples\n\nYou get the times that are not between two times by setting `start_time` later\nthan `end_time`:\n\n"}, {"name": "pandas.DataFrame.bfill", "path": "reference/api/pandas.dataframe.bfill", "type": "DataFrame", "text": "\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\nObject with missing values filled or None if `inplace=True`.\n\n"}, {"name": "pandas.DataFrame.bool", "path": "reference/api/pandas.dataframe.bool", "type": "DataFrame", "text": "\nReturn the bool of a single element Series or DataFrame.\n\nThis must be a boolean scalar value, either True or False. It will raise a\nValueError if the Series or DataFrame does not have exactly 1 element, or that\nelement is not boolean (integer values 0 and 1 will also raise an exception).\n\nThe value in the Series or DataFrame.\n\nSee also\n\nChange the data type of a Series, including to boolean.\n\nChange the data type of a DataFrame, including to boolean.\n\nNumPy boolean data type, used by pandas for boolean values.\n\nExamples\n\nThe method will only work for single element objects with a boolean value:\n\n"}, {"name": "pandas.DataFrame.boxplot", "path": "reference/api/pandas.dataframe.boxplot", "type": "DataFrame", "text": "\nMake a box plot from DataFrame columns.\n\nMake a box-and-whisker plot from DataFrame columns, optionally grouped by some\nother columns. A box plot is a method for graphically depicting groups of\nnumerical data through their quartiles. The box extends from the Q1 to Q3\nquartile values of the data, with a line at the median (Q2). The whiskers\nextend from the edges of box to show the range of the data. By default, they\nextend no more than 1.5 * IQR (IQR = Q3 - Q1) from the edges of the box,\nending at the farthest data point within that interval. Outliers are plotted\nas separate dots.\n\nFor further details see Wikipedia\u2019s entry for boxplot.\n\nColumn name or list of names, or vector. Can be any valid input to\n`pandas.DataFrame.groupby()`.\n\nColumn in the DataFrame to `pandas.DataFrame.groupby()`. One box-plot will be\ndone per value of columns in by.\n\nThe matplotlib axes to be used by boxplot.\n\nTick label font size in points or as a string (e.g., large).\n\nThe rotation angle of labels (in degrees) with respect to the screen\ncoordinate system.\n\nSetting this to True will show the grid.\n\nThe size of the figure to create in matplotlib.\n\nFor example, (3, 5) will display the subplots using 3 columns and 5 rows,\nstarting from the top-left.\n\nThe kind of object to return. The default is `axes`.\n\n\u2018axes\u2019 returns the matplotlib axes the boxplot is drawn on.\n\n\u2018dict\u2019 returns a dictionary whose values are the matplotlib Lines of the\nboxplot.\n\n\u2018both\u2019 returns a namedtuple with the axes and dict.\n\nwhen grouping with `by`, a Series mapping columns to `return_type` is\nreturned.\n\nIf `return_type` is None, a NumPy array of axes with the same shape as\n`layout` is returned.\n\nBackend to use instead of the backend specified in the option\n`plotting.backend`. For instance, \u2018matplotlib\u2019. Alternatively, to specify the\n`plotting.backend` for the whole session, set `pd.options.plotting.backend`.\n\nNew in version 1.0.0.\n\nAll other plotting keyword arguments to be passed to\n`matplotlib.pyplot.boxplot()`.\n\nSee Notes.\n\nSee also\n\nMake a histogram.\n\nMatplotlib equivalent plot.\n\nNotes\n\nThe return type depends on the return_type parameter:\n\n\u2018axes\u2019 : object of class matplotlib.axes.Axes\n\n\u2018dict\u2019 : dict of matplotlib.lines.Line2D objects\n\n\u2018both\u2019 : a namedtuple with structure (ax, lines)\n\nFor data grouped with `by`, return a Series of the above or a numpy array:\n\n`Series`\n\n`array` (for `return_type = None`)\n\nUse `return_type='dict'` when you want to tweak the appearance of the lines\nafter plotting. In this case a dict containing the Lines making up the boxes,\ncaps, fliers, medians, and whiskers is returned.\n\nExamples\n\nBoxplots can be created for every column in the dataframe by `df.boxplot()` or\nindicating the columns to be used:\n\nBoxplots of variables distributions grouped by the values of a third variable\ncan be created using the option `by`. For instance:\n\nA list of strings (i.e. `['X', 'Y']`) can be passed to boxplot in order to\ngroup the data by combination of the variables in the x-axis:\n\nThe layout of boxplot can be adjusted giving a tuple to `layout`:\n\nAdditional formatting can be done to the boxplot, like suppressing the grid\n(`grid=False`), rotating the labels in the x-axis (i.e. `rot=45`) or changing\nthe fontsize (i.e. `fontsize=15`):\n\nThe parameter `return_type` can be used to select the type of element returned\nby boxplot. When `return_type='axes'` is selected, the matplotlib axes on\nwhich the boxplot is drawn are returned:\n\nWhen grouping with `by`, a Series mapping columns to `return_type` is\nreturned:\n\nIf `return_type` is None, a NumPy array of axes with the same shape as\n`layout` is returned:\n\n"}, {"name": "pandas.DataFrame.clip", "path": "reference/api/pandas.dataframe.clip", "type": "DataFrame", "text": "\nTrim values at input threshold(s).\n\nAssigns values outside boundary to boundary values. Thresholds can be singular\nvalues or array like, and in the latter case the clipping is performed\nelement-wise in the specified axis.\n\nMinimum threshold value. All values below this threshold will be set to it. A\nmissing threshold (e.g NA) will not clip the value.\n\nMaximum threshold value. All values above this threshold will be set to it. A\nmissing threshold (e.g NA) will not clip the value.\n\nAlign object with lower and upper along the given axis.\n\nWhether to perform the operation in place on the data.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith numpy.\n\nSame type as calling object with the values outside the clip boundaries\nreplaced or None if `inplace=True`.\n\nSee also\n\nTrim values at input threshold in series.\n\nTrim values at input threshold in dataframe.\n\nClip (limit) the values in an array.\n\nExamples\n\nClips per column using lower and upper thresholds:\n\nClips using specific lower and upper thresholds per column element:\n\nClips using specific lower threshold per column element, with missing values:\n\n"}, {"name": "pandas.DataFrame.columns", "path": "reference/api/pandas.dataframe.columns", "type": "DataFrame", "text": "\nThe column labels of the DataFrame.\n\n"}, {"name": "pandas.DataFrame.combine", "path": "reference/api/pandas.dataframe.combine", "type": "DataFrame", "text": "\nPerform column-wise combine with another DataFrame.\n\nCombines a DataFrame with other DataFrame using func to element-wise combine\ncolumns. The row and column indexes of the resulting DataFrame will be the\nunion of the two.\n\nThe DataFrame to merge column-wise.\n\nFunction that takes two series as inputs and return a Series or a scalar. Used\nto merge the two dataframes column by columns.\n\nThe value to fill NaNs with prior to passing any column to the merge func.\n\nIf True, columns in self that do not exist in other will be overwritten with\nNaNs.\n\nCombination of the provided DataFrames.\n\nSee also\n\nCombine two DataFrame objects and default to non-null values in frame calling\nthe method.\n\nExamples\n\nCombine using a simple function that chooses the smaller column.\n\nExample using a true element-wise combine function.\n\nUsing fill_value fills Nones prior to passing the column to the merge\nfunction.\n\nHowever, if the same element in both dataframes is None, that None is\npreserved\n\nExample that demonstrates the use of overwrite and behavior when the axis\ndiffer between the dataframes.\n\nDemonstrating the preference of the passed in dataframe.\n\n"}, {"name": "pandas.DataFrame.combine_first", "path": "reference/api/pandas.dataframe.combine_first", "type": "DataFrame", "text": "\nUpdate null elements with value in the same location in other.\n\nCombine two DataFrame objects by filling null values in one DataFrame with\nnon-null values from other DataFrame. The row and column indexes of the\nresulting DataFrame will be the union of the two.\n\nProvided DataFrame to use to fill null values.\n\nThe result of combining the provided DataFrame with the other object.\n\nSee also\n\nPerform series-wise operation on two DataFrames using a given function.\n\nExamples\n\nNull values still persist if the location of that null value does not exist in\nother\n\n"}, {"name": "pandas.DataFrame.compare", "path": "reference/api/pandas.dataframe.compare", "type": "DataFrame", "text": "\nCompare to another DataFrame and show the differences.\n\nNew in version 1.1.0.\n\nObject to compare with.\n\nDetermine which axis to align the comparison on.\n\nwith rows drawn alternately from self and other.\n\nwith columns drawn alternately from self and other.\n\nIf true, all rows and columns are kept. Otherwise, only the ones with\ndifferent values are kept.\n\nIf true, the result keeps values that are equal. Otherwise, equal values are\nshown as NaNs.\n\nDataFrame that shows the differences stacked side by side.\n\nThe resulting index will be a MultiIndex with \u2018self\u2019 and \u2018other\u2019 stacked\nalternately at the inner level.\n\nWhen the two DataFrames don\u2019t have identical labels or shape.\n\nSee also\n\nCompare with another Series and show differences.\n\nTest whether two objects contain the same elements.\n\nNotes\n\nMatching NaNs will not appear as a difference.\n\nCan only compare identically-labeled (i.e. same shape, identical row and\ncolumn labels) DataFrames\n\nExamples\n\nAlign the differences on columns\n\nStack the differences on rows\n\nKeep the equal values\n\nKeep all original rows and columns\n\nKeep all original rows and columns and also all original values\n\n"}, {"name": "pandas.DataFrame.convert_dtypes", "path": "reference/api/pandas.dataframe.convert_dtypes", "type": "General utility functions", "text": "\nConvert columns to best possible dtypes using dtypes supporting `pd.NA`.\n\nNew in version 1.0.0.\n\nWhether object dtypes should be converted to the best possible types.\n\nWhether object dtypes should be converted to `StringDtype()`.\n\nWhether, if possible, conversion can be done to integer extension types.\n\nWhether object dtypes should be converted to `BooleanDtypes()`.\n\nWhether, if possible, conversion can be done to floating extension types. If\nconvert_integer is also True, preference will be give to integer dtypes if the\nfloats can be faithfully casted to integers.\n\nNew in version 1.2.0.\n\nCopy of input object with new dtype.\n\nSee also\n\nInfer dtypes of objects.\n\nConvert argument to datetime.\n\nConvert argument to timedelta.\n\nConvert argument to a numeric type.\n\nNotes\n\nBy default, `convert_dtypes` will attempt to convert a Series (or each Series\nin a DataFrame) to dtypes that support `pd.NA`. By using the options\n`convert_string`, `convert_integer`, `convert_boolean` and `convert_boolean`,\nit is possible to turn off individual conversions to `StringDtype`, the\ninteger extension types, `BooleanDtype` or floating extension types,\nrespectively.\n\nFor object-dtyped columns, if `infer_objects` is `True`, use the inference\nrules as during normal Series/DataFrame construction. Then, if possible,\nconvert to `StringDtype`, `BooleanDtype` or an appropriate integer or floating\nextension type, otherwise leave as `object`.\n\nIf the dtype is integer, convert to an appropriate integer extension type.\n\nIf the dtype is numeric, and consists of all integers, convert to an\nappropriate integer extension type. Otherwise, convert to an appropriate\nfloating extension type.\n\nChanged in version 1.2: Starting with pandas 1.2, this method also converts\nfloat columns to the nullable floating extension type.\n\nIn the future, as new dtypes are added that support `pd.NA`, the results of\nthis method will change to support those new dtypes.\n\nExamples\n\nStart with a DataFrame with default dtypes.\n\nConvert the DataFrame to use best possible dtypes.\n\nStart with a Series of strings and missing data represented by `np.nan`.\n\nObtain a Series with dtype `StringDtype`.\n\n"}, {"name": "pandas.DataFrame.copy", "path": "reference/api/pandas.dataframe.copy", "type": "DataFrame", "text": "\nMake a copy of this object\u2019s indices and data.\n\nWhen `deep=True` (default), a new object will be created with a copy of the\ncalling object\u2019s data and indices. Modifications to the data or indices of the\ncopy will not be reflected in the original object (see notes below).\n\nWhen `deep=False`, a new object will be created without copying the calling\nobject\u2019s data or index (only references to the data and index are copied). Any\nchanges to the data of the original will be reflected in the shallow copy (and\nvice versa).\n\nMake a deep copy, including a copy of the data and the indices. With\n`deep=False` neither the indices nor the data are copied.\n\nObject type matches caller.\n\nNotes\n\nWhen `deep=True`, data is copied but actual Python objects will not be copied\nrecursively, only the reference to the object. This is in contrast to\ncopy.deepcopy in the Standard Library, which recursively copies object data\n(see examples below).\n\nWhile `Index` objects are copied when `deep=True`, the underlying numpy array\nis not copied for performance reasons. Since `Index` is immutable, the\nunderlying data can be safely shared and a copy is not needed.\n\nExamples\n\nShallow copy versus default (deep) copy:\n\nShallow copy shares data and index with original.\n\nDeep copy has own copy of data and index.\n\nUpdates to the data shared by shallow copy and original is reflected in both;\ndeep copy remains unchanged.\n\nNote that when copying an object containing Python objects, a deep copy will\ncopy the data, but will not do so recursively. Updating a nested data object\nwill be reflected in the deep copy.\n\n"}, {"name": "pandas.DataFrame.corr", "path": "reference/api/pandas.dataframe.corr", "type": "DataFrame", "text": "\nCompute pairwise correlation of columns, excluding NA/null values.\n\nMethod of correlation:\n\npearson : standard correlation coefficient\n\nkendall : Kendall Tau correlation coefficient\n\nspearman : Spearman rank correlation\n\nand returning a float. Note that the returned matrix from corr will have 1\nalong the diagonals and will be symmetric regardless of the callable\u2019s\nbehavior.\n\nMinimum number of observations required per pair of columns to have a valid\nresult. Currently only available for Pearson and Spearman correlation.\n\nCorrelation matrix.\n\nSee also\n\nCompute pairwise correlation with another DataFrame or Series.\n\nCompute the correlation between two Series.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.corrwith", "path": "reference/api/pandas.dataframe.corrwith", "type": "DataFrame", "text": "\nCompute pairwise correlation.\n\nPairwise correlation is computed between rows or columns of DataFrame with\nrows or columns of Series or DataFrame. DataFrames are first aligned along\nboth axes before computing the correlations.\n\nObject with which to compute correlations.\n\nThe axis to use. 0 or \u2018index\u2019 to compute column-wise, 1 or \u2018columns\u2019 for row-\nwise.\n\nDrop missing indices from result.\n\nMethod of correlation:\n\npearson : standard correlation coefficient\n\nkendall : Kendall Tau correlation coefficient\n\nspearman : Spearman rank correlation\n\nand returning a float.\n\nPairwise correlations.\n\nSee also\n\nCompute pairwise correlation of columns.\n\n"}, {"name": "pandas.DataFrame.count", "path": "reference/api/pandas.dataframe.count", "type": "DataFrame", "text": "\nCount non-NA cells for each column or row.\n\nThe values None, NaN, NaT, and optionally numpy.inf (depending on\npandas.options.mode.use_inf_as_na) are considered NA.\n\nIf 0 or \u2018index\u2019 counts are generated for each column. If 1 or \u2018columns\u2019 counts\nare generated for each row.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a DataFrame. A str specifies the level name.\n\nInclude only float, int or boolean data.\n\nFor each column/row the number of non-NA/null entries. If level is specified\nreturns a DataFrame.\n\nSee also\n\nNumber of non-NA elements in a Series.\n\nCount unique combinations of columns.\n\nNumber of DataFrame rows and columns (including NA elements).\n\nBoolean same-sized DataFrame showing places of NA elements.\n\nExamples\n\nConstructing DataFrame from a dictionary:\n\nNotice the uncounted NA values:\n\nCounts for each row:\n\n"}, {"name": "pandas.DataFrame.cov", "path": "reference/api/pandas.dataframe.cov", "type": "DataFrame", "text": "\nCompute pairwise covariance of columns, excluding NA/null values.\n\nCompute the pairwise covariance among the series of a DataFrame. The returned\ndata frame is the covariance matrix of the columns of the DataFrame.\n\nBoth NA and null values are automatically excluded from the calculation. (See\nthe note below about bias from missing values.) A threshold can be set for the\nminimum number of observations for each value created. Comparisons with\nobservations below this threshold will be returned as `NaN`.\n\nThis method is generally used for the analysis of time series data to\nunderstand the relationship between different measures across time.\n\nMinimum number of observations required per pair of columns to have a valid\nresult.\n\nDelta degrees of freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nNew in version 1.1.0.\n\nThe covariance matrix of the series of the DataFrame.\n\nSee also\n\nCompute covariance with another Series.\n\nExponential weighted sample covariance.\n\nExpanding sample covariance.\n\nRolling sample covariance.\n\nNotes\n\nReturns the covariance matrix of the DataFrame\u2019s time series. The covariance\nis normalized by N-ddof.\n\nFor DataFrames that have Series that are missing data (assuming that data is\nmissing at random) the returned covariance matrix will be an unbiased estimate\nof the variance and covariance between the member Series.\n\nHowever, for many applications this estimate may not be acceptable because the\nestimate covariance matrix is not guaranteed to be positive semi-definite.\nThis could lead to estimate correlations having absolute values which are\ngreater than one, and/or a non-invertible covariance matrix. See Estimation of\ncovariance matrices for more details.\n\nExamples\n\nMinimum number of periods\n\nThis method also supports an optional `min_periods` keyword that specifies the\nrequired minimum number of non-NA observations for each column pair in order\nto have a valid result:\n\n"}, {"name": "pandas.DataFrame.cummax", "path": "reference/api/pandas.dataframe.cummax", "type": "DataFrame", "text": "\nReturn cumulative maximum over a DataFrame or Series axis.\n\nReturns a DataFrame or Series of the same size containing the cumulative\nmaximum.\n\nThe index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nReturn cumulative maximum of Series or DataFrame.\n\nSee also\n\nSimilar functionality but ignores `NaN` values.\n\nReturn the maximum over DataFrame axis.\n\nReturn cumulative maximum over DataFrame axis.\n\nReturn cumulative minimum over DataFrame axis.\n\nReturn cumulative sum over DataFrame axis.\n\nReturn cumulative product over DataFrame axis.\n\nExamples\n\nSeries\n\nBy default, NA values are ignored.\n\nTo include NA values in the operation, use `skipna=False`\n\nDataFrame\n\nBy default, iterates over rows and finds the maximum in each column. This is\nequivalent to `axis=None` or `axis='index'`.\n\nTo iterate over columns and find the maximum in each row, use `axis=1`\n\n"}, {"name": "pandas.DataFrame.cummin", "path": "reference/api/pandas.dataframe.cummin", "type": "DataFrame", "text": "\nReturn cumulative minimum over a DataFrame or Series axis.\n\nReturns a DataFrame or Series of the same size containing the cumulative\nminimum.\n\nThe index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nReturn cumulative minimum of Series or DataFrame.\n\nSee also\n\nSimilar functionality but ignores `NaN` values.\n\nReturn the minimum over DataFrame axis.\n\nReturn cumulative maximum over DataFrame axis.\n\nReturn cumulative minimum over DataFrame axis.\n\nReturn cumulative sum over DataFrame axis.\n\nReturn cumulative product over DataFrame axis.\n\nExamples\n\nSeries\n\nBy default, NA values are ignored.\n\nTo include NA values in the operation, use `skipna=False`\n\nDataFrame\n\nBy default, iterates over rows and finds the minimum in each column. This is\nequivalent to `axis=None` or `axis='index'`.\n\nTo iterate over columns and find the minimum in each row, use `axis=1`\n\n"}, {"name": "pandas.DataFrame.cumprod", "path": "reference/api/pandas.dataframe.cumprod", "type": "DataFrame", "text": "\nReturn cumulative product over a DataFrame or Series axis.\n\nReturns a DataFrame or Series of the same size containing the cumulative\nproduct.\n\nThe index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nReturn cumulative product of Series or DataFrame.\n\nSee also\n\nSimilar functionality but ignores `NaN` values.\n\nReturn the product over DataFrame axis.\n\nReturn cumulative maximum over DataFrame axis.\n\nReturn cumulative minimum over DataFrame axis.\n\nReturn cumulative sum over DataFrame axis.\n\nReturn cumulative product over DataFrame axis.\n\nExamples\n\nSeries\n\nBy default, NA values are ignored.\n\nTo include NA values in the operation, use `skipna=False`\n\nDataFrame\n\nBy default, iterates over rows and finds the product in each column. This is\nequivalent to `axis=None` or `axis='index'`.\n\nTo iterate over columns and find the product in each row, use `axis=1`\n\n"}, {"name": "pandas.DataFrame.cumsum", "path": "reference/api/pandas.dataframe.cumsum", "type": "DataFrame", "text": "\nReturn cumulative sum over a DataFrame or Series axis.\n\nReturns a DataFrame or Series of the same size containing the cumulative sum.\n\nThe index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nReturn cumulative sum of Series or DataFrame.\n\nSee also\n\nSimilar functionality but ignores `NaN` values.\n\nReturn the sum over DataFrame axis.\n\nReturn cumulative maximum over DataFrame axis.\n\nReturn cumulative minimum over DataFrame axis.\n\nReturn cumulative sum over DataFrame axis.\n\nReturn cumulative product over DataFrame axis.\n\nExamples\n\nSeries\n\nBy default, NA values are ignored.\n\nTo include NA values in the operation, use `skipna=False`\n\nDataFrame\n\nBy default, iterates over rows and finds the sum in each column. This is\nequivalent to `axis=None` or `axis='index'`.\n\nTo iterate over columns and find the sum in each row, use `axis=1`\n\n"}, {"name": "pandas.DataFrame.describe", "path": "reference/api/pandas.dataframe.describe", "type": "DataFrame", "text": "\nGenerate descriptive statistics.\n\nDescriptive statistics include those that summarize the central tendency,\ndispersion and shape of a dataset\u2019s distribution, excluding `NaN` values.\n\nAnalyzes both numeric and object series, as well as `DataFrame` column sets of\nmixed data types. The output will vary depending on what is provided. Refer to\nthe notes below for more detail.\n\nThe percentiles to include in the output. All should fall between 0 and 1. The\ndefault is `[.25, .5, .75]`, which returns the 25th, 50th, and 75th\npercentiles.\n\nA white list of data types to include in the result. Ignored for `Series`.\nHere are the options:\n\n\u2018all\u2019 : All columns of the input will be included in the output.\n\nA list-like of dtypes : Limits the results to the provided data types. To\nlimit the result to numeric types submit `numpy.number`. To limit it instead\nto object columns submit the `numpy.object` data type. Strings can also be\nused in the style of `select_dtypes` (e.g. `df.describe(include=['O'])`). To\nselect pandas categorical columns, use `'category'`\n\nNone (default) : The result will include all numeric columns.\n\nA black list of data types to omit from the result. Ignored for `Series`. Here\nare the options:\n\nA list-like of dtypes : Excludes the provided data types from the result. To\nexclude numeric types submit `numpy.number`. To exclude object columns submit\nthe data type `numpy.object`. Strings can also be used in the style of\n`select_dtypes` (e.g. `df.describe(exclude=['O'])`). To exclude pandas\ncategorical columns, use `'category'`\n\nNone (default) : The result will exclude nothing.\n\nWhether to treat datetime dtypes as numeric. This affects statistics\ncalculated for the column. For DataFrame input, this also controls whether\ndatetime columns are included by default.\n\nNew in version 1.1.0.\n\nSummary statistics of the Series or Dataframe provided.\n\nSee also\n\nCount number of non-NA/null observations.\n\nMaximum of the values in the object.\n\nMinimum of the values in the object.\n\nMean of the values.\n\nStandard deviation of the observations.\n\nSubset of a DataFrame including/excluding columns based on their dtype.\n\nNotes\n\nFor numeric data, the result\u2019s index will include `count`, `mean`, `std`,\n`min`, `max` as well as lower, `50` and upper percentiles. By default the\nlower percentile is `25` and the upper percentile is `75`. The `50` percentile\nis the same as the median.\n\nFor object data (e.g. strings or timestamps), the result\u2019s index will include\n`count`, `unique`, `top`, and `freq`. The `top` is the most common value. The\n`freq` is the most common value\u2019s frequency. Timestamps also include the\n`first` and `last` items.\n\nIf multiple object values have the highest count, then the `count` and `top`\nresults will be arbitrarily chosen from among those with the highest count.\n\nFor mixed data types provided via a `DataFrame`, the default is to return only\nan analysis of numeric columns. If the dataframe consists only of object and\ncategorical data without any numeric columns, the default is to return an\nanalysis of both the object and categorical columns. If `include='all'` is\nprovided as an option, the result will include a union of attributes of each\ntype.\n\nThe include and exclude parameters can be used to limit which columns in a\n`DataFrame` are analyzed for the output. The parameters are ignored when\nanalyzing a `Series`.\n\nExamples\n\nDescribing a numeric `Series`.\n\nDescribing a categorical `Series`.\n\nDescribing a timestamp `Series`.\n\nDescribing a `DataFrame`. By default only numeric fields are returned.\n\nDescribing all columns of a `DataFrame` regardless of data type.\n\nDescribing a column from a `DataFrame` by accessing it as an attribute.\n\nIncluding only numeric columns in a `DataFrame` description.\n\nIncluding only string columns in a `DataFrame` description.\n\nIncluding only categorical columns from a `DataFrame` description.\n\nExcluding numeric columns from a `DataFrame` description.\n\nExcluding object columns from a `DataFrame` description.\n\n"}, {"name": "pandas.DataFrame.diff", "path": "reference/api/pandas.dataframe.diff", "type": "DataFrame", "text": "\nFirst discrete difference of element.\n\nCalculates the difference of a Dataframe element compared with another element\nin the Dataframe (default is element in previous row).\n\nPeriods to shift for calculating difference, accepts negative values.\n\nTake difference over rows (0) or columns (1).\n\nFirst differences of the Series.\n\nSee also\n\nPercent change over given number of periods.\n\nShift index by desired number of periods with an optional time freq.\n\nFirst discrete difference of object.\n\nNotes\n\nFor boolean dtypes, this uses `operator.xor()` rather than `operator.sub()`.\nThe result is calculated according to current dtype in Dataframe, however\ndtype of the result is always float64.\n\nExamples\n\nDifference with previous row\n\nDifference with previous column\n\nDifference with 3rd previous row\n\nDifference with following row\n\nOverflow in input dtype\n\n"}, {"name": "pandas.DataFrame.div", "path": "reference/api/pandas.dataframe.div", "type": "DataFrame", "text": "\nGet Floating division of dataframe and other, element-wise (binary operator\ntruediv).\n\nEquivalent to `dataframe / other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, rtruediv.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.divide", "path": "reference/api/pandas.dataframe.divide", "type": "DataFrame", "text": "\nGet Floating division of dataframe and other, element-wise (binary operator\ntruediv).\n\nEquivalent to `dataframe / other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, rtruediv.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.dot", "path": "reference/api/pandas.dataframe.dot", "type": "DataFrame", "text": "\nCompute the matrix multiplication between the DataFrame and other.\n\nThis method computes the matrix product between the DataFrame and the values\nof an other Series, DataFrame or a numpy array.\n\nIt can also be called using `self @ other` in Python >= 3.5.\n\nThe other object to compute the matrix product with.\n\nIf other is a Series, return the matrix product between self and other as a\nSeries. If other is a DataFrame or a numpy.array, return the matrix product of\nself and other in a DataFrame of a np.array.\n\nSee also\n\nSimilar method for Series.\n\nNotes\n\nThe dimensions of DataFrame and other must be compatible in order to compute\nthe matrix multiplication. In addition, the column names of DataFrame and the\nindex of other must contain the same values, as they will be aligned prior to\nthe multiplication.\n\nThe dot method for Series computes the inner product, instead of the matrix\nproduct here.\n\nExamples\n\nHere we multiply a DataFrame with a Series.\n\nHere we multiply a DataFrame with another DataFrame.\n\nNote that the dot method give the same result as @\n\nThe dot method works also if other is an np.array.\n\nNote how shuffling of the objects does not change the result.\n\n"}, {"name": "pandas.DataFrame.drop", "path": "reference/api/pandas.dataframe.drop", "type": "DataFrame", "text": "\nDrop specified labels from rows or columns.\n\nRemove rows or columns by specifying label names and corresponding axis, or by\nspecifying directly index or column names. When using a multi-index, labels on\ndifferent levels can be removed by specifying the level. See the user guide\n<advanced.shown_levels> for more information about the now unused levels.\n\nIndex or column labels to drop. A tuple will be used as a single label and not\ntreated as a list-like.\n\nWhether to drop labels from the index (0 or \u2018index\u2019) or columns (1 or\n\u2018columns\u2019).\n\nAlternative to specifying axis (`labels, axis=0` is equivalent to\n`index=labels`).\n\nAlternative to specifying axis (`labels, axis=1` is equivalent to\n`columns=labels`).\n\nFor MultiIndex, level from which the labels will be removed.\n\nIf False, return a copy. Otherwise, do operation inplace and return None.\n\nIf \u2018ignore\u2019, suppress error and only existing labels are dropped.\n\nDataFrame without the removed index or column labels or None if\n`inplace=True`.\n\nIf any of the labels is not found in the selected axis.\n\nSee also\n\nLabel-location based indexer for selection by label.\n\nReturn DataFrame with labels on given axis omitted where (all or any) data are\nmissing.\n\nReturn DataFrame with duplicate rows removed, optionally only considering\ncertain columns.\n\nReturn Series with specified index labels removed.\n\nExamples\n\nDrop columns\n\nDrop a row by index\n\nDrop columns and/or rows of MultiIndex DataFrame\n\nDrop a specific index combination from the MultiIndex DataFrame, i.e., drop\nthe combination `'falcon'` and `'weight'`, which deletes only the\ncorresponding row\n\n"}, {"name": "pandas.DataFrame.drop_duplicates", "path": "reference/api/pandas.dataframe.drop_duplicates", "type": "DataFrame", "text": "\nReturn DataFrame with duplicate rows removed.\n\nConsidering certain columns is optional. Indexes, including time indexes are\nignored.\n\nOnly consider certain columns for identifying duplicates, by default use all\nof the columns.\n\nDetermines which duplicates (if any) to keep. - `first` : Drop duplicates\nexcept for the first occurrence. - `last` : Drop duplicates except for the\nlast occurrence. - False : Drop all duplicates.\n\nWhether to drop duplicates in place or to return a copy.\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nNew in version 1.0.0.\n\nDataFrame with duplicates removed or None if `inplace=True`.\n\nSee also\n\nCount unique combinations of columns.\n\nExamples\n\nConsider dataset containing ramen rating.\n\nBy default, it removes duplicate rows based on all columns.\n\nTo remove duplicates on specific column(s), use `subset`.\n\nTo remove duplicates and keep last occurrences, use `keep`.\n\n"}, {"name": "pandas.DataFrame.droplevel", "path": "reference/api/pandas.dataframe.droplevel", "type": "DataFrame", "text": "\nReturn Series/DataFrame with requested index / column level(s) removed.\n\nIf a string is given, must be the name of a level If list-like, elements must\nbe names or positional indexes of levels.\n\nAxis along which the level(s) is removed:\n\n0 or \u2018index\u2019: remove level(s) in column.\n\n1 or \u2018columns\u2019: remove level(s) in row.\n\nSeries/DataFrame with requested index / column level(s) removed.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.dropna", "path": "reference/api/pandas.dataframe.dropna", "type": "DataFrame", "text": "\nRemove missing values.\n\nSee the User Guide for more on which values are considered missing, and how to\nwork with missing data.\n\nDetermine if rows or columns which contain missing values are removed.\n\n0, or \u2018index\u2019 : Drop rows which contain missing values.\n\n1, or \u2018columns\u2019 : Drop columns which contain missing value.\n\nChanged in version 1.0.0: Pass tuple or list to drop on multiple axes. Only a\nsingle axis is allowed.\n\nDetermine if row or column is removed from DataFrame, when we have at least\none NA or all NA.\n\n\u2018any\u2019 : If any NA values are present, drop that row or column.\n\n\u2018all\u2019 : If all values are NA, drop that row or column.\n\nRequire that many non-NA values.\n\nLabels along other axis to consider, e.g. if you are dropping rows these would\nbe a list of columns to include.\n\nIf True, do operation inplace and return None.\n\nDataFrame with NA entries dropped from it or None if `inplace=True`.\n\nSee also\n\nIndicate missing values.\n\nIndicate existing (non-missing) values.\n\nReplace missing values.\n\nDrop missing values.\n\nDrop missing indices.\n\nExamples\n\nDrop the rows where at least one element is missing.\n\nDrop the columns where at least one element is missing.\n\nDrop the rows where all elements are missing.\n\nKeep only the rows with at least 2 non-NA values.\n\nDefine in which columns to look for missing values.\n\nKeep the DataFrame with valid entries in the same variable.\n\n"}, {"name": "pandas.DataFrame.dtypes", "path": "reference/api/pandas.dataframe.dtypes", "type": "General utility functions", "text": "\nReturn the dtypes in the DataFrame.\n\nThis returns a Series with the data type of each column. The result\u2019s index is\nthe original DataFrame\u2019s columns. Columns with mixed types are stored with the\n`object` dtype. See the User Guide for more.\n\nThe data type of each column.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.duplicated", "path": "reference/api/pandas.dataframe.duplicated", "type": "DataFrame", "text": "\nReturn boolean Series denoting duplicate rows.\n\nConsidering certain columns is optional.\n\nOnly consider certain columns for identifying duplicates, by default use all\nof the columns.\n\nDetermines which duplicates (if any) to mark.\n\n`first` : Mark duplicates as `True` except for the first occurrence.\n\n`last` : Mark duplicates as `True` except for the last occurrence.\n\nFalse : Mark all duplicates as `True`.\n\nBoolean series for each duplicated rows.\n\nSee also\n\nEquivalent method on index.\n\nEquivalent method on Series.\n\nRemove duplicate values from Series.\n\nRemove duplicate values from DataFrame.\n\nExamples\n\nConsider dataset containing ramen rating.\n\nBy default, for each set of duplicated values, the first occurrence is set on\nFalse and all others on True.\n\nBy using \u2018last\u2019, the last occurrence of each set of duplicated values is set\non False and all others on True.\n\nBy setting `keep` on False, all duplicates are True.\n\nTo find duplicates on specific column(s), use `subset`.\n\n"}, {"name": "pandas.DataFrame.empty", "path": "reference/api/pandas.dataframe.empty", "type": "DataFrame", "text": "\nIndicator whether Series/DataFrame is empty.\n\nTrue if Series/DataFrame is entirely empty (no items), meaning any of the axes\nare of length 0.\n\nIf Series/DataFrame is empty, return True, if not return False.\n\nSee also\n\nReturn series without null values.\n\nReturn DataFrame with labels on given axis omitted where (all or any) data are\nmissing.\n\nNotes\n\nIf Series/DataFrame contains only NaNs, it is still not considered empty. See\nthe example below.\n\nExamples\n\nAn example of an actual empty DataFrame. Notice the index is empty:\n\nIf we only have NaNs in our DataFrame, it is not considered empty! We will\nneed to drop the NaNs to make the DataFrame empty:\n\n"}, {"name": "pandas.DataFrame.eq", "path": "reference/api/pandas.dataframe.eq", "type": "DataFrame", "text": "\nGet Equal to of dataframe and other, element-wise (binary operator eq).\n\nAmong flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.\n\nEquivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or\ncolumns) and level for comparison.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nResult of the comparison.\n\nSee also\n\nCompare DataFrames for equality elementwise.\n\nCompare DataFrames for inequality elementwise.\n\nCompare DataFrames for less than inequality or equality elementwise.\n\nCompare DataFrames for strictly less than inequality elementwise.\n\nCompare DataFrames for greater than inequality or equality elementwise.\n\nCompare DataFrames for strictly greater than inequality elementwise.\n\nNotes\n\nMismatched indices will be unioned together. NaN values are considered\ndifferent (i.e. NaN != NaN).\n\nExamples\n\nComparison with a scalar, using either the operator or method:\n\nWhen other is a `Series`, the columns of a DataFrame are aligned with the\nindex of other and broadcast:\n\nUse the method to control the broadcast axis:\n\nWhen comparing to an arbitrary sequence, the number of columns must match the\nnumber elements in other:\n\nUse the method to control the axis:\n\nCompare to a DataFrame of different shape.\n\nCompare to a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.equals", "path": "reference/api/pandas.dataframe.equals", "type": "DataFrame", "text": "\nTest whether two objects contain the same elements.\n\nThis function allows two Series or DataFrames to be compared against each\nother to see if they have the same shape and elements. NaNs in the same\nlocation are considered equal.\n\nThe row/column index do not need to have the same type, as long as the values\nare considered equal. Corresponding columns must be of the same dtype.\n\nThe other Series or DataFrame to be compared with the first.\n\nTrue if all elements are the same in both objects, False otherwise.\n\nSee also\n\nCompare two Series objects of the same length and return a Series where each\nelement is True if the element in each Series is equal, False otherwise.\n\nCompare two DataFrame objects of the same shape and return a DataFrame where\neach element is True if the respective element in each DataFrame is equal,\nFalse otherwise.\n\nRaises an AssertionError if left and right are not equal. Provides an easy\ninterface to ignore inequality in dtypes, indexes and precision among others.\n\nLike assert_series_equal, but targets DataFrames.\n\nReturn True if two arrays have the same shape and elements, False otherwise.\n\nExamples\n\nDataFrames df and exactly_equal have the same types and values for their\nelements and column labels, which will return True.\n\nDataFrames df and different_column_type have the same element types and\nvalues, but have different types for the column labels, which will still\nreturn True.\n\nDataFrames df and different_data_type have different types for the same values\nfor their elements, and will return False even though their column labels are\nthe same values and types.\n\n"}, {"name": "pandas.DataFrame.eval", "path": "reference/api/pandas.dataframe.eval", "type": "DataFrame", "text": "\nEvaluate a string describing operations on DataFrame columns.\n\nOperates on columns only, not specific rows or elements. This allows eval to\nrun arbitrary code, which can make you vulnerable to code injection if you\npass user input to this function.\n\nThe expression string to evaluate.\n\nIf the expression contains an assignment, whether to perform the operation\ninplace and mutate the existing DataFrame. Otherwise, a new DataFrame is\nreturned.\n\nSee the documentation for `eval()` for complete details on the keyword\narguments accepted by `query()`.\n\nThe result of the evaluation or None if `inplace=True`.\n\nSee also\n\nEvaluates a boolean expression to query the columns of a frame.\n\nCan evaluate an expression or function to create new values for a column.\n\nEvaluate a Python expression as a string using various backends.\n\nNotes\n\nFor more details see the API documentation for `eval()`. For detailed examples\nsee enhancing performance with eval.\n\nExamples\n\nAssignment is allowed though by default the original DataFrame is not\nmodified.\n\nUse `inplace=True` to modify the original DataFrame.\n\nMultiple columns can be assigned to using multi-line expressions:\n\n"}, {"name": "pandas.DataFrame.ewm", "path": "reference/api/pandas.dataframe.ewm", "type": "DataFrame", "text": "\nProvide exponentially weighted (EW) calculations.\n\nExactly one parameter: `com`, `span`, `halflife`, or `alpha` must be provided.\n\nSpecify decay in terms of center of mass\n\n\\\\(\\alpha = 1 / (1 + com)\\\\), for \\\\(com \\geq 0\\\\).\n\nSpecify decay in terms of span\n\n\\\\(\\alpha = 2 / (span + 1)\\\\), for \\\\(span \\geq 1\\\\).\n\nSpecify decay in terms of half-life\n\n\\\\(\\alpha = 1 - \\exp\\left(-\\ln(2) / halflife\\right)\\\\), for \\\\(halflife >\n0\\\\).\n\nIf `times` is specified, the time unit (str or timedelta) over which an\nobservation decays to half its value. Only applicable to `mean()`, and\nhalflife value will not apply to the other functions.\n\nNew in version 1.1.0.\n\nSpecify smoothing factor \\\\(\\alpha\\\\) directly\n\n\\\\(0 < \\alpha \\leq 1\\\\).\n\nMinimum number of observations in window required to have a value; otherwise,\nresult is `np.nan`.\n\nDivide by decaying adjustment factor in beginning periods to account for\nimbalance in relative weightings (viewing EWMA as a moving average).\n\nWhen `adjust=True` (default), the EW function is calculated using weights\n\\\\(w_i = (1 - \\alpha)^i\\\\). For example, the EW moving average of the series\n[\\\\(x_0, x_1, ..., x_t\\\\)] would be:\n\nWhen `adjust=False`, the exponentially weighted function is calculated\nrecursively:\n\nIgnore missing values when calculating weights.\n\nWhen `ignore_na=False` (default), weights are based on absolute positions. For\nexample, the weights of \\\\(x_0\\\\) and \\\\(x_2\\\\) used in calculating the final\nweighted average of [\\\\(x_0\\\\), None, \\\\(x_2\\\\)] are \\\\((1-\\alpha)^2\\\\) and\n\\\\(1\\\\) if `adjust=True`, and \\\\((1-\\alpha)^2\\\\) and \\\\(\\alpha\\\\) if\n`adjust=False`.\n\nWhen `ignore_na=True`, weights are based on relative positions. For example,\nthe weights of \\\\(x_0\\\\) and \\\\(x_2\\\\) used in calculating the final weighted\naverage of [\\\\(x_0\\\\), None, \\\\(x_2\\\\)] are \\\\(1-\\alpha\\\\) and \\\\(1\\\\) if\n`adjust=True`, and \\\\(1-\\alpha\\\\) and \\\\(\\alpha\\\\) if `adjust=False`.\n\nIf `0` or `'index'`, calculate across the rows.\n\nIf `1` or `'columns'`, calculate across the columns.\n\nNew in version 1.1.0.\n\nOnly applicable to `mean()`.\n\nTimes corresponding to the observations. Must be monotonically increasing and\n`datetime64[ns]` dtype.\n\nIf 1-D array like, a sequence with the same shape as the observations.\n\nDeprecated since version 1.4.0: If str, the name of the column in the\nDataFrame representing the times.\n\nNew in version 1.4.0.\n\nExecute the rolling operation per single column or row (`'single'`) or over\nthe entire object (`'table'`).\n\nThis argument is only implemented when specifying `engine='numba'` in the\nmethod call.\n\nOnly applicable to `mean()`\n\nSee also\n\nProvides rolling window calculations.\n\nProvides expanding transformations.\n\nNotes\n\nSee Windowing Operations for further usage details and examples.\n\nExamples\n\nadjust\n\nignore_na\n\ntimes\n\nExponentially weighted mean with weights calculated with a timedelta\n`halflife` relative to `times`.\n\n"}, {"name": "pandas.DataFrame.expanding", "path": "reference/api/pandas.dataframe.expanding", "type": "DataFrame", "text": "\nProvide expanding window calculations.\n\nMinimum number of observations in window required to have a value; otherwise,\nresult is `np.nan`.\n\nIf False, set the window labels as the right edge of the window index.\n\nIf True, set the window labels as the center of the window index.\n\nDeprecated since version 1.1.0.\n\nIf `0` or `'index'`, roll across the rows.\n\nIf `1` or `'columns'`, roll across the columns.\n\nExecute the rolling operation per single column or row (`'single'`) or over\nthe entire object (`'table'`).\n\nThis argument is only implemented when specifying `engine='numba'` in the\nmethod call.\n\nNew in version 1.3.0.\n\nSee also\n\nProvides rolling window calculations.\n\nProvides exponential weighted functions.\n\nNotes\n\nSee Windowing Operations for further usage details and examples.\n\nExamples\n\nmin_periods\n\nExpanding sum with 1 vs 3 observations needed to calculate a value.\n\n"}, {"name": "pandas.DataFrame.explode", "path": "reference/api/pandas.dataframe.explode", "type": "DataFrame", "text": "\nTransform each element of a list-like to a row, replicating index values.\n\nNew in version 0.25.0.\n\nColumn(s) to explode. For multiple columns, specify a non-empty list with each\nelement be str or tuple, and all specified columns their list-like data on\nsame row of the frame must have matching length.\n\nNew in version 1.3.0: Multi-column explode\n\nIf True, the resulting index will be labeled 0, 1, \u2026, n - 1.\n\nNew in version 1.1.0.\n\nExploded lists to rows of the subset columns; index will be duplicated for\nthese rows.\n\nIf columns of the frame are not unique.\n\nIf specified columns to explode is empty list.\n\nIf specified columns to explode have not matching count of elements rowwise in\nthe frame.\n\nSee also\n\nPivot a level of the (necessarily hierarchical) index labels.\n\nUnpivot a DataFrame from wide format to long format.\n\nExplode a DataFrame from list-like columns to long format.\n\nNotes\n\nThis routine will explode list-likes including lists, tuples, sets, Series,\nand np.ndarray. The result dtype of the subset rows will be object. Scalars\nwill be returned unchanged, and empty list-likes will result in a np.nan for\nthat row. In addition, the ordering of rows in the output will be non-\ndeterministic when exploding sets.\n\nExamples\n\nSingle-column explode.\n\nMulti-column explode.\n\n"}, {"name": "pandas.DataFrame.ffill", "path": "reference/api/pandas.dataframe.ffill", "type": "DataFrame", "text": "\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\nObject with missing values filled or None if `inplace=True`.\n\n"}, {"name": "pandas.DataFrame.fillna", "path": "reference/api/pandas.dataframe.fillna", "type": "DataFrame", "text": "\nFill NA/NaN values using the specified method.\n\nValue to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of\nvalues specifying which value to use for each index (for a Series) or column\n(for a DataFrame). Values not in the dict/Series/DataFrame will not be filled.\nThis value cannot be a list.\n\nMethod to use for filling holes in reindexed Series pad / ffill: propagate\nlast valid observation forward to next valid backfill / bfill: use next valid\nobservation to fill gap.\n\nAxis along which to fill missing values.\n\nIf True, fill in-place. Note: this will modify any other views on this object\n(e.g., a no-copy slice for a column in a DataFrame).\n\nIf method is specified, this is the maximum number of consecutive NaN values\nto forward/backward fill. In other words, if there is a gap with more than\nthis number of consecutive NaNs, it will only be partially filled. If method\nis not specified, this is the maximum number of entries along the entire axis\nwhere NaNs will be filled. Must be greater than 0 if not None.\n\nA dict of item->dtype of what to downcast if possible, or the string \u2018infer\u2019\nwhich will try to downcast to an appropriate equal type (e.g. float64 to int64\nif possible).\n\nObject with missing values filled or None if `inplace=True`.\n\nSee also\n\nFill NaN values using interpolation.\n\nConform object to new index.\n\nConvert TimeSeries to specified frequency.\n\nExamples\n\nReplace all NaN elements with 0s.\n\nWe can also propagate non-null values forward or backward.\n\nReplace all NaN elements in column \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019, with 0, 1, 2, and 3\nrespectively.\n\nOnly replace the first NaN element.\n\nWhen filling using a DataFrame, replacement happens along the same column\nnames and same indices\n\nNote that column D is not affected since it is not present in df2.\n\n"}, {"name": "pandas.DataFrame.filter", "path": "reference/api/pandas.dataframe.filter", "type": "DataFrame", "text": "\nSubset the dataframe rows or columns according to the specified index labels.\n\nNote that this routine does not filter a dataframe on its contents. The filter\nis applied to the labels of the index.\n\nKeep labels from axis which are in items.\n\nKeep labels from axis for which \u201clike in label == True\u201d.\n\nKeep labels from axis for which re.search(regex, label) == True.\n\nThe axis to filter on, expressed either as an index (int) or axis name (str).\nBy default this is the info axis, \u2018index\u2019 for Series, \u2018columns\u2019 for DataFrame.\n\nSee also\n\nAccess a group of rows and columns by label(s) or a boolean array.\n\nNotes\n\nThe `items`, `like`, and `regex` parameters are enforced to be mutually\nexclusive.\n\n`axis` defaults to the info axis that is used when indexing with `[]`.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.first", "path": "reference/api/pandas.dataframe.first", "type": "DataFrame", "text": "\nSelect initial periods of time series data based on a date offset.\n\nWhen having a DataFrame with dates as index, this function can select the\nfirst few rows based on a date offset.\n\nThe offset length of the data that will be selected. For instance, \u20181M\u2019 will\ndisplay all the rows having their index within the first month.\n\nA subset of the caller.\n\nIf the index is not a `DatetimeIndex`\n\nSee also\n\nSelect final periods of time series based on a date offset.\n\nSelect values at a particular time of the day.\n\nSelect values between particular times of the day.\n\nExamples\n\nGet the rows for the first 3 days:\n\nNotice the data for 3 first calendar days were returned, not the first 3 days\nobserved in the dataset, and therefore data for 2018-04-13 was not returned.\n\n"}, {"name": "pandas.DataFrame.first_valid_index", "path": "reference/api/pandas.dataframe.first_valid_index", "type": "DataFrame", "text": "\nReturn index for first non-NA value or None, if no NA value is found.\n\nNotes\n\nIf all elements are non-NA/null, returns None. Also returns None for empty\nSeries/DataFrame.\n\n"}, {"name": "pandas.DataFrame.flags", "path": "reference/api/pandas.dataframe.flags", "type": "DataFrame", "text": "\nGet the properties associated with this pandas object.\n\nThe available flags are\n\n`Flags.allows_duplicate_labels`\n\nSee also\n\nFlags that apply to pandas objects.\n\nGlobal metadata applying to this dataset.\n\nNotes\n\n\u201cFlags\u201d differ from \u201cmetadata\u201d. Flags reflect properties of the pandas object\n(the Series or DataFrame). Metadata refer to properties of the dataset, and\nshould be stored in `DataFrame.attrs`.\n\nExamples\n\nFlags can be get or set using `.`\n\nOr by slicing with a key\n\n"}, {"name": "pandas.DataFrame.floordiv", "path": "reference/api/pandas.dataframe.floordiv", "type": "DataFrame", "text": "\nGet Integer division of dataframe and other, element-wise (binary operator\nfloordiv).\n\nEquivalent to `dataframe // other`, but with support to substitute a\nfill_value for missing data in one of the inputs. With reverse version,\nrfloordiv.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.from_dict", "path": "reference/api/pandas.dataframe.from_dict", "type": "DataFrame", "text": "\nConstruct DataFrame from dict of array-like or dicts.\n\nCreates DataFrame object from dictionary by columns or by index allowing dtype\nspecification.\n\nOf the form {field : array-like} or {field : dict}.\n\nThe \u201corientation\u201d of the data. If the keys of the passed dict should be the\ncolumns of the resulting DataFrame, pass \u2018columns\u2019 (default). Otherwise if the\nkeys should be rows, pass \u2018index\u2019. If \u2018tight\u2019, assume a dict with keys\n[\u2018index\u2019, \u2018columns\u2019, \u2018data\u2019, \u2018index_names\u2019, \u2018column_names\u2019].\n\nNew in version 1.4.0: \u2018tight\u2019 as an allowed value for the `orient` argument\n\nData type to force, otherwise infer.\n\nColumn labels to use when `orient='index'`. Raises a ValueError if used with\n`orient='columns'` or `orient='tight'`.\n\nSee also\n\nDataFrame from structured ndarray, sequence of tuples or dicts, or DataFrame.\n\nDataFrame object creation using constructor.\n\nConvert the DataFrame to a dictionary.\n\nExamples\n\nBy default the keys of the dict become the DataFrame columns:\n\nSpecify `orient='index'` to create the DataFrame using dictionary keys as\nrows:\n\nWhen using the \u2018index\u2019 orientation, the column names can be specified\nmanually:\n\nSpecify `orient='tight'` to create the DataFrame using a \u2018tight\u2019 format:\n\n"}, {"name": "pandas.DataFrame.from_records", "path": "reference/api/pandas.dataframe.from_records", "type": "DataFrame", "text": "\nConvert structured or record ndarray to DataFrame.\n\nCreates a DataFrame object from a structured ndarray, sequence of tuples or\ndicts, or DataFrame.\n\nStructured input data.\n\nField of array to use as the index, alternately a specific set of input labels\nto use.\n\nColumns or fields to exclude.\n\nColumn names to use. If the passed data do not have names associated with\nthem, this argument provides names for the columns. Otherwise this argument\nindicates the order of the columns in the result (any names not found in the\ndata will become all-NA columns).\n\nAttempt to convert values of non-string, non-numeric objects (like\ndecimal.Decimal) to floating point, useful for SQL result sets.\n\nNumber of rows to read if data is an iterator.\n\nSee also\n\nDataFrame from dict of array-like or dicts.\n\nDataFrame object creation using constructor.\n\nExamples\n\nData can be provided as a structured ndarray:\n\nData can be provided as a list of dicts:\n\nData can be provided as a list of tuples with corresponding columns:\n\n"}, {"name": "pandas.DataFrame.ge", "path": "reference/api/pandas.dataframe.ge", "type": "DataFrame", "text": "\nGet Greater than or equal to of dataframe and other, element-wise (binary\noperator ge).\n\nAmong flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.\n\nEquivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or\ncolumns) and level for comparison.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nResult of the comparison.\n\nSee also\n\nCompare DataFrames for equality elementwise.\n\nCompare DataFrames for inequality elementwise.\n\nCompare DataFrames for less than inequality or equality elementwise.\n\nCompare DataFrames for strictly less than inequality elementwise.\n\nCompare DataFrames for greater than inequality or equality elementwise.\n\nCompare DataFrames for strictly greater than inequality elementwise.\n\nNotes\n\nMismatched indices will be unioned together. NaN values are considered\ndifferent (i.e. NaN != NaN).\n\nExamples\n\nComparison with a scalar, using either the operator or method:\n\nWhen other is a `Series`, the columns of a DataFrame are aligned with the\nindex of other and broadcast:\n\nUse the method to control the broadcast axis:\n\nWhen comparing to an arbitrary sequence, the number of columns must match the\nnumber elements in other:\n\nUse the method to control the axis:\n\nCompare to a DataFrame of different shape.\n\nCompare to a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.get", "path": "reference/api/pandas.dataframe.get", "type": "DataFrame", "text": "\nGet item from object for given key (ex: DataFrame column).\n\nReturns default value if not found.\n\nExamples\n\nIf the key isn\u2019t found, the default value will be used.\n\n"}, {"name": "pandas.DataFrame.groupby", "path": "reference/api/pandas.dataframe.groupby", "type": "GroupBy", "text": "\nGroup DataFrame using a mapper or by a Series of columns.\n\nA groupby operation involves some combination of splitting the object,\napplying a function, and combining the results. This can be used to group\nlarge amounts of data and compute operations on these groups.\n\nUsed to determine the groups for the groupby. If `by` is a function, it\u2019s\ncalled on each value of the object\u2019s index. If a dict or Series is passed, the\nSeries or dict VALUES will be used to determine the groups (the Series\u2019 values\nare first aligned; see `.align()` method). If a list or ndarray of length\nequal to the selected axis is passed (see the groupby user guide), the values\nare used as-is to determine the groups. A label or list of labels may be\npassed to group by the columns in `self`. Notice that a tuple is interpreted\nas a (single) key.\n\nSplit along rows (0) or columns (1).\n\nIf the axis is a MultiIndex (hierarchical), group by a particular level or\nlevels.\n\nFor aggregated output, return object with group labels as the index. Only\nrelevant for DataFrame input. as_index=False is effectively \u201cSQL-style\u201d\ngrouped output.\n\nSort group keys. Get better performance by turning this off. Note this does\nnot influence the order of observations within each group. Groupby preserves\nthe order of rows within each group.\n\nWhen calling apply, add group keys to index to identify pieces.\n\nReduce the dimensionality of the return type if possible, otherwise return a\nconsistent type.\n\nDeprecated since version 1.1.0.\n\nThis only applies if any of the groupers are Categoricals. If True: only show\nobserved values for categorical groupers. If False: show all values for\ncategorical groupers.\n\nIf True, and if group keys contain NA values, NA values together with\nrow/column will be dropped. If False, NA values will also be treated as the\nkey in groups.\n\nNew in version 1.1.0.\n\nReturns a groupby object that contains information about the groups.\n\nSee also\n\nConvenience method for frequency conversion and resampling of time series.\n\nNotes\n\nSee the user guide for more detailed usage and examples, including splitting\nan object into groups, iterating through groups, selecting a group,\naggregation, and more.\n\nExamples\n\nHierarchical Indexes\n\nWe can groupby different levels of a hierarchical index using the level\nparameter:\n\nWe can also choose to include NA in group keys or not by setting dropna\nparameter, the default setting is True.\n\n"}, {"name": "pandas.DataFrame.gt", "path": "reference/api/pandas.dataframe.gt", "type": "DataFrame", "text": "\nGet Greater than of dataframe and other, element-wise (binary operator gt).\n\nAmong flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.\n\nEquivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or\ncolumns) and level for comparison.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nResult of the comparison.\n\nSee also\n\nCompare DataFrames for equality elementwise.\n\nCompare DataFrames for inequality elementwise.\n\nCompare DataFrames for less than inequality or equality elementwise.\n\nCompare DataFrames for strictly less than inequality elementwise.\n\nCompare DataFrames for greater than inequality or equality elementwise.\n\nCompare DataFrames for strictly greater than inequality elementwise.\n\nNotes\n\nMismatched indices will be unioned together. NaN values are considered\ndifferent (i.e. NaN != NaN).\n\nExamples\n\nComparison with a scalar, using either the operator or method:\n\nWhen other is a `Series`, the columns of a DataFrame are aligned with the\nindex of other and broadcast:\n\nUse the method to control the broadcast axis:\n\nWhen comparing to an arbitrary sequence, the number of columns must match the\nnumber elements in other:\n\nUse the method to control the axis:\n\nCompare to a DataFrame of different shape.\n\nCompare to a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.head", "path": "reference/api/pandas.dataframe.head", "type": "DataFrame", "text": "\nReturn the first n rows.\n\nThis function returns the first n rows for the object based on position. It is\nuseful for quickly testing if your object has the right type of data in it.\n\nFor negative values of n, this function returns all rows except the last n\nrows, equivalent to `df[:-n]`.\n\nNumber of rows to select.\n\nThe first n rows of the caller object.\n\nSee also\n\nReturns the last n rows.\n\nExamples\n\nViewing the first 5 lines\n\nViewing the first n lines (three in this case)\n\nFor negative values of n\n\n"}, {"name": "pandas.DataFrame.hist", "path": "reference/api/pandas.dataframe.hist", "type": "DataFrame", "text": "\nMake a histogram of the DataFrame\u2019s columns.\n\nA histogram is a representation of the distribution of data. This function\ncalls `matplotlib.pyplot.hist()`, on each series in the DataFrame, resulting\nin one histogram per column.\n\nThe pandas object holding the data.\n\nIf passed, will be used to limit data to a subset of columns.\n\nIf passed, then used to form histograms for separate groups.\n\nWhether to show axis grid lines.\n\nIf specified changes the x-axis label size.\n\nRotation of x axis labels. For example, a value of 90 displays the x labels\nrotated 90 degrees clockwise.\n\nIf specified changes the y-axis label size.\n\nRotation of y axis labels. For example, a value of 90 displays the y labels\nrotated 90 degrees clockwise.\n\nThe axes to plot the histogram on.\n\nIn case subplots=True, share x axis and set some x axis labels to invisible;\ndefaults to True if ax is None otherwise False if an ax is passed in. Note\nthat passing in both an ax and sharex=True will alter all x axis labels for\nall subplots in a figure.\n\nIn case subplots=True, share y axis and set some y axis labels to invisible.\n\nThe size in inches of the figure to create. Uses the value in\nmatplotlib.rcParams by default.\n\nTuple of (rows, columns) for the layout of the histograms.\n\nNumber of histogram bins to be used. If an integer is given, bins + 1 bin\nedges are calculated and returned. If bins is a sequence, gives bin edges,\nincluding left edge of first bin and right edge of last bin. In this case,\nbins is returned unmodified.\n\nBackend to use instead of the backend specified in the option\n`plotting.backend`. For instance, \u2018matplotlib\u2019. Alternatively, to specify the\n`plotting.backend` for the whole session, set `pd.options.plotting.backend`.\n\nNew in version 1.0.0.\n\nWhether to show the legend.\n\nNew in version 1.1.0.\n\nAll other plotting keyword arguments to be passed to\n`matplotlib.pyplot.hist()`.\n\nSee also\n\nPlot a histogram using matplotlib.\n\nExamples\n\nThis example draws a histogram based on the length and width of some animals,\ndisplayed in three bins\n\n"}, {"name": "pandas.DataFrame.iat", "path": "reference/api/pandas.dataframe.iat", "type": "DataFrame", "text": "\nAccess a single value for a row/column pair by integer position.\n\nSimilar to `iloc`, in that both provide integer-based lookups. Use `iat` if\nyou only need to get or set a single value in a DataFrame or Series.\n\nWhen integer position is out of bounds.\n\nSee also\n\nAccess a single value for a row/column label pair.\n\nAccess a group of rows and columns by label(s).\n\nAccess a group of rows and columns by integer position(s).\n\nExamples\n\nGet value at specified row/column pair\n\nSet value at specified row/column pair\n\nGet value within a series\n\n"}, {"name": "pandas.DataFrame.idxmax", "path": "reference/api/pandas.dataframe.idxmax", "type": "DataFrame", "text": "\nReturn index of first occurrence of maximum over requested axis.\n\nNA/null values are excluded.\n\nThe axis to use. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nIndexes of maxima along the specified axis.\n\nIf the row/column is empty\n\nSee also\n\nReturn index of the maximum element.\n\nNotes\n\nThis method is the DataFrame version of `ndarray.argmax`.\n\nExamples\n\nConsider a dataset containing food consumption in Argentina.\n\nBy default, it returns the index for the maximum value in each column.\n\nTo return the index for the maximum value in each row, use `axis=\"columns\"`.\n\n"}, {"name": "pandas.DataFrame.idxmin", "path": "reference/api/pandas.dataframe.idxmin", "type": "DataFrame", "text": "\nReturn index of first occurrence of minimum over requested axis.\n\nNA/null values are excluded.\n\nThe axis to use. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nIndexes of minima along the specified axis.\n\nIf the row/column is empty\n\nSee also\n\nReturn index of the minimum element.\n\nNotes\n\nThis method is the DataFrame version of `ndarray.argmin`.\n\nExamples\n\nConsider a dataset containing food consumption in Argentina.\n\nBy default, it returns the index for the minimum value in each column.\n\nTo return the index for the minimum value in each row, use `axis=\"columns\"`.\n\n"}, {"name": "pandas.DataFrame.iloc", "path": "reference/api/pandas.dataframe.iloc", "type": "DataFrame", "text": "\nPurely integer-location based indexing for selection by position.\n\n`.iloc[]` is primarily integer position based (from `0` to `length-1` of the\naxis), but may also be used with a boolean array.\n\nAllowed inputs are:\n\nAn integer, e.g. `5`.\n\nA list or array of integers, e.g. `[4, 3, 0]`.\n\nA slice object with ints, e.g. `1:7`.\n\nA boolean array.\n\nA `callable` function with one argument (the calling Series or DataFrame) and\nthat returns valid output for indexing (one of the above). This is useful in\nmethod chains, when you don\u2019t have a reference to the calling object, but\nwould like to base your selection on some value.\n\n`.iloc` will raise `IndexError` if a requested indexer is out-of-bounds,\nexcept slice indexers which allow out-of-bounds indexing (this conforms with\npython/numpy slice semantics).\n\nSee more at Selection by Position.\n\nSee also\n\nFast integer location scalar accessor.\n\nPurely label-location based indexer for selection by label.\n\nPurely integer-location based indexing for selection by position.\n\nExamples\n\nIndexing just the rows\n\nWith a scalar integer.\n\nWith a list of integers.\n\nWith a slice object.\n\nWith a boolean mask the same length as the index.\n\nWith a callable, useful in method chains. The x passed to the `lambda` is the\nDataFrame being sliced. This selects the rows whose index label even.\n\nIndexing both axes\n\nYou can mix the indexer types for the index and columns. Use `:` to select the\nentire axis.\n\nWith scalar integers.\n\nWith lists of integers.\n\nWith slice objects.\n\nWith a boolean array whose length matches the columns.\n\nWith a callable function that expects the Series or DataFrame.\n\n"}, {"name": "pandas.DataFrame.index", "path": "reference/api/pandas.dataframe.index", "type": "DataFrame", "text": "\nThe index (row labels) of the DataFrame.\n\n"}, {"name": "pandas.DataFrame.infer_objects", "path": "reference/api/pandas.dataframe.infer_objects", "type": "DataFrame", "text": "\nAttempt to infer better dtypes for object columns.\n\nAttempts soft conversion of object-dtyped columns, leaving non-object and\nunconvertible columns unchanged. The inference rules are the same as during\nnormal Series/DataFrame construction.\n\nSee also\n\nConvert argument to datetime.\n\nConvert argument to timedelta.\n\nConvert argument to numeric type.\n\nConvert argument to best possible dtype.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.info", "path": "reference/api/pandas.dataframe.info", "type": "DataFrame", "text": "\nPrint a concise summary of a DataFrame.\n\nThis method prints information about a DataFrame including the index dtype and\ncolumns, non-null values and memory usage.\n\nDataFrame to print information about.\n\nWhether to print the full summary. By default, the setting in\n`pandas.options.display.max_info_columns` is followed.\n\nWhere to send the output. By default, the output is printed to sys.stdout.\nPass a writable buffer if you need to further process the output. max_cols :\nint, optional When to switch from the verbose to the truncated output. If the\nDataFrame has more than max_cols columns, the truncated output is used. By\ndefault, the setting in `pandas.options.display.max_info_columns` is used.\n\nSpecifies whether total memory usage of the DataFrame elements (including the\nindex) should be displayed. By default, this follows the\n`pandas.options.display.memory_usage` setting.\n\nTrue always show memory usage. False never shows memory usage. A value of\n\u2018deep\u2019 is equivalent to \u201cTrue with deep introspection\u201d. Memory usage is shown\nin human-readable units (base-2 representation). Without deep introspection a\nmemory estimation is made based in column dtype and number of rows assuming\nvalues consume the same memory amount for corresponding dtypes. With deep\nmemory introspection, a real memory usage calculation is performed at the cost\nof computational resources.\n\nWhether to show the non-null counts. By default, this is shown only if the\nDataFrame is smaller than `pandas.options.display.max_info_rows` and\n`pandas.options.display.max_info_columns`. A value of True always shows the\ncounts, and False never shows the counts.\n\nDeprecated since version 1.2.0: Use show_counts instead.\n\nThis method prints a summary of a DataFrame and returns None.\n\nSee also\n\nGenerate descriptive statistics of DataFrame columns.\n\nMemory usage of DataFrame columns.\n\nExamples\n\nPrints information of all columns:\n\nPrints a summary of columns count and its dtypes but not per column\ninformation:\n\nPipe output of DataFrame.info to buffer instead of sys.stdout, get buffer\ncontent and writes to a text file:\n\nThe memory_usage parameter allows deep introspection mode, specially useful\nfor big DataFrames and fine-tune memory optimization:\n\n"}, {"name": "pandas.DataFrame.insert", "path": "reference/api/pandas.dataframe.insert", "type": "DataFrame", "text": "\nInsert column into DataFrame at specified location.\n\nRaises a ValueError if column is already contained in the DataFrame, unless\nallow_duplicates is set to True.\n\nInsertion index. Must verify 0 <= loc <= len(columns).\n\nLabel of the inserted column.\n\nSee also\n\nInsert new item by index.\n\nExamples\n\nNotice that pandas uses index alignment in case of value from type Series:\n\n"}, {"name": "pandas.DataFrame.interpolate", "path": "reference/api/pandas.dataframe.interpolate", "type": "DataFrame", "text": "\nFill NaN values using an interpolation method.\n\nPlease note that only `method='linear'` is supported for DataFrame/Series with\na MultiIndex.\n\nInterpolation technique to use. One of:\n\n\u2018linear\u2019: Ignore the index and treat the values as equally spaced. This is the\nonly method supported on MultiIndexes.\n\n\u2018time\u2019: Works on daily and higher resolution data to interpolate given length\nof interval.\n\n\u2018index\u2019, \u2018values\u2019: use the actual numerical values of the index.\n\n\u2018pad\u2019: Fill in NaNs using existing values.\n\n\u2018nearest\u2019, \u2018zero\u2019, \u2018slinear\u2019, \u2018quadratic\u2019, \u2018cubic\u2019, \u2018spline\u2019, \u2018barycentric\u2019,\n\u2018polynomial\u2019: Passed to scipy.interpolate.interp1d. These methods use the\nnumerical values of the index. Both \u2018polynomial\u2019 and \u2018spline\u2019 require that you\nalso specify an order (int), e.g. `df.interpolate(method='polynomial',\norder=5)`.\n\n\u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019, \u2018akima\u2019, \u2018cubicspline\u2019:\nWrappers around the SciPy interpolation methods of similar names. See Notes.\n\n\u2018from_derivatives\u2019: Refers to scipy.interpolate.BPoly.from_derivatives which\nreplaces \u2018piecewise_polynomial\u2019 interpolation method in scipy 0.18.\n\nAxis to interpolate along.\n\nMaximum number of consecutive NaNs to fill. Must be greater than 0.\n\nUpdate the data in place if possible.\n\nConsecutive NaNs will be filled in this direction.\n\nIf \u2018method\u2019 is \u2018pad\u2019 or \u2018ffill\u2019, \u2018limit_direction\u2019 must be \u2018forward\u2019.\n\nIf \u2018method\u2019 is \u2018backfill\u2019 or \u2018bfill\u2019, \u2018limit_direction\u2019 must be \u2018backwards\u2019.\n\nIf \u2018method\u2019 is \u2018backfill\u2019 or \u2018bfill\u2019, the default is \u2018backward\u2019\n\nelse the default is \u2018forward\u2019\n\nChanged in version 1.1.0: raises ValueError if limit_direction is \u2018forward\u2019 or\n\u2018both\u2019 and method is \u2018backfill\u2019 or \u2018bfill\u2019. raises ValueError if\nlimit_direction is \u2018backward\u2019 or \u2018both\u2019 and method is \u2018pad\u2019 or \u2018ffill\u2019.\n\nIf limit is specified, consecutive NaNs will be filled with this restriction.\n\n`None`: No fill restriction.\n\n\u2018inside\u2019: Only fill NaNs surrounded by valid values (interpolate).\n\n\u2018outside\u2019: Only fill NaNs outside valid values (extrapolate).\n\nDowncast dtypes if possible.\n\nKeyword arguments to pass on to the interpolating function.\n\nReturns the same object type as the caller, interpolated at some or all `NaN`\nvalues or None if `inplace=True`.\n\nSee also\n\nFill missing values using different methods.\n\nPiecewise cubic polynomials (Akima interpolator).\n\nPiecewise polynomial in the Bernstein basis.\n\nInterpolate a 1-D function.\n\nInterpolate polynomial (Krogh interpolator).\n\nPCHIP 1-d monotonic cubic interpolation.\n\nCubic spline data interpolator.\n\nNotes\n\nThe \u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019 and \u2018akima\u2019 methods are\nwrappers around the respective SciPy implementations of similar names. These\nuse the actual numerical values of the index. For more information on their\nbehavior, see the SciPy documentation and SciPy tutorial.\n\nExamples\n\nFilling in `NaN` in a `Series` via linear interpolation.\n\nFilling in `NaN` in a Series by padding, but filling at most two consecutive\n`NaN` at a time.\n\nFilling in `NaN` in a Series via polynomial interpolation or splines: Both\n\u2018polynomial\u2019 and \u2018spline\u2019 methods require that you also specify an `order`\n(int).\n\nFill the DataFrame forward (that is, going down) along each column using\nlinear interpolation.\n\nNote how the last entry in column \u2018a\u2019 is interpolated differently, because\nthere is no entry after it to use for interpolation. Note how the first entry\nin column \u2018b\u2019 remains `NaN`, because there is no entry before it to use for\ninterpolation.\n\nUsing polynomial interpolation.\n\n"}, {"name": "pandas.DataFrame.isin", "path": "reference/api/pandas.dataframe.isin", "type": "DataFrame", "text": "\nWhether each element in the DataFrame is contained in values.\n\nThe result will only be true at a location if all the labels match. If values\nis a Series, that\u2019s the index. If values is a dict, the keys must be the\ncolumn names, which must match. If values is a DataFrame, then both the index\nand column labels must match.\n\nDataFrame of booleans showing whether each element in the DataFrame is\ncontained in values.\n\nSee also\n\nEquality test for DataFrame.\n\nEquivalent method on Series.\n\nTest if pattern or regex is contained within a string of a Series or Index.\n\nExamples\n\nWhen `values` is a list check whether every value in the DataFrame is present\nin the list (which animals have 0 or 2 legs or wings)\n\nTo check if `values` is not in the DataFrame, use the `~` operator:\n\nWhen `values` is a dict, we can pass values to check for each column\nseparately:\n\nWhen `values` is a Series or DataFrame the index and column must match. Note\nthat \u2018falcon\u2019 does not match based on the number of legs in other.\n\n"}, {"name": "pandas.DataFrame.isna", "path": "reference/api/pandas.dataframe.isna", "type": "DataFrame", "text": "\nDetect missing values.\n\nReturn a boolean same-sized object indicating if the values are NA. NA values,\nsuch as None or `numpy.NaN`, gets mapped to True values. Everything else gets\nmapped to False values. Characters such as empty strings `''` or `numpy.inf`\nare not considered NA values (unless you set\n`pandas.options.mode.use_inf_as_na = True`).\n\nMask of bool values for each element in DataFrame that indicates whether an\nelement is an NA value.\n\nSee also\n\nAlias of isna.\n\nBoolean inverse of isna.\n\nOmit axes labels with missing values.\n\nTop-level isna.\n\nExamples\n\nShow which entries in a DataFrame are NA.\n\nShow which entries in a Series are NA.\n\n"}, {"name": "pandas.DataFrame.isnull", "path": "reference/api/pandas.dataframe.isnull", "type": "DataFrame", "text": "\nDataFrame.isnull is an alias for DataFrame.isna.\n\nDetect missing values.\n\nReturn a boolean same-sized object indicating if the values are NA. NA values,\nsuch as None or `numpy.NaN`, gets mapped to True values. Everything else gets\nmapped to False values. Characters such as empty strings `''` or `numpy.inf`\nare not considered NA values (unless you set\n`pandas.options.mode.use_inf_as_na = True`).\n\nMask of bool values for each element in DataFrame that indicates whether an\nelement is an NA value.\n\nSee also\n\nAlias of isna.\n\nBoolean inverse of isna.\n\nOmit axes labels with missing values.\n\nTop-level isna.\n\nExamples\n\nShow which entries in a DataFrame are NA.\n\nShow which entries in a Series are NA.\n\n"}, {"name": "pandas.DataFrame.items", "path": "reference/api/pandas.dataframe.items", "type": "DataFrame", "text": "\nIterate over (column name, Series) pairs.\n\nIterates over the DataFrame columns, returning a tuple with the column name\nand the content as a Series.\n\nThe column names for the DataFrame being iterated over.\n\nThe column entries belonging to each label, as a Series.\n\nSee also\n\nIterate over DataFrame rows as (index, Series) pairs.\n\nIterate over DataFrame rows as namedtuples of the values.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.iteritems", "path": "reference/api/pandas.dataframe.iteritems", "type": "DataFrame", "text": "\nIterate over (column name, Series) pairs.\n\nIterates over the DataFrame columns, returning a tuple with the column name\nand the content as a Series.\n\nThe column names for the DataFrame being iterated over.\n\nThe column entries belonging to each label, as a Series.\n\nSee also\n\nIterate over DataFrame rows as (index, Series) pairs.\n\nIterate over DataFrame rows as namedtuples of the values.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.iterrows", "path": "reference/api/pandas.dataframe.iterrows", "type": "DataFrame", "text": "\nIterate over DataFrame rows as (index, Series) pairs.\n\nThe index of the row. A tuple for a MultiIndex.\n\nThe data of the row as a Series.\n\nSee also\n\nIterate over DataFrame rows as namedtuples of the values.\n\nIterate over (column name, Series) pairs.\n\nNotes\n\nBecause `iterrows` returns a Series for each row, it does not preserve dtypes\nacross the rows (dtypes are preserved across columns for DataFrames). For\nexample,\n\nTo preserve dtypes while iterating over the rows, it is better to use\n`itertuples()` which returns namedtuples of the values and which is generally\nfaster than `iterrows`.\n\nYou should never modify something you are iterating over. This is not\nguaranteed to work in all cases. Depending on the data types, the iterator\nreturns a copy and not a view, and writing to it will have no effect.\n\n"}, {"name": "pandas.DataFrame.itertuples", "path": "reference/api/pandas.dataframe.itertuples", "type": "DataFrame", "text": "\nIterate over DataFrame rows as namedtuples.\n\nIf True, return the index as the first element of the tuple.\n\nThe name of the returned namedtuples or None to return regular tuples.\n\nAn object to iterate over namedtuples for each row in the DataFrame with the\nfirst field possibly being the index and following fields being the column\nvalues.\n\nSee also\n\nIterate over DataFrame rows as (index, Series) pairs.\n\nIterate over (column name, Series) pairs.\n\nNotes\n\nThe column names will be renamed to positional names if they are invalid\nPython identifiers, repeated, or start with an underscore. On python versions\n< 3.7 regular tuples are returned for DataFrames with a large number of\ncolumns (>254).\n\nExamples\n\nBy setting the index parameter to False we can remove the index as the first\nelement of the tuple:\n\nWith the name parameter set we set a custom name for the yielded namedtuples:\n\n"}, {"name": "pandas.DataFrame.join", "path": "reference/api/pandas.dataframe.join", "type": "DataFrame", "text": "\nJoin columns of another DataFrame.\n\nJoin columns with other DataFrame either on index or on a key column.\nEfficiently join multiple DataFrame objects by index at once by passing a\nlist.\n\nIndex should be similar to one of the columns in this one. If a Series is\npassed, its name attribute must be set, and that will be used as the column\nname in the resulting joined DataFrame.\n\nColumn or index level name(s) in the caller to join on the index in other,\notherwise joins index-on-index. If multiple values given, the other DataFrame\nmust have a MultiIndex. Can pass an array as the join key if it is not already\ncontained in the calling DataFrame. Like an Excel VLOOKUP operation.\n\nHow to handle the operation of the two objects.\n\nleft: use calling frame\u2019s index (or column if on is specified)\n\nright: use other\u2019s index.\n\nouter: form union of calling frame\u2019s index (or column if on is specified) with\nother\u2019s index, and sort it. lexicographically.\n\ninner: form intersection of calling frame\u2019s index (or column if on is\nspecified) with other\u2019s index, preserving the order of the calling\u2019s one.\n\ncross: creates the cartesian product from both frames, preserves the order of\nthe left keys.\n\nNew in version 1.2.0.\n\nSuffix to use from left frame\u2019s overlapping columns.\n\nSuffix to use from right frame\u2019s overlapping columns.\n\nOrder result DataFrame lexicographically by the join key. If False, the order\nof the join key depends on the join type (how keyword).\n\nA dataframe containing columns from both the caller and other.\n\nSee also\n\nFor column(s)-on-column(s) operations.\n\nNotes\n\nParameters on, lsuffix, and rsuffix are not supported when passing a list of\nDataFrame objects.\n\nSupport for specifying index levels as the on parameter was added in version\n0.23.0.\n\nExamples\n\nJoin DataFrames using their indexes.\n\nIf we want to join using the key columns, we need to set key to be the index\nin both df and other. The joined DataFrame will have key as its index.\n\nAnother option to join using the key columns is to use the on parameter.\nDataFrame.join always uses other\u2019s index but we can use any column in df. This\nmethod preserves the original DataFrame\u2019s index in the result.\n\nUsing non-unique key values shows how they are matched.\n\n"}, {"name": "pandas.DataFrame.keys", "path": "reference/api/pandas.dataframe.keys", "type": "DataFrame", "text": "\nGet the \u2018info axis\u2019 (see Indexing for more).\n\nThis is index for Series, columns for DataFrame.\n\nInfo axis.\n\n"}, {"name": "pandas.DataFrame.kurt", "path": "reference/api/pandas.dataframe.kurt", "type": "DataFrame", "text": "\nReturn unbiased kurtosis over requested axis.\n\nKurtosis obtained using Fisher\u2019s definition of kurtosis (kurtosis of normal ==\n0.0). Normalized by N-1.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.DataFrame.kurtosis", "path": "reference/api/pandas.dataframe.kurtosis", "type": "DataFrame", "text": "\nReturn unbiased kurtosis over requested axis.\n\nKurtosis obtained using Fisher\u2019s definition of kurtosis (kurtosis of normal ==\n0.0). Normalized by N-1.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.DataFrame.last", "path": "reference/api/pandas.dataframe.last", "type": "DataFrame", "text": "\nSelect final periods of time series data based on a date offset.\n\nFor a DataFrame with a sorted DatetimeIndex, this function selects the last\nfew rows based on a date offset.\n\nThe offset length of the data that will be selected. For instance, \u20183D\u2019 will\ndisplay all the rows having their index within the last 3 days.\n\nA subset of the caller.\n\nIf the index is not a `DatetimeIndex`\n\nSee also\n\nSelect initial periods of time series based on a date offset.\n\nSelect values at a particular time of the day.\n\nSelect values between particular times of the day.\n\nExamples\n\nGet the rows for the last 3 days:\n\nNotice the data for 3 last calendar days were returned, not the last 3\nobserved days in the dataset, and therefore data for 2018-04-11 was not\nreturned.\n\n"}, {"name": "pandas.DataFrame.last_valid_index", "path": "reference/api/pandas.dataframe.last_valid_index", "type": "DataFrame", "text": "\nReturn index for last non-NA value or None, if no NA value is found.\n\nNotes\n\nIf all elements are non-NA/null, returns None. Also returns None for empty\nSeries/DataFrame.\n\n"}, {"name": "pandas.DataFrame.le", "path": "reference/api/pandas.dataframe.le", "type": "DataFrame", "text": "\nGet Less than or equal to of dataframe and other, element-wise (binary\noperator le).\n\nAmong flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.\n\nEquivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or\ncolumns) and level for comparison.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nResult of the comparison.\n\nSee also\n\nCompare DataFrames for equality elementwise.\n\nCompare DataFrames for inequality elementwise.\n\nCompare DataFrames for less than inequality or equality elementwise.\n\nCompare DataFrames for strictly less than inequality elementwise.\n\nCompare DataFrames for greater than inequality or equality elementwise.\n\nCompare DataFrames for strictly greater than inequality elementwise.\n\nNotes\n\nMismatched indices will be unioned together. NaN values are considered\ndifferent (i.e. NaN != NaN).\n\nExamples\n\nComparison with a scalar, using either the operator or method:\n\nWhen other is a `Series`, the columns of a DataFrame are aligned with the\nindex of other and broadcast:\n\nUse the method to control the broadcast axis:\n\nWhen comparing to an arbitrary sequence, the number of columns must match the\nnumber elements in other:\n\nUse the method to control the axis:\n\nCompare to a DataFrame of different shape.\n\nCompare to a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.loc", "path": "reference/api/pandas.dataframe.loc", "type": "DataFrame", "text": "\nAccess a group of rows and columns by label(s) or a boolean array.\n\n`.loc[]` is primarily label based, but may also be used with a boolean array.\n\nAllowed inputs are:\n\nA single label, e.g. `5` or `'a'`, (note that `5` is interpreted as a label of\nthe index, and never as an integer position along the index).\n\nA list or array of labels, e.g. `['a', 'b', 'c']`.\n\nA slice object with labels, e.g. `'a':'f'`.\n\nWarning\n\nNote that contrary to usual python slices, both the start and the stop are\nincluded\n\nA boolean array of the same length as the axis being sliced, e.g. `[True,\nFalse, True]`.\n\nAn alignable boolean Series. The index of the key will be aligned before\nmasking.\n\nAn alignable Index. The Index of the returned selection will be the input.\n\nA `callable` function with one argument (the calling Series or DataFrame) and\nthat returns valid output for indexing (one of the above)\n\nSee more at Selection by Label.\n\nIf any items are not found.\n\nIf an indexed key is passed and its index is unalignable to the frame index.\n\nSee also\n\nAccess a single value for a row/column label pair.\n\nAccess group of rows and columns by integer position(s).\n\nReturns a cross-section (row(s) or column(s)) from the Series/DataFrame.\n\nAccess group of values using labels.\n\nExamples\n\nGetting values\n\nSingle label. Note this returns the row as a Series.\n\nList of labels. Note using `[[]]` returns a DataFrame.\n\nSingle label for row and column\n\nSlice with labels for row and single label for column. As mentioned above,\nnote that both the start and stop of the slice are included.\n\nBoolean list with the same length as the row axis\n\nAlignable boolean Series:\n\nIndex (same behavior as `df.reindex`)\n\nConditional that returns a boolean Series\n\nConditional that returns a boolean Series with column labels specified\n\nCallable that returns a boolean Series\n\nSetting values\n\nSet value for all items matching the list of labels\n\nSet value for an entire row\n\nSet value for an entire column\n\nSet value for rows matching callable condition\n\nGetting values on a DataFrame with an index that has integer labels\n\nAnother example using integers for the index\n\nSlice with integer labels for rows. As mentioned above, note that both the\nstart and stop of the slice are included.\n\nGetting values with a MultiIndex\n\nA number of examples using a DataFrame with a MultiIndex\n\nSingle label. Note this returns a DataFrame with a single index.\n\nSingle index tuple. Note this returns a Series.\n\nSingle label for row and column. Similar to passing in a tuple, this returns a\nSeries.\n\nSingle tuple. Note using `[[]]` returns a DataFrame.\n\nSingle tuple for the index with a single label for the column\n\nSlice from index tuple to single label\n\nSlice from index tuple to index tuple\n\n"}, {"name": "pandas.DataFrame.lookup", "path": "reference/api/pandas.dataframe.lookup", "type": "DataFrame", "text": "\nLabel-based \u201cfancy indexing\u201d function for DataFrame. Given equal-length arrays\nof row and column labels, return an array of the values corresponding to each\n(row, col) pair.\n\nDeprecated since version 1.2.0: DataFrame.lookup is deprecated, use\nDataFrame.melt and DataFrame.loc instead. For further details see Looking up\nvalues by index/column labels.\n\nThe row labels to use for lookup.\n\nThe column labels to use for lookup.\n\nThe found values.\n\n"}, {"name": "pandas.DataFrame.lt", "path": "reference/api/pandas.dataframe.lt", "type": "DataFrame", "text": "\nGet Less than of dataframe and other, element-wise (binary operator lt).\n\nAmong flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.\n\nEquivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or\ncolumns) and level for comparison.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nResult of the comparison.\n\nSee also\n\nCompare DataFrames for equality elementwise.\n\nCompare DataFrames for inequality elementwise.\n\nCompare DataFrames for less than inequality or equality elementwise.\n\nCompare DataFrames for strictly less than inequality elementwise.\n\nCompare DataFrames for greater than inequality or equality elementwise.\n\nCompare DataFrames for strictly greater than inequality elementwise.\n\nNotes\n\nMismatched indices will be unioned together. NaN values are considered\ndifferent (i.e. NaN != NaN).\n\nExamples\n\nComparison with a scalar, using either the operator or method:\n\nWhen other is a `Series`, the columns of a DataFrame are aligned with the\nindex of other and broadcast:\n\nUse the method to control the broadcast axis:\n\nWhen comparing to an arbitrary sequence, the number of columns must match the\nnumber elements in other:\n\nUse the method to control the axis:\n\nCompare to a DataFrame of different shape.\n\nCompare to a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.mad", "path": "reference/api/pandas.dataframe.mad", "type": "DataFrame", "text": "\nReturn the mean absolute deviation of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\n"}, {"name": "pandas.DataFrame.mask", "path": "reference/api/pandas.dataframe.mask", "type": "DataFrame", "text": "\nReplace values where the condition is True.\n\nWhere cond is False, keep the original value. Where True, replace with\ncorresponding value from other. If cond is callable, it is computed on the\nSeries/DataFrame and should return boolean Series/DataFrame or array. The\ncallable must not change input Series/DataFrame (though pandas doesn\u2019t check\nit).\n\nEntries where cond is True are replaced with corresponding value from other.\nIf other is callable, it is computed on the Series/DataFrame and should return\nscalar or Series/DataFrame. The callable must not change input\nSeries/DataFrame (though pandas doesn\u2019t check it).\n\nWhether to perform the operation in place on the data.\n\nAlignment axis if needed.\n\nAlignment level if needed.\n\nNote that currently this parameter won\u2019t affect the results and will always\ncoerce to a suitable dtype.\n\n\u2018raise\u2019 : allow exceptions to be raised.\n\n\u2018ignore\u2019 : suppress exceptions. On error return original object.\n\nTry to cast the result back to the input type (if possible).\n\nDeprecated since version 1.3.0: Manually cast back if necessary.\n\nSee also\n\nReturn an object of same shape as self.\n\nNotes\n\nThe mask method is an application of the if-then idiom. For each element in\nthe calling DataFrame, if `cond` is `False` the element is used; otherwise the\ncorresponding element from the DataFrame `other` is used.\n\nThe signature for `DataFrame.where()` differs from `numpy.where()`. Roughly\n`df1.where(m, df2)` is equivalent to `np.where(m, df1, df2)`.\n\nFor further details and examples see the `mask` documentation in indexing.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.max", "path": "reference/api/pandas.dataframe.max", "type": "DataFrame", "text": "\nReturn the maximum of the values over the requested axis.\n\nIf you want the index of the maximum, use `idxmax`. This is the equivalent of\nthe `numpy.ndarray` method `argmax`.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\nSee also\n\nReturn the sum.\n\nReturn the minimum.\n\nReturn the maximum.\n\nReturn the index of the minimum.\n\nReturn the index of the maximum.\n\nReturn the sum over the requested axis.\n\nReturn the minimum over the requested axis.\n\nReturn the maximum over the requested axis.\n\nReturn the index of the minimum over the requested axis.\n\nReturn the index of the maximum over the requested axis.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.mean", "path": "reference/api/pandas.dataframe.mean", "type": "DataFrame", "text": "\nReturn the mean of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.DataFrame.median", "path": "reference/api/pandas.dataframe.median", "type": "DataFrame", "text": "\nReturn the median of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.DataFrame.melt", "path": "reference/api/pandas.dataframe.melt", "type": "DataFrame", "text": "\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers\nset.\n\nThis function is useful to massage a DataFrame into a format where one or more\ncolumns are identifier variables (id_vars), while all other columns,\nconsidered measured variables (value_vars), are \u201cunpivoted\u201d to the row axis,\nleaving just two non-identifier columns, \u2018variable\u2019 and \u2018value\u2019.\n\nColumn(s) to use as identifier variables.\n\nColumn(s) to unpivot. If not specified, uses all columns that are not set as\nid_vars.\n\nName to use for the \u2018variable\u2019 column. If None it uses `frame.columns.name` or\n\u2018variable\u2019.\n\nName to use for the \u2018value\u2019 column.\n\nIf columns are a MultiIndex then use this level to melt.\n\nIf True, original index is ignored. If False, the original index is retained.\nIndex labels will be repeated as necessary.\n\nNew in version 1.1.0.\n\nUnpivoted DataFrame.\n\nSee also\n\nIdentical method.\n\nCreate a spreadsheet-style pivot table as a DataFrame.\n\nReturn reshaped DataFrame organized by given index / column values.\n\nExplode a DataFrame from list-like columns to long format.\n\nExamples\n\nThe names of \u2018variable\u2019 and \u2018value\u2019 columns can be customized:\n\nOriginal index values can be kept around:\n\nIf you have multi-index columns:\n\n"}, {"name": "pandas.DataFrame.memory_usage", "path": "reference/api/pandas.dataframe.memory_usage", "type": "DataFrame", "text": "\nReturn the memory usage of each column in bytes.\n\nThe memory usage can optionally include the contribution of the index and\nelements of object dtype.\n\nThis value is displayed in DataFrame.info by default. This can be suppressed\nby setting `pandas.options.display.memory_usage` to False.\n\nSpecifies whether to include the memory usage of the DataFrame\u2019s index in\nreturned Series. If `index=True`, the memory usage of the index is the first\nitem in the output.\n\nIf True, introspect the data deeply by interrogating object dtypes for system-\nlevel memory consumption, and include it in the returned values.\n\nA Series whose index is the original column names and whose values is the\nmemory usage of each column in bytes.\n\nSee also\n\nTotal bytes consumed by the elements of an ndarray.\n\nBytes consumed by a Series.\n\nMemory-efficient array for string values with many repeated values.\n\nConcise summary of a DataFrame.\n\nExamples\n\nThe memory footprint of object dtype columns is ignored by default:\n\nUse a Categorical for efficient storage of an object-dtype column with many\nrepeated values.\n\n"}, {"name": "pandas.DataFrame.merge", "path": "reference/api/pandas.dataframe.merge", "type": "DataFrame", "text": "\nMerge DataFrame or named Series objects with a database-style join.\n\nA named Series object is treated as a DataFrame with a single named column.\n\nThe join is done on columns or indexes. If joining columns on columns, the\nDataFrame indexes will be ignored. Otherwise if joining indexes on indexes or\nindexes on a column or columns, the index will be passed on. When performing a\ncross merge, no column specifications to merge on are allowed.\n\nWarning\n\nIf both key columns contain rows where the key is a null value, those rows\nwill be matched against each other. This is different from usual SQL join\nbehaviour and can lead to unexpected results.\n\nObject to merge with.\n\nType of merge to be performed.\n\nleft: use only keys from left frame, similar to a SQL left outer join;\npreserve key order.\n\nright: use only keys from right frame, similar to a SQL right outer join;\npreserve key order.\n\nouter: use union of keys from both frames, similar to a SQL full outer join;\nsort keys lexicographically.\n\ninner: use intersection of keys from both frames, similar to a SQL inner join;\npreserve the order of the left keys.\n\ncross: creates the cartesian product from both frames, preserves the order of\nthe left keys.\n\nNew in version 1.2.0.\n\nColumn or index level names to join on. These must be found in both\nDataFrames. If on is None and not merging on indexes then this defaults to the\nintersection of the columns in both DataFrames.\n\nColumn or index level names to join on in the left DataFrame. Can also be an\narray or list of arrays of the length of the left DataFrame. These arrays are\ntreated as if they are columns.\n\nColumn or index level names to join on in the right DataFrame. Can also be an\narray or list of arrays of the length of the right DataFrame. These arrays are\ntreated as if they are columns.\n\nUse the index from the left DataFrame as the join key(s). If it is a\nMultiIndex, the number of keys in the other DataFrame (either the index or a\nnumber of columns) must match the number of levels.\n\nUse the index from the right DataFrame as the join key. Same caveats as\nleft_index.\n\nSort the join keys lexicographically in the result DataFrame. If False, the\norder of the join keys depends on the join type (how keyword).\n\nA length-2 sequence where each element is optionally a string indicating the\nsuffix to add to overlapping column names in left and right respectively. Pass\na value of None instead of a string to indicate that the column name from left\nor right should be left as-is, with no suffix. At least one of the values must\nnot be None.\n\nIf False, avoid copy if possible.\n\nIf True, adds a column to the output DataFrame called \u201c_merge\u201d with\ninformation on the source of each row. The column can be given a different\nname by providing a string argument. The column will have a Categorical type\nwith the value of \u201cleft_only\u201d for observations whose merge key only appears in\nthe left DataFrame, \u201cright_only\u201d for observations whose merge key only appears\nin the right DataFrame, and \u201cboth\u201d if the observation\u2019s merge key is found in\nboth DataFrames.\n\nIf specified, checks if merge is of specified type.\n\n\u201cone_to_one\u201d or \u201c1:1\u201d: check if merge keys are unique in both left and right\ndatasets.\n\n\u201cone_to_many\u201d or \u201c1:m\u201d: check if merge keys are unique in left dataset.\n\n\u201cmany_to_one\u201d or \u201cm:1\u201d: check if merge keys are unique in right dataset.\n\n\u201cmany_to_many\u201d or \u201cm:m\u201d: allowed, but does not result in checks.\n\nA DataFrame of the two merged objects.\n\nSee also\n\nMerge with optional filling/interpolation.\n\nMerge on nearest keys.\n\nSimilar method using indices.\n\nNotes\n\nSupport for specifying index levels as the on, left_on, and right_on\nparameters was added in version 0.23.0 Support for merging named Series\nobjects was added in version 0.24.0\n\nExamples\n\nMerge df1 and df2 on the lkey and rkey columns. The value columns have the\ndefault suffixes, _x and _y, appended.\n\nMerge DataFrames df1 and df2 with specified left and right suffixes appended\nto any overlapping columns.\n\nMerge DataFrames df1 and df2, but raise an exception if the DataFrames have\nany overlapping columns.\n\n"}, {"name": "pandas.DataFrame.min", "path": "reference/api/pandas.dataframe.min", "type": "DataFrame", "text": "\nReturn the minimum of the values over the requested axis.\n\nIf you want the index of the minimum, use `idxmin`. This is the equivalent of\nthe `numpy.ndarray` method `argmin`.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\nSee also\n\nReturn the sum.\n\nReturn the minimum.\n\nReturn the maximum.\n\nReturn the index of the minimum.\n\nReturn the index of the maximum.\n\nReturn the sum over the requested axis.\n\nReturn the minimum over the requested axis.\n\nReturn the maximum over the requested axis.\n\nReturn the index of the minimum over the requested axis.\n\nReturn the index of the maximum over the requested axis.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.mod", "path": "reference/api/pandas.dataframe.mod", "type": "DataFrame", "text": "\nGet Modulo of dataframe and other, element-wise (binary operator mod).\n\nEquivalent to `dataframe % other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, rmod.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.mode", "path": "reference/api/pandas.dataframe.mode", "type": "DataFrame", "text": "\nGet the mode(s) of each element along the selected axis.\n\nThe mode of a set of values is the value that appears most often. It can be\nmultiple values.\n\nThe axis to iterate over while searching for the mode:\n\n0 or \u2018index\u2019 : get mode of each column\n\n1 or \u2018columns\u2019 : get mode of each row.\n\nIf True, only apply to numeric columns.\n\nDon\u2019t consider counts of NaN/NaT.\n\nThe modes of each column or row.\n\nSee also\n\nReturn the highest frequency value in a Series.\n\nReturn the counts of values in a Series.\n\nExamples\n\nBy default, missing values are not considered, and the mode of wings are both\n0 and 2. Because the resulting DataFrame has two rows, the second row of\n`species` and `legs` contains `NaN`.\n\nSetting `dropna=False` `NaN` values are considered and they can be the mode\n(like for wings).\n\nSetting `numeric_only=True`, only the mode of numeric columns is computed, and\ncolumns of other types are ignored.\n\nTo compute the mode over columns and not rows, use the axis parameter:\n\n"}, {"name": "pandas.DataFrame.mul", "path": "reference/api/pandas.dataframe.mul", "type": "DataFrame", "text": "\nGet Multiplication of dataframe and other, element-wise (binary operator mul).\n\nEquivalent to `dataframe * other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, rmul.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.multiply", "path": "reference/api/pandas.dataframe.multiply", "type": "DataFrame", "text": "\nGet Multiplication of dataframe and other, element-wise (binary operator mul).\n\nEquivalent to `dataframe * other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, rmul.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.ndim", "path": "reference/api/pandas.dataframe.ndim", "type": "DataFrame", "text": "\nReturn an int representing the number of axes / array dimensions.\n\nReturn 1 if Series. Otherwise return 2 if DataFrame.\n\nSee also\n\nNumber of array dimensions.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.ne", "path": "reference/api/pandas.dataframe.ne", "type": "DataFrame", "text": "\nGet Not equal to of dataframe and other, element-wise (binary operator ne).\n\nAmong flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.\n\nEquivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or\ncolumns) and level for comparison.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nResult of the comparison.\n\nSee also\n\nCompare DataFrames for equality elementwise.\n\nCompare DataFrames for inequality elementwise.\n\nCompare DataFrames for less than inequality or equality elementwise.\n\nCompare DataFrames for strictly less than inequality elementwise.\n\nCompare DataFrames for greater than inequality or equality elementwise.\n\nCompare DataFrames for strictly greater than inequality elementwise.\n\nNotes\n\nMismatched indices will be unioned together. NaN values are considered\ndifferent (i.e. NaN != NaN).\n\nExamples\n\nComparison with a scalar, using either the operator or method:\n\nWhen other is a `Series`, the columns of a DataFrame are aligned with the\nindex of other and broadcast:\n\nUse the method to control the broadcast axis:\n\nWhen comparing to an arbitrary sequence, the number of columns must match the\nnumber elements in other:\n\nUse the method to control the axis:\n\nCompare to a DataFrame of different shape.\n\nCompare to a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.nlargest", "path": "reference/api/pandas.dataframe.nlargest", "type": "DataFrame", "text": "\nReturn the first n rows ordered by columns in descending order.\n\nReturn the first n rows with the largest values in columns, in descending\norder. The columns that are not specified are returned as well, but not used\nfor ordering.\n\nThis method is equivalent to `df.sort_values(columns,\nascending=False).head(n)`, but more performant.\n\nNumber of rows to return.\n\nColumn label(s) to order by.\n\nWhere there are duplicate values:\n\n`first` : prioritize the first occurrence(s)\n\n`last` : prioritize the last occurrence(s)\n\n`all` : do not drop any duplicates, even it means selecting more than n items.\n\nThe first n rows ordered by the given columns in descending order.\n\nSee also\n\nReturn the first n rows ordered by columns in ascending order.\n\nSort DataFrame by the values.\n\nReturn the first n rows without re-ordering.\n\nNotes\n\nThis function cannot be used with all column types. For example, when\nspecifying columns with object or category dtypes, `TypeError` is raised.\n\nExamples\n\nIn the following example, we will use `nlargest` to select the three rows\nhaving the largest values in column \u201cpopulation\u201d.\n\nWhen using `keep='last'`, ties are resolved in reverse order:\n\nWhen using `keep='all'`, all duplicate items are maintained:\n\nTo order by the largest values in column \u201cpopulation\u201d and then \u201cGDP\u201d, we can\nspecify multiple columns like in the next example.\n\n"}, {"name": "pandas.DataFrame.notna", "path": "reference/api/pandas.dataframe.notna", "type": "DataFrame", "text": "\nDetect existing (non-missing) values.\n\nReturn a boolean same-sized object indicating if the values are not NA. Non-\nmissing values get mapped to True. Characters such as empty strings `''` or\n`numpy.inf` are not considered NA values (unless you set\n`pandas.options.mode.use_inf_as_na = True`). NA values, such as None or\n`numpy.NaN`, get mapped to False values.\n\nMask of bool values for each element in DataFrame that indicates whether an\nelement is not an NA value.\n\nSee also\n\nAlias of notna.\n\nBoolean inverse of notna.\n\nOmit axes labels with missing values.\n\nTop-level notna.\n\nExamples\n\nShow which entries in a DataFrame are not NA.\n\nShow which entries in a Series are not NA.\n\n"}, {"name": "pandas.DataFrame.notnull", "path": "reference/api/pandas.dataframe.notnull", "type": "DataFrame", "text": "\nDataFrame.notnull is an alias for DataFrame.notna.\n\nDetect existing (non-missing) values.\n\nReturn a boolean same-sized object indicating if the values are not NA. Non-\nmissing values get mapped to True. Characters such as empty strings `''` or\n`numpy.inf` are not considered NA values (unless you set\n`pandas.options.mode.use_inf_as_na = True`). NA values, such as None or\n`numpy.NaN`, get mapped to False values.\n\nMask of bool values for each element in DataFrame that indicates whether an\nelement is not an NA value.\n\nSee also\n\nAlias of notna.\n\nBoolean inverse of notna.\n\nOmit axes labels with missing values.\n\nTop-level notna.\n\nExamples\n\nShow which entries in a DataFrame are not NA.\n\nShow which entries in a Series are not NA.\n\n"}, {"name": "pandas.DataFrame.nsmallest", "path": "reference/api/pandas.dataframe.nsmallest", "type": "DataFrame", "text": "\nReturn the first n rows ordered by columns in ascending order.\n\nReturn the first n rows with the smallest values in columns, in ascending\norder. The columns that are not specified are returned as well, but not used\nfor ordering.\n\nThis method is equivalent to `df.sort_values(columns,\nascending=True).head(n)`, but more performant.\n\nNumber of items to retrieve.\n\nColumn name or names to order by.\n\nWhere there are duplicate values:\n\n`first` : take the first occurrence.\n\n`last` : take the last occurrence.\n\n`all` : do not drop any duplicates, even it means selecting more than n items.\n\nSee also\n\nReturn the first n rows ordered by columns in descending order.\n\nSort DataFrame by the values.\n\nReturn the first n rows without re-ordering.\n\nExamples\n\nIn the following example, we will use `nsmallest` to select the three rows\nhaving the smallest values in column \u201cpopulation\u201d.\n\nWhen using `keep='last'`, ties are resolved in reverse order:\n\nWhen using `keep='all'`, all duplicate items are maintained:\n\nTo order by the smallest values in column \u201cpopulation\u201d and then \u201cGDP\u201d, we can\nspecify multiple columns like in the next example.\n\n"}, {"name": "pandas.DataFrame.nunique", "path": "reference/api/pandas.dataframe.nunique", "type": "DataFrame", "text": "\nCount number of distinct elements in specified axis.\n\nReturn Series with number of distinct elements. Can ignore NaN values.\n\nThe axis to use. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.\n\nDon\u2019t include NaN in the counts.\n\nSee also\n\nMethod nunique for Series.\n\nCount non-NA cells for each column or row.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.pad", "path": "reference/api/pandas.dataframe.pad", "type": "DataFrame", "text": "\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\nObject with missing values filled or None if `inplace=True`.\n\n"}, {"name": "pandas.DataFrame.pct_change", "path": "reference/api/pandas.dataframe.pct_change", "type": "DataFrame", "text": "\nPercentage change between the current and a prior element.\n\nComputes the percentage change from the immediately previous row by default.\nThis is useful in comparing the percentage of change in a time series of\nelements.\n\nPeriods to shift for forming percent change.\n\nHow to handle NAs before computing percent changes.\n\nThe number of consecutive NAs to fill before stopping.\n\nIncrement to use from time series API (e.g. \u2018M\u2019 or BDay()).\n\nAdditional keyword arguments are passed into DataFrame.shift or Series.shift.\n\nThe same type as the calling object.\n\nSee also\n\nCompute the difference of two elements in a Series.\n\nCompute the difference of two elements in a DataFrame.\n\nShift the index by some number of periods.\n\nShift the index by some number of periods.\n\nExamples\n\nSeries\n\nSee the percentage change in a Series where filling NAs with last valid\nobservation forward to next valid.\n\nDataFrame\n\nPercentage change in French franc, Deutsche Mark, and Italian lira from\n1980-01-01 to 1980-03-01.\n\nPercentage of change in GOOG and APPL stock volume. Shows computing the\npercentage change between columns.\n\n"}, {"name": "pandas.DataFrame.pipe", "path": "reference/api/pandas.dataframe.pipe", "type": "DataFrame", "text": "\nApply chainable functions that expect Series or DataFrames.\n\nFunction to apply to the Series/DataFrame. `args`, and `kwargs` are passed\ninto `func`. Alternatively a `(callable, data_keyword)` tuple where\n`data_keyword` is a string indicating the keyword of `callable` that expects\nthe Series/DataFrame.\n\nPositional arguments passed into `func`.\n\nA dictionary of keyword arguments passed into `func`.\n\nSee also\n\nApply a function along input axis of DataFrame.\n\nApply a function elementwise on a whole DataFrame.\n\nApply a mapping correspondence on a `Series`.\n\nNotes\n\nUse `.pipe` when chaining together functions that expect Series, DataFrames or\nGroupBy objects. Instead of writing\n\nYou can write\n\nIf you have a function that takes the data as (say) the second argument, pass\na tuple indicating which keyword expects the data. For example, suppose `f`\ntakes its data as `arg2`:\n\n"}, {"name": "pandas.DataFrame.pivot", "path": "reference/api/pandas.dataframe.pivot", "type": "DataFrame", "text": "\nReturn reshaped DataFrame organized by given index / column values.\n\nReshape data (produce a \u201cpivot\u201d table) based on column values. Uses unique\nvalues from specified index / columns to form axes of the resulting DataFrame.\nThis function does not support data aggregation, multiple values will result\nin a MultiIndex in the columns. See the User Guide for more on reshaping.\n\nColumn to use to make new frame\u2019s index. If None, uses existing index.\n\nChanged in version 1.1.0: Also accept list of index names.\n\nColumn to use to make new frame\u2019s columns.\n\nChanged in version 1.1.0: Also accept list of columns names.\n\nColumn(s) to use for populating new frame\u2019s values. If not specified, all\nremaining columns will be used and the result will have hierarchically indexed\ncolumns.\n\nReturns reshaped DataFrame.\n\nWhen there are any index, columns combinations with multiple values.\nDataFrame.pivot_table when you need to aggregate.\n\nSee also\n\nGeneralization of pivot that can handle duplicate values for one index/column\npair.\n\nPivot based on the index values instead of a column.\n\nWide panel to long format. Less flexible but more user-friendly than melt.\n\nNotes\n\nFor finer-tuned control, see hierarchical indexing documentation along with\nthe related stack/unstack methods.\n\nExamples\n\nYou could also assign a list of column names or a list of index names.\n\nA ValueError is raised if there are any duplicates.\n\nNotice that the first two rows are the same for our index and columns\narguments.\n\n"}, {"name": "pandas.DataFrame.pivot_table", "path": "reference/api/pandas.dataframe.pivot_table", "type": "DataFrame", "text": "\nCreate a spreadsheet-style pivot table as a DataFrame.\n\nThe levels in the pivot table will be stored in MultiIndex objects\n(hierarchical indexes) on the index and columns of the result DataFrame.\n\nIf an array is passed, it must be the same length as the data. The list can\ncontain any of the other types (except list). Keys to group by on the pivot\ntable index. If an array is passed, it is being used as the same manner as\ncolumn values.\n\nIf an array is passed, it must be the same length as the data. The list can\ncontain any of the other types (except list). Keys to group by on the pivot\ntable column. If an array is passed, it is being used as the same manner as\ncolumn values.\n\nIf list of functions passed, the resulting pivot table will have hierarchical\ncolumns whose top level are the function names (inferred from the function\nobjects themselves) If dict is passed, the key is column to aggregate and\nvalue is function or list of functions.\n\nValue to replace missing values with (in the resulting pivot table, after\naggregation).\n\nAdd all row / columns (e.g. for subtotal / grand totals).\n\nDo not include columns whose entries are all NaN.\n\nName of the row / column that will contain the totals when margins is True.\n\nThis only applies if any of the groupers are Categoricals. If True: only show\nobserved values for categorical groupers. If False: show all values for\ncategorical groupers.\n\nChanged in version 0.25.0.\n\nSpecifies if the result should be sorted.\n\nNew in version 1.3.0.\n\nAn Excel style pivot table.\n\nSee also\n\nPivot without aggregation that can handle non-numeric data.\n\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers\nset.\n\nWide panel to long format. Less flexible but more user-friendly than melt.\n\nExamples\n\nThis first example aggregates values by taking the sum.\n\nWe can also fill missing values using the fill_value parameter.\n\nThe next example aggregates by taking the mean across multiple columns.\n\nWe can also calculate multiple types of aggregations for any given value\ncolumn.\n\n"}, {"name": "pandas.DataFrame.plot", "path": "reference/api/pandas.dataframe.plot", "type": "DataFrame", "text": "\nMake plots of Series or DataFrame.\n\nUses the backend specified by the option `plotting.backend`. By default,\nmatplotlib is used.\n\nThe object for which the method is called.\n\nOnly used if data is a DataFrame.\n\nAllows plotting of one column versus another. Only used if data is a\nDataFrame.\n\nThe kind of plot to produce:\n\n\u2018line\u2019 : line plot (default)\n\n\u2018bar\u2019 : vertical bar plot\n\n\u2018barh\u2019 : horizontal bar plot\n\n\u2018hist\u2019 : histogram\n\n\u2018box\u2019 : boxplot\n\n\u2018kde\u2019 : Kernel Density Estimation plot\n\n\u2018density\u2019 : same as \u2018kde\u2019\n\n\u2018area\u2019 : area plot\n\n\u2018pie\u2019 : pie plot\n\n\u2018scatter\u2019 : scatter plot (DataFrame only)\n\n\u2018hexbin\u2019 : hexbin plot (DataFrame only)\n\nAn axes of the current figure.\n\nMake separate subplots for each column.\n\nIn case `subplots=True`, share x axis and set some x axis labels to invisible;\ndefaults to True if ax is None otherwise False if an ax is passed in; Be\naware, that passing in both an ax and `sharex=True` will alter all x axis\nlabels for all axis in a figure.\n\nIn case `subplots=True`, share y axis and set some y axis labels to invisible.\n\n(rows, columns) for the layout of subplots.\n\nSize of a figure object.\n\nUse index as ticks for x axis.\n\nTitle to use for the plot. If a string is passed, print the string at the top\nof the figure. If a list is passed and subplots is True, print each item in\nthe list above the corresponding subplot.\n\nAxis grid lines.\n\nPlace legend on axis subplots.\n\nThe matplotlib line style per column.\n\nUse log scaling or symlog scaling on x axis. .. versionchanged:: 0.25.0\n\nUse log scaling or symlog scaling on y axis. .. versionchanged:: 0.25.0\n\nUse log scaling or symlog scaling on both x and y axes. .. versionchanged::\n0.25.0\n\nValues to use for the xticks.\n\nValues to use for the yticks.\n\nSet the x limits of the current axes.\n\nSet the y limits of the current axes.\n\nName to use for the xlabel on x-axis. Default uses index name as xlabel, or\nthe x-column name for planar plots.\n\nNew in version 1.1.0.\n\nChanged in version 1.2.0: Now applicable to planar plots (scatter, hexbin).\n\nName to use for the ylabel on y-axis. Default will show no ylabel, or the\ny-column name for planar plots.\n\nNew in version 1.1.0.\n\nChanged in version 1.2.0: Now applicable to planar plots (scatter, hexbin).\n\nRotation for ticks (xticks for vertical, yticks for horizontal plots).\n\nFont size for xticks and yticks.\n\nColormap to select colors from. If string, load colormap with that name from\nmatplotlib.\n\nIf True, plot colorbar (only relevant for \u2018scatter\u2019 and \u2018hexbin\u2019 plots).\n\nSpecify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1\n(right/top-end). Default is 0.5 (center).\n\nIf True, draw a table using the data in the DataFrame and the data will be\ntransposed to meet matplotlib\u2019s default layout. If a Series or DataFrame is\npassed, use passed data to draw a table.\n\nSee Plotting with Error Bars for detail.\n\nEquivalent to yerr.\n\nIf True, create stacked plot.\n\nSort column names to determine plot ordering.\n\nWhether to plot on the secondary y-axis if a list/tuple, which columns to plot\non secondary y-axis.\n\nWhen using a secondary_y axis, automatically mark the column labels with\n\u201c(right)\u201d in the legend.\n\nIf True, boolean values can be plotted.\n\nBackend to use instead of the backend specified in the option\n`plotting.backend`. For instance, \u2018matplotlib\u2019. Alternatively, to specify the\n`plotting.backend` for the whole session, set `pd.options.plotting.backend`.\n\nNew in version 1.0.0.\n\nOptions to pass to matplotlib plotting method.\n\nIf the backend is not the default matplotlib one, the return value will be the\nobject returned by the backend.\n\nNotes\n\nSee matplotlib documentation online for more on this subject\n\nIf kind = \u2018bar\u2019 or \u2018barh\u2019, you can specify relative alignments for bar plot\nlayout by position keyword. From 0 (left/bottom-end) to 1 (right/top-end).\nDefault is 0.5 (center)\n\n"}, {"name": "pandas.DataFrame.plot.area", "path": "reference/api/pandas.dataframe.plot.area", "type": "DataFrame", "text": "\nDraw a stacked area plot.\n\nAn area plot displays quantitative data visually. This function wraps the\nmatplotlib area function.\n\nCoordinates for the X axis. By default uses the index.\n\nColumn to plot. By default uses all columns.\n\nArea plots are stacked by default. Set to False to create a unstacked plot.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nArea plot, or array of area plots if subplots is True.\n\nSee also\n\nMake plots of DataFrame using matplotlib / pylab.\n\nExamples\n\nDraw an area plot based on basic business metrics:\n\nArea plots are stacked by default. To produce an unstacked plot, pass\n`stacked=False`:\n\nDraw an area plot for a single column:\n\nDraw with a different x:\n\n"}, {"name": "pandas.DataFrame.plot.bar", "path": "reference/api/pandas.dataframe.plot.bar", "type": "DataFrame", "text": "\nVertical bar plot.\n\nA bar plot is a plot that presents categorical data with rectangular bars with\nlengths proportional to the values that they represent. A bar plot shows\ncomparisons among discrete categories. One axis of the plot shows the specific\ncategories being compared, and the other axis represents a measured value.\n\nAllows plotting of one column versus another. If not specified, the index of\nthe DataFrame is used.\n\nAllows plotting of one column versus another. If not specified, all numerical\ncolumns are used.\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.\n\ncode, which will be used for each column recursively. For instance\n[\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow,\nalternatively. If there is only a single column to be plotted, then only the\nfirst color from the color list will be used.\n\ncolored accordingly. For example, if your columns are called a and b, then\npassing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and\nbars for column b in red.\n\nNew in version 1.1.0.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nAn ndarray is returned with one `matplotlib.axes.Axes` per column when\n`subplots=True`.\n\nSee also\n\nHorizontal bar plot.\n\nMake plots of a DataFrame.\n\nMake a bar plot with matplotlib.\n\nExamples\n\nBasic plot.\n\nPlot a whole dataframe to a bar plot. Each column is assigned a distinct\ncolor, and each row is nested in a group along the horizontal axis.\n\nPlot stacked bar charts for the DataFrame\n\nInstead of nesting, the figure can be split by column with `subplots=True`. In\nthis case, a `numpy.ndarray` of `matplotlib.axes.Axes` are returned.\n\nIf you don\u2019t like the default colours, you can specify how you\u2019d like each\ncolumn to be colored.\n\nPlot a single column.\n\nPlot only selected categories for the DataFrame.\n\n"}, {"name": "pandas.DataFrame.plot.barh", "path": "reference/api/pandas.dataframe.plot.barh", "type": "DataFrame", "text": "\nMake a horizontal bar plot.\n\nA horizontal bar plot is a plot that presents quantitative data with\nrectangular bars with lengths proportional to the values that they represent.\nA bar plot shows comparisons among discrete categories. One axis of the plot\nshows the specific categories being compared, and the other axis represents a\nmeasured value.\n\nAllows plotting of one column versus another. If not specified, the index of\nthe DataFrame is used.\n\nAllows plotting of one column versus another. If not specified, all numerical\ncolumns are used.\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.\n\ncode, which will be used for each column recursively. For instance\n[\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow,\nalternatively. If there is only a single column to be plotted, then only the\nfirst color from the color list will be used.\n\ncolored accordingly. For example, if your columns are called a and b, then\npassing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and\nbars for column b in red.\n\nNew in version 1.1.0.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nAn ndarray is returned with one `matplotlib.axes.Axes` per column when\n`subplots=True`.\n\nSee also\n\nVertical bar plot.\n\nMake plots of DataFrame using matplotlib.\n\nPlot a vertical bar plot using matplotlib.\n\nExamples\n\nBasic example\n\nPlot a whole DataFrame to a horizontal bar plot\n\nPlot stacked barh charts for the DataFrame\n\nWe can specify colors for each column\n\nPlot a column of the DataFrame to a horizontal bar plot\n\nPlot DataFrame versus the desired column\n\n"}, {"name": "pandas.DataFrame.plot.box", "path": "reference/api/pandas.dataframe.plot.box", "type": "DataFrame", "text": "\nMake a box plot of the DataFrame columns.\n\nA box plot is a method for graphically depicting groups of numerical data\nthrough their quartiles. The box extends from the Q1 to Q3 quartile values of\nthe data, with a line at the median (Q2). The whiskers extend from the edges\nof box to show the range of the data. The position of the whiskers is set by\ndefault to 1.5*IQR (IQR = Q3 - Q1) from the edges of the box. Outlier points\nare those past the end of the whiskers.\n\nFor further details see Wikipedia\u2019s entry for boxplot.\n\nA consideration when using this chart is that the box and the whiskers can\noverlap, which is very common when plotting small sets of data.\n\nColumn in the DataFrame to group by.\n\nChanged in version 1.4.0: Previously, by is silently ignore and makes no\ngroupings\n\nAdditional keywords are documented in `DataFrame.plot()`.\n\nSee also\n\nAnother method to draw a box plot.\n\nDraw a box plot from a Series object.\n\nDraw a box plot in matplotlib.\n\nExamples\n\nDraw a box plot from a DataFrame with four columns of randomly generated data.\n\nYou can also generate groupings if you specify the by parameter (which can\ntake a column name, or a list or tuple of column names):\n\nChanged in version 1.4.0.\n\n"}, {"name": "pandas.DataFrame.plot.density", "path": "reference/api/pandas.dataframe.plot.density", "type": "DataFrame", "text": "\nGenerate Kernel Density Estimate plot using Gaussian kernels.\n\nIn statistics, kernel density estimation (KDE) is a non-parametric way to\nestimate the probability density function (PDF) of a random variable. This\nfunction uses Gaussian kernels and includes automatic bandwidth determination.\n\nThe method used to calculate the estimator bandwidth. This can be \u2018scott\u2019,\n\u2018silverman\u2019, a scalar constant or a callable. If None (default), \u2018scott\u2019 is\nused. See `scipy.stats.gaussian_kde` for more information.\n\nEvaluation points for the estimated PDF. If None (default), 1000 equally\nspaced points are used. If ind is a NumPy array, the KDE is evaluated at the\npoints passed. If ind is an integer, ind number of equally spaced points are\nused.\n\nAdditional keyword arguments are documented in `pandas.%(this-\ndatatype)s.plot()`.\n\nSee also\n\nRepresentation of a kernel-density estimate using Gaussian kernels. This is\nthe function used internally to estimate the PDF.\n\nExamples\n\nGiven a Series of points randomly sampled from an unknown distribution,\nestimate its PDF using KDE with automatic bandwidth determination and plot the\nresults, evaluating them at 1000 equally spaced points (default):\n\nA scalar bandwidth can be specified. Using a small bandwidth value can lead to\nover-fitting, while using a large bandwidth value may result in under-fitting:\n\nFinally, the ind parameter determines the evaluation points for the plot of\nthe estimated PDF:\n\nFor DataFrame, it works in the same way:\n\nA scalar bandwidth can be specified. Using a small bandwidth value can lead to\nover-fitting, while using a large bandwidth value may result in under-fitting:\n\nFinally, the ind parameter determines the evaluation points for the plot of\nthe estimated PDF:\n\n"}, {"name": "pandas.DataFrame.plot.hexbin", "path": "reference/api/pandas.dataframe.plot.hexbin", "type": "DataFrame", "text": "\nGenerate a hexagonal binning plot.\n\nGenerate a hexagonal binning plot of x versus y. If C is None (the default),\nthis is a histogram of the number of occurrences of the observations at\n`(x[i], y[i])`.\n\nIf C is specified, specifies values at given coordinates `(x[i], y[i])`. These\nvalues are accumulated for each hexagonal bin and then reduced according to\nreduce_C_function, having as default the NumPy\u2019s mean function\n(`numpy.mean()`). (If C is specified, it must also be a 1-D sequence of the\nsame length as x and y, or a column label.)\n\nThe column label or position for x points.\n\nThe column label or position for y points.\n\nThe column label or position for the value of (x, y) point.\n\nFunction of one argument that reduces all the values in a bin to a single\nnumber (e.g. np.mean, np.max, np.sum, np.std).\n\nThe number of hexagons in the x-direction. The corresponding number of\nhexagons in the y-direction is chosen in a way that the hexagons are\napproximately regular. Alternatively, gridsize can be a tuple with two\nelements specifying the number of hexagons in the x-direction and the\ny-direction.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nThe matplotlib `Axes` on which the hexbin is plotted.\n\nSee also\n\nMake plots of a DataFrame.\n\nHexagonal binning plot using matplotlib, the matplotlib function that is used\nunder the hood.\n\nExamples\n\nThe following examples are generated with random data from a normal\ndistribution.\n\nThe next example uses C and np.sum as reduce_C_function. Note that\n\u2018observations\u2019 values ranges from 1 to 5 but the result plot shows values up\nto more than 25. This is because of the reduce_C_function.\n\n"}, {"name": "pandas.DataFrame.plot.hist", "path": "reference/api/pandas.dataframe.plot.hist", "type": "DataFrame", "text": "\nDraw one histogram of the DataFrame\u2019s columns.\n\nA histogram is a representation of the distribution of data. This function\ngroups the values of all given Series in the DataFrame into bins and draws all\nbins in one `matplotlib.axes.Axes`. This is useful when the DataFrame\u2019s Series\nare in a similar scale.\n\nColumn in the DataFrame to group by.\n\nChanged in version 1.4.0: Previously, by is silently ignore and makes no\ngroupings\n\nNumber of histogram bins to be used.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nReturn a histogram plot.\n\nSee also\n\nDraw histograms per DataFrame\u2019s Series.\n\nDraw a histogram with Series\u2019 data.\n\nExamples\n\nWhen we roll a die 6000 times, we expect to get each value around 1000 times.\nBut when we roll two dice and sum the result, the distribution is going to be\nquite different. A histogram illustrates those distributions.\n\nA grouped histogram can be generated by providing the parameter by (which can\nbe a column name, or a list of column names):\n\n"}, {"name": "pandas.DataFrame.plot.kde", "path": "reference/api/pandas.dataframe.plot.kde", "type": "DataFrame", "text": "\nGenerate Kernel Density Estimate plot using Gaussian kernels.\n\nIn statistics, kernel density estimation (KDE) is a non-parametric way to\nestimate the probability density function (PDF) of a random variable. This\nfunction uses Gaussian kernels and includes automatic bandwidth determination.\n\nThe method used to calculate the estimator bandwidth. This can be \u2018scott\u2019,\n\u2018silverman\u2019, a scalar constant or a callable. If None (default), \u2018scott\u2019 is\nused. See `scipy.stats.gaussian_kde` for more information.\n\nEvaluation points for the estimated PDF. If None (default), 1000 equally\nspaced points are used. If ind is a NumPy array, the KDE is evaluated at the\npoints passed. If ind is an integer, ind number of equally spaced points are\nused.\n\nAdditional keyword arguments are documented in `pandas.%(this-\ndatatype)s.plot()`.\n\nSee also\n\nRepresentation of a kernel-density estimate using Gaussian kernels. This is\nthe function used internally to estimate the PDF.\n\nExamples\n\nGiven a Series of points randomly sampled from an unknown distribution,\nestimate its PDF using KDE with automatic bandwidth determination and plot the\nresults, evaluating them at 1000 equally spaced points (default):\n\nA scalar bandwidth can be specified. Using a small bandwidth value can lead to\nover-fitting, while using a large bandwidth value may result in under-fitting:\n\nFinally, the ind parameter determines the evaluation points for the plot of\nthe estimated PDF:\n\nFor DataFrame, it works in the same way:\n\nA scalar bandwidth can be specified. Using a small bandwidth value can lead to\nover-fitting, while using a large bandwidth value may result in under-fitting:\n\nFinally, the ind parameter determines the evaluation points for the plot of\nthe estimated PDF:\n\n"}, {"name": "pandas.DataFrame.plot.line", "path": "reference/api/pandas.dataframe.plot.line", "type": "DataFrame", "text": "\nPlot Series or DataFrame as lines.\n\nThis function is useful to plot lines using DataFrame\u2019s values as coordinates.\n\nAllows plotting of one column versus another. If not specified, the index of\nthe DataFrame is used.\n\nAllows plotting of one column versus another. If not specified, all numerical\ncolumns are used.\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.\n\ncode, which will be used for each column recursively. For instance\n[\u2018green\u2019,\u2019yellow\u2019] each column\u2019s line will be filled in green or yellow,\nalternatively. If there is only a single column to be plotted, then only the\nfirst color from the color list will be used.\n\ncolored accordingly. For example, if your columns are called a and b, then\npassing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color lines for column a in green and\nlines for column b in red.\n\nNew in version 1.1.0.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nAn ndarray is returned with one `matplotlib.axes.Axes` per column when\n`subplots=True`.\n\nSee also\n\nPlot y versus x as lines and/or markers.\n\nExamples\n\nThe following example shows the populations for some animals over the years.\n\nAn example with subplots, so an array of axes is returned.\n\nLet\u2019s repeat the same example, but specifying colors for each column (in this\ncase, for each animal).\n\nThe following example shows the relationship between both populations.\n\n"}, {"name": "pandas.DataFrame.plot.pie", "path": "reference/api/pandas.dataframe.plot.pie", "type": "DataFrame", "text": "\nGenerate a pie plot.\n\nA pie plot is a proportional representation of the numerical data in a column.\nThis function wraps `matplotlib.pyplot.pie()` for the specified column. If no\ncolumn reference is passed and `subplots=True` a pie plot is drawn for each\nnumerical column independently.\n\nLabel or position of the column to plot. If not provided, `subplots=True`\nargument must be passed.\n\nKeyword arguments to pass on to `DataFrame.plot()`.\n\nA NumPy array is returned when subplots is True.\n\nSee also\n\nGenerate a pie plot for a Series.\n\nMake plots of a DataFrame.\n\nExamples\n\nIn the example below we have a DataFrame with the information about planet\u2019s\nmass and radius. We pass the \u2018mass\u2019 column to the pie function to get a pie\nplot.\n\n"}, {"name": "pandas.DataFrame.plot.scatter", "path": "reference/api/pandas.dataframe.plot.scatter", "type": "DataFrame", "text": "\nCreate a scatter plot with varying marker point size and color.\n\nThe coordinates of each point are defined by two dataframe columns and filled\ncircles are used to represent each point. This kind of plot is useful to see\ncomplex correlations between two variables. Points could be for instance\nnatural 2D coordinates like longitude and latitude in a map or, in general,\nany pair of metrics that can be plotted against each other.\n\nThe column name or column position to be used as horizontal coordinates for\neach point.\n\nThe column name or column position to be used as vertical coordinates for each\npoint.\n\nThe size of each point. Possible values are:\n\nA string with the name of the column to be used for marker\u2019s size.\n\nA single scalar so all points have the same size.\n\nA sequence of scalars, which will be used for each point\u2019s size recursively.\nFor instance, when passing [2,14] all points size will be either 2 or 14,\nalternatively.\n\nChanged in version 1.1.0.\n\nThe color of each point. Possible values are:\n\nA single color string referred to by name, RGB or RGBA code, for instance\n\u2018red\u2019 or \u2018#a98d19\u2019.\n\nA sequence of color strings referred to by name, RGB or RGBA code, which will\nbe used for each point\u2019s color recursively. For instance [\u2018green\u2019,\u2019yellow\u2019]\nall points will be filled in green or yellow, alternatively.\n\nA column name or position whose values will be used to color the marker points\naccording to a colormap.\n\nKeyword arguments to pass on to `DataFrame.plot()`.\n\nSee also\n\nScatter plot using multiple input data formats.\n\nExamples\n\nLet\u2019s see how to draw a scatter plot using coordinates from the values in a\nDataFrame\u2019s columns.\n\nAnd now with the color determined by a column as well.\n\n"}, {"name": "pandas.DataFrame.pop", "path": "reference/api/pandas.dataframe.pop", "type": "DataFrame", "text": "\nReturn item and drop from frame. Raise KeyError if not found.\n\nLabel of column to be popped.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.pow", "path": "reference/api/pandas.dataframe.pow", "type": "DataFrame", "text": "\nGet Exponential power of dataframe and other, element-wise (binary operator\npow).\n\nEquivalent to `dataframe ** other`, but with support to substitute a\nfill_value for missing data in one of the inputs. With reverse version, rpow.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.prod", "path": "reference/api/pandas.dataframe.prod", "type": "DataFrame", "text": "\nReturn the product of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nAdditional keyword arguments to be passed to the function.\n\nSee also\n\nReturn the sum.\n\nReturn the minimum.\n\nReturn the maximum.\n\nReturn the index of the minimum.\n\nReturn the index of the maximum.\n\nReturn the sum over the requested axis.\n\nReturn the minimum over the requested axis.\n\nReturn the maximum over the requested axis.\n\nReturn the index of the minimum over the requested axis.\n\nReturn the index of the maximum over the requested axis.\n\nExamples\n\nBy default, the product of an empty or all-NA Series is `1`\n\nThis can be controlled with the `min_count` parameter\n\nThanks to the `skipna` parameter, `min_count` handles all-NA and empty series\nidentically.\n\n"}, {"name": "pandas.DataFrame.product", "path": "reference/api/pandas.dataframe.product", "type": "DataFrame", "text": "\nReturn the product of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nAdditional keyword arguments to be passed to the function.\n\nSee also\n\nReturn the sum.\n\nReturn the minimum.\n\nReturn the maximum.\n\nReturn the index of the minimum.\n\nReturn the index of the maximum.\n\nReturn the sum over the requested axis.\n\nReturn the minimum over the requested axis.\n\nReturn the maximum over the requested axis.\n\nReturn the index of the minimum over the requested axis.\n\nReturn the index of the maximum over the requested axis.\n\nExamples\n\nBy default, the product of an empty or all-NA Series is `1`\n\nThis can be controlled with the `min_count` parameter\n\nThanks to the `skipna` parameter, `min_count` handles all-NA and empty series\nidentically.\n\n"}, {"name": "pandas.DataFrame.quantile", "path": "reference/api/pandas.dataframe.quantile", "type": "DataFrame", "text": "\nReturn values at the given quantile over requested axis.\n\nValue between 0 <= q <= 1, the quantile(s) to compute.\n\nEquals 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.\n\nIf False, the quantile of datetime and timedelta data will be computed as\nwell.\n\nThis optional parameter specifies the interpolation method to use, when the\ndesired quantile lies between two data points i and j:\n\nlinear: i + (j - i) * fraction, where fraction is the fractional part of the\nindex surrounded by i and j.\n\nlower: i.\n\nhigher: j.\n\nnearest: i or j whichever is nearest.\n\nmidpoint: (i \\+ j) / 2.\n\nindex is `q`, the columns are the columns of self, and the values are the\nquantiles.\n\nindex is the columns of self and the values are the quantiles.\n\nSee also\n\nRolling quantile.\n\nNumpy function to compute the percentile.\n\nExamples\n\nSpecifying numeric_only=False will also compute the quantile of datetime and\ntimedelta data.\n\n"}, {"name": "pandas.DataFrame.query", "path": "reference/api/pandas.dataframe.query", "type": "DataFrame", "text": "\nQuery the columns of a DataFrame with a boolean expression.\n\nThe query string to evaluate.\n\nYou can refer to variables in the environment by prefixing them with an \u2018@\u2019\ncharacter like `@a + b`.\n\nYou can refer to column names that are not valid Python variable names by\nsurrounding them in backticks. Thus, column names containing spaces or\npunctuations (besides underscores) or starting with digits must be surrounded\nby backticks. (For example, a column named \u201cArea (cm^2)\u201d would be referenced\nas ``Area (cm^2)``). Column names which are Python keywords (like \u201clist\u201d,\n\u201cfor\u201d, \u201cimport\u201d, etc) cannot be used.\n\nFor example, if one of your columns is called `a a` and you want to sum it\nwith `b`, your query should be ``a a` + b`.\n\nNew in version 0.25.0: Backtick quoting introduced.\n\nNew in version 1.0.0: Expanding functionality of backtick quoting for more\nthan only spaces.\n\nWhether the query should modify the data in place or return a modified copy.\n\nSee the documentation for `eval()` for complete details on the keyword\narguments accepted by `DataFrame.query()`.\n\nDataFrame resulting from the provided query expression or None if\n`inplace=True`.\n\nSee also\n\nEvaluate a string describing operations on DataFrame columns.\n\nEvaluate a string describing operations on DataFrame columns.\n\nNotes\n\nThe result of the evaluation of this expression is first passed to\n`DataFrame.loc` and if that fails because of a multidimensional key (e.g., a\nDataFrame) then the result will be passed to `DataFrame.__getitem__()`.\n\nThis method uses the top-level `eval()` function to evaluate the passed query.\n\nThe `query()` method uses a slightly modified Python syntax by default. For\nexample, the `&` and `|` (bitwise) operators have the precedence of their\nboolean cousins, `and` and `or`. This is syntactically valid Python, however\nthe semantics are different.\n\nYou can change the semantics of the expression by passing the keyword argument\n`parser='python'`. This enforces the same semantics as evaluation in Python\nspace. Likewise, you can pass `engine='python'` to evaluate an expression\nusing Python itself as a backend. This is not recommended as it is inefficient\ncompared to using `numexpr` as the engine.\n\nThe `DataFrame.index` and `DataFrame.columns` attributes of the `DataFrame`\ninstance are placed in the query namespace by default, which allows you to\ntreat both the index and columns of the frame as a column in the frame. The\nidentifier `index` is used for the frame index; you can also use the name of\nthe index to identify it in a query. Please note that Python keywords may not\nbe used as identifiers.\n\nFor further details and examples see the `query` documentation in indexing.\n\nBacktick quoted variables\n\nBacktick quoted variables are parsed as literal Python code and are converted\ninternally to a Python valid identifier. This can lead to the following\nproblems.\n\nDuring parsing a number of disallowed characters inside the backtick quoted\nstring are replaced by strings that are allowed as a Python identifier. These\ncharacters include all operators in Python, the space character, the question\nmark, the exclamation mark, the dollar sign, and the euro sign. For other\ncharacters that fall outside the ASCII range (U+0001..U+007F) and those that\nare not further specified in PEP 3131, the query parser will raise an error.\nThis excludes whitespace different than the space character, but also the\nhashtag (as it is used for comments) and the backtick itself (backtick can\nalso not be escaped).\n\nIn a special case, quotes that make a pair around a backtick can confuse the\nparser. For example, ``it's` > `that's`` will raise an error, as it forms a\nquoted string (`'s > `that'`) with a backtick inside.\n\nSee also the Python documentation about lexical analysis\n(https://docs.python.org/3/reference/lexical_analysis.html) in combination\nwith the source code in `pandas.core.computation.parsing`.\n\nExamples\n\nThe previous expression is equivalent to\n\nFor columns with spaces in their name, you can use backtick quoting.\n\nThe previous expression is equivalent to\n\n"}, {"name": "pandas.DataFrame.radd", "path": "reference/api/pandas.dataframe.radd", "type": "DataFrame", "text": "\nGet Addition of dataframe and other, element-wise (binary operator radd).\n\nEquivalent to `other + dataframe`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, add.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.rank", "path": "reference/api/pandas.dataframe.rank", "type": "DataFrame", "text": "\nCompute numerical data ranks (1 through n) along axis.\n\nBy default, equal values are assigned a rank that is the average of the ranks\nof those values.\n\nIndex to direct ranking.\n\nHow to rank the group of records that have the same value (i.e. ties):\n\naverage: average rank of the group\n\nmin: lowest rank in the group\n\nmax: highest rank in the group\n\nfirst: ranks assigned in order they appear in the array\n\ndense: like \u2018min\u2019, but rank always increases by 1 between groups.\n\nFor DataFrame objects, rank only numeric columns if set to True.\n\nHow to rank NaN values:\n\nkeep: assign NaN rank to NaN values\n\ntop: assign lowest rank to NaN values\n\nbottom: assign highest rank to NaN values\n\nWhether or not the elements should be ranked in ascending order.\n\nWhether or not to display the returned rankings in percentile form.\n\nReturn a Series or DataFrame with data ranks as values.\n\nSee also\n\nRank of values within each group.\n\nExamples\n\nThe following example shows how the method behaves with the above parameters:\n\ndefault_rank: this is the default behaviour obtained without using any\nparameter.\n\nmax_rank: setting `method = 'max'` the records that have the same values are\nranked using the highest rank (e.g.: since \u2018cat\u2019 and \u2018dog\u2019 are both in the 2nd\nand 3rd position, rank 3 is assigned.)\n\nNA_bottom: choosing `na_option = 'bottom'`, if there are records with NaN\nvalues they are placed at the bottom of the ranking.\n\npct_rank: when setting `pct = True`, the ranking is expressed as percentile\nrank.\n\n"}, {"name": "pandas.DataFrame.rdiv", "path": "reference/api/pandas.dataframe.rdiv", "type": "DataFrame", "text": "\nGet Floating division of dataframe and other, element-wise (binary operator\nrtruediv).\n\nEquivalent to `other / dataframe`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, truediv.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.reindex", "path": "reference/api/pandas.dataframe.reindex", "type": "DataFrame", "text": "\nConform Series/DataFrame to new index with optional filling logic.\n\nPlaces NA/NaN in locations having no value in the previous index. A new object\nis produced unless the new index is equivalent to the current one and\n`copy=False`.\n\nNew labels / index to conform to, should be specified using keywords.\nPreferably an Index object to avoid duplicating data.\n\nMethod to use for filling holes in reindexed DataFrame. Please note: this is\nonly applicable to DataFrames/Series with a monotonically\nincreasing/decreasing index.\n\nNone (default): don\u2019t fill gaps\n\npad / ffill: Propagate last valid observation forward to next valid.\n\nbackfill / bfill: Use next valid observation to fill gap.\n\nnearest: Use nearest valid observations to fill gap.\n\nReturn a new object, even if the passed indexes are the same.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nValue to use for missing values. Defaults to NaN, but can be any \u201ccompatible\u201d\nvalue.\n\nMaximum number of consecutive elements to forward or backward fill.\n\nMaximum distance between original and new labels for inexact matches. The\nvalues of the index at the matching locations most satisfy the equation\n`abs(index[indexer] - target) <= tolerance`.\n\nTolerance may be a scalar value, which applies the same tolerance to all\nvalues, or list-like, which applies variable tolerance per element. List-like\nincludes list, tuple, array, Series, and must be the same size as the index\nand its dtype must exactly match the index\u2019s type.\n\nSee also\n\nSet row labels.\n\nRemove row labels or move them to new columns.\n\nChange to same indices as other DataFrame.\n\nExamples\n\n`DataFrame.reindex` supports two calling conventions\n\n`(index=index_labels, columns=column_labels, ...)`\n\n`(labels, axis={'index', 'columns'}, ...)`\n\nWe highly recommend using keyword arguments to clarify your intent.\n\nCreate a dataframe with some fictional data.\n\nCreate a new index and reindex the dataframe. By default values in the new\nindex that do not have corresponding records in the dataframe are assigned\n`NaN`.\n\nWe can fill in the missing values by passing a value to the keyword\n`fill_value`. Because the index is not monotonically increasing or decreasing,\nwe cannot use arguments to the keyword `method` to fill the `NaN` values.\n\nWe can also reindex the columns.\n\nOr we can use \u201caxis-style\u201d keyword arguments\n\nTo further illustrate the filling functionality in `reindex`, we will create a\ndataframe with a monotonically increasing index (for example, a sequence of\ndates).\n\nSuppose we decide to expand the dataframe to cover a wider date range.\n\nThe index entries that did not have a value in the original data frame (for\nexample, \u20182009-12-29\u2019) are by default filled with `NaN`. If desired, we can\nfill in the missing values using one of several options.\n\nFor example, to back-propagate the last valid value to fill the `NaN` values,\npass `bfill` as an argument to the `method` keyword.\n\nPlease note that the `NaN` value present in the original dataframe (at index\nvalue 2010-01-03) will not be filled by any of the value propagation schemes.\nThis is because filling while reindexing does not look at dataframe values,\nbut only compares the original and desired indexes. If you do want to fill in\nthe `NaN` values present in the original dataframe, use the `fillna()` method.\n\nSee the user guide for more.\n\n"}, {"name": "pandas.DataFrame.reindex_like", "path": "reference/api/pandas.dataframe.reindex_like", "type": "DataFrame", "text": "\nReturn an object with matching indices as other object.\n\nConform the object to the same index on all axes. Optional filling logic,\nplacing NaN in locations having no value in the previous index. A new object\nis produced unless the new index is equivalent to the current one and\ncopy=False.\n\nIts row and column indices are used to define the new indices of this object.\n\nMethod to use for filling holes in reindexed DataFrame. Please note: this is\nonly applicable to DataFrames/Series with a monotonically\nincreasing/decreasing index.\n\nNone (default): don\u2019t fill gaps\n\npad / ffill: propagate last valid observation forward to next valid\n\nbackfill / bfill: use next valid observation to fill gap\n\nnearest: use nearest valid observations to fill gap.\n\nReturn a new object, even if the passed indexes are the same.\n\nMaximum number of consecutive labels to fill for inexact matches.\n\nMaximum distance between original and new labels for inexact matches. The\nvalues of the index at the matching locations must satisfy the equation\n`abs(index[indexer] - target) <= tolerance`.\n\nTolerance may be a scalar value, which applies the same tolerance to all\nvalues, or list-like, which applies variable tolerance per element. List-like\nincludes list, tuple, array, Series, and must be the same size as the index\nand its dtype must exactly match the index\u2019s type.\n\nSame type as caller, but with changed indices on each axis.\n\nSee also\n\nSet row labels.\n\nRemove row labels or move them to new columns.\n\nChange to new indices or expand indices.\n\nNotes\n\nSame as calling `.reindex(index=other.index, columns=other.columns,...)`.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.rename", "path": "reference/api/pandas.dataframe.rename", "type": "DataFrame", "text": "\nAlter axes labels.\n\nFunction / dict values must be unique (1-to-1). Labels not contained in a dict\n/ Series will be left as-is. Extra labels listed don\u2019t throw an error.\n\nSee the user guide for more.\n\nDict-like or function transformations to apply to that axis\u2019 values. Use\neither `mapper` and `axis` to specify the axis to target with `mapper`, or\n`index` and `columns`.\n\nAlternative to specifying axis (`mapper, axis=0` is equivalent to\n`index=mapper`).\n\nAlternative to specifying axis (`mapper, axis=1` is equivalent to\n`columns=mapper`).\n\nAxis to target with `mapper`. Can be either the axis name (\u2018index\u2019, \u2018columns\u2019)\nor number (0, 1). The default is \u2018index\u2019.\n\nAlso copy underlying data.\n\nWhether to return a new DataFrame. If True then value of copy is ignored.\n\nIn case of a MultiIndex, only rename labels in the specified level.\n\nIf \u2018raise\u2019, raise a KeyError when a dict-like mapper, index, or columns\ncontains labels that are not present in the Index being transformed. If\n\u2018ignore\u2019, existing keys will be renamed and extra keys will be ignored.\n\nDataFrame with the renamed axis labels or None if `inplace=True`.\n\nIf any of the labels is not found in the selected axis and \u201cerrors=\u2019raise\u2019\u201d.\n\nSee also\n\nSet the name of the axis.\n\nExamples\n\n`DataFrame.rename` supports two calling conventions\n\n`(index=index_mapper, columns=columns_mapper, ...)`\n\n`(mapper, axis={'index', 'columns'}, ...)`\n\nWe highly recommend using keyword arguments to clarify your intent.\n\nRename columns using a mapping:\n\nRename index using a mapping:\n\nCast index labels to a different type:\n\nUsing axis-style parameters:\n\n"}, {"name": "pandas.DataFrame.rename_axis", "path": "reference/api/pandas.dataframe.rename_axis", "type": "DataFrame", "text": "\nSet the name of the axis for the index or columns.\n\nValue to set the axis name attribute.\n\nA scalar, list-like, dict-like or functions transformations to apply to that\naxis\u2019 values. Note that the `columns` parameter is not allowed if the object\nis a Series. This parameter only apply for DataFrame type objects.\n\nUse either `mapper` and `axis` to specify the axis to target with `mapper`, or\n`index` and/or `columns`.\n\nThe axis to rename.\n\nAlso copy underlying data.\n\nModifies the object directly, instead of creating a new Series or DataFrame.\n\nThe same type as the caller or None if `inplace=True`.\n\nSee also\n\nAlter Series index labels or name.\n\nAlter DataFrame index labels or name.\n\nSet new names on index.\n\nNotes\n\n`DataFrame.rename_axis` supports two calling conventions\n\n`(index=index_mapper, columns=columns_mapper, ...)`\n\n`(mapper, axis={'index', 'columns'}, ...)`\n\nThe first calling convention will only modify the names of the index and/or\nthe names of the Index object that is the columns. In this case, the parameter\n`copy` is ignored.\n\nThe second calling convention will modify the names of the corresponding index\nif mapper is a list or a scalar. However, if mapper is dict-like or a\nfunction, it will use the deprecated behavior of modifying the axis labels.\n\nWe highly recommend using keyword arguments to clarify your intent.\n\nExamples\n\nSeries\n\nDataFrame\n\nMultiIndex\n\n"}, {"name": "pandas.DataFrame.reorder_levels", "path": "reference/api/pandas.dataframe.reorder_levels", "type": "DataFrame", "text": "\nRearrange index levels using input order. May not drop or duplicate levels.\n\nList representing new level order. Reference level by number (position) or by\nkey (label).\n\nWhere to reorder levels.\n\nExamples\n\nLet\u2019s reorder the levels of the index:\n\n"}, {"name": "pandas.DataFrame.replace", "path": "reference/api/pandas.dataframe.replace", "type": "DataFrame", "text": "\nReplace values given in to_replace with value.\n\nValues of the DataFrame are replaced with other values dynamically.\n\nThis differs from updating with `.loc` or `.iloc`, which require you to\nspecify a location to update with some value.\n\nHow to find the values that will be replaced.\n\nnumeric, str or regex:\n\nnumeric: numeric values equal to to_replace will be replaced with value\n\nstr: string exactly matching to_replace will be replaced with value\n\nregex: regexs matching to_replace will be replaced with value\n\nlist of str, regex, or numeric:\n\nFirst, if to_replace and value are both lists, they must be the same length.\n\nSecond, if `regex=True` then all of the strings in both lists will be\ninterpreted as regexs otherwise they will match directly. This doesn\u2019t matter\nmuch for value since there are only a few possible substitution regexes you\ncan use.\n\nstr, regex and numeric rules apply as above.\n\ndict:\n\nDicts can be used to specify different replacement values for different\nexisting values. For example, `{'a': 'b', 'y': 'z'}` replaces the value \u2018a\u2019\nwith \u2018b\u2019 and \u2018y\u2019 with \u2018z\u2019. To use a dict in this way the value parameter\nshould be None.\n\nFor a DataFrame a dict can specify that different values should be replaced in\ndifferent columns. For example, `{'a': 1, 'b': 'z'}` looks for the value 1 in\ncolumn \u2018a\u2019 and the value \u2018z\u2019 in column \u2018b\u2019 and replaces these values with\nwhatever is specified in value. The value parameter should not be `None` in\nthis case. You can treat this as a special case of passing two lists except\nthat you are specifying the column to search in.\n\nFor a DataFrame nested dictionaries, e.g., `{'a': {'b': np.nan}}`, are read as\nfollows: look in column \u2018a\u2019 for the value \u2018b\u2019 and replace it with NaN. The\nvalue parameter should be `None` to use a nested dict in this way. You can\nnest regular expressions as well. Note that column names (the top-level\ndictionary keys in a nested dictionary) cannot be regular expressions.\n\nNone:\n\nThis means that the regex argument must be a string, compiled regular\nexpression, or list, dict, ndarray or Series of such elements. If value is\nalso `None` then this must be a nested dictionary or Series.\n\nSee the examples section for examples of each of these.\n\nValue to replace any values matching to_replace with. For a DataFrame a dict\nof values can be used to specify which value to use for each column (columns\nnot in the dict will not be filled). Regular expressions, strings and lists or\ndicts of such objects are also allowed.\n\nIf True, performs operation inplace and returns None.\n\nMaximum size gap to forward or backward fill.\n\nWhether to interpret to_replace and/or value as regular expressions. If this\nis `True` then to_replace must be a string. Alternatively, this could be a\nregular expression or a list, dict, or array of regular expressions in which\ncase to_replace must be `None`.\n\nThe method to use when for replacement, when to_replace is a scalar, list or\ntuple and value is `None`.\n\nChanged in version 0.23.0: Added to DataFrame.\n\nObject after replacement.\n\nIf regex is not a `bool` and to_replace is not `None`.\n\nIf to_replace is not a scalar, array-like, `dict`, or `None`\n\nIf to_replace is a `dict` and value is not a `list`, `dict`, `ndarray`, or\n`Series`\n\nIf to_replace is `None` and regex is not compilable into a regular expression\nor is a list, dict, ndarray, or Series.\n\nWhen replacing multiple `bool` or `datetime64` objects and the arguments to\nto_replace does not match the type of the value being replaced\n\nIf a `list` or an `ndarray` is passed to to_replace and value but they are not\nthe same length.\n\nSee also\n\nFill NA values.\n\nReplace values based on boolean condition.\n\nSimple string replacement.\n\nNotes\n\nRegex substitution is performed under the hood with `re.sub`. The rules for\nsubstitution for `re.sub` are the same.\n\nRegular expressions will only substitute on strings, meaning you cannot\nprovide, for example, a regular expression matching floating point numbers and\nexpect the columns in your frame that have a numeric dtype to be matched.\nHowever, if those floating point numbers are strings, then you can do this.\n\nThis method has a lot of options. You are encouraged to experiment and play\nwith this method to gain intuition about how it works.\n\nWhen dict is used as the to_replace value, it is like key(s) in the dict are\nthe to_replace part and value(s) in the dict are the value parameter.\n\nExamples\n\nScalar `to_replace` and `value`\n\nList-like `to_replace`\n\ndict-like `to_replace`\n\nRegular expression `to_replace`\n\nCompare the behavior of `s.replace({'a': None})` and `s.replace('a', None)` to\nunderstand the peculiarities of the to_replace parameter:\n\nWhen one uses a dict as the to_replace value, it is like the value(s) in the\ndict are equal to the value parameter. `s.replace({'a': None})` is equivalent\nto `s.replace(to_replace={'a': None}, value=None, method=None)`:\n\nWhen `value` is not explicitly passed and to_replace is a scalar, list or\ntuple, replace uses the method parameter (default \u2018pad\u2019) to do the\nreplacement. So this is why the \u2018a\u2019 values are being replaced by 10 in rows 1\nand 2 and \u2018b\u2019 in row 4 in this case.\n\nOn the other hand, if `None` is explicitly passed for `value`, it will be\nrespected:\n\nChanged in version 1.4.0: Previously the explicit `None` was silently ignored.\n\n"}, {"name": "pandas.DataFrame.resample", "path": "reference/api/pandas.dataframe.resample", "type": "DataFrame", "text": "\nResample time-series data.\n\nConvenience method for frequency conversion and resampling of time series. The\nobject must have a datetime-like index (DatetimeIndex, PeriodIndex, or\nTimedeltaIndex), or the caller must pass the label of a datetime-like\nseries/index to the `on`/`level` keyword parameter.\n\nThe offset string or object representing target conversion.\n\nWhich axis to use for up- or down-sampling. For Series this will default to 0,\ni.e. along the rows. Must be DatetimeIndex, TimedeltaIndex or PeriodIndex.\n\nWhich side of bin interval is closed. The default is \u2018left\u2019 for all frequency\noffsets except for \u2018M\u2019, \u2018A\u2019, \u2018Q\u2019, \u2018BM\u2019, \u2018BA\u2019, \u2018BQ\u2019, and \u2018W\u2019 which all have a\ndefault of \u2018right\u2019.\n\nWhich bin edge label to label bucket with. The default is \u2018left\u2019 for all\nfrequency offsets except for \u2018M\u2019, \u2018A\u2019, \u2018Q\u2019, \u2018BM\u2019, \u2018BA\u2019, \u2018BQ\u2019, and \u2018W\u2019 which\nall have a default of \u2018right\u2019.\n\nFor PeriodIndex only, controls whether to use the start or end of rule.\n\nPass \u2018timestamp\u2019 to convert the resulting index to a DateTimeIndex or \u2018period\u2019\nto convert it to a PeriodIndex. By default the input representation is\nretained.\n\nAdjust the resampled time labels.\n\nDeprecated since version 1.1.0: You should add the loffset to the df.index\nafter the resample. See below.\n\nFor frequencies that evenly subdivide 1 day, the \u201corigin\u201d of the aggregated\nintervals. For example, for \u20185min\u2019 frequency, base could range from 0 through\n4. Defaults to 0.\n\nDeprecated since version 1.1.0: The new arguments that you should use are\n\u2018offset\u2019 or \u2018origin\u2019.\n\nFor a DataFrame, column to use instead of index for resampling. Column must be\ndatetime-like.\n\nFor a MultiIndex, level (name or number) to use for resampling. level must be\ndatetime-like.\n\nThe timestamp on which to adjust the grouping. The timezone of origin must\nmatch the timezone of the index. If string, must be one of the following:\n\n\u2018epoch\u2019: origin is 1970-01-01\n\n\u2018start\u2019: origin is the first value of the timeseries\n\n\u2018start_day\u2019: origin is the first day at midnight of the timeseries\n\nNew in version 1.1.0.\n\n\u2018end\u2019: origin is the last value of the timeseries\n\n\u2018end_day\u2019: origin is the ceiling midnight of the last day\n\nNew in version 1.3.0.\n\nAn offset timedelta added to the origin.\n\nNew in version 1.1.0.\n\n`Resampler` object.\n\nSee also\n\nResample a Series.\n\nResample a DataFrame.\n\nGroup DataFrame by mapping, function, label, or list of labels.\n\nReindex a DataFrame with the given frequency without grouping.\n\nNotes\n\nSee the user guide for more.\n\nTo learn more about the offset strings, please see this link.\n\nExamples\n\nStart by creating a series with 9 one minute timestamps.\n\nDownsample the series into 3 minute bins and sum the values of the timestamps\nfalling into a bin.\n\nDownsample the series into 3 minute bins as above, but label each bin using\nthe right edge instead of the left. Please note that the value in the bucket\nused as the label is not included in the bucket, which it labels. For example,\nin the original series the bucket `2000-01-01 00:03:00` contains the value 3,\nbut the summed value in the resampled bucket with the label `2000-01-01\n00:03:00` does not include 3 (if it did, the summed value would be 6, not 3).\nTo include this value close the right side of the bin interval as illustrated\nin the example below this one.\n\nDownsample the series into 3 minute bins as above, but close the right side of\nthe bin interval.\n\nUpsample the series into 30 second bins.\n\nUpsample the series into 30 second bins and fill the `NaN` values using the\n`pad` method.\n\nUpsample the series into 30 second bins and fill the `NaN` values using the\n`bfill` method.\n\nPass a custom function via `apply`\n\nFor a Series with a PeriodIndex, the keyword convention can be used to control\nwhether to use the start or end of rule.\n\nResample a year by quarter using \u2018start\u2019 convention. Values are assigned to\nthe first quarter of the period.\n\nResample quarters by month using \u2018end\u2019 convention. Values are assigned to the\nlast month of the period.\n\nFor DataFrame objects, the keyword on can be used to specify the column\ninstead of the index for resampling.\n\nFor a DataFrame with MultiIndex, the keyword level can be used to specify on\nwhich level the resampling needs to take place.\n\nIf you want to adjust the start of the bins based on a fixed timestamp:\n\nIf you want to adjust the start of the bins with an offset Timedelta, the two\nfollowing lines are equivalent:\n\nIf you want to take the largest Timestamp as the end of the bins:\n\nIn contrast with the start_day, you can use end_day to take the ceiling\nmidnight of the largest Timestamp as the end of the bins and drop the bins not\ncontaining data:\n\nTo replace the use of the deprecated base argument, you can now use offset, in\nthis example it is equivalent to have base=2:\n\nTo replace the use of the deprecated loffset argument:\n\n"}, {"name": "pandas.DataFrame.reset_index", "path": "reference/api/pandas.dataframe.reset_index", "type": "DataFrame", "text": "\nReset the index, or a level of it.\n\nReset the index of the DataFrame, and use the default one instead. If the\nDataFrame has a MultiIndex, this method can remove one or more levels.\n\nOnly remove the given levels from the index. Removes all levels by default.\n\nDo not try to insert index into dataframe columns. This resets the index to\nthe default integer index.\n\nModify the DataFrame in place (do not create a new object).\n\nIf the columns have multiple levels, determines which level the labels are\ninserted into. By default it is inserted into the first level.\n\nIf the columns have multiple levels, determines how the other levels are\nnamed. If None then the index name is repeated.\n\nDataFrame with the new index or None if `inplace=True`.\n\nSee also\n\nOpposite of reset_index.\n\nChange to new indices or expand indices.\n\nChange to same indices as other DataFrame.\n\nExamples\n\nWhen we reset the index, the old index is added as a column, and a new\nsequential index is used:\n\nWe can use the drop parameter to avoid the old index being added as a column:\n\nYou can also use reset_index with MultiIndex.\n\nIf the index has multiple levels, we can reset a subset of them:\n\nIf we are not dropping the index, by default, it is placed in the top level.\nWe can place it in another level:\n\nWhen the index is inserted under another level, we can specify under which one\nwith the parameter col_fill:\n\nIf we specify a nonexistent level for col_fill, it is created:\n\n"}, {"name": "pandas.DataFrame.rfloordiv", "path": "reference/api/pandas.dataframe.rfloordiv", "type": "DataFrame", "text": "\nGet Integer division of dataframe and other, element-wise (binary operator\nrfloordiv).\n\nEquivalent to `other // dataframe`, but with support to substitute a\nfill_value for missing data in one of the inputs. With reverse version,\nfloordiv.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.rmod", "path": "reference/api/pandas.dataframe.rmod", "type": "DataFrame", "text": "\nGet Modulo of dataframe and other, element-wise (binary operator rmod).\n\nEquivalent to `other % dataframe`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, mod.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.rmul", "path": "reference/api/pandas.dataframe.rmul", "type": "DataFrame", "text": "\nGet Multiplication of dataframe and other, element-wise (binary operator\nrmul).\n\nEquivalent to `other * dataframe`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, mul.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.rolling", "path": "reference/api/pandas.dataframe.rolling", "type": "DataFrame", "text": "\nProvide rolling window calculations.\n\nSize of the moving window.\n\nIf an integer, the fixed number of observations used for each window.\n\nIf an offset, the time period of each window. Each window will be a variable\nsized based on the observations included in the time-period. This is only\nvalid for datetimelike indexes. To learn more about the offsets & frequency\nstrings, please see this link.\n\nIf a BaseIndexer subclass, the window boundaries based on the defined\n`get_window_bounds` method. Additional rolling keyword arguments, namely\n`min_periods`, `center`, and `closed` will be passed to `get_window_bounds`.\n\nMinimum number of observations in window required to have a value; otherwise,\nresult is `np.nan`.\n\nFor a window that is specified by an offset, `min_periods` will default to 1.\n\nFor a window that is specified by an integer, `min_periods` will default to\nthe size of the window.\n\nIf False, set the window labels as the right edge of the window index.\n\nIf True, set the window labels as the center of the window index.\n\nIf `None`, all points are evenly weighted.\n\nIf a string, it must be a valid scipy.signal window function.\n\nCertain Scipy window types require additional parameters to be passed in the\naggregation function. The additional parameters must match the keywords\nspecified in the Scipy window type method signature.\n\nFor a DataFrame, a column label or Index level on which to calculate the\nrolling window, rather than the DataFrame\u2019s index.\n\nProvided integer column is ignored and excluded from result since an integer\nindex is not used to calculate the rolling window.\n\nIf `0` or `'index'`, roll across the rows.\n\nIf `1` or `'columns'`, roll across the columns.\n\nIf `'right'`, the first point in the window is excluded from calculations.\n\nIf `'left'`, the last point in the window is excluded from calculations.\n\nIf `'both'`, the no points in the window are excluded from calculations.\n\nIf `'neither'`, the first and last points in the window are excluded from\ncalculations.\n\nDefault `None` (`'right'`).\n\nChanged in version 1.2.0: The closed parameter with fixed windows is now\nsupported.\n\nNew in version 1.3.0.\n\nExecute the rolling operation per single column or row (`'single'`) or over\nthe entire object (`'table'`).\n\nThis argument is only implemented when specifying `engine='numba'` in the\nmethod call.\n\nSee also\n\nProvides expanding transformations.\n\nProvides exponential weighted functions.\n\nNotes\n\nSee Windowing Operations for further usage details and examples.\n\nExamples\n\nwindow\n\nRolling sum with a window length of 2 observations.\n\nRolling sum with a window span of 2 seconds.\n\nRolling sum with forward looking windows with 2 observations.\n\nmin_periods\n\nRolling sum with a window length of 2 observations, but only needs a minimum\nof 1 observation to calculate a value.\n\ncenter\n\nRolling sum with the result assigned to the center of the window index.\n\nwin_type\n\nRolling sum with a window length of 2, using the Scipy `'gaussian'` window\ntype. `std` is required in the aggregation function.\n\n"}, {"name": "pandas.DataFrame.round", "path": "reference/api/pandas.dataframe.round", "type": "DataFrame", "text": "\nRound a DataFrame to a variable number of decimal places.\n\nNumber of decimal places to round each column to. If an int is given, round\neach column to the same number of places. Otherwise dict and Series round to\nvariable numbers of places. Column names should be in the keys if decimals is\na dict-like, or in the index if decimals is a Series. Any columns not included\nin decimals will be left as is. Elements of decimals which are not columns of\nthe input will be ignored.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith numpy.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith numpy.\n\nA DataFrame with the affected columns rounded to the specified number of\ndecimal places.\n\nSee also\n\nRound a numpy array to the given number of decimals.\n\nRound a Series to the given number of decimals.\n\nExamples\n\nBy providing an integer each column is rounded to the same number of decimal\nplaces\n\nWith a dict, the number of places for specific columns can be specified with\nthe column names as key and the number of decimal places as value\n\nUsing a Series, the number of places for specific columns can be specified\nwith the column names as index and the number of decimal places as value\n\n"}, {"name": "pandas.DataFrame.rpow", "path": "reference/api/pandas.dataframe.rpow", "type": "DataFrame", "text": "\nGet Exponential power of dataframe and other, element-wise (binary operator\nrpow).\n\nEquivalent to `other ** dataframe`, but with support to substitute a\nfill_value for missing data in one of the inputs. With reverse version, pow.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.rsub", "path": "reference/api/pandas.dataframe.rsub", "type": "DataFrame", "text": "\nGet Subtraction of dataframe and other, element-wise (binary operator rsub).\n\nEquivalent to `other - dataframe`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, sub.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.rtruediv", "path": "reference/api/pandas.dataframe.rtruediv", "type": "DataFrame", "text": "\nGet Floating division of dataframe and other, element-wise (binary operator\nrtruediv).\n\nEquivalent to `other / dataframe`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, truediv.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.sample", "path": "reference/api/pandas.dataframe.sample", "type": "DataFrame", "text": "\nReturn a random sample of items from an axis of object.\n\nYou can use random_state for reproducibility.\n\nNumber of items from axis to return. Cannot be used with frac. Default = 1 if\nfrac = None.\n\nFraction of axis items to return. Cannot be used with n.\n\nAllow or disallow sampling of the same row more than once.\n\nDefault \u2018None\u2019 results in equal probability weighting. If passed a Series,\nwill align with target object on index. Index values in weights not found in\nsampled object will be ignored and index values in sampled object not in\nweights will be assigned weights of zero. If called on a DataFrame, will\naccept the name of a column when axis = 0. Unless weights are a Series,\nweights must be same length as axis being sampled. If weights do not sum to 1,\nthey will be normalized to sum to 1. Missing values in the weights column will\nbe treated as zero. Infinite values not allowed.\n\nIf int, array-like, or BitGenerator, seed for random number generator. If\nnp.random.RandomState or np.random.Generator, use as given.\n\nChanged in version 1.1.0: array-like and BitGenerator object now passed to\nnp.random.RandomState() as seed\n\nChanged in version 1.4.0: np.random.Generator objects now accepted\n\nAxis to sample. Accepts axis number or name. Default is stat axis for given\ndata type (0 for Series and DataFrames).\n\nIf True, the resulting index will be labeled 0, 1, \u2026, n - 1.\n\nNew in version 1.3.0.\n\nA new object of same type as caller containing n items randomly sampled from\nthe caller object.\n\nSee also\n\nGenerates random samples from each group of a DataFrame object.\n\nGenerates random samples from each group of a Series object.\n\nGenerates a random sample from a given 1-D numpy array.\n\nNotes\n\nIf frac > 1, replacement should be set to True.\n\nExamples\n\nExtract 3 random elements from the `Series` `df['num_legs']`: Note that we use\nrandom_state to ensure the reproducibility of the examples.\n\nA random 50% sample of the `DataFrame` with replacement:\n\nAn upsample sample of the `DataFrame` with replacement: Note that replace\nparameter has to be True for frac parameter > 1.\n\nUsing a DataFrame column as weights. Rows with larger value in the\nnum_specimen_seen column are more likely to be sampled.\n\n"}, {"name": "pandas.DataFrame.select_dtypes", "path": "reference/api/pandas.dataframe.select_dtypes", "type": "General utility functions", "text": "\nReturn a subset of the DataFrame\u2019s columns based on the column dtypes.\n\nA selection of dtypes or strings to be included/excluded. At least one of\nthese parameters must be supplied.\n\nThe subset of the frame including the dtypes in `include` and excluding the\ndtypes in `exclude`.\n\nIf both of `include` and `exclude` are empty\n\nIf `include` and `exclude` have overlapping elements\n\nIf any kind of string dtype is passed in.\n\nSee also\n\nReturn Series with the data type of each column.\n\nNotes\n\nTo select all numeric types, use `np.number` or `'number'`\n\nTo select strings you must use the `object` dtype, but note that this will\nreturn all object dtype columns\n\nSee the numpy dtype hierarchy\n\nTo select datetimes, use `np.datetime64`, `'datetime'` or `'datetime64'`\n\nTo select timedeltas, use `np.timedelta64`, `'timedelta'` or `'timedelta64'`\n\nTo select Pandas categorical dtypes, use `'category'`\n\nTo select Pandas datetimetz dtypes, use `'datetimetz'` (new in 0.20.0) or\n`'datetime64[ns, tz]'`\n\nExamples\n\n"}, {"name": "pandas.DataFrame.sem", "path": "reference/api/pandas.dataframe.sem", "type": "DataFrame", "text": "\nReturn unbiased standard error of the mean over requested axis.\n\nNormalized by N-1 by default. This can be changed using the ddof argument\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nDelta Degrees of Freedom. The divisor used in calculations is N - ddof, where\nN represents the number of elements.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\n"}, {"name": "pandas.DataFrame.set_axis", "path": "reference/api/pandas.dataframe.set_axis", "type": "DataFrame", "text": "\nAssign desired index to given axis.\n\nIndexes for column or row labels can be changed by assigning a list-like or\nIndex.\n\nThe values for the new index.\n\nThe axis to update. The value 0 identifies the rows, and 1 identifies the\ncolumns.\n\nWhether to return a new DataFrame instance.\n\nAn object of type DataFrame or None if `inplace=True`.\n\nSee also\n\nAlter the name of the index or columns.\n\nExamples\n\nChange the row labels.\n\nChange the column labels.\n\nNow, update the labels inplace.\n\n"}, {"name": "pandas.DataFrame.set_flags", "path": "reference/api/pandas.dataframe.set_flags", "type": "DataFrame", "text": "\nReturn a new object with updated flags.\n\nWhether the returned object allows duplicate labels.\n\nThe same type as the caller.\n\nSee also\n\nGlobal metadata applying to this dataset.\n\nGlobal flags applying to this object.\n\nNotes\n\nThis method returns a new object that\u2019s a view on the same data as the input.\nMutating the input or the output values will be reflected in the other.\n\nThis method is intended to be used in method chains.\n\n\u201cFlags\u201d differ from \u201cmetadata\u201d. Flags reflect properties of the pandas object\n(the Series or DataFrame). Metadata refer to properties of the dataset, and\nshould be stored in `DataFrame.attrs`.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.set_index", "path": "reference/api/pandas.dataframe.set_index", "type": "DataFrame", "text": "\nSet the DataFrame index using existing columns.\n\nSet the DataFrame index (row labels) using one or more existing columns or\narrays (of the correct length). The index can replace the existing index or\nexpand on it.\n\nThis parameter can be either a single column key, a single array of the same\nlength as the calling DataFrame, or a list containing an arbitrary combination\nof column keys and arrays. Here, \u201carray\u201d encompasses `Series`, `Index`,\n`np.ndarray`, and instances of `Iterator`.\n\nDelete columns to be used as the new index.\n\nWhether to append columns to existing index.\n\nIf True, modifies the DataFrame in place (do not create a new object).\n\nCheck the new index for duplicates. Otherwise defer the check until necessary.\nSetting to False will improve the performance of this method.\n\nChanged row labels or None if `inplace=True`.\n\nSee also\n\nOpposite of set_index.\n\nChange to new indices or expand indices.\n\nChange to same indices as other DataFrame.\n\nExamples\n\nSet the index to become the \u2018month\u2019 column:\n\nCreate a MultiIndex using columns \u2018year\u2019 and \u2018month\u2019:\n\nCreate a MultiIndex using an Index and a column:\n\nCreate a MultiIndex using two Series:\n\n"}, {"name": "pandas.DataFrame.shape", "path": "reference/api/pandas.dataframe.shape", "type": "DataFrame", "text": "\nReturn a tuple representing the dimensionality of the DataFrame.\n\nSee also\n\nTuple of array dimensions.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.shift", "path": "reference/api/pandas.dataframe.shift", "type": "DataFrame", "text": "\nShift index by desired number of periods with an optional time freq.\n\nWhen freq is not passed, shift the index without realigning the data. If freq\nis passed (in this case, the index must be date or datetime, or it will raise\na NotImplementedError), the index will be increased using the periods and the\nfreq. freq can be inferred when specified as \u201cinfer\u201d as long as either freq or\ninferred_freq attribute is set in the index.\n\nNumber of periods to shift. Can be positive or negative.\n\nOffset to use from the tseries module or time rule (e.g. \u2018EOM\u2019). If freq is\nspecified then the index values are shifted but the data is not realigned.\nThat is, use freq if you would like to extend the index when shifting and\npreserve the original data. If freq is specified as \u201cinfer\u201d then it will be\ninferred from the freq or inferred_freq attributes of the index. If neither of\nthose attributes exist, a ValueError is thrown.\n\nShift direction.\n\nThe scalar value to use for newly introduced missing values. the default\ndepends on the dtype of self. For numeric data, `np.nan` is used. For\ndatetime, timedelta, or period data, etc. `NaT` is used. For extension dtypes,\n`self.dtype.na_value` is used.\n\nChanged in version 1.1.0.\n\nCopy of input object, shifted.\n\nSee also\n\nShift values of Index.\n\nShift values of DatetimeIndex.\n\nShift values of PeriodIndex.\n\nShift the time index, using the index\u2019s frequency if available.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.size", "path": "reference/api/pandas.dataframe.size", "type": "DataFrame", "text": "\nReturn an int representing the number of elements in this object.\n\nReturn the number of rows if Series. Otherwise return the number of rows times\nnumber of columns if DataFrame.\n\nSee also\n\nNumber of elements in the array.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.skew", "path": "reference/api/pandas.dataframe.skew", "type": "DataFrame", "text": "\nReturn unbiased skew over requested axis.\n\nNormalized by N-1.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.DataFrame.slice_shift", "path": "reference/api/pandas.dataframe.slice_shift", "type": "DataFrame", "text": "\nEquivalent to shift without copying data. The shifted data will not include\nthe dropped periods and the shifted axis will be smaller than the original.\n\nDeprecated since version 1.2.0: slice_shift is deprecated, use\nDataFrame/Series.shift instead.\n\nNumber of periods to move, can be positive or negative.\n\nNotes\n\nWhile the slice_shift is faster than shift, you may pay for it later during\nalignment.\n\n"}, {"name": "pandas.DataFrame.sort_index", "path": "reference/api/pandas.dataframe.sort_index", "type": "DataFrame", "text": "\nSort object by labels (along an axis).\n\nReturns a new DataFrame sorted by label if inplace argument is `False`,\notherwise updates the original DataFrame and returns None.\n\nThe axis along which to sort. The value 0 identifies the rows, and 1\nidentifies the columns.\n\nIf not None, sort on values in specified index level(s).\n\nSort ascending vs. descending. When the index is a MultiIndex the sort\ndirection can be controlled for each level individually.\n\nIf True, perform operation in-place.\n\nChoice of sorting algorithm. See also `numpy.sort()` for more information.\nmergesort and stable are the only stable algorithms. For DataFrames, this\noption is only applied when sorting on a single column or label.\n\nPuts NaNs at the beginning if first; last puts NaNs at the end. Not\nimplemented for MultiIndex.\n\nIf True and sorting by level and index is multilevel, sort by other levels too\n(in order) after sorting by specified level.\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nNew in version 1.0.0.\n\nIf not None, apply the key function to the index values before sorting. This\nis similar to the key argument in the builtin `sorted()` function, with the\nnotable difference that this key function should be vectorized. It should\nexpect an `Index` and return an `Index` of the same shape. For MultiIndex\ninputs, the key is applied per level.\n\nNew in version 1.1.0.\n\nThe original DataFrame sorted by the labels or None if `inplace=True`.\n\nSee also\n\nSort Series by the index.\n\nSort DataFrame by the value.\n\nSort Series by the value.\n\nExamples\n\nBy default, it sorts in ascending order, to sort in descending order, use\n`ascending=False`\n\nA key function can be specified which is applied to the index before sorting.\nFor a `MultiIndex` this is applied to each level separately.\n\n"}, {"name": "pandas.DataFrame.sort_values", "path": "reference/api/pandas.dataframe.sort_values", "type": "DataFrame", "text": "\nSort by the values along either axis.\n\nName or list of names to sort by.\n\nif axis is 0 or \u2018index\u2019 then by may contain index levels and/or column labels.\n\nif axis is 1 or \u2018columns\u2019 then by may contain column levels and/or index\nlabels.\n\nAxis to be sorted.\n\nSort ascending vs. descending. Specify list for multiple sort orders. If this\nis a list of bools, must match the length of the by.\n\nIf True, perform operation in-place.\n\nChoice of sorting algorithm. See also `numpy.sort()` for more information.\nmergesort and stable are the only stable algorithms. For DataFrames, this\noption is only applied when sorting on a single column or label.\n\nPuts NaNs at the beginning if first; last puts NaNs at the end.\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nNew in version 1.0.0.\n\nApply the key function to the values before sorting. This is similar to the\nkey argument in the builtin `sorted()` function, with the notable difference\nthat this key function should be vectorized. It should expect a `Series` and\nreturn a Series with the same shape as the input. It will be applied to each\ncolumn in by independently.\n\nNew in version 1.1.0.\n\nDataFrame with sorted values or None if `inplace=True`.\n\nSee also\n\nSort a DataFrame by the index.\n\nSimilar method for a Series.\n\nExamples\n\nSort by col1\n\nSort by multiple columns\n\nSort Descending\n\nPutting NAs first\n\nSorting with a key function\n\nNatural sort with the key argument, using the natsort\n<https://github.com/SethMMorton/natsort> package.\n\n"}, {"name": "pandas.DataFrame.sparse", "path": "reference/api/pandas.dataframe.sparse", "type": "DataFrame", "text": "\nDataFrame accessor for sparse data.\n\nNew in version 0.25.0.\n\n"}, {"name": "pandas.DataFrame.sparse.density", "path": "reference/api/pandas.dataframe.sparse.density", "type": "DataFrame", "text": "\nRatio of non-sparse points to total (dense) data points.\n\n"}, {"name": "pandas.DataFrame.sparse.from_spmatrix", "path": "reference/api/pandas.dataframe.sparse.from_spmatrix", "type": "DataFrame", "text": "\nCreate a new DataFrame from a scipy sparse matrix.\n\nNew in version 0.25.0.\n\nMust be convertible to csc format.\n\nRow and column labels to use for the resulting DataFrame. Defaults to a\nRangeIndex.\n\nEach column of the DataFrame is stored as a `arrays.SparseArray`.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.sparse.to_coo", "path": "reference/api/pandas.dataframe.sparse.to_coo", "type": "DataFrame", "text": "\nReturn the contents of the frame as a sparse SciPy COO matrix.\n\nNew in version 0.25.0.\n\nIf the caller is heterogeneous and contains booleans or objects, the result\nwill be of dtype=object. See Notes.\n\nNotes\n\nThe dtype will be the lowest-common-denominator type (implicit upcasting);\nthat is to say if the dtypes (even of numeric types) are mixed, the one that\naccommodates all will be chosen.\n\ne.g. If the dtypes are float16 and float32, dtype will be upcast to float32.\nBy numpy.find_common_type convention, mixing int64 and and uint64 will result\nin a float64 dtype.\n\n"}, {"name": "pandas.DataFrame.sparse.to_dense", "path": "reference/api/pandas.dataframe.sparse.to_dense", "type": "DataFrame", "text": "\nConvert a DataFrame with sparse values to dense.\n\nNew in version 0.25.0.\n\nA DataFrame with the same values stored as dense arrays.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.squeeze", "path": "reference/api/pandas.dataframe.squeeze", "type": "DataFrame", "text": "\nSqueeze 1 dimensional axis objects into scalars.\n\nSeries or DataFrames with a single element are squeezed to a scalar.\nDataFrames with a single column or a single row are squeezed to a Series.\nOtherwise the object is unchanged.\n\nThis method is most useful when you don\u2019t know if your object is a Series or\nDataFrame, but you do know it has just a single column. In that case you can\nsafely call squeeze to ensure you have a Series.\n\nA specific axis to squeeze. By default, all length-1 axes are squeezed.\n\nThe projection after squeezing axis or all the axes.\n\nSee also\n\nInteger-location based indexing for selecting scalars.\n\nInteger-location based indexing for selecting Series.\n\nInverse of DataFrame.squeeze for a single-column DataFrame.\n\nExamples\n\nSlicing might produce a Series with a single value:\n\nSqueezing objects with more than one value in every axis does nothing:\n\nSqueezing is even more effective when used with DataFrames.\n\nSlicing a single column will produce a DataFrame with the columns having only\none value:\n\nSo the columns can be squeezed down, resulting in a Series:\n\nSlicing a single row from a single column will produce a single scalar\nDataFrame:\n\nSqueezing the rows produces a single scalar Series:\n\nSqueezing all axes will project directly into a scalar:\n\n"}, {"name": "pandas.DataFrame.stack", "path": "reference/api/pandas.dataframe.stack", "type": "DataFrame", "text": "\nStack the prescribed level(s) from columns to index.\n\nReturn a reshaped DataFrame or Series having a multi-level index with one or\nmore new inner-most levels compared to the current DataFrame. The new inner-\nmost levels are created by pivoting the columns of the current dataframe:\n\nif the columns have a single level, the output is a Series;\n\nif the columns have multiple levels, the new index level(s) is (are) taken\nfrom the prescribed level(s) and the output is a DataFrame.\n\nLevel(s) to stack from the column axis onto the index axis, defined as one\nindex or label, or a list of indices or labels.\n\nWhether to drop rows in the resulting Frame/Series with missing values.\nStacking a column level onto the index axis can create combinations of index\nand column values that are missing from the original dataframe. See Examples\nsection.\n\nStacked dataframe or series.\n\nSee also\n\nUnstack prescribed level(s) from index axis onto column axis.\n\nReshape dataframe from long format to wide format.\n\nCreate a spreadsheet-style pivot table as a DataFrame.\n\nNotes\n\nThe function is named by analogy with a collection of books being reorganized\nfrom being side by side on a horizontal position (the columns of the\ndataframe) to being stacked vertically on top of each other (in the index of\nthe dataframe).\n\nExamples\n\nSingle level columns\n\nStacking a dataframe with a single level column axis returns a Series:\n\nMulti level columns: simple case\n\nStacking a dataframe with a multi-level column axis:\n\nMissing values\n\nIt is common to have missing values when stacking a dataframe with multi-level\ncolumns, as the stacked dataframe typically has more values than the original\ndataframe. Missing values are filled with NaNs:\n\nPrescribing the level(s) to be stacked\n\nThe first parameter controls which level or levels are stacked:\n\nDropping missing values\n\nNote that rows where all values are missing are dropped by default but this\nbehaviour can be controlled via the dropna keyword parameter:\n\n"}, {"name": "pandas.DataFrame.std", "path": "reference/api/pandas.dataframe.std", "type": "DataFrame", "text": "\nReturn sample standard deviation over requested axis.\n\nNormalized by N-1 by default. This can be changed using the ddof argument.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nDelta Degrees of Freedom. The divisor used in calculations is N - ddof, where\nN represents the number of elements.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nNotes\n\nTo have the same behaviour as numpy.std, use ddof=0 (instead of the default\nddof=1)\n\nExamples\n\nThe standard deviation of the columns can be found as follows:\n\nAlternatively, ddof=0 can be set to normalize by N instead of N-1:\n\n"}, {"name": "pandas.DataFrame.style", "path": "reference/api/pandas.dataframe.style", "type": "Style", "text": "\nReturns a Styler object.\n\nContains methods for building a styled HTML representation of the DataFrame.\n\nSee also\n\nHelps style a DataFrame or Series according to the data with HTML and CSS.\n\n"}, {"name": "pandas.DataFrame.sub", "path": "reference/api/pandas.dataframe.sub", "type": "DataFrame", "text": "\nGet Subtraction of dataframe and other, element-wise (binary operator sub).\n\nEquivalent to `dataframe - other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, rsub.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.subtract", "path": "reference/api/pandas.dataframe.subtract", "type": "DataFrame", "text": "\nGet Subtraction of dataframe and other, element-wise (binary operator sub).\n\nEquivalent to `dataframe - other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, rsub.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.sum", "path": "reference/api/pandas.dataframe.sum", "type": "DataFrame", "text": "\nReturn the sum of the values over the requested axis.\n\nThis is equivalent to the method `numpy.sum`.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nAdditional keyword arguments to be passed to the function.\n\nSee also\n\nReturn the sum.\n\nReturn the minimum.\n\nReturn the maximum.\n\nReturn the index of the minimum.\n\nReturn the index of the maximum.\n\nReturn the sum over the requested axis.\n\nReturn the minimum over the requested axis.\n\nReturn the maximum over the requested axis.\n\nReturn the index of the minimum over the requested axis.\n\nReturn the index of the maximum over the requested axis.\n\nExamples\n\nBy default, the sum of an empty or all-NA Series is `0`.\n\nThis can be controlled with the `min_count` parameter. For example, if you\u2019d\nlike the sum of an empty series to be NaN, pass `min_count=1`.\n\nThanks to the `skipna` parameter, `min_count` handles all-NA and empty series\nidentically.\n\n"}, {"name": "pandas.DataFrame.swapaxes", "path": "reference/api/pandas.dataframe.swapaxes", "type": "DataFrame", "text": "\nInterchange axes and swap values axes appropriately.\n\n"}, {"name": "pandas.DataFrame.swaplevel", "path": "reference/api/pandas.dataframe.swaplevel", "type": "DataFrame", "text": "\nSwap levels i and j in a `MultiIndex`.\n\nDefault is to swap the two innermost levels of the index.\n\nLevels of the indices to be swapped. Can pass level name as string.\n\nThe axis to swap levels on. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for\ncolumn-wise.\n\nDataFrame with levels swapped in MultiIndex.\n\nExamples\n\nIn the following example, we will swap the levels of the indices. Here, we\nwill swap the levels column-wise, but levels can be swapped row-wise in a\nsimilar manner. Note that column-wise is the default behaviour. By not\nsupplying any arguments for i and j, we swap the last and second to last\nindices.\n\nBy supplying one argument, we can choose which index to swap the last index\nwith. We can for example swap the first index with the last one as follows.\n\nWe can also define explicitly which indices we want to swap by supplying\nvalues for both i and j. Here, we for example swap the first and second\nindices.\n\n"}, {"name": "pandas.DataFrame.T", "path": "reference/api/pandas.dataframe.t", "type": "DataFrame", "text": "\n\n"}, {"name": "pandas.DataFrame.tail", "path": "reference/api/pandas.dataframe.tail", "type": "DataFrame", "text": "\nReturn the last n rows.\n\nThis function returns last n rows from the object based on position. It is\nuseful for quickly verifying data, for example, after sorting or appending\nrows.\n\nFor negative values of n, this function returns all rows except the first n\nrows, equivalent to `df[n:]`.\n\nNumber of rows to select.\n\nThe last n rows of the caller object.\n\nSee also\n\nThe first n rows of the caller object.\n\nExamples\n\nViewing the last 5 lines\n\nViewing the last n lines (three in this case)\n\nFor negative values of n\n\n"}, {"name": "pandas.DataFrame.take", "path": "reference/api/pandas.dataframe.take", "type": "DataFrame", "text": "\nReturn the elements in the given positional indices along an axis.\n\nThis means that we are not indexing according to actual values in the index\nattribute of the object. We are indexing according to the actual position of\nthe element in the object.\n\nAn array of ints indicating which positions to take.\n\nThe axis on which to select elements. `0` means that we are selecting rows,\n`1` means that we are selecting columns.\n\nBefore pandas 1.0, `is_copy=False` can be specified to ensure that the return\nvalue is an actual copy. Starting with pandas 1.0, `take` always returns a\ncopy, and the keyword is therefore deprecated.\n\nDeprecated since version 1.0.0.\n\nFor compatibility with `numpy.take()`. Has no effect on the output.\n\nAn array-like containing the elements taken from the object.\n\nSee also\n\nSelect a subset of a DataFrame by labels.\n\nSelect a subset of a DataFrame by positions.\n\nTake elements from an array along an axis.\n\nExamples\n\nTake elements at positions 0 and 3 along the axis 0 (default).\n\nNote how the actual indices selected (0 and 1) do not correspond to our\nselected indices 0 and 3. That\u2019s because we are selecting the 0th and 3rd\nrows, not rows whose indices equal 0 and 3.\n\nTake elements at indices 1 and 2 along the axis 1 (column selection).\n\nWe may take elements using negative integers for positive indices, starting\nfrom the end of the object, just like with Python lists.\n\n"}, {"name": "pandas.DataFrame.to_clipboard", "path": "reference/api/pandas.dataframe.to_clipboard", "type": "DataFrame", "text": "\nCopy object to the system clipboard.\n\nWrite a text representation of object to the system clipboard. This can be\npasted into Excel, for example.\n\nProduce output in a csv format for easy pasting into excel.\n\nTrue, use the provided separator for csv pasting.\n\nFalse, write a string representation of the object to the clipboard.\n\nField delimiter.\n\nThese parameters will be passed to DataFrame.to_csv.\n\nSee also\n\nWrite a DataFrame to a comma-separated values (csv) file.\n\nRead text from clipboard and pass to read_csv.\n\nNotes\n\nRequirements for your platform.\n\nLinux : xclip, or xsel (with PyQt4 modules)\n\nWindows : none\n\nmacOS : none\n\nExamples\n\nCopy the contents of a DataFrame to the clipboard.\n\nWe can omit the index by passing the keyword index and setting it to false.\n\n"}, {"name": "pandas.DataFrame.to_csv", "path": "reference/api/pandas.dataframe.to_csv", "type": "DataFrame", "text": "\nWrite object to a comma-separated values (csv) file.\n\nString, path object (implementing os.PathLike[str]), or file-like object\nimplementing a write() function. If None, the result is returned as a string.\nIf a non-binary file object is passed, it should be opened with newline=\u2019\u2019,\ndisabling universal newlines. If a binary file object is passed, mode might\nneed to contain a \u2018b\u2019.\n\nChanged in version 1.2.0: Support for binary file objects was introduced.\n\nString of length 1. Field delimiter for the output file.\n\nMissing data representation.\n\nFormat string for floating point numbers.\n\nColumns to write.\n\nWrite out the column names. If a list of strings is given it is assumed to be\naliases for the column names.\n\nWrite row names (index).\n\nColumn label for index column(s) if desired. If None is given, and header and\nindex are True, then the index names are used. A sequence should be given if\nthe object uses MultiIndex. If False do not print fields for index names. Use\nindex_label=False for easier importing in R.\n\nPython write mode, default \u2018w\u2019.\n\nA string representing the encoding to use in the output file, defaults to\n\u2018utf-8\u2019. encoding is not supported if path_or_buf is a non-binary file object.\n\nFor on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018%s\u2019 path-like,\nthen detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019,\n\u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to `None` for no compression.\nCan also be a dict with key `'method'` set to one of {`'zip'`, `'gzip'`,\n`'bz2'`, `'zstd'`} and other key-value pairs are forwarded to\n`zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or\n`zstandard.ZstdDecompressor`, respectively. As an example, the following could\nbe passed for faster compression and to create a reproducible gzip archive:\n`compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}`.\n\nChanged in version 1.0.0: May now be a dict with key \u2018method\u2019 as compression\nmode and other entries as additional compression options if compression mode\nis \u2018zip\u2019.\n\nChanged in version 1.1.0: Passing compression options as keys in dict is\nsupported for compression modes \u2018gzip\u2019, \u2018bz2\u2019, \u2018zstd\u2019, and \u2018zip\u2019.\n\nChanged in version 1.2.0: Compression is supported for binary file objects.\n\nChanged in version 1.2.0: Previous versions forwarded dict entries for \u2018gzip\u2019\nto gzip.open instead of gzip.GzipFile which prevented setting mtime.\n\nDefaults to csv.QUOTE_MINIMAL. If you have set a float_format then floats are\nconverted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-\nnumeric.\n\nString of length 1. Character used to quote fields.\n\nThe newline character or character sequence to use in the output file.\nDefaults to os.linesep, which depends on the OS in which this method is called\n(\u2019\\n\u2019 for linux, \u2018\\r\\n\u2019 for Windows, i.e.).\n\nRows to write at a time.\n\nFormat string for datetime objects.\n\nControl quoting of quotechar inside a field.\n\nString of length 1. Character used to escape sep and quotechar when\nappropriate.\n\nCharacter recognized as decimal separator. E.g. use \u2018,\u2019 for European data.\n\nSpecifies how encoding and decoding errors are to be handled. See the errors\nargument for `open()` for a full list of options.\n\nNew in version 1.1.0.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nIf path_or_buf is None, returns the resulting csv format as a string.\nOtherwise returns None.\n\nSee also\n\nLoad a CSV file into a DataFrame.\n\nWrite DataFrame to an Excel file.\n\nExamples\n\nCreate \u2018out.zip\u2019 containing \u2018out.csv\u2019\n\nTo write a csv file to a new folder or nested folder you will first need to\ncreate it using either Pathlib or os:\n\n"}, {"name": "pandas.DataFrame.to_dict", "path": "reference/api/pandas.dataframe.to_dict", "type": "DataFrame", "text": "\nConvert the DataFrame to a dictionary.\n\nThe type of the key-value pairs can be customized with the parameters (see\nbelow).\n\nDetermines the type of the values of the dictionary.\n\n\u2018dict\u2019 (default) : dict like {column -> {index -> value}}\n\n\u2018list\u2019 : dict like {column -> [values]}\n\n\u2018series\u2019 : dict like {column -> Series(values)}\n\n\u2018split\u2019 : dict like {\u2018index\u2019 -> [index], \u2018columns\u2019 -> [columns], \u2018data\u2019 ->\n[values]}\n\n\u2018tight\u2019 : dict like {\u2018index\u2019 -> [index], \u2018columns\u2019 -> [columns], \u2018data\u2019 ->\n[values], \u2018index_names\u2019 -> [index.names], \u2018column_names\u2019 -> [column.names]}\n\n\u2018records\u2019 : list like [{column -> value}, \u2026 , {column -> value}]\n\n\u2018index\u2019 : dict like {index -> {column -> value}}\n\nAbbreviations are allowed. s indicates series and sp indicates split.\n\nNew in version 1.4.0: \u2018tight\u2019 as an allowed value for the `orient` argument\n\nThe collections.abc.Mapping subclass used for all Mappings in the return\nvalue. Can be the actual class or an empty instance of the mapping type you\nwant. If you want a collections.defaultdict, you must pass it initialized.\n\nReturn a collections.abc.Mapping object representing the DataFrame. The\nresulting transformation depends on the orient parameter.\n\nSee also\n\nCreate a DataFrame from a dictionary.\n\nConvert a DataFrame to JSON format.\n\nExamples\n\nYou can specify the return orientation.\n\nYou can also specify the mapping type.\n\nIf you want a defaultdict, you need to initialize it:\n\n"}, {"name": "pandas.DataFrame.to_excel", "path": "reference/api/pandas.dataframe.to_excel", "type": "DataFrame", "text": "\nWrite object to an Excel sheet.\n\nTo write a single object to an Excel .xlsx file it is only necessary to\nspecify a target file name. To write to multiple sheets it is necessary to\ncreate an ExcelWriter object with a target file name, and specify a sheet in\nthe file to write to.\n\nMultiple sheets may be written to by specifying unique sheet_name. With all\ndata written to the file it is necessary to save the changes. Note that\ncreating an ExcelWriter object with a file name that already exists will\nresult in the contents of the existing file being erased.\n\nFile path or existing ExcelWriter.\n\nName of sheet which will contain DataFrame.\n\nMissing data representation.\n\nFormat string for floating point numbers. For example `float_format=\"%.2f\"`\nwill format 0.1234 to 0.12.\n\nColumns to write.\n\nWrite out the column names. If a list of string is given it is assumed to be\naliases for the column names.\n\nWrite row names (index).\n\nColumn label for index column(s) if desired. If not specified, and header and\nindex are True, then the index names are used. A sequence should be given if\nthe DataFrame uses MultiIndex.\n\nUpper left cell row to dump data frame.\n\nUpper left cell column to dump data frame.\n\nWrite engine to use, \u2018openpyxl\u2019 or \u2018xlsxwriter\u2019. You can also set this via the\noptions `io.excel.xlsx.writer`, `io.excel.xls.writer`, and\n`io.excel.xlsm.writer`.\n\nDeprecated since version 1.2.0: As the xlwt package is no longer maintained,\nthe `xlwt` engine will be removed in a future version of pandas.\n\nWrite MultiIndex and Hierarchical Rows as merged cells.\n\nEncoding of the resulting excel file. Only necessary for xlwt, other writers\nsupport unicode natively.\n\nRepresentation for infinity (there is no native representation for infinity in\nExcel).\n\nDisplay more information in the error logs.\n\nSpecifies the one-based bottommost row and rightmost column that is to be\nfrozen.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nSee also\n\nWrite DataFrame to a comma-separated values (csv) file.\n\nClass for writing DataFrame objects into excel sheets.\n\nRead an Excel file into a pandas DataFrame.\n\nRead a comma-separated values (csv) file into DataFrame.\n\nNotes\n\nFor compatibility with `to_csv()`, to_excel serializes lists and dicts to\nstrings before writing.\n\nOnce a workbook has been saved it is not possible to write further data\nwithout rewriting the whole workbook.\n\nExamples\n\nCreate, write to and save a workbook:\n\nTo specify the sheet name:\n\nIf you wish to write to more than one sheet in the workbook, it is necessary\nto specify an ExcelWriter object:\n\nExcelWriter can also be used to append to an existing Excel file:\n\nTo set the library that is used to write the Excel file, you can pass the\nengine keyword (the default engine is automatically chosen depending on the\nfile extension):\n\n"}, {"name": "pandas.DataFrame.to_feather", "path": "reference/api/pandas.dataframe.to_feather", "type": "DataFrame", "text": "\nWrite a DataFrame to the binary Feather format.\n\nString, path object (implementing `os.PathLike[str]`), or file-like object\nimplementing a binary `write()` function. If a string or a path, it will be\nused as Root Directory path when writing a partitioned dataset.\n\nAdditional keywords passed to `pyarrow.feather.write_feather()`. Starting with\npyarrow 0.17, this includes the compression, compression_level, chunksize and\nversion keywords.\n\nNew in version 1.1.0.\n\nNotes\n\nThis function writes the dataframe as a feather file. Requires a default\nindex. For saving the DataFrame with your custom index use a method that\nsupports custom indices e.g. to_parquet.\n\n"}, {"name": "pandas.DataFrame.to_gbq", "path": "reference/api/pandas.dataframe.to_gbq", "type": "DataFrame", "text": "\nWrite a DataFrame to a Google BigQuery table.\n\nThis function requires the pandas-gbq package.\n\nSee the How to authenticate with Google BigQuery guide for authentication\ninstructions.\n\nName of table to be written, in the form `dataset.tablename`.\n\nGoogle BigQuery Account project ID. Optional when available from the\nenvironment.\n\nNumber of rows to be inserted in each chunk from the dataframe. Set to `None`\nto load the whole dataframe at once.\n\nForce Google BigQuery to re-authenticate the user. This is useful if multiple\naccounts are used.\n\nBehavior when the destination table exists. Value can be one of:\n\nIf table exists raise pandas_gbq.gbq.TableCreationError.\n\nIf table exists, drop it, recreate it, and insert data.\n\nIf table exists, insert data. Create if does not exist.\n\nUse the local webserver flow instead of the console flow when getting user\ncredentials.\n\nNew in version 0.2.0 of pandas-gbq.\n\nList of BigQuery table fields to which according DataFrame columns conform to,\ne.g. `[{'name': 'col1', 'type': 'STRING'},...]`. If schema is not provided, it\nwill be generated according to dtypes of DataFrame columns. See BigQuery API\ndocumentation on available names of a field.\n\nNew in version 0.3.1 of pandas-gbq.\n\nLocation where the load job should run. See the BigQuery locations\ndocumentation for a list of available locations. The location must match that\nof the target dataset.\n\nNew in version 0.5.0 of pandas-gbq.\n\nUse the library tqdm to show the progress bar for the upload, chunk by chunk.\n\nNew in version 0.5.0 of pandas-gbq.\n\nCredentials for accessing Google APIs. Use this parameter to override default\ncredentials, such as to use Compute Engine\n`google.auth.compute_engine.Credentials` or Service Account\n`google.oauth2.service_account.Credentials` directly.\n\nNew in version 0.8.0 of pandas-gbq.\n\nSee also\n\nThis function in the pandas-gbq library.\n\nRead a DataFrame from Google BigQuery.\n\n"}, {"name": "pandas.DataFrame.to_hdf", "path": "reference/api/pandas.dataframe.to_hdf", "type": "DataFrame", "text": "\nWrite the contained data to an HDF5 file using HDFStore.\n\nHierarchical Data Format (HDF) is self-describing, allowing an application to\ninterpret the structure and contents of a file with no outside information.\nOne HDF file can hold a mix of related objects which can be accessed as a\ngroup or as individual objects.\n\nIn order to add another DataFrame or Series to an existing HDF file please use\nappend mode and a different a key.\n\nWarning\n\nOne can store a subclass of `DataFrame` or `Series` to HDF5, but the type of\nthe subclass is lost upon storing.\n\nFor more information see the user guide.\n\nFile path or HDFStore object.\n\nIdentifier for the group in the store.\n\nMode to open file:\n\n\u2018w\u2019: write, a new file is created (an existing file with the same name would\nbe deleted).\n\n\u2018a\u2019: append, an existing file is opened for reading and writing, and if the\nfile does not exist it is created.\n\n\u2018r+\u2019: similar to \u2018a\u2019, but the file must already exist.\n\nSpecifies a compression level for data. A value of 0 or None disables\ncompression.\n\nSpecifies the compression library to be used. As of v0.20.2 these additional\ncompressors for Blosc are supported (default if no compressor specified:\n\u2018blosc:blosclz\u2019): {\u2018blosc:blosclz\u2019, \u2018blosc:lz4\u2019, \u2018blosc:lz4hc\u2019,\n\u2018blosc:snappy\u2019, \u2018blosc:zlib\u2019, \u2018blosc:zstd\u2019}. Specifying a compression library\nwhich is not available issues a ValueError.\n\nFor Table formats, append the input data to the existing.\n\nPossible values:\n\n\u2018fixed\u2019: Fixed format. Fast writing/reading. Not-appendable, nor searchable.\n\n\u2018table\u2019: Table format. Write as a PyTables Table structure which may perform\nworse but allow more flexible operations like searching / selecting subsets of\nthe data.\n\nIf None, pd.get_option(\u2018io.hdf.default_format\u2019) is checked, followed by\nfallback to \u201cfixed\u201d.\n\nSpecifies how encoding and decoding errors are to be handled. See the errors\nargument for `open()` for a full list of options.\n\nMap column names to minimum string sizes for columns.\n\nHow to represent null values as str. Not allowed with append=True.\n\nList of columns to create as indexed data columns for on-disk queries, or True\nto use all columns. By default only the axes of the object are indexed. See\nQuery via data columns. Applicable only to format=\u2019table\u2019.\n\nSee also\n\nRead from HDF file.\n\nWrite a DataFrame to the binary parquet format.\n\nWrite to a SQL table.\n\nWrite out feather-format for DataFrames.\n\nWrite out to a csv file.\n\nExamples\n\nWe can add another object to the same file:\n\nReading from HDF file:\n\n"}, {"name": "pandas.DataFrame.to_html", "path": "reference/api/pandas.dataframe.to_html", "type": "DataFrame", "text": "\nRender a DataFrame as an HTML table.\n\nBuffer to write to. If None, the output is returned as a string.\n\nThe subset of columns to write. Writes all columns by default.\n\nThe minimum width of each column in CSS length units. An int is assumed to be\npx units.\n\nNew in version 0.25.0: Ability to use str.\n\nWhether to print column labels, default True.\n\nWhether to print index (row) labels.\n\nString representation of `NaN` to use.\n\nFormatter functions to apply to columns\u2019 elements by position or name. The\nresult of each function must be a unicode string. List/tuple must be of length\nequal to the number of columns.\n\nFormatter function to apply to columns\u2019 elements if they are floats. This\nfunction must return a unicode string and will be applied only to the\nnon-`NaN` elements, with `NaN` being handled by `na_rep`.\n\nChanged in version 1.2.0.\n\nSet to False for a DataFrame with a hierarchical index to print every\nmultiindex key at each row.\n\nPrints the names of the indexes.\n\nHow to justify the column labels. If None uses the option from the print\nconfiguration (controlled by set_option), \u2018right\u2019 out of the box. Valid values\nare\n\nleft\n\nright\n\ncenter\n\njustify\n\njustify-all\n\nstart\n\nend\n\ninherit\n\nmatch-parent\n\ninitial\n\nunset.\n\nMaximum number of rows to display in the console.\n\nMaximum number of columns to display in the console.\n\nDisplay DataFrame dimensions (number of rows by number of columns).\n\nCharacter recognized as decimal separator, e.g. \u2018,\u2019 in Europe.\n\nMake the row labels bold in the output.\n\nCSS class(es) to apply to the resulting html table.\n\nConvert the characters <, >, and & to HTML-safe sequences.\n\nWhether the generated HTML is for IPython Notebook.\n\nA `border=border` attribute is included in the opening <table> tag. Default\n`pd.options.display.html.border`.\n\nA css id is included in the opening <table> tag if specified.\n\nConvert URLs to HTML links.\n\nSet character encoding.\n\nNew in version 1.0.\n\nIf buf is None, returns the result as a string. Otherwise returns None.\n\nSee also\n\nConvert DataFrame to a string.\n\n"}, {"name": "pandas.DataFrame.to_json", "path": "reference/api/pandas.dataframe.to_json", "type": "DataFrame", "text": "\nConvert the object to a JSON string.\n\nNote NaN\u2019s and None will be converted to null and datetime objects will be\nconverted to UNIX timestamps.\n\nString, path object (implementing os.PathLike[str]), or file-like object\nimplementing a write() function. If None, the result is returned as a string.\n\nIndication of expected JSON string format.\n\nSeries:\n\ndefault is \u2018index\u2019\n\nallowed values are: {\u2018split\u2019, \u2018records\u2019, \u2018index\u2019, \u2018table\u2019}.\n\nDataFrame:\n\ndefault is \u2018columns\u2019\n\nallowed values are: {\u2018split\u2019, \u2018records\u2019, \u2018index\u2019, \u2018columns\u2019, \u2018values\u2019,\n\u2018table\u2019}.\n\nThe format of the JSON string:\n\n\u2018split\u2019 : dict like {\u2018index\u2019 -> [index], \u2018columns\u2019 -> [columns], \u2018data\u2019 ->\n[values]}\n\n\u2018records\u2019 : list like [{column -> value}, \u2026 , {column -> value}]\n\n\u2018index\u2019 : dict like {index -> {column -> value}}\n\n\u2018columns\u2019 : dict like {column -> {index -> value}}\n\n\u2018values\u2019 : just the values array\n\n\u2018table\u2019 : dict like {\u2018schema\u2019: {schema}, \u2018data\u2019: {data}}\n\nDescribing the data, where data component is like `orient='records'`.\n\nType of date conversion. \u2018epoch\u2019 = epoch milliseconds, \u2018iso\u2019 = ISO8601. The\ndefault depends on the orient. For `orient='table'`, the default is \u2018iso\u2019. For\nall other orients, the default is \u2018epoch\u2019.\n\nThe number of decimal places to use when encoding floating point values.\n\nForce encoded string to be ASCII.\n\nThe time unit to encode to, governs timestamp and ISO8601 precision. One of\n\u2018s\u2019, \u2018ms\u2019, \u2018us\u2019, \u2018ns\u2019 for second, millisecond, microsecond, and nanosecond\nrespectively.\n\nHandler to call if object cannot otherwise be converted to a suitable format\nfor JSON. Should receive a single argument which is the object to convert and\nreturn a serialisable object.\n\nIf \u2018orient\u2019 is \u2018records\u2019 write out line-delimited json format. Will throw\nValueError if incorrect \u2018orient\u2019 since others are not list-like.\n\nFor on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018path_or_buf\u2019\npath-like, then detect compression from the following extensions: \u2018.gz\u2019,\n\u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to `None` for\nno compression. Can also be a dict with key `'method'` set to one of {`'zip'`,\n`'gzip'`, `'bz2'`, `'zstd'`} and other key-value pairs are forwarded to\n`zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or\n`zstandard.ZstdDecompressor`, respectively. As an example, the following could\nbe passed for faster compression and to create a reproducible gzip archive:\n`compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}`.\n\nChanged in version 1.4.0: Zstandard support.\n\nWhether to include the index values in the JSON string. Not including the\nindex (`index=False`) is only supported when orient is \u2018split\u2019 or \u2018table\u2019.\n\nLength of whitespace used to indent each record.\n\nNew in version 1.0.0.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nIf path_or_buf is None, returns the resulting json format as a string.\nOtherwise returns None.\n\nSee also\n\nConvert a JSON string to pandas object.\n\nNotes\n\nThe behavior of `indent=0` varies from the stdlib, which does not indent the\noutput but does insert newlines. Currently, `indent=0` and the default\n`indent=None` are equivalent in pandas, though this may change in a future\nrelease.\n\n`orient='table'` contains a \u2018pandas_version\u2019 field under \u2018schema\u2019. This stores\nthe version of pandas used in the latest revision of the schema.\n\nExamples\n\nEncoding/decoding a Dataframe using `'records'` formatted JSON. Note that\nindex labels are not preserved with this encoding.\n\nEncoding/decoding a Dataframe using `'index'` formatted JSON:\n\nEncoding/decoding a Dataframe using `'columns'` formatted JSON:\n\nEncoding/decoding a Dataframe using `'values'` formatted JSON:\n\nEncoding with Table Schema:\n\n"}, {"name": "pandas.DataFrame.to_latex", "path": "reference/api/pandas.dataframe.to_latex", "type": "DataFrame", "text": "\nRender object to a LaTeX tabular, longtable, or nested table.\n\nRequires `\\usepackage{booktabs}`. The output can be copy/pasted into a main\nLaTeX document or read from an external file with `\\input{table.tex}`.\n\nChanged in version 1.0.0: Added caption and label arguments.\n\nChanged in version 1.2.0: Added position argument, changed meaning of caption\nargument.\n\nBuffer to write to. If None, the output is returned as a string.\n\nThe subset of columns to write. Writes all columns by default.\n\nThe minimum width of each column.\n\nWrite out the column names. If a list of strings is given, it is assumed to be\naliases for the column names.\n\nWrite row names (index).\n\nMissing data representation.\n\nFormatter functions to apply to columns\u2019 elements by position or name. The\nresult of each function must be a unicode string. List must be of length equal\nto the number of columns.\n\nFormatter for floating point numbers. For example `float_format=\"%.2f\"` and\n`float_format=\"{:0.2f}\".format` will both result in 0.1234 being formatted as\n0.12.\n\nSet to False for a DataFrame with a hierarchical index to print every\nmultiindex key at each row. By default, the value will be read from the config\nmodule.\n\nPrints the names of the indexes.\n\nMake the row labels bold in the output.\n\nThe columns format as specified in LaTeX table format e.g. \u2018rcl\u2019 for 3\ncolumns. By default, \u2018l\u2019 will be used for all columns except columns of\nnumbers, which default to \u2018r\u2019.\n\nBy default, the value will be read from the pandas config module. Use a\nlongtable environment instead of tabular. Requires adding a\nusepackage{longtable} to your LaTeX preamble.\n\nBy default, the value will be read from the pandas config module. When set to\nFalse prevents from escaping latex special characters in column names.\n\nA string representing the encoding to use in the output file, defaults to\n\u2018utf-8\u2019.\n\nCharacter recognized as decimal separator, e.g. \u2018,\u2019 in Europe.\n\nUse multicolumn to enhance MultiIndex columns. The default will be read from\nthe config module.\n\nThe alignment for multicolumns, similar to column_format The default will be\nread from the config module.\n\nUse multirow to enhance MultiIndex rows. Requires adding a\nusepackage{multirow} to your LaTeX preamble. Will print centered labels\n(instead of top-aligned) across the contained rows, separating groups via\nclines. The default will be read from the pandas config module.\n\nTuple (full_caption, short_caption), which results in\n`\\caption[short_caption]{full_caption}`; if a single string is passed, no\nshort caption will be set.\n\nNew in version 1.0.0.\n\nChanged in version 1.2.0: Optionally allow caption to be a tuple\n`(full_caption, short_caption)`.\n\nThe LaTeX label to be placed inside `\\label{}` in the output. This is used\nwith `\\ref{}` in the main `.tex` file.\n\nNew in version 1.0.0.\n\nThe LaTeX positional argument for tables, to be placed after `\\begin{}` in the\noutput.\n\nNew in version 1.2.0.\n\nIf buf is None, returns the result as a string. Otherwise returns None.\n\nSee also\n\nRender a DataFrame to LaTeX with conditional formatting.\n\nRender a DataFrame to a console-friendly tabular output.\n\nRender a DataFrame as an HTML table.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.to_markdown", "path": "reference/api/pandas.dataframe.to_markdown", "type": "DataFrame", "text": "\nPrint DataFrame in Markdown-friendly format.\n\nNew in version 1.0.0.\n\nBuffer to write to. If None, the output is returned as a string.\n\nMode in which file is opened, \u201cwt\u201d by default.\n\nAdd index (row) labels.\n\nNew in version 1.1.0.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nThese parameters will be passed to tabulate.\n\nDataFrame in Markdown-friendly format.\n\nNotes\n\nRequires the tabulate package.\n\nExamples\n\nOutput markdown with a tabulate option.\n\n"}, {"name": "pandas.DataFrame.to_numpy", "path": "reference/api/pandas.dataframe.to_numpy", "type": "DataFrame", "text": "\nConvert the DataFrame to a NumPy array.\n\nBy default, the dtype of the returned array will be the common NumPy dtype of\nall types in the DataFrame. For example, if the dtypes are `float16` and\n`float32`, the results dtype will be `float32`. This may require copying data\nand coercing values, which may be expensive.\n\nThe dtype to pass to `numpy.asarray()`.\n\nWhether to ensure that the returned value is not a view on another array. Note\nthat `copy=False` does not ensure that `to_numpy()` is no-copy. Rather,\n`copy=True` ensure that a copy is made, even if not strictly necessary.\n\nThe value to use for missing values. The default value depends on dtype and\nthe dtypes of the DataFrame columns.\n\nNew in version 1.1.0.\n\nSee also\n\nSimilar method for Series.\n\nExamples\n\nWith heterogeneous data, the lowest common type will have to be used.\n\nFor a mix of numeric and non-numeric types, the output array will have object\ndtype.\n\n"}, {"name": "pandas.DataFrame.to_parquet", "path": "reference/api/pandas.dataframe.to_parquet", "type": "DataFrame", "text": "\nWrite a DataFrame to the binary parquet format.\n\nThis function writes the dataframe as a parquet file. You can choose different\nparquet backends, and have the option of compression. See the user guide for\nmore details.\n\nString, path object (implementing `os.PathLike[str]`), or file-like object\nimplementing a binary `write()` function. If None, the result is returned as\nbytes. If a string or path, it will be used as Root Directory path when\nwriting a partitioned dataset.\n\nChanged in version 1.2.0.\n\nPreviously this was \u201cfname\u201d\n\nParquet library to use. If \u2018auto\u2019, then the option `io.parquet.engine` is\nused. The default `io.parquet.engine` behavior is to try \u2018pyarrow\u2019, falling\nback to \u2018fastparquet\u2019 if \u2018pyarrow\u2019 is unavailable.\n\nName of the compression to use. Use `None` for no compression.\n\nIf `True`, include the dataframe\u2019s index(es) in the file output. If `False`,\nthey will not be written to the file. If `None`, similar to `True` the\ndataframe\u2019s index(es) will be saved. However, instead of being saved as\nvalues, the RangeIndex will be stored as a range in the metadata so it doesn\u2019t\nrequire much space and is faster. Other indexes will be included as columns in\nthe file output.\n\nColumn names by which to partition the dataset. Columns are partitioned in the\norder they are given. Must be None if path is not a string.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nAdditional arguments passed to the parquet library. See pandas io for more\ndetails.\n\nSee also\n\nRead a parquet file.\n\nWrite a csv file.\n\nWrite to a sql table.\n\nWrite to hdf.\n\nNotes\n\nThis function requires either the fastparquet or pyarrow library.\n\nExamples\n\nIf you want to get a buffer to the parquet content you can use a io.BytesIO\nobject, as long as you don\u2019t use partition_cols, which creates multiple files.\n\n"}, {"name": "pandas.DataFrame.to_period", "path": "reference/api/pandas.dataframe.to_period", "type": "Input/output", "text": "\nConvert DataFrame from DatetimeIndex to PeriodIndex.\n\nConvert DataFrame from DatetimeIndex to PeriodIndex with desired frequency\n(inferred from index if not passed).\n\nFrequency of the PeriodIndex.\n\nThe axis to convert (the index by default).\n\nIf False then underlying input data is not copied.\n\nExamples\n\nFor the yearly frequency\n\n"}, {"name": "pandas.DataFrame.to_pickle", "path": "reference/api/pandas.dataframe.to_pickle", "type": "DataFrame", "text": "\nPickle (serialize) object to file.\n\nFile path where the pickled object will be stored.\n\nFor on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018path\u2019 path-\nlike, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019,\n\u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to `None` for no\ncompression. Can also be a dict with key `'method'` set to one of {`'zip'`,\n`'gzip'`, `'bz2'`, `'zstd'`} and other key-value pairs are forwarded to\n`zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or\n`zstandard.ZstdDecompressor`, respectively. As an example, the following could\nbe passed for faster compression and to create a reproducible gzip archive:\n`compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}`.\n\nInt which indicates which protocol should be used by the pickler, default\nHIGHEST_PROTOCOL (see [1] paragraph 12.1.2). The possible values are 0, 1, 2,\n3, 4, 5. A negative value for the protocol parameter is equivalent to setting\nits value to HIGHEST_PROTOCOL.\n\nhttps://docs.python.org/3/library/pickle.html.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nSee also\n\nLoad pickled pandas object (or any object) from file.\n\nWrite DataFrame to an HDF5 file.\n\nWrite DataFrame to a SQL database.\n\nWrite a DataFrame to the binary parquet format.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.to_records", "path": "reference/api/pandas.dataframe.to_records", "type": "DataFrame", "text": "\nConvert DataFrame to a NumPy record array.\n\nIndex will be included as the first field of the record array if requested.\n\nInclude index in resulting record array, stored in \u2018index\u2019 field or using the\nindex label, if set.\n\nIf a string or type, the data type to store all columns. If a dictionary, a\nmapping of column names and indices (zero-indexed) to specific data types.\n\nIf a string or type, the data type to store all index levels. If a dictionary,\na mapping of index level names and indices (zero-indexed) to specific data\ntypes.\n\nThis mapping is applied only if index=True.\n\nNumPy ndarray with the DataFrame labels as fields and each row of the\nDataFrame as entries.\n\nSee also\n\nConvert structured or record ndarray to DataFrame.\n\nAn ndarray that allows field access using attributes, analogous to typed\ncolumns in a spreadsheet.\n\nExamples\n\nIf the DataFrame index has no label then the recarray field name is set to\n\u2018index\u2019. If the index has a label then this is used as the field name:\n\nThe index can be excluded from the record array:\n\nData types can be specified for the columns:\n\nAs well as for the index:\n\n"}, {"name": "pandas.DataFrame.to_sql", "path": "reference/api/pandas.dataframe.to_sql", "type": "DataFrame", "text": "\nWrite records stored in a DataFrame to a SQL database.\n\nDatabases supported by SQLAlchemy [1] are supported. Tables can be newly\ncreated, appended to, or overwritten.\n\nName of SQL table.\n\nUsing SQLAlchemy makes it possible to use any DB supported by that library.\nLegacy support is provided for sqlite3.Connection objects. The user is\nresponsible for engine disposal and connection closure for the SQLAlchemy\nconnectable See here.\n\nSpecify the schema (if database flavor supports this). If None, use default\nschema.\n\nHow to behave if the table already exists.\n\nfail: Raise a ValueError.\n\nreplace: Drop the table before inserting new values.\n\nappend: Insert new values to the existing table.\n\nWrite DataFrame index as a column. Uses index_label as the column name in the\ntable.\n\nColumn label for index column(s). If None is given (default) and index is\nTrue, then the index names are used. A sequence should be given if the\nDataFrame uses MultiIndex.\n\nSpecify the number of rows in each batch to be written at a time. By default,\nall rows will be written at once.\n\nSpecifying the datatype for columns. If a dictionary is used, the keys should\nbe the column names and the values should be the SQLAlchemy types or strings\nfor the sqlite3 legacy mode. If a scalar is provided, it will be applied to\nall columns.\n\nControls the SQL insertion clause used:\n\nNone : Uses standard SQL `INSERT` clause (one per row).\n\n\u2018multi\u2019: Pass multiple values in a single `INSERT` clause.\n\ncallable with signature `(pd_table, conn, keys, data_iter)`.\n\nDetails and a sample callable implementation can be found in the section\ninsert method.\n\nNumber of rows affected by to_sql. None is returned if the callable passed\ninto `method` does not return the number of rows.\n\nThe number of returned rows affected is the sum of the `rowcount` attribute of\n`sqlite3.Cursor` or SQLAlchemy connectable which may not reflect the exact\nnumber of written rows as stipulated in the sqlite3 or SQLAlchemy.\n\nNew in version 1.4.0.\n\nWhen the table already exists and if_exists is \u2018fail\u2019 (the default).\n\nSee also\n\nRead a DataFrame from a table.\n\nNotes\n\nTimezone aware datetime columns will be written as `Timestamp with timezone`\ntype with SQLAlchemy if supported by the database. Otherwise, the datetimes\nwill be stored as timezone unaware timestamps local to the original timezone.\n\nReferences\n\nhttps://docs.sqlalchemy.org\n\nhttps://www.python.org/dev/peps/pep-0249/\n\nExamples\n\nCreate an in-memory SQLite database.\n\nCreate a table from scratch with 3 rows.\n\nAn sqlalchemy.engine.Connection can also be passed to con:\n\nThis is allowed to support operations that require that the same DBAPI\nconnection is used for the entire operation.\n\nOverwrite the table with just `df2`.\n\nSpecify the dtype (especially useful for integers with missing values). Notice\nthat while pandas is forced to store the data as floating point, the database\nsupports nullable integers. When fetching the data with Python, we get back\ninteger scalars.\n\n"}, {"name": "pandas.DataFrame.to_stata", "path": "reference/api/pandas.dataframe.to_stata", "type": "DataFrame", "text": "\nExport DataFrame object to Stata dta format.\n\nWrites the DataFrame to a Stata dataset file. \u201cdta\u201d files contain a Stata\ndataset.\n\nString, path object (implementing `os.PathLike[str]`), or file-like object\nimplementing a binary `write()` function.\n\nChanged in version 1.0.0.\n\nPreviously this was \u201cfname\u201d\n\nDictionary mapping columns containing datetime types to stata internal format\nto use when writing the dates. Options are \u2018tc\u2019, \u2018td\u2019, \u2018tm\u2019, \u2018tw\u2019, \u2018th\u2019, \u2018tq\u2019,\n\u2018ty\u2019. Column can be either an integer or a name. Datetime columns that do not\nhave a conversion type specified will be converted to \u2018tc\u2019. Raises\nNotImplementedError if a datetime column has timezone information.\n\nWrite the index to Stata dataset.\n\nCan be \u201c>\u201d, \u201c<\u201d, \u201clittle\u201d, or \u201cbig\u201d. default is sys.byteorder.\n\nA datetime to use as file creation date. Default is the current time.\n\nA label for the data set. Must be 80 characters or smaller.\n\nDictionary containing columns as keys and variable labels as values. Each\nlabel must be 80 characters or smaller.\n\nVersion to use in the output dta file. Set to None to let pandas decide\nbetween 118 or 119 formats depending on the number of columns in the frame.\nVersion 114 can be read by Stata 10 and later. Version 117 can be read by\nStata 13 or later. Version 118 is supported in Stata 14 and later. Version 119\nis supported in Stata 15 and later. Version 114 limits string variables to 244\ncharacters or fewer while versions 117 and later allow strings with lengths up\nto 2,000,000 characters. Versions 118 and 119 support Unicode characters, and\nversion 119 supports more than 32,767 variables.\n\nVersion 119 should usually only be used when the number of variables exceeds\nthe capacity of dta format 118. Exporting smaller datasets in format 119 may\nhave unintended consequences, and, as of November 2020, Stata SE cannot read\nversion 119 files.\n\nChanged in version 1.0.0: Added support for formats 118 and 119.\n\nList of column names to convert to string columns to Stata StrL format. Only\navailable if version is 117. Storing strings in the StrL format can produce\nsmaller dta files if strings have more than 8 characters and values are\nrepeated.\n\nFor on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018path\u2019 path-\nlike, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019,\n\u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to `None` for no\ncompression. Can also be a dict with key `'method'` set to one of {`'zip'`,\n`'gzip'`, `'bz2'`, `'zstd'`} and other key-value pairs are forwarded to\n`zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or\n`zstandard.ZstdDecompressor`, respectively. As an example, the following could\nbe passed for faster compression and to create a reproducible gzip archive:\n`compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}`.\n\nNew in version 1.1.0.\n\nChanged in version 1.4.0: Zstandard support.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nDictionary containing columns as keys and dictionaries of column value to\nlabels as values. Labels for a single variable must be 32,000 characters or\nsmaller.\n\nNew in version 1.4.0.\n\nIf datetimes contain timezone information\n\nColumn dtype is not representable in Stata\n\nColumns listed in convert_dates are neither datetime64[ns] or\ndatetime.datetime\n\nColumn listed in convert_dates is not in DataFrame\n\nCategorical label contains more than 32,000 characters\n\nSee also\n\nImport Stata data files.\n\nLow-level writer for Stata data files.\n\nLow-level writer for version 117 files.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.to_string", "path": "reference/api/pandas.dataframe.to_string", "type": "DataFrame", "text": "\nRender a DataFrame to a console-friendly tabular output.\n\nBuffer to write to. If None, the output is returned as a string.\n\nThe subset of columns to write. Writes all columns by default.\n\nThe minimum width of each column. If a list of ints is given every integers\ncorresponds with one column. If a dict is given, the key references the\ncolumn, while the value defines the space to use..\n\nWrite out the column names. If a list of strings is given, it is assumed to be\naliases for the column names.\n\nWhether to print index (row) labels.\n\nString representation of `NaN` to use.\n\nFormatter functions to apply to columns\u2019 elements by position or name. The\nresult of each function must be a unicode string. List/tuple must be of length\nequal to the number of columns.\n\nFormatter function to apply to columns\u2019 elements if they are floats. This\nfunction must return a unicode string and will be applied only to the\nnon-`NaN` elements, with `NaN` being handled by `na_rep`.\n\nChanged in version 1.2.0.\n\nSet to False for a DataFrame with a hierarchical index to print every\nmultiindex key at each row.\n\nPrints the names of the indexes.\n\nHow to justify the column labels. If None uses the option from the print\nconfiguration (controlled by set_option), \u2018right\u2019 out of the box. Valid values\nare\n\nleft\n\nright\n\ncenter\n\njustify\n\njustify-all\n\nstart\n\nend\n\ninherit\n\nmatch-parent\n\ninitial\n\nunset.\n\nMaximum number of rows to display in the console.\n\nMaximum number of columns to display in the console.\n\nDisplay DataFrame dimensions (number of rows by number of columns).\n\nCharacter recognized as decimal separator, e.g. \u2018,\u2019 in Europe.\n\nWidth to wrap a line in characters.\n\nThe number of rows to display in the console in a truncated repr (when number\nof rows is above max_rows).\n\nMax width to truncate each column in characters. By default, no limit.\n\nNew in version 1.0.0.\n\nSet character encoding.\n\nNew in version 1.0.\n\nIf buf is None, returns the result as a string. Otherwise returns None.\n\nSee also\n\nConvert DataFrame to HTML.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.to_timestamp", "path": "reference/api/pandas.dataframe.to_timestamp", "type": "DataFrame", "text": "\nCast to DatetimeIndex of timestamps, at beginning of period.\n\nDesired frequency.\n\nConvention for converting period to timestamp; start of period vs. end.\n\nThe axis to convert (the index by default).\n\nIf False then underlying input data is not copied.\n\n"}, {"name": "pandas.DataFrame.to_xarray", "path": "reference/api/pandas.dataframe.to_xarray", "type": "DataFrame", "text": "\nReturn an xarray object from the pandas object.\n\nData in the pandas structure converted to Dataset if the object is a\nDataFrame, or a DataArray if the object is a Series.\n\nSee also\n\nWrite DataFrame to an HDF5 file.\n\nWrite a DataFrame to the binary parquet format.\n\nNotes\n\nSee the xarray docs\n\nExamples\n\n"}, {"name": "pandas.DataFrame.to_xml", "path": "reference/api/pandas.dataframe.to_xml", "type": "DataFrame", "text": "\nRender a DataFrame to an XML document.\n\nNew in version 1.3.0.\n\nString, path object (implementing `os.PathLike[str]`), or file-like object\nimplementing a `write()` function. If None, the result is returned as a\nstring.\n\nWhether to include index in XML document.\n\nThe name of root element in XML document.\n\nThe name of row element in XML document.\n\nMissing data representation.\n\nList of columns to write as attributes in row element. Hierarchical columns\nwill be flattened with underscore delimiting the different levels.\n\nList of columns to write as children in row element. By default, all columns\noutput as children of row element. Hierarchical columns will be flattened with\nunderscore delimiting the different levels.\n\nAll namespaces to be defined in root element. Keys of dict should be prefix\nnames and values of dict corresponding URIs. Default namespaces should be\ngiven empty string key. For example,\n\nNamespace prefix to be used for every element and/or attribute in document.\nThis should be one of the keys in `namespaces` dict.\n\nEncoding of the resulting document.\n\nWhether to include the XML declaration at start of document.\n\nWhether output should be pretty printed with indentation and line breaks.\n\nParser module to use for building of tree. Only \u2018lxml\u2019 and \u2018etree\u2019 are\nsupported. With \u2018lxml\u2019, the ability to use XSLT stylesheet is supported.\n\nA URL, file-like object, or a raw string containing an XSLT script used to\ntransform the raw XML output. Script should use layout of elements and\nattributes from original output. This argument requires `lxml` to be\ninstalled. Only XSLT 1.0 scripts and not later versions is currently\nsupported.\n\nFor on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018path_or_buffer\u2019\npath-like, then detect compression from the following extensions: \u2018.gz\u2019,\n\u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to `None` for\nno compression. Can also be a dict with key `'method'` set to one of {`'zip'`,\n`'gzip'`, `'bz2'`, `'zstd'`} and other key-value pairs are forwarded to\n`zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or\n`zstandard.ZstdDecompressor`, respectively. As an example, the following could\nbe passed for faster compression and to create a reproducible gzip archive:\n`compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}`.\n\nChanged in version 1.4.0: Zstandard support.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nIf `io` is None, returns the resulting XML format as a string. Otherwise\nreturns None.\n\nSee also\n\nConvert the pandas object to a JSON string.\n\nConvert DataFrame to a html.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.transform", "path": "reference/api/pandas.dataframe.transform", "type": "DataFrame", "text": "\nCall `func` on self producing a DataFrame with the same axis shape as self.\n\nFunction to use for transforming the data. If a function, must either work\nwhen passed a DataFrame or when passed to DataFrame.apply. If func is both\nlist-like and dict-like, dict-like behavior takes precedence.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist-like of functions and/or function names, e.g. `[np.exp, 'sqrt']`\n\ndict-like of axis labels -> functions, function names or list-like of such.\n\nIf 0 or \u2018index\u2019: apply function to each column. If 1 or \u2018columns\u2019: apply\nfunction to each row.\n\nPositional arguments to pass to func.\n\nKeyword arguments to pass to func.\n\nA DataFrame that must have the same length as self.\n\nSee also\n\nOnly perform aggregating type operations.\n\nInvoke function on a DataFrame.\n\nNotes\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nExamples\n\nEven though the resulting DataFrame must have the same length as the input\nDataFrame, it is possible to provide several input functions:\n\nYou can call transform on a GroupBy object:\n\n"}, {"name": "pandas.DataFrame.transpose", "path": "reference/api/pandas.dataframe.transpose", "type": "DataFrame", "text": "\nTranspose index and columns.\n\nReflect the DataFrame over its main diagonal by writing rows as columns and\nvice-versa. The property `T` is an accessor to the method `transpose()`.\n\nAccepted for compatibility with NumPy.\n\nWhether to copy the data after transposing, even for DataFrames with a single\ndtype.\n\nNote that a copy is always required for mixed dtype DataFrames, or for\nDataFrames with any extension types.\n\nThe transposed DataFrame.\n\nSee also\n\nPermute the dimensions of a given array.\n\nNotes\n\nTransposing a DataFrame with mixed dtypes will result in a homogeneous\nDataFrame with the object dtype. In such a case, a copy of the data is always\nmade.\n\nExamples\n\nSquare DataFrame with homogeneous dtype\n\nWhen the dtype is homogeneous in the original DataFrame, we get a transposed\nDataFrame with the same dtype:\n\nNon-square DataFrame with mixed dtypes\n\nWhen the DataFrame has mixed dtypes, we get a transposed DataFrame with the\nobject dtype:\n\n"}, {"name": "pandas.DataFrame.truediv", "path": "reference/api/pandas.dataframe.truediv", "type": "DataFrame", "text": "\nGet Floating division of dataframe and other, element-wise (binary operator\ntruediv).\n\nEquivalent to `dataframe / other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, rtruediv.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.truncate", "path": "reference/api/pandas.dataframe.truncate", "type": "DataFrame", "text": "\nTruncate a Series or DataFrame before and after some index value.\n\nThis is a useful shorthand for boolean indexing based on index values above or\nbelow certain thresholds.\n\nTruncate all rows before this index value.\n\nTruncate all rows after this index value.\n\nAxis to truncate. Truncates the index (rows) by default.\n\nReturn a copy of the truncated section.\n\nThe truncated Series or DataFrame.\n\nSee also\n\nSelect a subset of a DataFrame by label.\n\nSelect a subset of a DataFrame by position.\n\nNotes\n\nIf the index being truncated contains only datetime values, before and after\nmay be specified as strings instead of Timestamps.\n\nExamples\n\nThe columns of a DataFrame can be truncated.\n\nFor Series, only rows can be truncated.\n\nThe index values in `truncate` can be datetimes or string dates.\n\nBecause the index is a DatetimeIndex containing only dates, we can specify\nbefore and after as strings. They will be coerced to Timestamps before\ntruncation.\n\nNote that `truncate` assumes a 0 value for any unspecified time component\n(midnight). This differs from partial string slicing, which returns any\npartially matching dates.\n\n"}, {"name": "pandas.DataFrame.tshift", "path": "reference/api/pandas.dataframe.tshift", "type": "DataFrame", "text": "\nShift the time index, using the index\u2019s frequency if available.\n\nDeprecated since version 1.1.0: Use shift instead.\n\nNumber of periods to move, can be positive or negative.\n\nIncrement to use from the tseries module or time rule expressed as a string\n(e.g. \u2018EOM\u2019).\n\nCorresponds to the axis that contains the Index.\n\nNotes\n\nIf freq is not specified then tries to use the freq or inferred_freq\nattributes of the index. If neither of those attributes exist, a ValueError is\nthrown\n\n"}, {"name": "pandas.DataFrame.tz_convert", "path": "reference/api/pandas.dataframe.tz_convert", "type": "DataFrame", "text": "\nConvert tz-aware axis to target time zone.\n\nIf axis is a MultiIndex, convert a specific level. Otherwise must be None.\n\nAlso make a copy of the underlying data.\n\nObject with time zone converted axis.\n\nIf the axis is tz-naive.\n\n"}, {"name": "pandas.DataFrame.tz_localize", "path": "reference/api/pandas.dataframe.tz_localize", "type": "DataFrame", "text": "\nLocalize tz-naive index of a Series or DataFrame to target time zone.\n\nThis operation localizes the Index. To localize the values in a timezone-naive\nSeries, use `Series.dt.tz_localize()`.\n\nIf axis ia a MultiIndex, localize a specific level. Otherwise must be None.\n\nAlso make a copy of the underlying data.\n\nWhen clocks moved backward due to DST, ambiguous times may arise. For example\nin Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST,\n02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a\nsituation, the ambiguous parameter dictates how ambiguous times should be\nhandled.\n\n\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order\n\nbool-ndarray where True signifies a DST time, False designates a non-DST time\n(note that this flag is only applicable for ambiguous times)\n\n\u2018NaT\u2019 will return NaT where there are ambiguous times\n\n\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST. Valid values are:\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times\n\ntimedelta objects will shift nonexistent times by the timedelta\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nSame type as the input.\n\nIf the TimeSeries is tz-aware and tz is not None.\n\nExamples\n\nLocalize local times:\n\nBe careful with DST changes. When there is sequential data, pandas can infer\nthe DST time:\n\nIn some cases, inferring the DST is impossible. In such cases, you can pass an\nndarray to the ambiguous parameter to set the DST explicitly\n\nIf the DST transition causes nonexistent times, you can shift these dates\nforward or backward with a timedelta object or \u2018shift_forward\u2019 or\n\u2018shift_backward\u2019.\n\n"}, {"name": "pandas.DataFrame.unstack", "path": "reference/api/pandas.dataframe.unstack", "type": "DataFrame", "text": "\nPivot a level of the (necessarily hierarchical) index labels.\n\nReturns a DataFrame having a new level of column labels whose inner-most level\nconsists of the pivoted index labels.\n\nIf the index is not a MultiIndex, the output will be a Series (the analogue of\nstack when the columns are not a MultiIndex).\n\nLevel(s) of index to unstack, can pass level name.\n\nReplace NaN with this value if the unstack produces missing values.\n\nSee also\n\nPivot a table based on column values.\n\nPivot a level of the column labels (inverse operation from unstack).\n\nExamples\n\n"}, {"name": "pandas.DataFrame.update", "path": "reference/api/pandas.dataframe.update", "type": "DataFrame", "text": "\nModify in place using non-NA values from another DataFrame.\n\nAligns on indices. There is no return value.\n\nShould have at least one matching index/column label with the original\nDataFrame. If a Series is passed, its name attribute must be set, and that\nwill be used as the column name to align with the original DataFrame.\n\nOnly left join is implemented, keeping the index and columns of the original\nobject.\n\nHow to handle non-NA values for overlapping keys:\n\nTrue: overwrite original DataFrame\u2019s values with values from other.\n\nFalse: only update values that are NA in the original DataFrame.\n\nCan choose to replace values other than NA. Return True for values that should\nbe updated.\n\nIf \u2018raise\u2019, will raise a ValueError if the DataFrame and other both contain\nnon-NA data in the same place.\n\nWhen errors=\u2019raise\u2019 and there\u2019s overlapping non-NA data.\n\nWhen errors is not either \u2018ignore\u2019 or \u2018raise\u2019\n\nIf join != \u2018left\u2019\n\nSee also\n\nSimilar method for dictionaries.\n\nFor column(s)-on-column(s) operations.\n\nExamples\n\nThe DataFrame\u2019s length does not increase as a result of the update, only\nvalues at matching index/column labels are updated.\n\nFor Series, its name attribute must be set.\n\nIf other contains NaNs the corresponding values are not updated in the\noriginal dataframe.\n\n"}, {"name": "pandas.DataFrame.value_counts", "path": "reference/api/pandas.dataframe.value_counts", "type": "DataFrame", "text": "\nReturn a Series containing counts of unique rows in the DataFrame.\n\nNew in version 1.1.0.\n\nColumns to use when counting unique combinations.\n\nReturn proportions rather than frequencies.\n\nSort by frequencies.\n\nSort in ascending order.\n\nDon\u2019t include counts of rows that contain NA values.\n\nNew in version 1.3.0.\n\nSee also\n\nEquivalent method on Series.\n\nNotes\n\nThe returned Series will have a MultiIndex with one level per input column. By\ndefault, rows that contain any NA values are omitted from the result. By\ndefault, the resulting Series will be in descending order so that the first\nelement is the most frequently-occurring row.\n\nExamples\n\nWith dropna set to False we can also count rows with NA values.\n\n"}, {"name": "pandas.DataFrame.values", "path": "reference/api/pandas.dataframe.values", "type": "DataFrame", "text": "\nReturn a Numpy representation of the DataFrame.\n\nWarning\n\nWe recommend using `DataFrame.to_numpy()` instead.\n\nOnly the values in the DataFrame will be returned, the axes labels will be\nremoved.\n\nThe values of the DataFrame.\n\nSee also\n\nRecommended alternative to this method.\n\nRetrieve the index labels.\n\nRetrieving the column names.\n\nNotes\n\nThe dtype will be a lower-common-denominator dtype (implicit upcasting); that\nis to say if the dtypes (even of numeric types) are mixed, the one that\naccommodates all will be chosen. Use this with care if you are not dealing\nwith the blocks.\n\ne.g. If the dtypes are float16 and float32, dtype will be upcast to float32.\nIf dtypes are int32 and uint8, dtype will be upcast to int32. By\n`numpy.find_common_type()` convention, mixing int64 and uint64 will result in\na float64 dtype.\n\nExamples\n\nA DataFrame where all columns are the same type (e.g., int64) results in an\narray of the same type.\n\nA DataFrame with mixed type columns(e.g., str/object, int64, float32) results\nin an ndarray of the broadest type that accommodates these mixed types (e.g.,\nobject).\n\n"}, {"name": "pandas.DataFrame.var", "path": "reference/api/pandas.dataframe.var", "type": "DataFrame", "text": "\nReturn unbiased variance over requested axis.\n\nNormalized by N-1 by default. This can be changed using the ddof argument.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nDelta Degrees of Freedom. The divisor used in calculations is N - ddof, where\nN represents the number of elements.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nExamples\n\nAlternatively, `ddof=0` can be set to normalize by N instead of N-1:\n\n"}, {"name": "pandas.DataFrame.where", "path": "reference/api/pandas.dataframe.where", "type": "DataFrame", "text": "\nReplace values where the condition is False.\n\nWhere cond is True, keep the original value. Where False, replace with\ncorresponding value from other. If cond is callable, it is computed on the\nSeries/DataFrame and should return boolean Series/DataFrame or array. The\ncallable must not change input Series/DataFrame (though pandas doesn\u2019t check\nit).\n\nEntries where cond is False are replaced with corresponding value from other.\nIf other is callable, it is computed on the Series/DataFrame and should return\nscalar or Series/DataFrame. The callable must not change input\nSeries/DataFrame (though pandas doesn\u2019t check it).\n\nWhether to perform the operation in place on the data.\n\nAlignment axis if needed.\n\nAlignment level if needed.\n\nNote that currently this parameter won\u2019t affect the results and will always\ncoerce to a suitable dtype.\n\n\u2018raise\u2019 : allow exceptions to be raised.\n\n\u2018ignore\u2019 : suppress exceptions. On error return original object.\n\nTry to cast the result back to the input type (if possible).\n\nDeprecated since version 1.3.0: Manually cast back if necessary.\n\nSee also\n\nReturn an object of same shape as self.\n\nNotes\n\nThe where method is an application of the if-then idiom. For each element in\nthe calling DataFrame, if `cond` is `True` the element is used; otherwise the\ncorresponding element from the DataFrame `other` is used.\n\nThe signature for `DataFrame.where()` differs from `numpy.where()`. Roughly\n`df1.where(m, df2)` is equivalent to `np.where(m, df1, df2)`.\n\nFor further details and examples see the `where` documentation in indexing.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.xs", "path": "reference/api/pandas.dataframe.xs", "type": "DataFrame", "text": "\nReturn cross-section from the Series/DataFrame.\n\nThis method takes a key argument to select data at a particular level of a\nMultiIndex.\n\nLabel contained in the index, or partially in a MultiIndex.\n\nAxis to retrieve cross-section on.\n\nIn case of a key partially contained in a MultiIndex, indicate which levels\nare used. Levels can be referred by label or position.\n\nIf False, returns object with same levels as self.\n\nCross-section from the original Series or DataFrame corresponding to the\nselected index levels.\n\nSee also\n\nAccess a group of rows and columns by label(s) or a boolean array.\n\nPurely integer-location based indexing for selection by position.\n\nNotes\n\nxs can not be used to set values.\n\nMultiIndex Slicers is a generic way to get/set values on any level or levels.\nIt is a superset of xs functionality, see MultiIndex Slicers.\n\nExamples\n\nGet values at specified index\n\nGet values at several indexes\n\nGet values at specified index and level\n\nGet values at several indexes and levels\n\nGet values at specified column and axis\n\n"}, {"name": "pandas.date_range", "path": "reference/api/pandas.date_range", "type": "General functions", "text": "\nReturn a fixed frequency DatetimeIndex.\n\nReturns the range of equally spaced time points (where the difference between\nany two adjacent points is specified by the given frequency) such that they\nall satisfy start <[=] x <[=] end, where the first one and the last one are,\nresp., the first and last time points in that range that fall on the boundary\nof `freq` (if given as a frequency string) or that are valid for `freq` (if\ngiven as a `pandas.tseries.offsets.DateOffset`). (If exactly one of `start`,\n`end`, or `freq` is not specified, this missing parameter can be computed\ngiven `periods`, the number of timesteps in the range. See the note below.)\n\nLeft bound for generating dates.\n\nRight bound for generating dates.\n\nNumber of periods to generate.\n\nFrequency strings can have multiples, e.g. \u20185H\u2019. See here for a list of\nfrequency aliases.\n\nTime zone name for returning localized DatetimeIndex, for example\n\u2018Asia/Hong_Kong\u2019. By default, the resulting DatetimeIndex is timezone-naive.\n\nNormalize start/end dates to midnight before generating date range.\n\nName of the resulting DatetimeIndex.\n\nMake the interval closed with respect to the given frequency to the \u2018left\u2019,\n\u2018right\u2019, or both sides (None, the default).\n\nDeprecated since version 1.4.0: Argument closed has been deprecated to\nstandardize boundary inputs. Use inclusive instead, to set each bound as\nclosed or open.\n\nInclude boundaries; Whether to set each bound as closed or open.\n\nNew in version 1.4.0.\n\nFor compatibility. Has no effect on the result.\n\nSee also\n\nAn immutable container for datetimes.\n\nReturn a fixed frequency TimedeltaIndex.\n\nReturn a fixed frequency PeriodIndex.\n\nReturn a fixed frequency IntervalIndex.\n\nNotes\n\nOf the four parameters `start`, `end`, `periods`, and `freq`, exactly three\nmust be specified. If `freq` is omitted, the resulting `DatetimeIndex` will\nhave `periods` linearly spaced elements between `start` and `end` (closed on\nboth sides).\n\nTo learn more about the frequency strings, please see this link.\n\nExamples\n\nSpecifying the values\n\nThe next four examples generate the same DatetimeIndex, but vary the\ncombination of start, end and periods.\n\nSpecify start and end, with the default daily frequency.\n\nSpecify start and periods, the number of periods (days).\n\nSpecify end and periods, the number of periods (days).\n\nSpecify start, end, and periods; the frequency is generated automatically\n(linearly spaced).\n\nOther Parameters\n\nChanged the freq (frequency) to `'M'` (month end frequency).\n\nMultiples are allowed\n\nfreq can also be specified as an Offset object.\n\nSpecify tz to set the timezone.\n\ninclusive controls whether to include start and end that are on the boundary.\nThe default, \u201cboth\u201d, includes boundary points on either end.\n\nUse `inclusive='left'` to exclude end if it falls on the boundary.\n\nUse `inclusive='right'` to exclude start if it falls on the boundary, and\nsimilarly `inclusive='neither'` will exclude both start and end.\n\n"}, {"name": "pandas.DatetimeIndex", "path": "reference/api/pandas.datetimeindex", "type": "Index Objects", "text": "\nImmutable ndarray-like of datetime64 data.\n\nRepresented internally as int64, and which can be boxed to Timestamp objects\nthat are subclasses of datetime and carry metadata.\n\nOptional datetime-like data to construct index with.\n\nOne of pandas date offset strings or corresponding objects. The string \u2018infer\u2019\ncan be passed in order to set the frequency of the index as the inferred\nfrequency upon creation.\n\nSet the Timezone of the data.\n\nNormalize start/end dates to midnight before generating date range.\n\nSet whether to include start and end that are on the boundary. The default\nincludes boundary points on either end.\n\nWhen clocks moved backward due to DST, ambiguous times may arise. For example\nin Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST,\n02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a\nsituation, the ambiguous parameter dictates how ambiguous times should be\nhandled.\n\n\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order\n\nbool-ndarray where True signifies a DST time, False signifies a non-DST time\n(note that this flag is only applicable for ambiguous times)\n\n\u2018NaT\u2019 will return NaT where there are ambiguous times\n\n\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.\n\nIf True, parse dates in data with the day first order.\n\nIf True parse dates in data with the year first order.\n\nNote that the only NumPy dtype allowed is \u2018datetime64[ns]\u2019.\n\nMake a copy of input ndarray.\n\nName to be stored in the index.\n\nSee also\n\nThe base pandas Index type.\n\nIndex of timedelta64 data.\n\nIndex of Period data.\n\nConvert argument to datetime.\n\nCreate a fixed-frequency DatetimeIndex.\n\nNotes\n\nTo learn more about the frequency strings, please see this link.\n\nAttributes\n\n`year`\n\nThe year of the datetime.\n\n`month`\n\nThe month as January=1, December=12.\n\n`day`\n\nThe day of the datetime.\n\n`hour`\n\nThe hours of the datetime.\n\n`minute`\n\nThe minutes of the datetime.\n\n`second`\n\nThe seconds of the datetime.\n\n`microsecond`\n\nThe microseconds of the datetime.\n\n`nanosecond`\n\nThe nanoseconds of the datetime.\n\n`date`\n\nReturns numpy array of python `datetime.date` objects.\n\n`time`\n\nReturns numpy array of `datetime.time` objects.\n\n`timetz`\n\nReturns numpy array of `datetime.time` objects with timezone information.\n\n`dayofyear`\n\nThe ordinal day of the year.\n\n`day_of_year`\n\nThe ordinal day of the year.\n\n`weekofyear`\n\n(DEPRECATED) The week ordinal of the year.\n\n`week`\n\n(DEPRECATED) The week ordinal of the year.\n\n`dayofweek`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`day_of_week`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`weekday`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`quarter`\n\nThe quarter of the date.\n\n`tz`\n\nReturn the timezone.\n\n`freq`\n\nReturn the frequency object if it is set, otherwise None.\n\n`freqstr`\n\nReturn the frequency object as a string if its set, otherwise None.\n\n`is_month_start`\n\nIndicates whether the date is the first day of the month.\n\n`is_month_end`\n\nIndicates whether the date is the last day of the month.\n\n`is_quarter_start`\n\nIndicator for whether the date is the first day of a quarter.\n\n`is_quarter_end`\n\nIndicator for whether the date is the last day of a quarter.\n\n`is_year_start`\n\nIndicate whether the date is the first day of a year.\n\n`is_year_end`\n\nIndicate whether the date is the last day of the year.\n\n`is_leap_year`\n\nBoolean indicator if the date belongs to a leap year.\n\n`inferred_freq`\n\nTries to return a string representing a frequency guess, generated by\ninfer_freq.\n\nMethods\n\n`normalize`(*args, **kwargs)\n\nConvert times to midnight.\n\n`strftime`(*args, **kwargs)\n\nConvert to Index using specified date_format.\n\n`snap`([freq])\n\nSnap time stamps to nearest occurring frequency.\n\n`tz_convert`(tz)\n\nConvert tz-aware Datetime Array/Index from one time zone to another.\n\n`tz_localize`(tz[, ambiguous, nonexistent])\n\nLocalize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index.\n\n`round`(*args, **kwargs)\n\nPerform round operation on the data to the specified freq.\n\n`floor`(*args, **kwargs)\n\nPerform floor operation on the data to the specified freq.\n\n`ceil`(*args, **kwargs)\n\nPerform ceil operation on the data to the specified freq.\n\n`to_period`(*args, **kwargs)\n\nCast to PeriodArray/Index at a particular frequency.\n\n`to_perioddelta`(freq)\n\nCalculate TimedeltaArray of difference between index values and index\nconverted to PeriodArray at specified freq.\n\n`to_pydatetime`(*args, **kwargs)\n\nReturn Datetime Array/Index as object ndarray of datetime.datetime objects.\n\n`to_series`([keep_tz, index, name])\n\nCreate a Series with both index and values equal to the index keys useful with\nmap for returning an indexer based on an index.\n\n`to_frame`([index, name])\n\nCreate a DataFrame with a column containing the Index.\n\n`month_name`(*args, **kwargs)\n\nReturn the month names of the DateTimeIndex with specified locale.\n\n`day_name`(*args, **kwargs)\n\nReturn the day names of the DateTimeIndex with specified locale.\n\n`mean`(*args, **kwargs)\n\nReturn the mean value of the Array.\n\n`std`(*args, **kwargs)\n\nReturn sample standard deviation over requested axis.\n\n"}, {"name": "pandas.DatetimeIndex.ceil", "path": "reference/api/pandas.datetimeindex.ceil", "type": "Index Objects", "text": "\nPerform ceil operation on the data to the specified freq.\n\nThe frequency level to ceil the index to. Must be a fixed frequency like \u2018S\u2019\n(second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible\nfreq values.\n\nOnly relevant for DatetimeIndex:\n\n\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order\n\nbool-ndarray where True signifies a DST time, False designates a non-DST time\n(note that this flag is only applicable for ambiguous times)\n\n\u2018NaT\u2019 will return NaT where there are ambiguous times\n\n\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST.\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times\n\ntimedelta objects will shift nonexistent times by the timedelta\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with\nthe same index for a Series.\n\nNotes\n\nIf the timestamps have a timezone, ceiling will take place relative to the\nlocal (\u201cwall\u201d) time and re-localized to the same timezone. When ceiling near\ndaylight savings time, use `nonexistent` and `ambiguous` to control the re-\nlocalization behavior.\n\nExamples\n\nDatetimeIndex\n\nSeries\n\nWhen rounding near a daylight savings time transition, use `ambiguous` or\n`nonexistent` to control how the timestamp should be re-localized.\n\n"}, {"name": "pandas.DatetimeIndex.date", "path": "reference/api/pandas.datetimeindex.date", "type": "Index Objects", "text": "\nReturns numpy array of python `datetime.date` objects.\n\nNamely, the date part of Timestamps without time and timezone information.\n\n"}, {"name": "pandas.DatetimeIndex.day", "path": "reference/api/pandas.datetimeindex.day", "type": "Index Objects", "text": "\nThe day of the datetime.\n\nExamples\n\n"}, {"name": "pandas.DatetimeIndex.day_name", "path": "reference/api/pandas.datetimeindex.day_name", "type": "Index Objects", "text": "\nReturn the day names of the DateTimeIndex with specified locale.\n\nLocale determining the language in which to return the day name. Default is\nEnglish locale.\n\nIndex of day names.\n\nExamples\n\n"}, {"name": "pandas.DatetimeIndex.day_of_week", "path": "reference/api/pandas.datetimeindex.day_of_week", "type": "Index Objects", "text": "\nThe day of the week with Monday=0, Sunday=6.\n\nReturn the day of the week. It is assumed the week starts on Monday, which is\ndenoted by 0 and ends on Sunday which is denoted by 6. This method is\navailable on both Series with datetime values (using the dt accessor) or\nDatetimeIndex.\n\nContaining integers indicating the day number.\n\nSee also\n\nAlias.\n\nAlias.\n\nReturns the name of the day of the week.\n\nExamples\n\n"}, {"name": "pandas.DatetimeIndex.day_of_year", "path": "reference/api/pandas.datetimeindex.day_of_year", "type": "Index Objects", "text": "\nThe ordinal day of the year.\n\n"}, {"name": "pandas.DatetimeIndex.dayofweek", "path": "reference/api/pandas.datetimeindex.dayofweek", "type": "Index Objects", "text": "\nThe day of the week with Monday=0, Sunday=6.\n\nReturn the day of the week. It is assumed the week starts on Monday, which is\ndenoted by 0 and ends on Sunday which is denoted by 6. This method is\navailable on both Series with datetime values (using the dt accessor) or\nDatetimeIndex.\n\nContaining integers indicating the day number.\n\nSee also\n\nAlias.\n\nAlias.\n\nReturns the name of the day of the week.\n\nExamples\n\n"}, {"name": "pandas.DatetimeIndex.dayofyear", "path": "reference/api/pandas.datetimeindex.dayofyear", "type": "Index Objects", "text": "\nThe ordinal day of the year.\n\n"}, {"name": "pandas.DatetimeIndex.floor", "path": "reference/api/pandas.datetimeindex.floor", "type": "Index Objects", "text": "\nPerform floor operation on the data to the specified freq.\n\nThe frequency level to floor the index to. Must be a fixed frequency like \u2018S\u2019\n(second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible\nfreq values.\n\nOnly relevant for DatetimeIndex:\n\n\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order\n\nbool-ndarray where True signifies a DST time, False designates a non-DST time\n(note that this flag is only applicable for ambiguous times)\n\n\u2018NaT\u2019 will return NaT where there are ambiguous times\n\n\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST.\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times\n\ntimedelta objects will shift nonexistent times by the timedelta\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with\nthe same index for a Series.\n\nNotes\n\nIf the timestamps have a timezone, flooring will take place relative to the\nlocal (\u201cwall\u201d) time and re-localized to the same timezone. When flooring near\ndaylight savings time, use `nonexistent` and `ambiguous` to control the re-\nlocalization behavior.\n\nExamples\n\nDatetimeIndex\n\nSeries\n\nWhen rounding near a daylight savings time transition, use `ambiguous` or\n`nonexistent` to control how the timestamp should be re-localized.\n\n"}, {"name": "pandas.DatetimeIndex.freq", "path": "reference/api/pandas.datetimeindex.freq", "type": "Index Objects", "text": "\nReturn the frequency object if it is set, otherwise None.\n\n"}, {"name": "pandas.DatetimeIndex.freqstr", "path": "reference/api/pandas.datetimeindex.freqstr", "type": "Index Objects", "text": "\nReturn the frequency object as a string if its set, otherwise None.\n\n"}, {"name": "pandas.DatetimeIndex.hour", "path": "reference/api/pandas.datetimeindex.hour", "type": "Index Objects", "text": "\nThe hours of the datetime.\n\nExamples\n\n"}, {"name": "pandas.DatetimeIndex.indexer_at_time", "path": "reference/api/pandas.datetimeindex.indexer_at_time", "type": "Index Objects", "text": "\nReturn index locations of values at particular time of day (e.g. 9:30AM).\n\nTime passed in either as object (datetime.time) or as string in appropriate\nformat (\u201c%H:%M\u201d, \u201c%H%M\u201d, \u201c%I:%M%p\u201d, \u201c%I%M%p\u201d, \u201c%H:%M:%S\u201d, \u201c%H%M%S\u201d,\n\u201c%I:%M:%S%p\u201d, \u201c%I%M%S%p\u201d).\n\nSee also\n\nGet index locations of values between particular times of day.\n\nSelect values at particular time of day.\n\n"}, {"name": "pandas.DatetimeIndex.indexer_between_time", "path": "reference/api/pandas.datetimeindex.indexer_between_time", "type": "Index Objects", "text": "\nReturn index locations of values between particular times of day (e.g.,\n9:00-9:30AM).\n\nTime passed either as object (datetime.time) or as string in appropriate\nformat (\u201c%H:%M\u201d, \u201c%H%M\u201d, \u201c%I:%M%p\u201d, \u201c%I%M%p\u201d, \u201c%H:%M:%S\u201d, \u201c%H%M%S\u201d,\n\u201c%I:%M:%S%p\u201d,\u201d%I%M%S%p\u201d).\n\nSee also\n\nGet index locations of values at particular time of day.\n\nSelect values between particular times of day.\n\n"}, {"name": "pandas.DatetimeIndex.inferred_freq", "path": "reference/api/pandas.datetimeindex.inferred_freq", "type": "Index Objects", "text": "\nTries to return a string representing a frequency guess, generated by\ninfer_freq. Returns None if it can\u2019t autodetect the frequency.\n\n"}, {"name": "pandas.DatetimeIndex.is_leap_year", "path": "reference/api/pandas.datetimeindex.is_leap_year", "type": "Index Objects", "text": "\nBoolean indicator if the date belongs to a leap year.\n\nA leap year is a year, which has 366 days (instead of 365) including 29th of\nFebruary as an intercalary day. Leap years are years which are multiples of\nfour with the exception of years divisible by 100 but not by 400.\n\nBooleans indicating if dates belong to a leap year.\n\nExamples\n\nThis method is available on Series with datetime values under the `.dt`\naccessor, and directly on DatetimeIndex.\n\n"}, {"name": "pandas.DatetimeIndex.is_month_end", "path": "reference/api/pandas.datetimeindex.is_month_end", "type": "Index Objects", "text": "\nIndicates whether the date is the last day of the month.\n\nFor Series, returns a Series with boolean values. For DatetimeIndex, returns a\nboolean array.\n\nSee also\n\nReturn a boolean indicating whether the date is the first day of the month.\n\nReturn a boolean indicating whether the date is the last day of the month.\n\nExamples\n\nThis method is available on Series with datetime values under the `.dt`\naccessor, and directly on DatetimeIndex.\n\n"}, {"name": "pandas.DatetimeIndex.is_month_start", "path": "reference/api/pandas.datetimeindex.is_month_start", "type": "Index Objects", "text": "\nIndicates whether the date is the first day of the month.\n\nFor Series, returns a Series with boolean values. For DatetimeIndex, returns a\nboolean array.\n\nSee also\n\nReturn a boolean indicating whether the date is the first day of the month.\n\nReturn a boolean indicating whether the date is the last day of the month.\n\nExamples\n\nThis method is available on Series with datetime values under the `.dt`\naccessor, and directly on DatetimeIndex.\n\n"}, {"name": "pandas.DatetimeIndex.is_quarter_end", "path": "reference/api/pandas.datetimeindex.is_quarter_end", "type": "Index Objects", "text": "\nIndicator for whether the date is the last day of a quarter.\n\nThe same type as the original data with boolean values. Series will have the\nsame name and index. DatetimeIndex will have the same name.\n\nSee also\n\nReturn the quarter of the date.\n\nSimilar property indicating the quarter start.\n\nExamples\n\nThis method is available on Series with datetime values under the `.dt`\naccessor, and directly on DatetimeIndex.\n\n"}, {"name": "pandas.DatetimeIndex.is_quarter_start", "path": "reference/api/pandas.datetimeindex.is_quarter_start", "type": "Index Objects", "text": "\nIndicator for whether the date is the first day of a quarter.\n\nThe same type as the original data with boolean values. Series will have the\nsame name and index. DatetimeIndex will have the same name.\n\nSee also\n\nReturn the quarter of the date.\n\nSimilar property for indicating the quarter start.\n\nExamples\n\nThis method is available on Series with datetime values under the `.dt`\naccessor, and directly on DatetimeIndex.\n\n"}, {"name": "pandas.DatetimeIndex.is_year_end", "path": "reference/api/pandas.datetimeindex.is_year_end", "type": "Index Objects", "text": "\nIndicate whether the date is the last day of the year.\n\nThe same type as the original data with boolean values. Series will have the\nsame name and index. DatetimeIndex will have the same name.\n\nSee also\n\nSimilar property indicating the start of the year.\n\nExamples\n\nThis method is available on Series with datetime values under the `.dt`\naccessor, and directly on DatetimeIndex.\n\n"}, {"name": "pandas.DatetimeIndex.is_year_start", "path": "reference/api/pandas.datetimeindex.is_year_start", "type": "Index Objects", "text": "\nIndicate whether the date is the first day of a year.\n\nThe same type as the original data with boolean values. Series will have the\nsame name and index. DatetimeIndex will have the same name.\n\nSee also\n\nSimilar property indicating the last day of the year.\n\nExamples\n\nThis method is available on Series with datetime values under the `.dt`\naccessor, and directly on DatetimeIndex.\n\n"}, {"name": "pandas.DatetimeIndex.mean", "path": "reference/api/pandas.datetimeindex.mean", "type": "Index Objects", "text": "\nReturn the mean value of the Array.\n\nNew in version 0.25.0.\n\nWhether to ignore any NaT elements.\n\nTimestamp or Timedelta.\n\nSee also\n\nReturns the average of array elements along a given axis.\n\nReturn the mean value in a Series.\n\nNotes\n\nmean is only defined for Datetime and Timedelta dtypes, not for Period.\n\n"}, {"name": "pandas.DatetimeIndex.microsecond", "path": "reference/api/pandas.datetimeindex.microsecond", "type": "Index Objects", "text": "\nThe microseconds of the datetime.\n\nExamples\n\n"}, {"name": "pandas.DatetimeIndex.minute", "path": "reference/api/pandas.datetimeindex.minute", "type": "Index Objects", "text": "\nThe minutes of the datetime.\n\nExamples\n\n"}, {"name": "pandas.DatetimeIndex.month", "path": "reference/api/pandas.datetimeindex.month", "type": "Index Objects", "text": "\nThe month as January=1, December=12.\n\nExamples\n\n"}, {"name": "pandas.DatetimeIndex.month_name", "path": "reference/api/pandas.datetimeindex.month_name", "type": "Index Objects", "text": "\nReturn the month names of the DateTimeIndex with specified locale.\n\nLocale determining the language in which to return the month name. Default is\nEnglish locale.\n\nIndex of month names.\n\nExamples\n\n"}, {"name": "pandas.DatetimeIndex.nanosecond", "path": "reference/api/pandas.datetimeindex.nanosecond", "type": "Index Objects", "text": "\nThe nanoseconds of the datetime.\n\nExamples\n\n"}, {"name": "pandas.DatetimeIndex.normalize", "path": "reference/api/pandas.datetimeindex.normalize", "type": "Index Objects", "text": "\nConvert times to midnight.\n\nThe time component of the date-time is converted to midnight i.e. 00:00:00.\nThis is useful in cases, when the time does not matter. Length is unaltered.\nThe timezones are unaffected.\n\nThis method is available on Series with datetime values under the `.dt`\naccessor, and directly on Datetime Array/Index.\n\nThe same type as the original data. Series will have the same name and index.\nDatetimeIndex will have the same name.\n\nSee also\n\nFloor the datetimes to the specified freq.\n\nCeil the datetimes to the specified freq.\n\nRound the datetimes to the specified freq.\n\nExamples\n\n"}, {"name": "pandas.DatetimeIndex.quarter", "path": "reference/api/pandas.datetimeindex.quarter", "type": "Index Objects", "text": "\nThe quarter of the date.\n\n"}, {"name": "pandas.DatetimeIndex.round", "path": "reference/api/pandas.datetimeindex.round", "type": "Index Objects", "text": "\nPerform round operation on the data to the specified freq.\n\nThe frequency level to round the index to. Must be a fixed frequency like \u2018S\u2019\n(second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible\nfreq values.\n\nOnly relevant for DatetimeIndex:\n\n\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order\n\nbool-ndarray where True signifies a DST time, False designates a non-DST time\n(note that this flag is only applicable for ambiguous times)\n\n\u2018NaT\u2019 will return NaT where there are ambiguous times\n\n\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST.\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times\n\ntimedelta objects will shift nonexistent times by the timedelta\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with\nthe same index for a Series.\n\nNotes\n\nIf the timestamps have a timezone, rounding will take place relative to the\nlocal (\u201cwall\u201d) time and re-localized to the same timezone. When rounding near\ndaylight savings time, use `nonexistent` and `ambiguous` to control the re-\nlocalization behavior.\n\nExamples\n\nDatetimeIndex\n\nSeries\n\nWhen rounding near a daylight savings time transition, use `ambiguous` or\n`nonexistent` to control how the timestamp should be re-localized.\n\n"}, {"name": "pandas.DatetimeIndex.second", "path": "reference/api/pandas.datetimeindex.second", "type": "Index Objects", "text": "\nThe seconds of the datetime.\n\nExamples\n\n"}, {"name": "pandas.DatetimeIndex.snap", "path": "reference/api/pandas.datetimeindex.snap", "type": "Index Objects", "text": "\nSnap time stamps to nearest occurring frequency.\n\n"}, {"name": "pandas.DatetimeIndex.std", "path": "reference/api/pandas.datetimeindex.std", "type": "Index Objects", "text": "\nReturn sample standard deviation over requested axis.\n\nNormalized by N-1 by default. This can be changed using the ddof argument\n\nAxis for the function to be applied on.\n\nDegrees of Freedom. The divisor used in calculations is N - ddof, where N\nrepresents the number of elements.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\n"}, {"name": "pandas.DatetimeIndex.strftime", "path": "reference/api/pandas.datetimeindex.strftime", "type": "Index Objects", "text": "\nConvert to Index using specified date_format.\n\nReturn an Index of formatted strings specified by date_format, which supports\nthe same string format as the python standard library. Details of the string\nformat can be found in python string format doc.\n\nDate format string (e.g. \u201c%Y-%m-%d\u201d).\n\nNumPy ndarray of formatted strings.\n\nSee also\n\nConvert the given argument to datetime.\n\nReturn DatetimeIndex with times to midnight.\n\nRound the DatetimeIndex to the specified freq.\n\nFloor the DatetimeIndex to the specified freq.\n\nExamples\n\n"}, {"name": "pandas.DatetimeIndex.time", "path": "reference/api/pandas.datetimeindex.time", "type": "Index Objects", "text": "\nReturns numpy array of `datetime.time` objects.\n\nThe time part of the Timestamps.\n\n"}, {"name": "pandas.DatetimeIndex.timetz", "path": "reference/api/pandas.datetimeindex.timetz", "type": "Index Objects", "text": "\nReturns numpy array of `datetime.time` objects with timezone information.\n\nThe time part of the Timestamps.\n\n"}, {"name": "pandas.DatetimeIndex.to_frame", "path": "reference/api/pandas.datetimeindex.to_frame", "type": "DataFrame", "text": "\nCreate a DataFrame with a column containing the Index.\n\nSet the index of the returned DataFrame as the original Index.\n\nThe passed name should substitute for the index name (if it has one).\n\nDataFrame containing the original Index data.\n\nSee also\n\nConvert an Index to a Series.\n\nConvert Series to DataFrame.\n\nExamples\n\nBy default, the original Index is reused. To enforce a new Index:\n\nTo override the name of the resulting column, specify name:\n\n"}, {"name": "pandas.DatetimeIndex.to_period", "path": "reference/api/pandas.datetimeindex.to_period", "type": "Input/output", "text": "\nCast to PeriodArray/Index at a particular frequency.\n\nConverts DatetimeArray/Index to PeriodArray/Index.\n\nOne of pandas\u2019 offset strings or an Offset object. Will be inferred by\ndefault.\n\nWhen converting a DatetimeArray/Index with non-regular values, so that a\nfrequency cannot be inferred.\n\nSee also\n\nImmutable ndarray holding ordinal values.\n\nReturn DatetimeIndex as object.\n\nExamples\n\nInfer the daily frequency\n\n"}, {"name": "pandas.DatetimeIndex.to_perioddelta", "path": "reference/api/pandas.datetimeindex.to_perioddelta", "type": "Input/output", "text": "\nCalculate TimedeltaArray of difference between index values and index\nconverted to PeriodArray at specified freq. Used for vectorized offsets.\n\n"}, {"name": "pandas.DatetimeIndex.to_pydatetime", "path": "reference/api/pandas.datetimeindex.to_pydatetime", "type": "Index Objects", "text": "\nReturn Datetime Array/Index as object ndarray of datetime.datetime objects.\n\n"}, {"name": "pandas.DatetimeIndex.to_series", "path": "reference/api/pandas.datetimeindex.to_series", "type": "Index Objects", "text": "\nCreate a Series with both index and values equal to the index keys useful with\nmap for returning an indexer based on an index.\n\nReturn the data keeping the timezone.\n\nIf keep_tz is True:\n\nIf the timezone is not set, the resulting Series will have a datetime64[ns]\ndtype.\n\nOtherwise the Series will have an datetime64[ns, tz] dtype; the tz will be\npreserved.\n\nIf keep_tz is False:\n\nSeries will have a datetime64[ns] dtype. TZ aware objects will have the tz\nremoved.\n\nChanged in version 1.0.0: The default value is now True. In a future version,\nthis keyword will be removed entirely. Stop passing the argument to obtain the\nfuture behavior and silence the warning.\n\nIndex of resulting Series. If None, defaults to original index.\n\nName of resulting Series. If None, defaults to name of original index.\n\n"}, {"name": "pandas.DatetimeIndex.tz", "path": "reference/api/pandas.datetimeindex.tz", "type": "Index Objects", "text": "\nReturn the timezone.\n\nReturns None when the array is tz-naive.\n\n"}, {"name": "pandas.DatetimeIndex.tz_convert", "path": "reference/api/pandas.datetimeindex.tz_convert", "type": "Index Objects", "text": "\nConvert tz-aware Datetime Array/Index from one time zone to another.\n\nTime zone for time. Corresponding timestamps would be converted to this time\nzone of the Datetime Array/Index. A tz of None will convert to UTC and remove\nthe timezone information.\n\nIf Datetime Array/Index is tz-naive.\n\nSee also\n\nA timezone that has a variable offset from UTC.\n\nLocalize tz-naive DatetimeIndex to a given time zone, or remove timezone from\na tz-aware DatetimeIndex.\n\nExamples\n\nWith the tz parameter, we can change the DatetimeIndex to other time zones:\n\nWith the `tz=None`, we can remove the timezone (after converting to UTC if\nnecessary):\n\n"}, {"name": "pandas.DatetimeIndex.tz_localize", "path": "reference/api/pandas.datetimeindex.tz_localize", "type": "Index Objects", "text": "\nLocalize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index.\n\nThis method takes a time zone (tz) naive Datetime Array/Index object and makes\nthis time zone aware. It does not move the time to another time zone.\n\nThis method can also be used to do the inverse \u2013 to create a time zone unaware\nobject from an aware object. To that end, pass tz=None.\n\nTime zone to convert timestamps to. Passing `None` will remove the time zone\ninformation preserving local time.\n\nWhen clocks moved backward due to DST, ambiguous times may arise. For example\nin Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST,\n02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a\nsituation, the ambiguous parameter dictates how ambiguous times should be\nhandled.\n\n\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order\n\nbool-ndarray where True signifies a DST time, False signifies a non-DST time\n(note that this flag is only applicable for ambiguous times)\n\n\u2018NaT\u2019 will return NaT where there are ambiguous times\n\n\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST.\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times\n\ntimedelta objects will shift nonexistent times by the timedelta\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nArray/Index converted to the specified time zone.\n\nIf the Datetime Array/Index is tz-aware and tz is not None.\n\nSee also\n\nConvert tz-aware DatetimeIndex from one time zone to another.\n\nExamples\n\nLocalize DatetimeIndex in US/Eastern time zone:\n\nWith the `tz=None`, we can remove the time zone information while keeping the\nlocal time (not converted to UTC):\n\nBe careful with DST changes. When there is sequential data, pandas can infer\nthe DST time:\n\nIn some cases, inferring the DST is impossible. In such cases, you can pass an\nndarray to the ambiguous parameter to set the DST explicitly\n\nIf the DST transition causes nonexistent times, you can shift these dates\nforward or backwards with a timedelta object or \u2018shift_forward\u2019 or\n\u2018shift_backwards\u2019.\n\n"}, {"name": "pandas.DatetimeIndex.week", "path": "reference/api/pandas.datetimeindex.week", "type": "Index Objects", "text": "\nThe week ordinal of the year.\n\nDeprecated since version 1.1.0.\n\nweekofyear and week have been deprecated. Please use\nDatetimeIndex.isocalendar().week instead.\n\n"}, {"name": "pandas.DatetimeIndex.weekday", "path": "reference/api/pandas.datetimeindex.weekday", "type": "Index Objects", "text": "\nThe day of the week with Monday=0, Sunday=6.\n\nReturn the day of the week. It is assumed the week starts on Monday, which is\ndenoted by 0 and ends on Sunday which is denoted by 6. This method is\navailable on both Series with datetime values (using the dt accessor) or\nDatetimeIndex.\n\nContaining integers indicating the day number.\n\nSee also\n\nAlias.\n\nAlias.\n\nReturns the name of the day of the week.\n\nExamples\n\n"}, {"name": "pandas.DatetimeIndex.weekofyear", "path": "reference/api/pandas.datetimeindex.weekofyear", "type": "Index Objects", "text": "\nThe week ordinal of the year.\n\nDeprecated since version 1.1.0.\n\nweekofyear and week have been deprecated. Please use\nDatetimeIndex.isocalendar().week instead.\n\n"}, {"name": "pandas.DatetimeIndex.year", "path": "reference/api/pandas.datetimeindex.year", "type": "Index Objects", "text": "\nThe year of the datetime.\n\nExamples\n\n"}, {"name": "pandas.DatetimeTZDtype", "path": "reference/api/pandas.datetimetzdtype", "type": "Pandas arrays", "text": "\nAn ExtensionDtype for timezone-aware datetime data.\n\nThis is not an actual numpy dtype, but a duck type.\n\nThe precision of the datetime data. Currently limited to `\"ns\"`.\n\nThe timezone.\n\nWhen the requested timezone cannot be found.\n\nExamples\n\nAttributes\n\n`unit`\n\nThe precision of the datetime data.\n\n`tz`\n\nThe timezone.\n\nMethods\n\nNone\n\n"}, {"name": "pandas.DatetimeTZDtype.tz", "path": "reference/api/pandas.datetimetzdtype.tz", "type": "Pandas arrays", "text": "\nThe timezone.\n\n"}, {"name": "pandas.DatetimeTZDtype.unit", "path": "reference/api/pandas.datetimetzdtype.unit", "type": "Pandas arrays", "text": "\nThe precision of the datetime data.\n\n"}, {"name": "pandas.describe_option", "path": "reference/api/pandas.describe_option", "type": "General utility functions", "text": "\nPrints the description for one or more registered options.\n\nCall with no arguments to get a listing for all registered options.\n\nAvailable options:\n\ncompute.[use_bottleneck, use_numba, use_numexpr]\n\ndisplay.[chop_threshold, colheader_justify, column_space, date_dayfirst,\ndate_yearfirst, encoding, expand_frame_repr, float_format]\n\ndisplay.html.[border, table_schema, use_mathjax]\n\ndisplay.[large_repr]\n\ndisplay.latex.[escape, longtable, multicolumn, multicolumn_format, multirow,\nrepr]\n\ndisplay.[max_categories, max_columns, max_colwidth, max_dir_items,\nmax_info_columns, max_info_rows, max_rows, max_seq_items, memory_usage,\nmin_rows, multi_sparse, notebook_repr_html, pprint_nest_depth, precision,\nshow_dimensions]\n\ndisplay.unicode.[ambiguous_as_wide, east_asian_width]\n\ndisplay.[width]\n\nio.excel.ods.[reader, writer]\n\nio.excel.xls.[reader, writer]\n\nio.excel.xlsb.[reader]\n\nio.excel.xlsm.[reader, writer]\n\nio.excel.xlsx.[reader, writer]\n\nio.hdf.[default_format, dropna_table]\n\nio.parquet.[engine]\n\nio.sql.[engine]\n\nmode.[chained_assignment, data_manager, sim_interactive, string_storage,\nuse_inf_as_na, use_inf_as_null]\n\nplotting.[backend]\n\nplotting.matplotlib.[register_converters]\n\nstyler.format.[decimal, escape, formatter, na_rep, precision, thousands]\n\nstyler.html.[mathjax]\n\nstyler.latex.[environment, hrules, multicol_align, multirow_align]\n\nstyler.render.[encoding, max_columns, max_elements, max_rows, repr]\n\nstyler.sparse.[columns, index]\n\nRegexp pattern. All matching keys will have their description displayed.\n\nIf True (default) the description(s) will be printed to stdout. Otherwise, the\ndescription(s) will be returned as a unicode string (for testing).\n\nNotes\n\nThe available options with its descriptions:\n\nUse the bottleneck library to accelerate if it is installed, the default is\nTrue Valid values: False,True [default: True] [currently: True]\n\nUse the numba engine option for select operations if it is installed, the\ndefault is False Valid values: False,True [default: False] [currently: False]\n\nUse the numexpr library to accelerate computation if it is installed, the\ndefault is True Valid values: False,True [default: True] [currently: True]\n\nif set to a float value, all float values smaller then the given threshold\nwill be displayed as exactly 0 by repr and friends. [default: None]\n[currently: None]\n\nControls the justification of column headers. used by DataFrameFormatter.\n[default: right] [currently: right]\n\n[default: 12] [currently: 12]\n\nWhen True, prints and parses dates with the day first, eg 20/01/2005 [default:\nFalse] [currently: False]\n\nWhen True, prints and parses dates with the year first, eg 2005/01/20\n[default: False] [currently: False]\n\nDefaults to the detected encoding of the console. Specifies the encoding to be\nused for strings returned by to_string, these are generally strings meant to\nbe displayed on the console. [default: utf-8] [currently: utf-8]\n\nWhether to print out the full DataFrame repr for wide DataFrames across\nmultiple lines, max_columns is still respected, but the output will wrap-\naround across multiple \u201cpages\u201d if its width exceeds display.width. [default:\nTrue] [currently: True]\n\nThe callable should accept a floating point number and return a string with\nthe desired format of the number. This is used in some places like\nSeriesFormatter. See formats.format.EngFormatter for an example. [default:\nNone] [currently: None]\n\nA `border=value` attribute is inserted in the `<table>` tag for the DataFrame\nHTML repr. [default: 1] [currently: 1]\n\nWhether to publish a Table Schema representation for frontends that support\nit. (default: False) [default: False] [currently: False]\n\nWhen True, Jupyter notebook will process table contents using MathJax,\nrendering mathematical expressions enclosed by the dollar symbol. (default:\nTrue) [default: True] [currently: True]\n\nFor DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show\na truncated table (the default from 0.13), or switch to the view from\ndf.info() (the behaviour in earlier versions of pandas). [default: truncate]\n[currently: truncate]\n\nThis specifies if the to_latex method of a Dataframe uses escapes special\ncharacters. Valid values: False,True [default: True] [currently: True]\n\nThis specifies if the to_latex method of a Dataframe uses the longtable\nformat. Valid values: False,True [default: False] [currently: False]\n\nThis specifies if the to_latex method of a Dataframe uses multicolumns to\npretty-print MultiIndex columns. Valid values: False,True [default: True]\n[currently: True]\n\nThis specifies if the to_latex method of a Dataframe uses multicolumns to\npretty-print MultiIndex columns. Valid values: False,True [default: l]\n[currently: l]\n\nThis specifies if the to_latex method of a Dataframe uses multirows to pretty-\nprint MultiIndex rows. Valid values: False,True [default: False] [currently:\nFalse]\n\nWhether to produce a latex DataFrame representation for jupyter environments\nthat support it. (default: False) [default: False] [currently: False]\n\nThis sets the maximum number of categories pandas should output when printing\nout a Categorical or a Series of dtype \u201ccategory\u201d. [default: 8] [currently: 8]\n\nIf max_cols is exceeded, switch to truncate view. Depending on large_repr,\nobjects are either centrally truncated or printed as a summary view. \u2018None\u2019\nvalue means unlimited.\n\nIn case python/IPython is running in a terminal and large_repr equals\n\u2018truncate\u2019 this can be set to 0 and pandas will auto-detect the width of the\nterminal and print a truncated object which fits the screen width. The IPython\nnotebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is\nnot possible to do correct auto-detection. [default: 0] [currently: 0]\n\nThe maximum width in characters of a column in the repr of a pandas data\nstructure. When the column overflows, a \u201c\u2026\u201d placeholder is embedded in the\noutput. A \u2018None\u2019 value means unlimited. [default: 50] [currently: 50]\n\nThe number of items that will be added to dir(\u2026). \u2018None\u2019 value means\nunlimited. Because dir is cached, changing this option will not immediately\naffect already existing dataframes until a column is deleted or added.\n\nThis is for instance used to suggest columns from a dataframe to tab\ncompletion. [default: 100] [currently: 100]\n\nmax_info_columns is used in DataFrame.info method to decide if per column\ninformation will be printed. [default: 100] [currently: 100]\n\ndf.info() will usually show null-counts for each column. For large frames this\ncan be quite slow. max_info_rows and max_info_cols limit this null check only\nto frames with smaller dimensions than specified. [default: 1690785]\n[currently: 1690785]\n\nIf max_rows is exceeded, switch to truncate view. Depending on large_repr,\nobjects are either centrally truncated or printed as a summary view. \u2018None\u2019\nvalue means unlimited.\n\nIn case python/IPython is running in a terminal and large_repr equals\n\u2018truncate\u2019 this can be set to 0 and pandas will auto-detect the height of the\nterminal and print a truncated object which fits the screen height. The\nIPython notebook, IPython qtconsole, or IDLE do not run in a terminal and\nhence it is not possible to do correct auto-detection. [default: 60]\n[currently: 60]\n\nWhen pretty-printing a long sequence, no more then max_seq_items will be\nprinted. If items are omitted, they will be denoted by the addition of \u201c\u2026\u201d to\nthe resulting string.\n\nIf set to None, the number of items to be printed is unlimited. [default: 100]\n[currently: 100]\n\nThis specifies if the memory usage of a DataFrame should be displayed when\ndf.info() is called. Valid values True,False,\u2019deep\u2019 [default: True]\n[currently: True]\n\nThe numbers of rows to show in a truncated view (when max_rows is exceeded).\nIgnored when max_rows is set to None or 0. When set to None, follows the value\nof max_rows. [default: 10] [currently: 10]\n\n\u201csparsify\u201d MultiIndex display (don\u2019t display repeated elements in outer levels\nwithin groups) [default: True] [currently: True]\n\nWhen True, IPython notebook will use html representation for pandas objects\n(if it is available). [default: True] [currently: True]\n\nControls the number of nested levels to process when pretty-printing [default:\n3] [currently: 3]\n\nFloating point output precision in terms of number of places after the\ndecimal, for regular formatting as well as scientific notation. Similar to\n`precision` in `numpy.set_printoptions()`. [default: 6] [currently: 6]\n\nWhether to print out dimensions at the end of DataFrame repr. If \u2018truncate\u2019 is\nspecified, only print out the dimensions if the frame is truncated (e.g. not\ndisplay all rows and/or columns) [default: truncate] [currently: truncate]\n\nWhether to use the Unicode East Asian Width to calculate the display text\nwidth. Enabling this may affect to the performance (default: False) [default:\nFalse] [currently: False]\n\nWhether to use the Unicode East Asian Width to calculate the display text\nwidth. Enabling this may affect to the performance (default: False) [default:\nFalse] [currently: False]\n\nWidth of the display in characters. In case python/IPython is running in a\nterminal this can be set to None and pandas will correctly auto-detect the\nwidth. Note that the IPython notebook, IPython qtconsole, or IDLE do not run\nin a terminal and hence it is not possible to correctly detect the width.\n[default: 80] [currently: 80]\n\nThe default Excel reader engine for \u2018ods\u2019 files. Available options: auto, odf.\n[default: auto] [currently: auto]\n\nThe default Excel writer engine for \u2018ods\u2019 files. Available options: auto, odf.\n[default: auto] [currently: auto]\n\nThe default Excel reader engine for \u2018xls\u2019 files. Available options: auto,\nxlrd. [default: auto] [currently: auto]\n\nThe default Excel writer engine for \u2018xls\u2019 files. Available options: auto,\nxlwt. [default: auto] [currently: auto] (Deprecated, use `` instead.)\n\nThe default Excel reader engine for \u2018xlsb\u2019 files. Available options: auto,\npyxlsb. [default: auto] [currently: auto]\n\nThe default Excel reader engine for \u2018xlsm\u2019 files. Available options: auto,\nxlrd, openpyxl. [default: auto] [currently: auto]\n\nThe default Excel writer engine for \u2018xlsm\u2019 files. Available options: auto,\nopenpyxl. [default: auto] [currently: auto]\n\nThe default Excel reader engine for \u2018xlsx\u2019 files. Available options: auto,\nxlrd, openpyxl. [default: auto] [currently: auto]\n\nThe default Excel writer engine for \u2018xlsx\u2019 files. Available options: auto,\nopenpyxl, xlsxwriter. [default: auto] [currently: auto]\n\ndefault format writing format, if None, then put will default to \u2018fixed\u2019 and\nappend will default to \u2018table\u2019 [default: None] [currently: None]\n\ndrop ALL nan rows when appending to a table [default: False] [currently:\nFalse]\n\nThe default parquet reader/writer engine. Available options: \u2018auto\u2019,\n\u2018pyarrow\u2019, \u2018fastparquet\u2019, the default is \u2018auto\u2019 [default: auto] [currently:\nauto]\n\nThe default sql reader/writer engine. Available options: \u2018auto\u2019, \u2018sqlalchemy\u2019,\nthe default is \u2018auto\u2019 [default: auto] [currently: auto]\n\nRaise an exception, warn, or no action if trying to use chained assignment,\nThe default is warn [default: warn] [currently: warn]\n\nInternal data manager type; can be \u201cblock\u201d or \u201carray\u201d. Defaults to \u201cblock\u201d,\nunless overridden by the \u2018PANDAS_DATA_MANAGER\u2019 environment variable (needs to\nbe set before pandas is imported). [default: block] [currently: block]\n\nWhether to simulate interactive mode for purposes of testing [default: False]\n[currently: False]\n\nThe default storage for StringDtype. [default: python] [currently: python]\n\nTrue means treat None, NaN, INF, -INF as NA (old way), False means None and\nNaN are null, but INF, -INF are not NA (new way). [default: False] [currently:\nFalse]\n\nuse_inf_as_null had been deprecated and will be removed in a future version.\nUse use_inf_as_na instead. [default: False] [currently: False] (Deprecated,\nuse mode.use_inf_as_na instead.)\n\nThe plotting backend to use. The default value is \u201cmatplotlib\u201d, the backend\nprovided with pandas. Other backends can be specified by providing the name of\nthe module that implements the backend. [default: matplotlib] [currently:\nmatplotlib]\n\nWhether to register converters with matplotlib\u2019s units registry for dates,\ntimes, datetimes, and Periods. Toggling to False will remove the converters,\nrestoring any converters that pandas overwrote. [default: auto] [currently:\nauto]\n\nThe character representation for the decimal separator for floats and complex.\n[default: .] [currently: .]\n\nWhether to escape certain characters according to the given context; html or\nlatex. [default: None] [currently: None]\n\nA formatter object to be used as default within `Styler.format`. [default:\nNone] [currently: None]\n\nThe string representation for values identified as missing. [default: None]\n[currently: None]\n\nThe precision for floats and complex numbers. [default: 6] [currently: 6]\n\nThe character representation for thousands separator for floats, int and\ncomplex. [default: None] [currently: None]\n\nIf False will render special CSS classes to table attributes that indicate\nMathjax will not be used in Jupyter Notebook. [default: True] [currently:\nTrue]\n\nThe environment to replace `\\begin{table}`. If \u201clongtable\u201d is used results in\na specific longtable environment format. [default: None] [currently: None]\n\nWhether to add horizontal rules on top and bottom and below the headers.\n[default: False] [currently: False]\n\nThe specifier for horizontal alignment of sparsified LaTeX multicolumns. Pipe\ndecorators can also be added to non-naive values to draw vertical rules, e.g.\n\u201c|r\u201d will draw a rule on the left side of right aligned merged cells.\n[default: r] [currently: r]\n\nThe specifier for vertical alignment of sparsified LaTeX multirows. [default:\nc] [currently: c]\n\nThe encoding used for output HTML and LaTeX files. [default: utf-8]\n[currently: utf-8]\n\nThe maximum number of columns that will be rendered. May still be reduced to\nsatsify `max_elements`, which takes precedence. [default: None] [currently:\nNone]\n\nThe maximum number of data-cell (<td>) elements that will be rendered before\ntrimming will occur over columns, rows or both if needed. [default: 262144]\n[currently: 262144]\n\nThe maximum number of rows that will be rendered. May still be reduced to\nsatsify `max_elements`, which takes precedence. [default: None] [currently:\nNone]\n\nDetermine which output to use in Jupyter Notebook in {\u201chtml\u201d, \u201clatex\u201d}.\n[default: html] [currently: html]\n\nWhether to sparsify the display of hierarchical columns. Setting to False will\ndisplay each explicit level element in a hierarchical key for each column.\n[default: True] [currently: True]\n\nWhether to sparsify the display of a hierarchical index. Setting to False will\ndisplay each explicit level element in a hierarchical key for each row.\n[default: True] [currently: True]\n\n"}, {"name": "pandas.errors.AbstractMethodError", "path": "reference/api/pandas.errors.abstractmethoderror", "type": "General utility functions", "text": "\nRaise this error instead of NotImplementedError for abstract methods while\nkeeping compatibility with Python 2 and Python 3.\n\n"}, {"name": "pandas.errors.AccessorRegistrationWarning", "path": "reference/api/pandas.errors.accessorregistrationwarning", "type": "General utility functions", "text": "\nWarning for attribute conflicts in accessor registration.\n\n"}, {"name": "pandas.errors.DtypeWarning", "path": "reference/api/pandas.errors.dtypewarning", "type": "General utility functions", "text": "\nWarning raised when reading different dtypes in a column from a file.\n\nRaised for a dtype incompatibility. This can happen whenever read_csv or\nread_table encounter non-uniform dtypes in a column(s) of a given CSV file.\n\nSee also\n\nRead CSV (comma-separated) file into a DataFrame.\n\nRead general delimited file into a DataFrame.\n\nNotes\n\nThis warning is issued when dealing with larger files because the dtype\nchecking happens per chunk read.\n\nDespite the warning, the CSV file is read with mixed types in a single column\nwhich will be an object type. See the examples below to better understand this\nissue.\n\nExamples\n\nThis example creates and reads a large CSV file with a column that contains\nint and str.\n\nImportant to notice that `df2` will contain both str and int for the same\ninput, \u20181\u2019.\n\nOne way to solve this issue is using the dtype parameter in the read_csv and\nread_table functions to explicit the conversion:\n\nNo warning was issued.\n\n"}, {"name": "pandas.errors.DuplicateLabelError", "path": "reference/api/pandas.errors.duplicatelabelerror", "type": "General utility functions", "text": "\nError raised when an operation would introduce duplicate labels.\n\nNew in version 1.2.0.\n\nExamples\n\n"}, {"name": "pandas.errors.EmptyDataError", "path": "reference/api/pandas.errors.emptydataerror", "type": "General utility functions", "text": "\nException that is thrown in pd.read_csv (by both the C and Python engines)\nwhen empty data or header is encountered.\n\n"}, {"name": "pandas.errors.IntCastingNaNError", "path": "reference/api/pandas.errors.intcastingnanerror", "type": "General utility functions", "text": "\nRaised when attempting an astype operation on an array with NaN to an integer\ndtype.\n\n"}, {"name": "pandas.errors.InvalidIndexError", "path": "reference/api/pandas.errors.invalidindexerror", "type": "General utility functions", "text": "\nException raised when attempting to use an invalid index key.\n\nNew in version 1.1.0.\n\n"}, {"name": "pandas.errors.MergeError", "path": "reference/api/pandas.errors.mergeerror", "type": "General utility functions", "text": "\nError raised when problems arise during merging due to problems with input\ndata. Subclass of ValueError.\n\n"}, {"name": "pandas.errors.NullFrequencyError", "path": "reference/api/pandas.errors.nullfrequencyerror", "type": "General utility functions", "text": "\nError raised when a null freq attribute is used in an operation that needs a\nnon-null frequency, particularly DatetimeIndex.shift, TimedeltaIndex.shift,\nPeriodIndex.shift.\n\n"}, {"name": "pandas.errors.NumbaUtilError", "path": "reference/api/pandas.errors.numbautilerror", "type": "General utility functions", "text": "\nError raised for unsupported Numba engine routines.\n\n"}, {"name": "pandas.errors.OptionError", "path": "reference/api/pandas.errors.optionerror", "type": "General utility functions", "text": "\nException for pandas.options, backwards compatible with KeyError checks.\n\n"}, {"name": "pandas.errors.OutOfBoundsDatetime", "path": "reference/api/pandas.errors.outofboundsdatetime", "type": "General utility functions", "text": "\n\n"}, {"name": "pandas.errors.OutOfBoundsTimedelta", "path": "reference/api/pandas.errors.outofboundstimedelta", "type": "General utility functions", "text": "\nRaised when encountering a timedelta value that cannot be represented as a\ntimedelta64[ns].\n\n"}, {"name": "pandas.errors.ParserError", "path": "reference/api/pandas.errors.parsererror", "type": "General utility functions", "text": "\nException that is raised by an error encountered in parsing file contents.\n\nThis is a generic error raised for errors encountered when functions like\nread_csv or read_html are parsing contents of a file.\n\nSee also\n\nRead CSV (comma-separated) file into a DataFrame.\n\nRead HTML table into a DataFrame.\n\n"}, {"name": "pandas.errors.ParserWarning", "path": "reference/api/pandas.errors.parserwarning", "type": "General utility functions", "text": "\nWarning raised when reading a file that doesn\u2019t use the default \u2018c\u2019 parser.\n\nRaised by pd.read_csv and pd.read_table when it is necessary to change\nparsers, generally from the default \u2018c\u2019 parser to \u2018python\u2019.\n\nIt happens due to a lack of support or functionality for parsing a particular\nattribute of a CSV file with the requested engine.\n\nCurrently, \u2018c\u2019 unsupported options include the following parameters:\n\nsep other than a single character (e.g. regex separators)\n\nskipfooter higher than 0\n\nsep=None with delim_whitespace=False\n\nThe warning can be avoided by adding engine=\u2019python\u2019 as a parameter in\npd.read_csv and pd.read_table methods.\n\nSee also\n\nRead CSV (comma-separated) file into DataFrame.\n\nRead general delimited file into DataFrame.\n\nExamples\n\nUsing a sep in pd.read_csv other than a single character:\n\nAdding engine=\u2019python\u2019 to pd.read_csv removes the Warning:\n\n"}, {"name": "pandas.errors.PerformanceWarning", "path": "reference/api/pandas.errors.performancewarning", "type": "General utility functions", "text": "\nWarning raised when there is a possible performance impact.\n\n"}, {"name": "pandas.errors.UnsortedIndexError", "path": "reference/api/pandas.errors.unsortedindexerror", "type": "General utility functions", "text": "\nError raised when attempting to get a slice of a MultiIndex, and the index has\nnot been lexsorted. Subclass of KeyError.\n\n"}, {"name": "pandas.errors.UnsupportedFunctionCall", "path": "reference/api/pandas.errors.unsupportedfunctioncall", "type": "General utility functions", "text": "\nException raised when attempting to call a numpy function on a pandas object,\nbut that function is not supported by the object e.g.\n`np.cumsum(groupby_object)`.\n\n"}, {"name": "pandas.eval", "path": "reference/api/pandas.eval", "type": "General functions", "text": "\nEvaluate a Python expression as a string using various backends.\n\nThe following arithmetic operations are supported: `+`, `-`, `*`, `/`, `**`,\n`%`, `//` (python engine only) along with the following boolean operations:\n`|` (or), `&` (and), and `~` (not). Additionally, the `'pandas'` parser allows\nthe use of `and`, `or`, and `not` with the same semantics as the corresponding\nbitwise operators. `Series` and `DataFrame` objects are supported and behave\nas they would with plain ol\u2019 Python evaluation.\n\nThe expression to evaluate. This string cannot contain any Python statements,\nonly Python expressions.\n\nThe parser to use to construct the syntax tree from the expression. The\ndefault of `'pandas'` parses code slightly different than standard Python.\nAlternatively, you can parse an expression using the `'python'` parser to\nretain strict Python semantics. See the enhancing performance documentation\nfor more details.\n\nThe engine used to evaluate the expression. Supported engines are\n\nNone : tries to use `numexpr`, falls back to `python`\n\nnumexpr for large speed ups in complex expressions with large frames.\n\nlevel python. This engine is generally not that useful.\n\nMore backends may be available in the future.\n\nWhether to use true division, like in Python >= 3.\n\nDeprecated since version 1.0.0.\n\nA dictionary of local variables, taken from locals() by default.\n\nA dictionary of global variables, taken from globals() by default.\n\nA list of objects implementing the `__getitem__` special method that you can\nuse to inject an additional collection of namespaces to use for variable\nlookup. For example, this is used in the `query()` method to inject the\n`DataFrame.index` and `DataFrame.columns` variables that refer to their\nrespective `DataFrame` instance attributes.\n\nThe number of prior stack frames to traverse and add to the current scope.\nMost users will not need to change this parameter.\n\nThis is the target object for assignment. It is used when there is variable\nassignment in the expression. If so, then target must support item assignment\nwith string keys, and if a copy is being returned, it must also support\n.copy().\n\nIf target is provided, and the expression mutates target, whether to modify\ntarget inplace. Otherwise, return a copy of target with the mutation.\n\nThe completion value of evaluating the given code or None if `inplace=True`.\n\nThere are many instances where such an error can be raised:\n\ntarget=None, but the expression is multiline.\n\nThe expression is multiline, but not all them have item assignment. An example\nof such an arrangement is this:\n\na = b + 1 a + 2\n\nHere, there are expressions on different lines, making it multiline, but the\nlast line has no variable assigned to the output of a + 2.\n\ninplace=True, but the expression is missing item assignment.\n\nItem assignment is provided, but the target does not support string item\nassignment.\n\nItem assignment is provided and inplace=False, but the target does not support\nthe .copy() method\n\nSee also\n\nEvaluates a boolean expression to query the columns of a frame.\n\nEvaluate a string describing operations on DataFrame columns.\n\nNotes\n\nThe `dtype` of any objects involved in an arithmetic `%` operation are\nrecursively cast to `float64`.\n\nSee the enhancing performance documentation for more details.\n\nExamples\n\nWe can add a new column using `pd.eval`:\n\n"}, {"name": "pandas.ExcelFile.parse", "path": "reference/api/pandas.excelfile.parse", "type": "General functions", "text": "\nParse specified sheet(s) into a DataFrame.\n\nEquivalent to read_excel(ExcelFile, \u2026) See the read_excel docstring for more\ninfo on accepted parameters.\n\nDataFrame from the passed in Excel file.\n\n"}, {"name": "pandas.ExcelWriter", "path": "reference/api/pandas.excelwriter", "type": "General functions", "text": "\nClass for writing DataFrame objects into excel sheets.\n\nDefault is to use : * xlwt for xls * xlsxwriter for xlsx if xlsxwriter is\ninstalled otherwise openpyxl * odf for ods. See DataFrame.to_excel for typical\nusage.\n\nThe writer should be used as a context manager. Otherwise, call close() to\nsave and close any opened file handles.\n\nPath to xls or xlsx or ods file.\n\nEngine to use for writing. If None, defaults to `io.excel.<extension>.writer`.\nNOTE: can only be passed as a keyword argument.\n\nDeprecated since version 1.2.0: As the xlwt package is no longer maintained,\nthe `xlwt` engine will be removed in a future version of pandas.\n\nFormat string for dates written into Excel files (e.g. \u2018YYYY-MM-DD\u2019).\n\nFormat string for datetime objects written into Excel files. (e.g. \u2018YYYY-MM-DD\nHH:MM:SS\u2019).\n\nFile mode to use (write or append). Append does not work with fsspec URLs.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc., if using a URL that will be parsed by\n`fsspec`, e.g., starting \u201cs3://\u201d, \u201cgcs://\u201d.\n\nNew in version 1.2.0.\n\nHow to behave when trying to write to a sheet that already exists (append mode\nonly).\n\nerror: raise a ValueError.\n\nnew: Create a new sheet, with a name determined by the engine.\n\nreplace: Delete the contents of the sheet before writing to it.\n\noverlay: Write contents to the existing sheet without removing the old\ncontents.\n\nNew in version 1.3.0.\n\nChanged in version 1.4.0: Added `overlay` option\n\nKeyword arguments to be passed into the engine. These will be passed to the\nfollowing functions of the respective engines:\n\nxlsxwriter: `xlsxwriter.Workbook(file, **engine_kwargs)`\n\nopenpyxl (write mode): `openpyxl.Workbook(**engine_kwargs)`\n\nopenpyxl (append mode): `openpyxl.load_workbook(file, **engine_kwargs)`\n\nodswriter: `odf.opendocument.OpenDocumentSpreadsheet(**engine_kwargs)`\n\nNew in version 1.3.0.\n\nKeyword arguments to be passed into the engine.\n\nDeprecated since version 1.3.0: Use engine_kwargs instead.\n\nNotes\n\nNone of the methods and properties are considered public.\n\nFor compatibility with CSV writers, ExcelWriter serializes lists and dicts to\nstrings before writing.\n\nExamples\n\nDefault usage:\n\nTo write to separate sheets in a single file:\n\nYou can set the date format or datetime format:\n\nYou can also append to an existing Excel file:\n\nHere, the if_sheet_exists parameter can be set to replace a sheet if it\nalready exists:\n\nYou can also write multiple DataFrames to a single sheet. Note that the\n`if_sheet_exists` parameter needs to be set to `overlay`:\n\nYou can store Excel file in RAM:\n\nYou can pack Excel file into zip archive:\n\nYou can specify additional arguments to the underlying engine:\n\nIn append mode, `engine_kwargs` are passed through to openpyxl\u2019s\n`load_workbook`:\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.factorize", "path": "reference/api/pandas.factorize", "type": "General functions", "text": "\nEncode the object as an enumerated type or categorical variable.\n\nThis method is useful for obtaining a numeric representation of an array when\nall that matters is identifying distinct values. factorize is available as\nboth a top-level function `pandas.factorize()`, and as a method\n`Series.factorize()` and `Index.factorize()`.\n\nA 1-D sequence. Sequences that aren\u2019t pandas objects are coerced to ndarrays\nbefore factorization.\n\nSort uniques and shuffle codes to maintain the relationship.\n\nValue to mark \u201cnot found\u201d. If None, will not drop the NaN from the uniques of\nthe values.\n\nChanged in version 1.1.2.\n\nHint to the hashtable sizer.\n\nAn integer ndarray that\u2019s an indexer into uniques. `uniques.take(codes)` will\nhave the same values as values.\n\nThe unique valid values. When values is Categorical, uniques is a Categorical.\nWhen values is some other pandas object, an Index is returned. Otherwise, a\n1-D ndarray is returned.\n\nNote\n\nEven if there\u2019s a missing value in values, uniques will not contain an entry\nfor it.\n\nSee also\n\nDiscretize continuous-valued array.\n\nFind the unique value in an array.\n\nExamples\n\nThese examples all show factorize as a top-level method like\n`pd.factorize(values)`. The results are identical for methods like\n`Series.factorize()`.\n\nWith `sort=True`, the uniques will be sorted, and codes will be shuffled so\nthat the relationship is the maintained.\n\nMissing values are indicated in codes with na_sentinel (`-1` by default). Note\nthat missing values are never included in uniques.\n\nThus far, we\u2019ve only factorized lists (which are internally coerced to NumPy\narrays). When factorizing pandas objects, the type of uniques will differ. For\nCategoricals, a Categorical is returned.\n\nNotice that `'b'` is in `uniques.categories`, despite not being present in\n`cat.values`.\n\nFor all other pandas objects, an Index of the appropriate type is returned.\n\nIf NaN is in the values, and we want to include NaN in the uniques of the\nvalues, it can be achieved by setting `na_sentinel=None`.\n\n"}, {"name": "pandas.Flags", "path": "reference/api/pandas.flags", "type": "General functions", "text": "\nFlags that apply to pandas objects.\n\nNew in version 1.2.0.\n\nThe object these flags are associated with.\n\nWhether to allow duplicate labels in this object. By default, duplicate labels\nare permitted. Setting this to `False` will cause an\n`errors.DuplicateLabelError` to be raised when index (or columns for\nDataFrame) is not unique, or any subsequent operation on introduces\nduplicates. See Disallowing Duplicate Labels for more.\n\nWarning\n\nThis is an experimental feature. Currently, many methods fail to propagate the\n`allows_duplicate_labels` value. In future versions it is expected that every\nmethod taking or returning one or more DataFrame or Series objects will\npropagate `allows_duplicate_labels`.\n\nNotes\n\nAttributes can be set in two ways\n\nAttributes\n\n`allows_duplicate_labels`\n\nWhether this object allows duplicate labels.\n\n"}, {"name": "pandas.Flags.allows_duplicate_labels", "path": "reference/api/pandas.flags.allows_duplicate_labels", "type": "General functions", "text": "\nWhether this object allows duplicate labels.\n\nSetting `allows_duplicate_labels=False` ensures that the index (and columns of\na DataFrame) are unique. Most methods that accept and return a Series or\nDataFrame will propagate the value of `allows_duplicate_labels`.\n\nSee Duplicate Labels for more.\n\nSee also\n\nSet global metadata on this object.\n\nSet global flags on this object.\n\nExamples\n\n"}, {"name": "pandas.Float64Index", "path": "reference/api/pandas.float64index", "type": "Index Objects", "text": "\nImmutable sequence used for indexing and alignment. The basic object storing\naxis labels for all pandas objects. Float64Index is a special case of Index\nwith purely float labels. .\n\nDeprecated since version 1.4.0: In pandas v2.0 Float64Index will be removed\nand `NumericIndex` used instead. Float64Index will remain fully functional for\nthe duration of pandas 1.x.\n\nMake a copy of input ndarray.\n\nName to be stored in the index.\n\nSee also\n\nThe base pandas Index type.\n\nIndex of numpy int/uint/float data.\n\nNotes\n\nAn Index instance can only contain hashable objects.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.get_dummies", "path": "reference/api/pandas.get_dummies", "type": "General functions", "text": "\nConvert categorical variable into dummy/indicator variables.\n\nData of which to get dummy indicators.\n\nString to append DataFrame column names. Pass a list with length equal to the\nnumber of columns when calling get_dummies on a DataFrame. Alternatively,\nprefix can be a dictionary mapping column names to prefixes.\n\nIf appending prefix, separator/delimiter to use. Or pass a list or dictionary\nas with prefix.\n\nAdd a column to indicate NaNs, if False NaNs are ignored.\n\nColumn names in the DataFrame to be encoded. If columns is None then all the\ncolumns with object or category dtype will be converted.\n\nWhether the dummy-encoded columns should be backed by a `SparseArray` (True)\nor a regular NumPy array (False).\n\nWhether to get k-1 dummies out of k categorical levels by removing the first\nlevel.\n\nData type for new columns. Only a single dtype is allowed.\n\nDummy-coded data.\n\nSee also\n\nConvert Series to dummy codes.\n\nExamples\n\n"}, {"name": "pandas.get_option", "path": "reference/api/pandas.get_option", "type": "General utility functions", "text": "\nRetrieves the value of the specified option.\n\nAvailable options:\n\ncompute.[use_bottleneck, use_numba, use_numexpr]\n\ndisplay.[chop_threshold, colheader_justify, column_space, date_dayfirst,\ndate_yearfirst, encoding, expand_frame_repr, float_format]\n\ndisplay.html.[border, table_schema, use_mathjax]\n\ndisplay.[large_repr]\n\ndisplay.latex.[escape, longtable, multicolumn, multicolumn_format, multirow,\nrepr]\n\ndisplay.[max_categories, max_columns, max_colwidth, max_dir_items,\nmax_info_columns, max_info_rows, max_rows, max_seq_items, memory_usage,\nmin_rows, multi_sparse, notebook_repr_html, pprint_nest_depth, precision,\nshow_dimensions]\n\ndisplay.unicode.[ambiguous_as_wide, east_asian_width]\n\ndisplay.[width]\n\nio.excel.ods.[reader, writer]\n\nio.excel.xls.[reader, writer]\n\nio.excel.xlsb.[reader]\n\nio.excel.xlsm.[reader, writer]\n\nio.excel.xlsx.[reader, writer]\n\nio.hdf.[default_format, dropna_table]\n\nio.parquet.[engine]\n\nio.sql.[engine]\n\nmode.[chained_assignment, data_manager, sim_interactive, string_storage,\nuse_inf_as_na, use_inf_as_null]\n\nplotting.[backend]\n\nplotting.matplotlib.[register_converters]\n\nstyler.format.[decimal, escape, formatter, na_rep, precision, thousands]\n\nstyler.html.[mathjax]\n\nstyler.latex.[environment, hrules, multicol_align, multirow_align]\n\nstyler.render.[encoding, max_columns, max_elements, max_rows, repr]\n\nstyler.sparse.[columns, index]\n\nRegexp which should match a single option. Note: partial matches are supported\nfor convenience, but unless you use the full option name (e.g.\nx.y.z.option_name), your code may break in future versions if new options with\nsimilar names are introduced.\n\nNotes\n\nThe available options with its descriptions:\n\nUse the bottleneck library to accelerate if it is installed, the default is\nTrue Valid values: False,True [default: True] [currently: True]\n\nUse the numba engine option for select operations if it is installed, the\ndefault is False Valid values: False,True [default: False] [currently: False]\n\nUse the numexpr library to accelerate computation if it is installed, the\ndefault is True Valid values: False,True [default: True] [currently: True]\n\nif set to a float value, all float values smaller then the given threshold\nwill be displayed as exactly 0 by repr and friends. [default: None]\n[currently: None]\n\nControls the justification of column headers. used by DataFrameFormatter.\n[default: right] [currently: right]\n\n[default: 12] [currently: 12]\n\nWhen True, prints and parses dates with the day first, eg 20/01/2005 [default:\nFalse] [currently: False]\n\nWhen True, prints and parses dates with the year first, eg 2005/01/20\n[default: False] [currently: False]\n\nDefaults to the detected encoding of the console. Specifies the encoding to be\nused for strings returned by to_string, these are generally strings meant to\nbe displayed on the console. [default: utf-8] [currently: utf-8]\n\nWhether to print out the full DataFrame repr for wide DataFrames across\nmultiple lines, max_columns is still respected, but the output will wrap-\naround across multiple \u201cpages\u201d if its width exceeds display.width. [default:\nTrue] [currently: True]\n\nThe callable should accept a floating point number and return a string with\nthe desired format of the number. This is used in some places like\nSeriesFormatter. See formats.format.EngFormatter for an example. [default:\nNone] [currently: None]\n\nA `border=value` attribute is inserted in the `<table>` tag for the DataFrame\nHTML repr. [default: 1] [currently: 1]\n\nWhether to publish a Table Schema representation for frontends that support\nit. (default: False) [default: False] [currently: False]\n\nWhen True, Jupyter notebook will process table contents using MathJax,\nrendering mathematical expressions enclosed by the dollar symbol. (default:\nTrue) [default: True] [currently: True]\n\nFor DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show\na truncated table (the default from 0.13), or switch to the view from\ndf.info() (the behaviour in earlier versions of pandas). [default: truncate]\n[currently: truncate]\n\nThis specifies if the to_latex method of a Dataframe uses escapes special\ncharacters. Valid values: False,True [default: True] [currently: True]\n\nThis specifies if the to_latex method of a Dataframe uses the longtable\nformat. Valid values: False,True [default: False] [currently: False]\n\nThis specifies if the to_latex method of a Dataframe uses multicolumns to\npretty-print MultiIndex columns. Valid values: False,True [default: True]\n[currently: True]\n\nThis specifies if the to_latex method of a Dataframe uses multicolumns to\npretty-print MultiIndex columns. Valid values: False,True [default: l]\n[currently: l]\n\nThis specifies if the to_latex method of a Dataframe uses multirows to pretty-\nprint MultiIndex rows. Valid values: False,True [default: False] [currently:\nFalse]\n\nWhether to produce a latex DataFrame representation for jupyter environments\nthat support it. (default: False) [default: False] [currently: False]\n\nThis sets the maximum number of categories pandas should output when printing\nout a Categorical or a Series of dtype \u201ccategory\u201d. [default: 8] [currently: 8]\n\nIf max_cols is exceeded, switch to truncate view. Depending on large_repr,\nobjects are either centrally truncated or printed as a summary view. \u2018None\u2019\nvalue means unlimited.\n\nIn case python/IPython is running in a terminal and large_repr equals\n\u2018truncate\u2019 this can be set to 0 and pandas will auto-detect the width of the\nterminal and print a truncated object which fits the screen width. The IPython\nnotebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is\nnot possible to do correct auto-detection. [default: 0] [currently: 0]\n\nThe maximum width in characters of a column in the repr of a pandas data\nstructure. When the column overflows, a \u201c\u2026\u201d placeholder is embedded in the\noutput. A \u2018None\u2019 value means unlimited. [default: 50] [currently: 50]\n\nThe number of items that will be added to dir(\u2026). \u2018None\u2019 value means\nunlimited. Because dir is cached, changing this option will not immediately\naffect already existing dataframes until a column is deleted or added.\n\nThis is for instance used to suggest columns from a dataframe to tab\ncompletion. [default: 100] [currently: 100]\n\nmax_info_columns is used in DataFrame.info method to decide if per column\ninformation will be printed. [default: 100] [currently: 100]\n\ndf.info() will usually show null-counts for each column. For large frames this\ncan be quite slow. max_info_rows and max_info_cols limit this null check only\nto frames with smaller dimensions than specified. [default: 1690785]\n[currently: 1690785]\n\nIf max_rows is exceeded, switch to truncate view. Depending on large_repr,\nobjects are either centrally truncated or printed as a summary view. \u2018None\u2019\nvalue means unlimited.\n\nIn case python/IPython is running in a terminal and large_repr equals\n\u2018truncate\u2019 this can be set to 0 and pandas will auto-detect the height of the\nterminal and print a truncated object which fits the screen height. The\nIPython notebook, IPython qtconsole, or IDLE do not run in a terminal and\nhence it is not possible to do correct auto-detection. [default: 60]\n[currently: 60]\n\nWhen pretty-printing a long sequence, no more then max_seq_items will be\nprinted. If items are omitted, they will be denoted by the addition of \u201c\u2026\u201d to\nthe resulting string.\n\nIf set to None, the number of items to be printed is unlimited. [default: 100]\n[currently: 100]\n\nThis specifies if the memory usage of a DataFrame should be displayed when\ndf.info() is called. Valid values True,False,\u2019deep\u2019 [default: True]\n[currently: True]\n\nThe numbers of rows to show in a truncated view (when max_rows is exceeded).\nIgnored when max_rows is set to None or 0. When set to None, follows the value\nof max_rows. [default: 10] [currently: 10]\n\n\u201csparsify\u201d MultiIndex display (don\u2019t display repeated elements in outer levels\nwithin groups) [default: True] [currently: True]\n\nWhen True, IPython notebook will use html representation for pandas objects\n(if it is available). [default: True] [currently: True]\n\nControls the number of nested levels to process when pretty-printing [default:\n3] [currently: 3]\n\nFloating point output precision in terms of number of places after the\ndecimal, for regular formatting as well as scientific notation. Similar to\n`precision` in `numpy.set_printoptions()`. [default: 6] [currently: 6]\n\nWhether to print out dimensions at the end of DataFrame repr. If \u2018truncate\u2019 is\nspecified, only print out the dimensions if the frame is truncated (e.g. not\ndisplay all rows and/or columns) [default: truncate] [currently: truncate]\n\nWhether to use the Unicode East Asian Width to calculate the display text\nwidth. Enabling this may affect to the performance (default: False) [default:\nFalse] [currently: False]\n\nWhether to use the Unicode East Asian Width to calculate the display text\nwidth. Enabling this may affect to the performance (default: False) [default:\nFalse] [currently: False]\n\nWidth of the display in characters. In case python/IPython is running in a\nterminal this can be set to None and pandas will correctly auto-detect the\nwidth. Note that the IPython notebook, IPython qtconsole, or IDLE do not run\nin a terminal and hence it is not possible to correctly detect the width.\n[default: 80] [currently: 80]\n\nThe default Excel reader engine for \u2018ods\u2019 files. Available options: auto, odf.\n[default: auto] [currently: auto]\n\nThe default Excel writer engine for \u2018ods\u2019 files. Available options: auto, odf.\n[default: auto] [currently: auto]\n\nThe default Excel reader engine for \u2018xls\u2019 files. Available options: auto,\nxlrd. [default: auto] [currently: auto]\n\nThe default Excel writer engine for \u2018xls\u2019 files. Available options: auto,\nxlwt. [default: auto] [currently: auto] (Deprecated, use `` instead.)\n\nThe default Excel reader engine for \u2018xlsb\u2019 files. Available options: auto,\npyxlsb. [default: auto] [currently: auto]\n\nThe default Excel reader engine for \u2018xlsm\u2019 files. Available options: auto,\nxlrd, openpyxl. [default: auto] [currently: auto]\n\nThe default Excel writer engine for \u2018xlsm\u2019 files. Available options: auto,\nopenpyxl. [default: auto] [currently: auto]\n\nThe default Excel reader engine for \u2018xlsx\u2019 files. Available options: auto,\nxlrd, openpyxl. [default: auto] [currently: auto]\n\nThe default Excel writer engine for \u2018xlsx\u2019 files. Available options: auto,\nopenpyxl, xlsxwriter. [default: auto] [currently: auto]\n\ndefault format writing format, if None, then put will default to \u2018fixed\u2019 and\nappend will default to \u2018table\u2019 [default: None] [currently: None]\n\ndrop ALL nan rows when appending to a table [default: False] [currently:\nFalse]\n\nThe default parquet reader/writer engine. Available options: \u2018auto\u2019,\n\u2018pyarrow\u2019, \u2018fastparquet\u2019, the default is \u2018auto\u2019 [default: auto] [currently:\nauto]\n\nThe default sql reader/writer engine. Available options: \u2018auto\u2019, \u2018sqlalchemy\u2019,\nthe default is \u2018auto\u2019 [default: auto] [currently: auto]\n\nRaise an exception, warn, or no action if trying to use chained assignment,\nThe default is warn [default: warn] [currently: warn]\n\nInternal data manager type; can be \u201cblock\u201d or \u201carray\u201d. Defaults to \u201cblock\u201d,\nunless overridden by the \u2018PANDAS_DATA_MANAGER\u2019 environment variable (needs to\nbe set before pandas is imported). [default: block] [currently: block]\n\nWhether to simulate interactive mode for purposes of testing [default: False]\n[currently: False]\n\nThe default storage for StringDtype. [default: python] [currently: python]\n\nTrue means treat None, NaN, INF, -INF as NA (old way), False means None and\nNaN are null, but INF, -INF are not NA (new way). [default: False] [currently:\nFalse]\n\nuse_inf_as_null had been deprecated and will be removed in a future version.\nUse use_inf_as_na instead. [default: False] [currently: False] (Deprecated,\nuse mode.use_inf_as_na instead.)\n\nThe plotting backend to use. The default value is \u201cmatplotlib\u201d, the backend\nprovided with pandas. Other backends can be specified by providing the name of\nthe module that implements the backend. [default: matplotlib] [currently:\nmatplotlib]\n\nWhether to register converters with matplotlib\u2019s units registry for dates,\ntimes, datetimes, and Periods. Toggling to False will remove the converters,\nrestoring any converters that pandas overwrote. [default: auto] [currently:\nauto]\n\nThe character representation for the decimal separator for floats and complex.\n[default: .] [currently: .]\n\nWhether to escape certain characters according to the given context; html or\nlatex. [default: None] [currently: None]\n\nA formatter object to be used as default within `Styler.format`. [default:\nNone] [currently: None]\n\nThe string representation for values identified as missing. [default: None]\n[currently: None]\n\nThe precision for floats and complex numbers. [default: 6] [currently: 6]\n\nThe character representation for thousands separator for floats, int and\ncomplex. [default: None] [currently: None]\n\nIf False will render special CSS classes to table attributes that indicate\nMathjax will not be used in Jupyter Notebook. [default: True] [currently:\nTrue]\n\nThe environment to replace `\\begin{table}`. If \u201clongtable\u201d is used results in\na specific longtable environment format. [default: None] [currently: None]\n\nWhether to add horizontal rules on top and bottom and below the headers.\n[default: False] [currently: False]\n\nThe specifier for horizontal alignment of sparsified LaTeX multicolumns. Pipe\ndecorators can also be added to non-naive values to draw vertical rules, e.g.\n\u201c|r\u201d will draw a rule on the left side of right aligned merged cells.\n[default: r] [currently: r]\n\nThe specifier for vertical alignment of sparsified LaTeX multirows. [default:\nc] [currently: c]\n\nThe encoding used for output HTML and LaTeX files. [default: utf-8]\n[currently: utf-8]\n\nThe maximum number of columns that will be rendered. May still be reduced to\nsatsify `max_elements`, which takes precedence. [default: None] [currently:\nNone]\n\nThe maximum number of data-cell (<td>) elements that will be rendered before\ntrimming will occur over columns, rows or both if needed. [default: 262144]\n[currently: 262144]\n\nThe maximum number of rows that will be rendered. May still be reduced to\nsatsify `max_elements`, which takes precedence. [default: None] [currently:\nNone]\n\nDetermine which output to use in Jupyter Notebook in {\u201chtml\u201d, \u201clatex\u201d}.\n[default: html] [currently: html]\n\nWhether to sparsify the display of hierarchical columns. Setting to False will\ndisplay each explicit level element in a hierarchical key for each column.\n[default: True] [currently: True]\n\nWhether to sparsify the display of a hierarchical index. Setting to False will\ndisplay each explicit level element in a hierarchical key for each row.\n[default: True] [currently: True]\n\n"}, {"name": "pandas.Grouper", "path": "reference/api/pandas.grouper", "type": "GroupBy", "text": "\nA Grouper allows the user to specify a groupby instruction for an object.\n\nThis specification will select a column via the key parameter, or if the level\nand/or axis parameters are given, a level of the index of the target object.\n\nIf axis and/or level are passed as keywords to both Grouper and groupby, the\nvalues passed to Grouper take precedence.\n\nGroupby key, which selects the grouping column of the target.\n\nThe level for the target index.\n\nThis will groupby the specified frequency if the target selection (via key or\nlevel) is a datetime-like object. For full specification of available\nfrequencies, please see here.\n\nNumber/name of the axis.\n\nWhether to sort the resulting labels.\n\nClosed end of interval. Only when freq parameter is passed.\n\nInterval boundary to use for labeling. Only when freq parameter is passed.\n\nIf grouper is PeriodIndex and freq parameter is passed.\n\nOnly when freq parameter is passed. For frequencies that evenly subdivide 1\nday, the \u201corigin\u201d of the aggregated intervals. For example, for \u20185min\u2019\nfrequency, base could range from 0 through 4. Defaults to 0.\n\nDeprecated since version 1.1.0: The new arguments that you should use are\n\u2018offset\u2019 or \u2018origin\u2019.\n\nOnly when freq parameter is passed.\n\nDeprecated since version 1.1.0: loffset is only working for `.resample(...)`\nand not for Grouper (GH28302). However, loffset is also deprecated for\n`.resample(...)` See: `DataFrame.resample`\n\nThe timestamp on which to adjust the grouping. The timezone of origin must\nmatch the timezone of the index. If string, must be one of the following:\n\n\u2018epoch\u2019: origin is 1970-01-01\n\n\u2018start\u2019: origin is the first value of the timeseries\n\n\u2018start_day\u2019: origin is the first day at midnight of the timeseries\n\nNew in version 1.1.0.\n\n\u2018end\u2019: origin is the last value of the timeseries\n\n\u2018end_day\u2019: origin is the ceiling midnight of the last day\n\nNew in version 1.3.0.\n\nAn offset timedelta added to the origin.\n\nNew in version 1.1.0.\n\nIf True, and if group keys contain NA values, NA values together with\nrow/column will be dropped. If False, NA values will also be treated as the\nkey in groups.\n\nNew in version 1.2.0.\n\nExamples\n\nSyntactic sugar for `df.groupby('A')`\n\nSpecify a resample operation on the column \u2018Publish date\u2019\n\nIf you want to adjust the start of the bins based on a fixed timestamp:\n\nIf you want to adjust the start of the bins with an offset Timedelta, the two\nfollowing lines are equivalent:\n\nTo replace the use of the deprecated base argument, you can now use offset, in\nthis example it is equivalent to have base=2:\n\nAttributes\n\nax\n\ngroups\n\n"}, {"name": "pandas.HDFStore.append", "path": "reference/api/pandas.hdfstore.append", "type": "Input/output", "text": "\nAppend to Table in file. Node must already exist and be Table format.\n\nFormat to use when storing object in HDFStore. Value can be one of:\n\nTable format. Write as a PyTables Table structure which may perform worse but\nallow more flexible operations like searching / selecting subsets of the data.\n\nAppend the input data to the existing.\n\nList of columns to create as indexed data columns for on-disk queries, or True\nto use all columns. By default only the axes of the object are indexed. See\nhere.\n\nDo not write an ALL nan row to the store settable by the option\n\u2018io.hdf.dropna_table\u2019.\n\nNotes\n\nDoes not check if data being appended overlaps with existing data in the\ntable, so be careful\n\n"}, {"name": "pandas.HDFStore.get", "path": "reference/api/pandas.hdfstore.get", "type": "Input/output", "text": "\nRetrieve pandas object stored in file.\n\nSame type as object stored in file.\n\n"}, {"name": "pandas.HDFStore.groups", "path": "reference/api/pandas.hdfstore.groups", "type": "Input/output", "text": "\nReturn a list of all the top-level nodes.\n\nEach node returned is not a pandas storage object.\n\nList of objects.\n\n"}, {"name": "pandas.HDFStore.info", "path": "reference/api/pandas.hdfstore.info", "type": "Input/output", "text": "\nPrint detailed information on the store.\n\n"}, {"name": "pandas.HDFStore.keys", "path": "reference/api/pandas.hdfstore.keys", "type": "Input/output", "text": "\nReturn a list of keys corresponding to objects stored in HDFStore.\n\nWhen kind equals \u2018pandas\u2019 return pandas objects. When kind equals \u2018native\u2019\nreturn native HDF5 Table objects.\n\nNew in version 1.1.0.\n\nList of ABSOLUTE path-names (e.g. have the leading \u2018/\u2019).\n\n"}, {"name": "pandas.HDFStore.put", "path": "reference/api/pandas.hdfstore.put", "type": "Input/output", "text": "\nStore object in HDFStore.\n\nFormat to use when storing object in HDFStore. Value can be one of:\n\nFixed format. Fast writing/reading. Not-appendable, nor searchable.\n\nTable format. Write as a PyTables Table structure which may perform worse but\nallow more flexible operations like searching / selecting subsets of the data.\n\nThis will force Table format, append the input data to the existing.\n\nList of columns to create as data columns, or True to use all columns. See\nhere.\n\nProvide an encoding for strings.\n\nParameter is propagated to \u2018create_table\u2019 method of \u2018PyTables\u2019. If set to\nFalse it enables to have the same h5 files (same hashes) independent on\ncreation time.\n\nNew in version 1.1.0.\n\n"}, {"name": "pandas.HDFStore.select", "path": "reference/api/pandas.hdfstore.select", "type": "Input/output", "text": "\nRetrieve pandas object stored in file, optionally based on where criteria.\n\nWarning\n\nPandas uses PyTables for reading and writing HDF5 files, which allows\nserializing object-dtype data with pickle when using the \u201cfixed\u201d format.\nLoading pickled data received from untrusted sources can be unsafe.\n\nSee: https://docs.python.org/3/library/pickle.html for more.\n\nObject being retrieved from file.\n\nList of Term (or convertible) objects, optional.\n\nRow number to start selection.\n\nRow number to stop selection.\n\nA list of columns that if not None, will limit the return columns.\n\nReturns an iterator.\n\nNumber or rows to include in iteration, return an iterator.\n\nShould automatically close the store when finished.\n\nRetrieved object from file.\n\n"}, {"name": "pandas.HDFStore.walk", "path": "reference/api/pandas.hdfstore.walk", "type": "Input/output", "text": "\nWalk the pytables group hierarchy for pandas objects.\n\nThis generator will yield the group path, subgroups and pandas object names\nfor each group.\n\nAny non-pandas PyTables objects that are not a group will be ignored.\n\nThe where group itself is listed first (preorder), then each of its child\ngroups (following an alphanumerical order) is also traversed, following the\nsame procedure.\n\nGroup where to start walking.\n\nFull path to a group (without trailing \u2018/\u2019).\n\nNames (strings) of the groups contained in path.\n\nNames (strings) of the pandas objects contained in path.\n\n"}, {"name": "pandas.Index", "path": "reference/api/pandas.index", "type": "Index Objects", "text": "\nImmutable sequence used for indexing and alignment. The basic object storing\naxis labels for all pandas objects.\n\nIf dtype is None, we find the dtype that best fits the data. If an actual\ndtype is provided, we coerce to that dtype if it\u2019s safe. Otherwise, an error\nwill be raised.\n\nMake a copy of input ndarray.\n\nName to be stored in the index.\n\nWhen True, attempt to create a MultiIndex if possible.\n\nSee also\n\nIndex implementing a monotonic integer range.\n\nIndex of `Categorical` s.\n\nA multi-level, or hierarchical Index.\n\nAn Index of `Interval` s.\n\nIndex of datetime64 data.\n\nIndex of timedelta64 data.\n\nIndex of Period data.\n\nIndex of numpy int/uint/float data.\n\nIndex of purely int64 labels (deprecated).\n\nIndex of purely uint64 labels (deprecated).\n\nIndex of purely float64 labels (deprecated).\n\nNotes\n\nAn Index instance can only contain hashable objects\n\nExamples\n\nAttributes\n\n`T`\n\nReturn the transpose, which is by definition self.\n\n`array`\n\nThe ExtensionArray of the data backing this Series or Index.\n\n`asi8`\n\nInteger representation of the values.\n\n`dtype`\n\nReturn the dtype object of the underlying data.\n\n`has_duplicates`\n\nCheck if the Index has duplicate values.\n\n`hasnans`\n\nReturn True if there are any NaNs.\n\n`inferred_type`\n\nReturn a string of the type inferred from the values.\n\n`is_all_dates`\n\nWhether or not the index values only consist of dates.\n\n`is_monotonic`\n\nAlias for is_monotonic_increasing.\n\n`is_monotonic_decreasing`\n\nReturn if the index is monotonic decreasing (only equal or decreasing) values.\n\n`is_monotonic_increasing`\n\nReturn if the index is monotonic increasing (only equal or increasing) values.\n\n`is_unique`\n\nReturn if the index has unique values.\n\n`name`\n\nReturn Index or MultiIndex name.\n\n`nbytes`\n\nReturn the number of bytes in the underlying data.\n\n`ndim`\n\nNumber of dimensions of the underlying data, by definition 1.\n\n`nlevels`\n\nNumber of levels.\n\n`shape`\n\nReturn a tuple of the shape of the underlying data.\n\n`size`\n\nReturn the number of elements in the underlying data.\n\n`values`\n\nReturn an array representing the data in the Index.\n\nempty\n\nnames\n\nMethods\n\n`all`(*args, **kwargs)\n\nReturn whether all elements are Truthy.\n\n`any`(*args, **kwargs)\n\nReturn whether any element is Truthy.\n\n`append`(other)\n\nAppend a collection of Index options together.\n\n`argmax`([axis, skipna])\n\nReturn int position of the largest value in the Series.\n\n`argmin`([axis, skipna])\n\nReturn int position of the smallest value in the Series.\n\n`argsort`(*args, **kwargs)\n\nReturn the integer indices that would sort the index.\n\n`asof`(label)\n\nReturn the label from the index, or, if not present, the previous one.\n\n`asof_locs`(where, mask)\n\nReturn the locations (indices) of labels in the index.\n\n`astype`(dtype[, copy])\n\nCreate an Index with values cast to dtypes.\n\n`copy`([name, deep, dtype, names])\n\nMake a copy of this object.\n\n`delete`(loc)\n\nMake new Index with passed location(-s) deleted.\n\n`difference`(other[, sort])\n\nReturn a new Index with elements of index not in other.\n\n`drop`(labels[, errors])\n\nMake new Index with passed list of labels deleted.\n\n`drop_duplicates`([keep])\n\nReturn Index with duplicate values removed.\n\n`droplevel`([level])\n\nReturn index with requested level(s) removed.\n\n`dropna`([how])\n\nReturn Index without NA/NaN values.\n\n`duplicated`([keep])\n\nIndicate duplicate index values.\n\n`equals`(other)\n\nDetermine if two Index object are equal.\n\n`factorize`([sort, na_sentinel])\n\nEncode the object as an enumerated type or categorical variable.\n\n`fillna`([value, downcast])\n\nFill NA/NaN values with the specified value.\n\n`format`([name, formatter, na_rep])\n\nRender a string representation of the Index.\n\n`get_indexer`(target[, method, limit, tolerance])\n\nCompute indexer and mask for new index given the current index.\n\n`get_indexer_for`(target)\n\nGuaranteed return of an indexer even when non-unique.\n\n`get_indexer_non_unique`(target)\n\nCompute indexer and mask for new index given the current index.\n\n`get_level_values`(level)\n\nReturn an Index of values for requested level.\n\n`get_loc`(key[, method, tolerance])\n\nGet integer location, slice or boolean mask for requested label.\n\n`get_slice_bound`(label, side[, kind])\n\nCalculate slice bound that corresponds to given label.\n\n`get_value`(series, key)\n\nFast lookup of value from 1-dimensional ndarray.\n\n`groupby`(values)\n\nGroup the index labels by a given array of values.\n\n`holds_integer`()\n\nWhether the type is an integer type.\n\n`identical`(other)\n\nSimilar to equals, but checks that object attributes and types are also equal.\n\n`insert`(loc, item)\n\nMake new Index inserting new item at location.\n\n`intersection`(other[, sort])\n\nForm the intersection of two Index objects.\n\n`is_`(other)\n\nMore flexible, faster check like `is` but that works through views.\n\n`is_boolean`()\n\nCheck if the Index only consists of booleans.\n\n`is_categorical`()\n\nCheck if the Index holds categorical data.\n\n`is_floating`()\n\nCheck if the Index is a floating type.\n\n`is_integer`()\n\nCheck if the Index only consists of integers.\n\n`is_interval`()\n\nCheck if the Index holds Interval objects.\n\n`is_mixed`()\n\nCheck if the Index holds data with mixed data types.\n\n`is_numeric`()\n\nCheck if the Index only consists of numeric data.\n\n`is_object`()\n\nCheck if the Index is of the object dtype.\n\n`is_type_compatible`(kind)\n\nWhether the index type is compatible with the provided type.\n\n`isin`(values[, level])\n\nReturn a boolean array where the index values are in values.\n\n`isna`()\n\nDetect missing values.\n\n`isnull`()\n\nDetect missing values.\n\n`item`()\n\nReturn the first element of the underlying data as a Python scalar.\n\n`join`(other[, how, level, return_indexers, sort])\n\nCompute join_index and indexers to conform data structures to the new index.\n\n`map`(mapper[, na_action])\n\nMap values using an input mapping or function.\n\n`max`([axis, skipna])\n\nReturn the maximum value of the Index.\n\n`memory_usage`([deep])\n\nMemory usage of the values.\n\n`min`([axis, skipna])\n\nReturn the minimum value of the Index.\n\n`notna`()\n\nDetect existing (non-missing) values.\n\n`notnull`()\n\nDetect existing (non-missing) values.\n\n`nunique`([dropna])\n\nReturn number of unique elements in the object.\n\n`putmask`(mask, value)\n\nReturn a new Index of the values set with the mask.\n\n`ravel`([order])\n\nReturn an ndarray of the flattened values of the underlying data.\n\n`reindex`(target[, method, level, limit, ...])\n\nCreate index with target's values.\n\n`rename`(name[, inplace])\n\nAlter Index or MultiIndex name.\n\n`repeat`(repeats[, axis])\n\nRepeat elements of a Index.\n\n`searchsorted`(value[, side, sorter])\n\nFind indices where elements should be inserted to maintain order.\n\n`set_names`(names[, level, inplace])\n\nSet Index or MultiIndex name.\n\n`set_value`(arr, key, value)\n\n(DEPRECATED) Fast lookup of value from 1-dimensional ndarray.\n\n`shift`([periods, freq])\n\nShift index by desired number of time frequency increments.\n\n`slice_indexer`([start, end, step, kind])\n\nCompute the slice indexer for input labels and step.\n\n`slice_locs`([start, end, step, kind])\n\nCompute slice locations for input labels.\n\n`sort`(*args, **kwargs)\n\nUse sort_values instead.\n\n`sort_values`([return_indexer, ascending, ...])\n\nReturn a sorted copy of the index.\n\n`sortlevel`([level, ascending, sort_remaining])\n\nFor internal compatibility with the Index API.\n\n`str`\n\nalias of `pandas.core.strings.accessor.StringMethods`\n\n`symmetric_difference`(other[, result_name, sort])\n\nCompute the symmetric difference of two Index objects.\n\n`take`(indices[, axis, allow_fill, fill_value])\n\nReturn a new Index of the values selected by the indices.\n\n`to_flat_index`()\n\nIdentity method.\n\n`to_frame`([index, name])\n\nCreate a DataFrame with a column containing the Index.\n\n`to_list`()\n\nReturn a list of the values.\n\n`to_native_types`([slicer])\n\n(DEPRECATED) Format specified values of self and return them.\n\n`to_numpy`([dtype, copy, na_value])\n\nA NumPy ndarray representing the values in this Series or Index.\n\n`to_series`([index, name])\n\nCreate a Series with both index and values equal to the index keys.\n\n`tolist`()\n\nReturn a list of the values.\n\n`transpose`(*args, **kwargs)\n\nReturn the transpose, which is by definition self.\n\n`union`(other[, sort])\n\nForm the union of two Index objects.\n\n`unique`([level])\n\nReturn unique values in the index.\n\n`value_counts`([normalize, sort, ascending, ...])\n\nReturn a Series containing counts of unique values.\n\n`where`(cond[, other])\n\nReplace values where the condition is False.\n\nview\n\n"}, {"name": "pandas.Index.all", "path": "reference/api/pandas.index.all", "type": "Index Objects", "text": "\nReturn whether all elements are Truthy.\n\nRequired for compatibility with numpy.\n\nRequired for compatibility with numpy.\n\nA single element array-like may be converted to bool.\n\nSee also\n\nReturn whether any element in an Index is True.\n\nReturn whether any element in a Series is True.\n\nReturn whether all elements in a Series are True.\n\nNotes\n\nNot a Number (NaN), positive infinity and negative infinity evaluate to True\nbecause these are not equal to zero.\n\nExamples\n\nTrue, because nonzero integers are considered True.\n\nFalse, because `0` is considered False.\n\n"}, {"name": "pandas.Index.any", "path": "reference/api/pandas.index.any", "type": "Index Objects", "text": "\nReturn whether any element is Truthy.\n\nRequired for compatibility with numpy.\n\nRequired for compatibility with numpy.\n\nA single element array-like may be converted to bool.\n\nSee also\n\nReturn whether all elements are True.\n\nReturn whether all elements are True.\n\nNotes\n\nNot a Number (NaN), positive infinity and negative infinity evaluate to True\nbecause these are not equal to zero.\n\nExamples\n\n"}, {"name": "pandas.Index.append", "path": "reference/api/pandas.index.append", "type": "Index Objects", "text": "\nAppend a collection of Index options together.\n\n"}, {"name": "pandas.Index.argmax", "path": "reference/api/pandas.index.argmax", "type": "Index Objects", "text": "\nReturn int position of the largest value in the Series.\n\nIf the maximum is achieved in multiple locations, the first row position is\nreturned.\n\nDummy argument for consistency with Series.\n\nExclude NA/null values when showing the result.\n\nAdditional arguments and keywords for compatibility with NumPy.\n\nRow position of the maximum value.\n\nSee also\n\nReturn position of the maximum value.\n\nReturn position of the minimum value.\n\nEquivalent method for numpy arrays.\n\nReturn index label of the maximum values.\n\nReturn index label of the minimum values.\n\nExamples\n\nConsider dataset containing cereal calories\n\nThe maximum cereal calories is the third element and the minimum cereal\ncalories is the first element, since series is zero-indexed.\n\n"}, {"name": "pandas.Index.argmin", "path": "reference/api/pandas.index.argmin", "type": "Index Objects", "text": "\nReturn int position of the smallest value in the Series.\n\nIf the minimum is achieved in multiple locations, the first row position is\nreturned.\n\nDummy argument for consistency with Series.\n\nExclude NA/null values when showing the result.\n\nAdditional arguments and keywords for compatibility with NumPy.\n\nRow position of the minimum value.\n\nSee also\n\nReturn position of the minimum value.\n\nReturn position of the maximum value.\n\nEquivalent method for numpy arrays.\n\nReturn index label of the maximum values.\n\nReturn index label of the minimum values.\n\nExamples\n\nConsider dataset containing cereal calories\n\nThe maximum cereal calories is the third element and the minimum cereal\ncalories is the first element, since series is zero-indexed.\n\n"}, {"name": "pandas.Index.argsort", "path": "reference/api/pandas.index.argsort", "type": "Index Objects", "text": "\nReturn the integer indices that would sort the index.\n\nPassed to numpy.ndarray.argsort.\n\nPassed to numpy.ndarray.argsort.\n\nInteger indices that would sort the index if used as an indexer.\n\nSee also\n\nSimilar method for NumPy arrays.\n\nReturn sorted copy of Index.\n\nExamples\n\n"}, {"name": "pandas.Index.array", "path": "reference/api/pandas.index.array", "type": "Index Objects", "text": "\nThe ExtensionArray of the data backing this Series or Index.\n\nAn ExtensionArray of the values stored within. For extension types, this is\nthe actual array. For NumPy native types, this is a thin (no copy) wrapper\naround `numpy.ndarray`.\n\n`.array` differs `.values` which may require converting the data to a\ndifferent form.\n\nSee also\n\nSimilar method that always returns a NumPy array.\n\nSimilar method that always returns a NumPy array.\n\nNotes\n\nThis table lays out the different array types for each extension dtype within\npandas.\n\ndtype\n\narray type\n\ncategory\n\nCategorical\n\nperiod\n\nPeriodArray\n\ninterval\n\nIntervalArray\n\nIntegerNA\n\nIntegerArray\n\nstring\n\nStringArray\n\nboolean\n\nBooleanArray\n\ndatetime64[ns, tz]\n\nDatetimeArray\n\nFor any 3rd-party extension types, the array type will be an ExtensionArray.\n\nFor all remaining dtypes `.array` will be a `arrays.NumpyExtensionArray`\nwrapping the actual ndarray stored within. If you absolutely need a NumPy\narray (possibly with copying / coercing data), then use `Series.to_numpy()`\ninstead.\n\nExamples\n\nFor regular NumPy types like int, and float, a PandasArray is returned.\n\nFor extension types, like Categorical, the actual ExtensionArray is returned\n\n"}, {"name": "pandas.Index.asi8", "path": "reference/api/pandas.index.asi8", "type": "Index Objects", "text": "\nInteger representation of the values.\n\nAn ndarray with int64 dtype.\n\n"}, {"name": "pandas.Index.asof", "path": "reference/api/pandas.index.asof", "type": "Index Objects", "text": "\nReturn the label from the index, or, if not present, the previous one.\n\nAssuming that the index is sorted, return the passed index label if it is in\nthe index, or return the previous index label if the passed one is not in the\nindex.\n\nThe label up to which the method returns the latest index label.\n\nThe passed label if it is in the index. The previous label if the passed label\nis not in the sorted index or NaN if there is no such label.\n\nSee also\n\nReturn the latest value in a Series up to the passed index.\n\nPerform an asof merge (similar to left join but it matches on nearest key\nrather than equal key).\n\nAn asof is a thin wrapper around get_loc with method=\u2019pad\u2019.\n\nExamples\n\nIndex.asof returns the latest index label up to the passed label.\n\nIf the label is in the index, the method returns the passed label.\n\nIf all of the labels in the index are later than the passed label, NaN is\nreturned.\n\nIf the index is not sorted, an error is raised.\n\n"}, {"name": "pandas.Index.asof_locs", "path": "reference/api/pandas.index.asof_locs", "type": "Index Objects", "text": "\nReturn the locations (indices) of labels in the index.\n\nAs in the asof function, if the label (a particular entry in where) is not in\nthe index, the latest index label up to the passed label is chosen and its\nindex returned.\n\nIf all of the labels in the index are later than a label in where, -1 is\nreturned.\n\nmask is used to ignore NA values in the index during calculation.\n\nAn Index consisting of an array of timestamps.\n\nArray of booleans denoting where values in the original data are not NA.\n\nAn array of locations (indices) of the labels from the Index which correspond\nto the return values of the asof function for every element in where.\n\n"}, {"name": "pandas.Index.astype", "path": "reference/api/pandas.index.astype", "type": "Index Objects", "text": "\nCreate an Index with values cast to dtypes.\n\nThe class of a new Index is determined by dtype. When conversion is\nimpossible, a TypeError exception is raised.\n\nNote that any signed integer dtype is treated as `'int64'`, and any unsigned\ninteger dtype is treated as `'uint64'`, regardless of the size.\n\nBy default, astype always returns a newly allocated object. If copy is set to\nFalse and internal requirements on dtype are satisfied, the original data is\nused to create a new Index or the original Index is returned.\n\nIndex with values cast to specified dtype.\n\n"}, {"name": "pandas.Index.copy", "path": "reference/api/pandas.index.copy", "type": "Index Objects", "text": "\nMake a copy of this object.\n\nName and dtype sets those attributes on the new object.\n\nSet name for new object.\n\nSet dtype for new object.\n\nDeprecated since version 1.2.0: use `astype` method instead.\n\nKept for compatibility with MultiIndex. Should not be used.\n\nDeprecated since version 1.4.0: use `name` instead.\n\nIndex refer to new object which is a copy of this object.\n\nNotes\n\nIn most cases, there should be no functional difference from using `deep`, but\nif `deep` is passed it will attempt to deepcopy.\n\n"}, {"name": "pandas.Index.delete", "path": "reference/api/pandas.index.delete", "type": "Index Objects", "text": "\nMake new Index with passed location(-s) deleted.\n\nLocation of item(-s) which will be deleted. Use a list of locations to delete\nmore than one value at the same time.\n\nWill be same type as self, except for RangeIndex.\n\nSee also\n\nDelete any rows and column from NumPy array (ndarray).\n\nExamples\n\n"}, {"name": "pandas.Index.difference", "path": "reference/api/pandas.index.difference", "type": "Index Objects", "text": "\nReturn a new Index with elements of index not in other.\n\nThis is the set difference of two Index objects.\n\nWhether to sort the resulting index. By default, the values are attempted to\nbe sorted, but any TypeError from incomparable elements is caught by pandas.\n\nNone : Attempt to sort the result, but catch any TypeErrors from comparing\nincomparable elements.\n\nFalse : Do not sort the result.\n\nExamples\n\n"}, {"name": "pandas.Index.drop", "path": "reference/api/pandas.index.drop", "type": "Index Objects", "text": "\nMake new Index with passed list of labels deleted.\n\nIf \u2018ignore\u2019, suppress error and existing labels are dropped.\n\nWill be same type as self, except for RangeIndex.\n\nIf not all of the labels are found in the selected axis\n\n"}, {"name": "pandas.Index.drop_duplicates", "path": "reference/api/pandas.index.drop_duplicates", "type": "Index Objects", "text": "\nReturn Index with duplicate values removed.\n\n\u2018first\u2019 : Drop duplicates except for the first occurrence.\n\n\u2018last\u2019 : Drop duplicates except for the last occurrence.\n\n`False` : Drop all duplicates.\n\nSee also\n\nEquivalent method on Series.\n\nEquivalent method on DataFrame.\n\nRelated method on Index, indicating duplicate Index values.\n\nExamples\n\nGenerate an pandas.Index with duplicate values.\n\nThe keep parameter controls which duplicate values are removed. The value\n\u2018first\u2019 keeps the first occurrence for each set of duplicated entries. The\ndefault value of keep is \u2018first\u2019.\n\nThe value \u2018last\u2019 keeps the last occurrence for each set of duplicated entries.\n\nThe value `False` discards all sets of duplicated entries.\n\n"}, {"name": "pandas.Index.droplevel", "path": "reference/api/pandas.index.droplevel", "type": "Index Objects", "text": "\nReturn index with requested level(s) removed.\n\nIf resulting index has only 1 level left, the result will be of Index type,\nnot MultiIndex.\n\nIf a string is given, must be the name of a level If list-like, elements must\nbe names or indexes of levels.\n\nExamples\n\n"}, {"name": "pandas.Index.dropna", "path": "reference/api/pandas.index.dropna", "type": "Index Objects", "text": "\nReturn Index without NA/NaN values.\n\nIf the Index is a MultiIndex, drop the value when any or all levels are NaN.\n\n"}, {"name": "pandas.Index.dtype", "path": "reference/api/pandas.index.dtype", "type": "Index Objects", "text": "\nReturn the dtype object of the underlying data.\n\n"}, {"name": "pandas.Index.duplicated", "path": "reference/api/pandas.index.duplicated", "type": "Index Objects", "text": "\nIndicate duplicate index values.\n\nDuplicated values are indicated as `True` values in the resulting array.\nEither all duplicates, all except the first, or all except the last occurrence\nof duplicates can be indicated.\n\nThe value or values in a set of duplicates to mark as missing.\n\n\u2018first\u2019 : Mark duplicates as `True` except for the first occurrence.\n\n\u2018last\u2019 : Mark duplicates as `True` except for the last occurrence.\n\n`False` : Mark all duplicates as `True`.\n\nSee also\n\nEquivalent method on pandas.Series.\n\nEquivalent method on pandas.DataFrame.\n\nRemove duplicate values from Index.\n\nExamples\n\nBy default, for each set of duplicated values, the first occurrence is set to\nFalse and all others to True:\n\nwhich is equivalent to\n\nBy using \u2018last\u2019, the last occurrence of each set of duplicated values is set\non False and all others on True:\n\nBy setting keep on `False`, all duplicates are True:\n\n"}, {"name": "pandas.Index.empty", "path": "reference/api/pandas.index.empty", "type": "Index Objects", "text": "\n\n"}, {"name": "pandas.Index.equals", "path": "reference/api/pandas.index.equals", "type": "Index Objects", "text": "\nDetermine if two Index object are equal.\n\nThe things that are being compared are:\n\nThe elements inside the Index object.\n\nThe order of the elements inside the Index object.\n\nThe other object to compare against.\n\nTrue if \u201cother\u201d is an Index and it has the same elements and order as the\ncalling index; False otherwise.\n\nExamples\n\nThe elements inside are compared\n\nThe order is compared\n\nThe dtype is not compared\n\n"}, {"name": "pandas.Index.factorize", "path": "reference/api/pandas.index.factorize", "type": "Index Objects", "text": "\nEncode the object as an enumerated type or categorical variable.\n\nThis method is useful for obtaining a numeric representation of an array when\nall that matters is identifying distinct values. factorize is available as\nboth a top-level function `pandas.factorize()`, and as a method\n`Series.factorize()` and `Index.factorize()`.\n\nSort uniques and shuffle codes to maintain the relationship.\n\nValue to mark \u201cnot found\u201d. If None, will not drop the NaN from the uniques of\nthe values.\n\nChanged in version 1.1.2.\n\nAn integer ndarray that\u2019s an indexer into uniques. `uniques.take(codes)` will\nhave the same values as values.\n\nThe unique valid values. When values is Categorical, uniques is a Categorical.\nWhen values is some other pandas object, an Index is returned. Otherwise, a\n1-D ndarray is returned.\n\nNote\n\nEven if there\u2019s a missing value in values, uniques will not contain an entry\nfor it.\n\nSee also\n\nDiscretize continuous-valued array.\n\nFind the unique value in an array.\n\nExamples\n\nThese examples all show factorize as a top-level method like\n`pd.factorize(values)`. The results are identical for methods like\n`Series.factorize()`.\n\nWith `sort=True`, the uniques will be sorted, and codes will be shuffled so\nthat the relationship is the maintained.\n\nMissing values are indicated in codes with na_sentinel (`-1` by default). Note\nthat missing values are never included in uniques.\n\nThus far, we\u2019ve only factorized lists (which are internally coerced to NumPy\narrays). When factorizing pandas objects, the type of uniques will differ. For\nCategoricals, a Categorical is returned.\n\nNotice that `'b'` is in `uniques.categories`, despite not being present in\n`cat.values`.\n\nFor all other pandas objects, an Index of the appropriate type is returned.\n\nIf NaN is in the values, and we want to include NaN in the uniques of the\nvalues, it can be achieved by setting `na_sentinel=None`.\n\n"}, {"name": "pandas.Index.fillna", "path": "reference/api/pandas.index.fillna", "type": "Index Objects", "text": "\nFill NA/NaN values with the specified value.\n\nScalar value to use to fill holes (e.g. 0). This value cannot be a list-likes.\n\nA dict of item->dtype of what to downcast if possible, or the string \u2018infer\u2019\nwhich will try to downcast to an appropriate equal type (e.g. float64 to int64\nif possible).\n\nSee also\n\nFill NaN values of a DataFrame.\n\nFill NaN Values of a Series.\n\n"}, {"name": "pandas.Index.format", "path": "reference/api/pandas.index.format", "type": "Index Objects", "text": "\nRender a string representation of the Index.\n\n"}, {"name": "pandas.Index.get_indexer", "path": "reference/api/pandas.index.get_indexer", "type": "Index Objects", "text": "\nCompute indexer and mask for new index given the current index. The indexer\nshould be then used as an input to ndarray.take to align the current data to\nthe new index.\n\ndefault: exact matches only.\n\npad / ffill: find the PREVIOUS index value if no exact match.\n\nbackfill / bfill: use NEXT index value if no exact match\n\nnearest: use the NEAREST index value if no exact match. Tied distances are\nbroken by preferring the larger index value.\n\nMaximum number of consecutive labels in `target` to match for inexact matches.\n\nMaximum distance between original and new labels for inexact matches. The\nvalues of the index at the matching locations must satisfy the equation\n`abs(index[indexer] - target) <= tolerance`.\n\nTolerance may be a scalar value, which applies the same tolerance to all\nvalues, or list-like, which applies variable tolerance per element. List-like\nincludes list, tuple, array, Series, and must be the same size as the index\nand its dtype must exactly match the index\u2019s type.\n\nIntegers from 0 to n - 1 indicating that the index at these positions matches\nthe corresponding target values. Missing values in the target are marked by\n-1.\n\nNotes\n\nReturns -1 for unmatched values, for further explanation see the example\nbelow.\n\nExamples\n\nNotice that the return value is an array of locations in `index` and `x` is\nmarked by -1, as it is not in `index`.\n\n"}, {"name": "pandas.Index.get_indexer_for", "path": "reference/api/pandas.index.get_indexer_for", "type": "Index Objects", "text": "\nGuaranteed return of an indexer even when non-unique.\n\nThis dispatches to get_indexer or get_indexer_non_unique as appropriate.\n\nList of indices.\n\nExamples\n\n"}, {"name": "pandas.Index.get_indexer_non_unique", "path": "reference/api/pandas.index.get_indexer_non_unique", "type": "Index Objects", "text": "\nCompute indexer and mask for new index given the current index. The indexer\nshould be then used as an input to ndarray.take to align the current data to\nthe new index.\n\nIntegers from 0 to n - 1 indicating that the index at these positions matches\nthe corresponding target values. Missing values in the target are marked by\n-1.\n\nAn indexer into the target of the values not found. These correspond to the -1\nin the indexer array.\n\n"}, {"name": "pandas.Index.get_level_values", "path": "reference/api/pandas.index.get_level_values", "type": "Index Objects", "text": "\nReturn an Index of values for requested level.\n\nThis is primarily useful to get an individual level of values from a\nMultiIndex, but is provided on Index as well for compatibility.\n\nIt is either the integer position or the name of the level.\n\nCalling object, as there is only one level in the Index.\n\nSee also\n\nGet values for a level of a MultiIndex.\n\nNotes\n\nFor Index, level should be 0, since there are no multiple levels.\n\nExamples\n\nGet level values by supplying level as integer:\n\n"}, {"name": "pandas.Index.get_loc", "path": "reference/api/pandas.index.get_loc", "type": "Index Objects", "text": "\nGet integer location, slice or boolean mask for requested label.\n\ndefault: exact matches only.\n\npad / ffill: find the PREVIOUS index value if no exact match.\n\nbackfill / bfill: use NEXT index value if no exact match\n\nnearest: use the NEAREST index value if no exact match. Tied distances are\nbroken by preferring the larger index value.\n\nMaximum distance from index value for inexact matches. The value of the index\nat the matching location must satisfy the equation `abs(index[loc] - key) <=\ntolerance`.\n\nExamples\n\n"}, {"name": "pandas.Index.get_slice_bound", "path": "reference/api/pandas.index.get_slice_bound", "type": "Index Objects", "text": "\nCalculate slice bound that corresponds to given label.\n\nReturns leftmost (one-past-the-rightmost if `side=='right'`) position of given\nlabel.\n\nDeprecated since version 1.4.0.\n\nIndex of label.\n\n"}, {"name": "pandas.Index.get_value", "path": "reference/api/pandas.index.get_value", "type": "Index Objects", "text": "\nFast lookup of value from 1-dimensional ndarray.\n\nOnly use this if you know what you\u2019re doing.\n\n"}, {"name": "pandas.Index.groupby", "path": "reference/api/pandas.index.groupby", "type": "GroupBy", "text": "\nGroup the index labels by a given array of values.\n\nValues used to determine the groups.\n\n{group name -> group labels}\n\n"}, {"name": "pandas.Index.has_duplicates", "path": "reference/api/pandas.index.has_duplicates", "type": "Index Objects", "text": "\nCheck if the Index has duplicate values.\n\nWhether or not the Index has duplicate values.\n\nExamples\n\n"}, {"name": "pandas.Index.hasnans", "path": "reference/api/pandas.index.hasnans", "type": "Index Objects", "text": "\nReturn True if there are any NaNs.\n\nEnables various performance speedups.\n\n"}, {"name": "pandas.Index.holds_integer", "path": "reference/api/pandas.index.holds_integer", "type": "Index Objects", "text": "\nWhether the type is an integer type.\n\n"}, {"name": "pandas.Index.identical", "path": "reference/api/pandas.index.identical", "type": "Index Objects", "text": "\nSimilar to equals, but checks that object attributes and types are also equal.\n\nIf two Index objects have equal elements and same type True, otherwise False.\n\n"}, {"name": "pandas.Index.inferred_type", "path": "reference/api/pandas.index.inferred_type", "type": "Index Objects", "text": "\nReturn a string of the type inferred from the values.\n\n"}, {"name": "pandas.Index.insert", "path": "reference/api/pandas.index.insert", "type": "Index Objects", "text": "\nMake new Index inserting new item at location.\n\nFollows Python numpy.insert semantics for negative values.\n\n"}, {"name": "pandas.Index.intersection", "path": "reference/api/pandas.index.intersection", "type": "Input/output", "text": "\nForm the intersection of two Index objects.\n\nThis returns a new Index with elements common to the index and other.\n\nWhether to sort the resulting index.\n\nFalse : do not sort the result.\n\nNone : sort the result, except when self and other are equal or when the\nvalues cannot be compared.\n\nExamples\n\n"}, {"name": "pandas.Index.is_", "path": "reference/api/pandas.index.is_", "type": "Index Objects", "text": "\nMore flexible, faster check like `is` but that works through views.\n\nNote: this is not the same as `Index.identical()`, which checks that metadata\nis also the same.\n\nOther object to compare against.\n\nTrue if both have same underlying data, False otherwise.\n\nSee also\n\nWorks like `Index.is_` but also checks metadata.\n\n"}, {"name": "pandas.Index.is_all_dates", "path": "reference/api/pandas.index.is_all_dates", "type": "Index Objects", "text": "\nWhether or not the index values only consist of dates.\n\n"}, {"name": "pandas.Index.is_boolean", "path": "reference/api/pandas.index.is_boolean", "type": "Index Objects", "text": "\nCheck if the Index only consists of booleans.\n\nWhether or not the Index only consists of booleans.\n\nSee also\n\nCheck if the Index only consists of integers.\n\nCheck if the Index is a floating type.\n\nCheck if the Index only consists of numeric data.\n\nCheck if the Index is of the object dtype.\n\nCheck if the Index holds categorical data.\n\nCheck if the Index holds Interval objects.\n\nCheck if the Index holds data with mixed data types.\n\nExamples\n\n"}, {"name": "pandas.Index.is_categorical", "path": "reference/api/pandas.index.is_categorical", "type": "Index Objects", "text": "\nCheck if the Index holds categorical data.\n\nTrue if the Index is categorical.\n\nSee also\n\nIndex for categorical data.\n\nCheck if the Index only consists of booleans.\n\nCheck if the Index only consists of integers.\n\nCheck if the Index is a floating type.\n\nCheck if the Index only consists of numeric data.\n\nCheck if the Index is of the object dtype.\n\nCheck if the Index holds Interval objects.\n\nCheck if the Index holds data with mixed data types.\n\nExamples\n\n"}, {"name": "pandas.Index.is_floating", "path": "reference/api/pandas.index.is_floating", "type": "Index Objects", "text": "\nCheck if the Index is a floating type.\n\nThe Index may consist of only floats, NaNs, or a mix of floats, integers, or\nNaNs.\n\nWhether or not the Index only consists of only consists of floats, NaNs, or a\nmix of floats, integers, or NaNs.\n\nSee also\n\nCheck if the Index only consists of booleans.\n\nCheck if the Index only consists of integers.\n\nCheck if the Index only consists of numeric data.\n\nCheck if the Index is of the object dtype.\n\nCheck if the Index holds categorical data.\n\nCheck if the Index holds Interval objects.\n\nCheck if the Index holds data with mixed data types.\n\nExamples\n\n"}, {"name": "pandas.Index.is_integer", "path": "reference/api/pandas.index.is_integer", "type": "Index Objects", "text": "\nCheck if the Index only consists of integers.\n\nWhether or not the Index only consists of integers.\n\nSee also\n\nCheck if the Index only consists of booleans.\n\nCheck if the Index is a floating type.\n\nCheck if the Index only consists of numeric data.\n\nCheck if the Index is of the object dtype.\n\nCheck if the Index holds categorical data.\n\nCheck if the Index holds Interval objects.\n\nCheck if the Index holds data with mixed data types.\n\nExamples\n\n"}, {"name": "pandas.Index.is_interval", "path": "reference/api/pandas.index.is_interval", "type": "Index Objects", "text": "\nCheck if the Index holds Interval objects.\n\nWhether or not the Index holds Interval objects.\n\nSee also\n\nIndex for Interval objects.\n\nCheck if the Index only consists of booleans.\n\nCheck if the Index only consists of integers.\n\nCheck if the Index is a floating type.\n\nCheck if the Index only consists of numeric data.\n\nCheck if the Index is of the object dtype.\n\nCheck if the Index holds categorical data.\n\nCheck if the Index holds data with mixed data types.\n\nExamples\n\n"}, {"name": "pandas.Index.is_mixed", "path": "reference/api/pandas.index.is_mixed", "type": "Index Objects", "text": "\nCheck if the Index holds data with mixed data types.\n\nWhether or not the Index holds data with mixed data types.\n\nSee also\n\nCheck if the Index only consists of booleans.\n\nCheck if the Index only consists of integers.\n\nCheck if the Index is a floating type.\n\nCheck if the Index only consists of numeric data.\n\nCheck if the Index is of the object dtype.\n\nCheck if the Index holds categorical data.\n\nCheck if the Index holds Interval objects.\n\nExamples\n\n"}, {"name": "pandas.Index.is_monotonic", "path": "reference/api/pandas.index.is_monotonic", "type": "Index Objects", "text": "\nAlias for is_monotonic_increasing.\n\n"}, {"name": "pandas.Index.is_monotonic_decreasing", "path": "reference/api/pandas.index.is_monotonic_decreasing", "type": "Index Objects", "text": "\nReturn if the index is monotonic decreasing (only equal or decreasing) values.\n\nExamples\n\n"}, {"name": "pandas.Index.is_monotonic_increasing", "path": "reference/api/pandas.index.is_monotonic_increasing", "type": "Index Objects", "text": "\nReturn if the index is monotonic increasing (only equal or increasing) values.\n\nExamples\n\n"}, {"name": "pandas.Index.is_numeric", "path": "reference/api/pandas.index.is_numeric", "type": "Index Objects", "text": "\nCheck if the Index only consists of numeric data.\n\nWhether or not the Index only consists of numeric data.\n\nSee also\n\nCheck if the Index only consists of booleans.\n\nCheck if the Index only consists of integers.\n\nCheck if the Index is a floating type.\n\nCheck if the Index is of the object dtype.\n\nCheck if the Index holds categorical data.\n\nCheck if the Index holds Interval objects.\n\nCheck if the Index holds data with mixed data types.\n\nExamples\n\n"}, {"name": "pandas.Index.is_object", "path": "reference/api/pandas.index.is_object", "type": "Index Objects", "text": "\nCheck if the Index is of the object dtype.\n\nWhether or not the Index is of the object dtype.\n\nSee also\n\nCheck if the Index only consists of booleans.\n\nCheck if the Index only consists of integers.\n\nCheck if the Index is a floating type.\n\nCheck if the Index only consists of numeric data.\n\nCheck if the Index holds categorical data.\n\nCheck if the Index holds Interval objects.\n\nCheck if the Index holds data with mixed data types.\n\nExamples\n\n"}, {"name": "pandas.Index.is_type_compatible", "path": "reference/api/pandas.index.is_type_compatible", "type": "Index Objects", "text": "\nWhether the index type is compatible with the provided type.\n\n"}, {"name": "pandas.Index.is_unique", "path": "reference/api/pandas.index.is_unique", "type": "Index Objects", "text": "\nReturn if the index has unique values.\n\n"}, {"name": "pandas.Index.isin", "path": "reference/api/pandas.index.isin", "type": "Index Objects", "text": "\nReturn a boolean array where the index values are in values.\n\nCompute boolean array of whether each index value is found in the passed set\nof values. The length of the returned boolean array matches the length of the\nindex.\n\nSought values.\n\nName or position of the index level to use (if the index is a MultiIndex).\n\nNumPy array of boolean values.\n\nSee also\n\nSame for Series.\n\nSame method for DataFrames.\n\nNotes\n\nIn the case of MultiIndex you must either specify values as a list-like object\ncontaining tuples that are the same length as the number of levels, or specify\nlevel. Otherwise it will raise a `ValueError`.\n\nIf level is specified:\n\nif it is the name of one and only one index level, use that level;\n\notherwise it should be a number indicating level position.\n\nExamples\n\nCheck whether each index value in a list of values.\n\nCheck whether the strings in the \u2018color\u2019 level of the MultiIndex are in a list\nof colors.\n\nTo check across the levels of a MultiIndex, pass a list of tuples:\n\nFor a DatetimeIndex, string values in values are converted to Timestamps.\n\n"}, {"name": "pandas.Index.isna", "path": "reference/api/pandas.index.isna", "type": "Index Objects", "text": "\nDetect missing values.\n\nReturn a boolean same-sized object indicating if the values are NA. NA values,\nsuch as `None`, `numpy.NaN` or `pd.NaT`, get mapped to `True` values.\nEverything else get mapped to `False` values. Characters such as empty strings\n\u2018\u2019 or `numpy.inf` are not considered NA values (unless you set\n`pandas.options.mode.use_inf_as_na = True`).\n\nA boolean array of whether my values are NA.\n\nSee also\n\nBoolean inverse of isna.\n\nOmit entries with missing values.\n\nTop-level isna.\n\nDetect missing values in Series object.\n\nExamples\n\nShow which entries in a pandas.Index are NA. The result is an array.\n\nEmpty strings are not considered NA values. None is considered an NA value.\n\nFor datetimes, NaT (Not a Time) is considered as an NA value.\n\n"}, {"name": "pandas.Index.isnull", "path": "reference/api/pandas.index.isnull", "type": "Index Objects", "text": "\nDetect missing values.\n\nReturn a boolean same-sized object indicating if the values are NA. NA values,\nsuch as `None`, `numpy.NaN` or `pd.NaT`, get mapped to `True` values.\nEverything else get mapped to `False` values. Characters such as empty strings\n\u2018\u2019 or `numpy.inf` are not considered NA values (unless you set\n`pandas.options.mode.use_inf_as_na = True`).\n\nA boolean array of whether my values are NA.\n\nSee also\n\nBoolean inverse of isna.\n\nOmit entries with missing values.\n\nTop-level isna.\n\nDetect missing values in Series object.\n\nExamples\n\nShow which entries in a pandas.Index are NA. The result is an array.\n\nEmpty strings are not considered NA values. None is considered an NA value.\n\nFor datetimes, NaT (Not a Time) is considered as an NA value.\n\n"}, {"name": "pandas.Index.item", "path": "reference/api/pandas.index.item", "type": "Index Objects", "text": "\nReturn the first element of the underlying data as a Python scalar.\n\nThe first element of %(klass)s.\n\nIf the data is not length-1.\n\n"}, {"name": "pandas.Index.join", "path": "reference/api/pandas.index.join", "type": "Index Objects", "text": "\nCompute join_index and indexers to conform data structures to the new index.\n\nSort the join keys lexicographically in the result Index. If False, the order\nof the join keys depends on the join type (how keyword).\n\n"}, {"name": "pandas.Index.map", "path": "reference/api/pandas.index.map", "type": "Index Objects", "text": "\nMap values using an input mapping or function.\n\nMapping correspondence.\n\nIf \u2018ignore\u2019, propagate NA values, without passing them to the mapping\ncorrespondence.\n\nThe output of the mapping function applied to the index. If the function\nreturns a tuple with more than one element a MultiIndex will be returned.\n\n"}, {"name": "pandas.Index.max", "path": "reference/api/pandas.index.max", "type": "Index Objects", "text": "\nReturn the maximum value of the Index.\n\nFor compatibility with NumPy. Only 0 or None are allowed.\n\nExclude NA/null values when showing the result.\n\nAdditional arguments and keywords for compatibility with NumPy.\n\nMaximum value.\n\nSee also\n\nReturn the minimum value in an Index.\n\nReturn the maximum value in a Series.\n\nReturn the maximum values in a DataFrame.\n\nExamples\n\nFor a MultiIndex, the maximum is determined lexicographically.\n\n"}, {"name": "pandas.Index.memory_usage", "path": "reference/api/pandas.index.memory_usage", "type": "Index Objects", "text": "\nMemory usage of the values.\n\nIntrospect the data deeply, interrogate object dtypes for system-level memory\nconsumption.\n\nSee also\n\nTotal bytes consumed by the elements of the array.\n\nNotes\n\nMemory usage does not include memory consumed by elements that are not\ncomponents of the array if deep=False or if used on PyPy\n\n"}, {"name": "pandas.Index.min", "path": "reference/api/pandas.index.min", "type": "Index Objects", "text": "\nReturn the minimum value of the Index.\n\nDummy argument for consistency with Series.\n\nExclude NA/null values when showing the result.\n\nAdditional arguments and keywords for compatibility with NumPy.\n\nMinimum value.\n\nSee also\n\nReturn the maximum value of the object.\n\nReturn the minimum value in a Series.\n\nReturn the minimum values in a DataFrame.\n\nExamples\n\nFor a MultiIndex, the minimum is determined lexicographically.\n\n"}, {"name": "pandas.Index.name", "path": "reference/api/pandas.index.name", "type": "Index Objects", "text": "\nReturn Index or MultiIndex name.\n\n"}, {"name": "pandas.Index.names", "path": "reference/api/pandas.index.names", "type": "Index Objects", "text": "\n\n"}, {"name": "pandas.Index.nbytes", "path": "reference/api/pandas.index.nbytes", "type": "Index Objects", "text": "\nReturn the number of bytes in the underlying data.\n\n"}, {"name": "pandas.Index.ndim", "path": "reference/api/pandas.index.ndim", "type": "Index Objects", "text": "\nNumber of dimensions of the underlying data, by definition 1.\n\n"}, {"name": "pandas.Index.nlevels", "path": "reference/api/pandas.index.nlevels", "type": "Index Objects", "text": "\nNumber of levels.\n\n"}, {"name": "pandas.Index.notna", "path": "reference/api/pandas.index.notna", "type": "Index Objects", "text": "\nDetect existing (non-missing) values.\n\nReturn a boolean same-sized object indicating if the values are not NA. Non-\nmissing values get mapped to `True`. Characters such as empty strings `''` or\n`numpy.inf` are not considered NA values (unless you set\n`pandas.options.mode.use_inf_as_na = True`). NA values, such as None or\n`numpy.NaN`, get mapped to `False` values.\n\nBoolean array to indicate which entries are not NA.\n\nSee also\n\nAlias of notna.\n\nInverse of notna.\n\nTop-level notna.\n\nExamples\n\nShow which entries in an Index are not NA. The result is an array.\n\nEmpty strings are not considered NA values. None is considered a NA value.\n\n"}, {"name": "pandas.Index.notnull", "path": "reference/api/pandas.index.notnull", "type": "Index Objects", "text": "\nDetect existing (non-missing) values.\n\nReturn a boolean same-sized object indicating if the values are not NA. Non-\nmissing values get mapped to `True`. Characters such as empty strings `''` or\n`numpy.inf` are not considered NA values (unless you set\n`pandas.options.mode.use_inf_as_na = True`). NA values, such as None or\n`numpy.NaN`, get mapped to `False` values.\n\nBoolean array to indicate which entries are not NA.\n\nSee also\n\nAlias of notna.\n\nInverse of notna.\n\nTop-level notna.\n\nExamples\n\nShow which entries in an Index are not NA. The result is an array.\n\nEmpty strings are not considered NA values. None is considered a NA value.\n\n"}, {"name": "pandas.Index.nunique", "path": "reference/api/pandas.index.nunique", "type": "Index Objects", "text": "\nReturn number of unique elements in the object.\n\nExcludes NA values by default.\n\nDon\u2019t include NaN in the count.\n\nSee also\n\nMethod nunique for DataFrame.\n\nCount non-NA/null observations in the Series.\n\nExamples\n\n"}, {"name": "pandas.Index.putmask", "path": "reference/api/pandas.index.putmask", "type": "Index Objects", "text": "\nReturn a new Index of the values set with the mask.\n\nSee also\n\nChanges elements of an array based on conditional and input values.\n\n"}, {"name": "pandas.Index.ravel", "path": "reference/api/pandas.index.ravel", "type": "Index Objects", "text": "\nReturn an ndarray of the flattened values of the underlying data.\n\nFlattened array.\n\nSee also\n\nReturn a flattened array.\n\n"}, {"name": "pandas.Index.reindex", "path": "reference/api/pandas.index.reindex", "type": "Index Objects", "text": "\nCreate index with target\u2019s values.\n\ndefault: exact matches only.\n\npad / ffill: find the PREVIOUS index value if no exact match.\n\nbackfill / bfill: use NEXT index value if no exact match\n\nnearest: use the NEAREST index value if no exact match. Tied distances are\nbroken by preferring the larger index value.\n\nLevel of multiindex.\n\nMaximum number of consecutive labels in `target` to match for inexact matches.\n\nMaximum distance between original and new labels for inexact matches. The\nvalues of the index at the matching locations must satisfy the equation\n`abs(index[indexer] - target) <= tolerance`.\n\nTolerance may be a scalar value, which applies the same tolerance to all\nvalues, or list-like, which applies variable tolerance per element. List-like\nincludes list, tuple, array, Series, and must be the same size as the index\nand its dtype must exactly match the index\u2019s type.\n\nResulting index.\n\nIndices of output values in original index.\n\nIf `method` passed along with `level`.\n\nIf non-unique multi-index\n\nIf non-unique index and `method` or `limit` passed.\n\nSee also\n\nExamples\n\n"}, {"name": "pandas.Index.rename", "path": "reference/api/pandas.index.rename", "type": "Index Objects", "text": "\nAlter Index or MultiIndex name.\n\nAble to set new names without level. Defaults to returning new index. Length\nof names must match number of levels in MultiIndex.\n\nName(s) to set.\n\nModifies the object directly, instead of creating a new Index or MultiIndex.\n\nThe same type as the caller or None if `inplace=True`.\n\nSee also\n\nAble to set new names partially and by level.\n\nExamples\n\n"}, {"name": "pandas.Index.repeat", "path": "reference/api/pandas.index.repeat", "type": "Index Objects", "text": "\nRepeat elements of a Index.\n\nReturns a new Index where each element of the current Index is repeated\nconsecutively a given number of times.\n\nThe number of repetitions for each element. This should be a non-negative\ninteger. Repeating 0 times will return an empty Index.\n\nMust be `None`. Has no effect but is accepted for compatibility with numpy.\n\nNewly created Index with repeated elements.\n\nSee also\n\nEquivalent function for Series.\n\nSimilar method for `numpy.ndarray`.\n\nExamples\n\n"}, {"name": "pandas.Index.searchsorted", "path": "reference/api/pandas.index.searchsorted", "type": "Index Objects", "text": "\nFind indices where elements should be inserted to maintain order.\n\nFind the indices into a sorted Index self such that, if the corresponding\nelements in value were inserted before the indices, the order of self would be\npreserved.\n\nNote\n\nThe Index must be monotonically sorted, otherwise wrong locations will likely\nbe returned. Pandas does not check this for you.\n\nValues to insert into self.\n\nIf \u2018left\u2019, the index of the first suitable location found is given. If\n\u2018right\u2019, return the last such index. If there is no suitable index, return\neither 0 or N (where N is the length of self).\n\nOptional array of integer indices that sort self into ascending order. They\nare typically the result of `np.argsort`.\n\nA scalar or array of insertion points with the same shape as value.\n\nSee also\n\nSort by the values along either axis.\n\nSimilar method from NumPy.\n\nNotes\n\nBinary search is used to find the required insertion points.\n\nExamples\n\nIf the values are not monotonically sorted, wrong locations may be returned:\n\n"}, {"name": "pandas.Index.set_names", "path": "reference/api/pandas.index.set_names", "type": "Index Objects", "text": "\nSet Index or MultiIndex name.\n\nAble to set new names partially and by level.\n\nName(s) to set.\n\nChanged in version 1.3.0.\n\nIf the index is a MultiIndex and names is not dict-like, level(s) to set (None\nfor all levels). Otherwise level must be None.\n\nChanged in version 1.3.0.\n\nModifies the object directly, instead of creating a new Index or MultiIndex.\n\nThe same type as the caller or None if `inplace=True`.\n\nSee also\n\nAble to set new names without level.\n\nExamples\n\nWhen renaming levels with a dict, levels can not be passed.\n\n"}, {"name": "pandas.Index.set_value", "path": "reference/api/pandas.index.set_value", "type": "Index Objects", "text": "\nFast lookup of value from 1-dimensional ndarray.\n\nDeprecated since version 1.0.\n\nNotes\n\nOnly use this if you know what you\u2019re doing.\n\n"}, {"name": "pandas.Index.shape", "path": "reference/api/pandas.index.shape", "type": "Index Objects", "text": "\nReturn a tuple of the shape of the underlying data.\n\n"}, {"name": "pandas.Index.shift", "path": "reference/api/pandas.index.shift", "type": "Index Objects", "text": "\nShift index by desired number of time frequency increments.\n\nThis method is for shifting the values of datetime-like indexes by a specified\ntime increment a given number of times.\n\nNumber of periods (or increments) to shift by, can be positive or negative.\n\nFrequency increment to shift by. If None, the index is shifted by its own freq\nattribute. Offset aliases are valid strings, e.g., \u2018D\u2019, \u2018W\u2019, \u2018M\u2019 etc.\n\nShifted index.\n\nSee also\n\nShift values of Series.\n\nNotes\n\nThis method is only implemented for datetime-like index classes, i.e.,\nDatetimeIndex, PeriodIndex and TimedeltaIndex.\n\nExamples\n\nPut the first 5 month starts of 2011 into an index.\n\nShift the index by 10 days.\n\nThe default value of freq is the freq attribute of the index, which is \u2018MS\u2019\n(month start) in this example.\n\n"}, {"name": "pandas.Index.size", "path": "reference/api/pandas.index.size", "type": "Index Objects", "text": "\nReturn the number of elements in the underlying data.\n\n"}, {"name": "pandas.Index.slice_indexer", "path": "reference/api/pandas.index.slice_indexer", "type": "Index Objects", "text": "\nCompute the slice indexer for input labels and step.\n\nIndex needs to be ordered and unique.\n\nIf None, defaults to the beginning.\n\nIf None, defaults to the end.\n\nDeprecated since version 1.4.0.\n\nnot ordered.\n\nNotes\n\nThis function assumes that the data is sorted, so use at your own peril\n\nExamples\n\nThis is a method on all index types. For example you can do:\n\n"}, {"name": "pandas.Index.slice_locs", "path": "reference/api/pandas.index.slice_locs", "type": "Index Objects", "text": "\nCompute slice locations for input labels.\n\nIf None, defaults to the beginning.\n\nIf None, defaults to the end.\n\nIf None, defaults to 1.\n\nDeprecated since version 1.4.0.\n\nSee also\n\nGet location for a single label.\n\nNotes\n\nThis method only works if the index is monotonic or unique.\n\nExamples\n\n"}, {"name": "pandas.Index.sort", "path": "reference/api/pandas.index.sort", "type": "Index Objects", "text": "\nUse sort_values instead.\n\n"}, {"name": "pandas.Index.sort_values", "path": "reference/api/pandas.index.sort_values", "type": "Index Objects", "text": "\nReturn a sorted copy of the index.\n\nReturn a sorted copy of the index, and optionally return the indices that\nsorted the index itself.\n\nShould the indices that would sort the index be returned.\n\nShould the index values be sorted in an ascending order.\n\nArgument \u2018first\u2019 puts NaNs at the beginning, \u2018last\u2019 puts NaNs at the end.\n\nNew in version 1.2.0.\n\nIf not None, apply the key function to the index values before sorting. This\nis similar to the key argument in the builtin `sorted()` function, with the\nnotable difference that this key function should be vectorized. It should\nexpect an `Index` and return an `Index` of the same shape.\n\nNew in version 1.1.0.\n\nSorted copy of the index.\n\nThe indices that the index itself was sorted by.\n\nSee also\n\nSort values of a Series.\n\nSort values in a DataFrame.\n\nExamples\n\nSort values in ascending order (default behavior).\n\nSort values in descending order, and also get the indices idx was sorted by.\n\n"}, {"name": "pandas.Index.sortlevel", "path": "reference/api/pandas.index.sortlevel", "type": "Index Objects", "text": "\nFor internal compatibility with the Index API.\n\nSort the Index. This is for compat with MultiIndex\n\nFalse to sort in descending order\n\n"}, {"name": "pandas.Index.str", "path": "reference/api/pandas.index.str", "type": "Index Objects", "text": "\nVectorized string functions for Series and Index.\n\nNAs stay NA unless handled otherwise by a particular method. Patterned after\nPython\u2019s string methods, with some inspiration from R\u2019s stringr package.\n\nExamples\n\n"}, {"name": "pandas.Index.symmetric_difference", "path": "reference/api/pandas.index.symmetric_difference", "type": "Index Objects", "text": "\nCompute the symmetric difference of two Index objects.\n\nWhether to sort the resulting index. By default, the values are attempted to\nbe sorted, but any TypeError from incomparable elements is caught by pandas.\n\nNone : Attempt to sort the result, but catch any TypeErrors from comparing\nincomparable elements.\n\nFalse : Do not sort the result.\n\nNotes\n\n`symmetric_difference` contains elements that appear in either `idx1` or\n`idx2` but not both. Equivalent to the Index created by `idx1.difference(idx2)\n| idx2.difference(idx1)` with duplicates dropped.\n\nExamples\n\n"}, {"name": "pandas.Index.T", "path": "reference/api/pandas.index.t", "type": "Index Objects", "text": "\nReturn the transpose, which is by definition self.\n\n"}, {"name": "pandas.Index.take", "path": "reference/api/pandas.index.take", "type": "Index Objects", "text": "\nReturn a new Index of the values selected by the indices.\n\nFor internal compatibility with numpy arrays.\n\nIndices to be taken.\n\nThe axis over which to select values, always 0.\n\nIf allow_fill=True and fill_value is not None, indices specified by -1 are\nregarded as NA. If Index doesn\u2019t hold NA, raise ValueError.\n\nAn index formed of elements at the given indices. Will be the same type as\nself, except for RangeIndex.\n\nSee also\n\nReturn an array formed from the elements of a at the given indices.\n\n"}, {"name": "pandas.Index.to_flat_index", "path": "reference/api/pandas.index.to_flat_index", "type": "Index Objects", "text": "\nIdentity method.\n\nThis is implemented for compatibility with subclass implementations when\nchaining.\n\nCaller.\n\nSee also\n\nSubclass implementation.\n\n"}, {"name": "pandas.Index.to_frame", "path": "reference/api/pandas.index.to_frame", "type": "DataFrame", "text": "\nCreate a DataFrame with a column containing the Index.\n\nSet the index of the returned DataFrame as the original Index.\n\nThe passed name should substitute for the index name (if it has one).\n\nDataFrame containing the original Index data.\n\nSee also\n\nConvert an Index to a Series.\n\nConvert Series to DataFrame.\n\nExamples\n\nBy default, the original Index is reused. To enforce a new Index:\n\nTo override the name of the resulting column, specify name:\n\n"}, {"name": "pandas.Index.to_list", "path": "reference/api/pandas.index.to_list", "type": "Index Objects", "text": "\nReturn a list of the values.\n\nThese are each a scalar type, which is a Python scalar (for str, int, float)\nor a pandas scalar (for Timestamp/Timedelta/Interval/Period)\n\nSee also\n\nReturn the array as an a.ndim-levels deep nested list of Python scalars.\n\n"}, {"name": "pandas.Index.to_native_types", "path": "reference/api/pandas.index.to_native_types", "type": "General utility functions", "text": "\nFormat specified values of self and return them.\n\nDeprecated since version 1.2.0.\n\nAn indexer into self that specifies which values are used in the formatting\nprocess.\n\nOptions for specifying how the values should be formatted. These options\ninclude the following:\n\nThe value that serves as a placeholder for NULL values\n\nWhether or not there are quoted values in self\n\nThe format used to represent date-like values.\n\nFormatted values.\n\n"}, {"name": "pandas.Index.to_numpy", "path": "reference/api/pandas.index.to_numpy", "type": "Index Objects", "text": "\nA NumPy ndarray representing the values in this Series or Index.\n\nThe dtype to pass to `numpy.asarray()`.\n\nWhether to ensure that the returned value is not a view on another array. Note\nthat `copy=False` does not ensure that `to_numpy()` is no-copy. Rather,\n`copy=True` ensure that a copy is made, even if not strictly necessary.\n\nThe value to use for missing values. The default value depends on dtype and\nthe type of the array.\n\nNew in version 1.0.0.\n\nAdditional keywords passed through to the `to_numpy` method of the underlying\narray (for extension arrays).\n\nNew in version 1.0.0.\n\nSee also\n\nGet the actual data stored within.\n\nGet the actual data stored within.\n\nSimilar method for DataFrame.\n\nNotes\n\nThe returned array will be the same up to equality (values equal in self will\nbe equal in the returned array; likewise for values that are not equal). When\nself contains an ExtensionArray, the dtype may be different. For example, for\na category-dtype Series, `to_numpy()` will return a NumPy array and the\ncategorical dtype will be lost.\n\nFor NumPy dtypes, this will be a reference to the actual data stored in this\nSeries or Index (assuming `copy=False`). Modifying the result in place will\nmodify the data stored in the Series or Index (not that we recommend doing\nthat).\n\nFor extension types, `to_numpy()` may require copying data and coercing the\nresult to a NumPy type (possibly object), which may be expensive. When you\nneed a no-copy reference to the underlying data, `Series.array` should be used\ninstead.\n\nThis table lays out the different dtypes and default return types of\n`to_numpy()` for various dtypes within pandas.\n\ndtype\n\narray type\n\ncategory[T]\n\nndarray[T] (same dtype as input)\n\nperiod\n\nndarray[object] (Periods)\n\ninterval\n\nndarray[object] (Intervals)\n\nIntegerNA\n\nndarray[object]\n\ndatetime64[ns]\n\ndatetime64[ns]\n\ndatetime64[ns, tz]\n\nndarray[object] (Timestamps)\n\nExamples\n\nSpecify the dtype to control how datetime-aware data is represented. Use\n`dtype=object` to return an ndarray of pandas `Timestamp` objects, each with\nthe correct `tz`.\n\nOr `dtype='datetime64[ns]'` to return an ndarray of native datetime64 values.\nThe values are converted to UTC and the timezone info is dropped.\n\n"}, {"name": "pandas.Index.to_series", "path": "reference/api/pandas.index.to_series", "type": "Index Objects", "text": "\nCreate a Series with both index and values equal to the index keys.\n\nUseful with map for returning an indexer based on an index.\n\nIndex of resulting Series. If None, defaults to original index.\n\nName of resulting Series. If None, defaults to name of original index.\n\nThe dtype will be based on the type of the Index values.\n\nSee also\n\nConvert an Index to a DataFrame.\n\nConvert Series to DataFrame.\n\nExamples\n\nBy default, the original Index and original name is reused.\n\nTo enforce a new Index, specify new labels to `index`:\n\nTo override the name of the resulting column, specify name:\n\n"}, {"name": "pandas.Index.tolist", "path": "reference/api/pandas.index.tolist", "type": "Index Objects", "text": "\nReturn a list of the values.\n\nThese are each a scalar type, which is a Python scalar (for str, int, float)\nor a pandas scalar (for Timestamp/Timedelta/Interval/Period)\n\nSee also\n\nReturn the array as an a.ndim-levels deep nested list of Python scalars.\n\n"}, {"name": "pandas.Index.transpose", "path": "reference/api/pandas.index.transpose", "type": "Index Objects", "text": "\nReturn the transpose, which is by definition self.\n\n"}, {"name": "pandas.Index.union", "path": "reference/api/pandas.index.union", "type": "Input/output", "text": "\nForm the union of two Index objects.\n\nIf the Index objects are incompatible, both Index objects will be cast to\ndtype(\u2018object\u2019) first.\n\nChanged in version 0.25.0.\n\nWhether to sort the resulting Index.\n\nNone : Sort the result, except when\n\nself and other are equal.\n\nself or other has length 0.\n\nSome values in self or other cannot be compared. A RuntimeWarning is issued in\nthis case.\n\nFalse : do not sort the result.\n\nExamples\n\nUnion matching dtypes\n\nUnion mismatched dtypes\n\nMultiIndex case\n\n"}, {"name": "pandas.Index.unique", "path": "reference/api/pandas.index.unique", "type": "Index Objects", "text": "\nReturn unique values in the index.\n\nUnique values are returned in order of appearance, this does NOT sort.\n\nOnly return values from specified level (for MultiIndex). If int, gets the\nlevel by integer position, else by level name.\n\nSee also\n\nNumpy array of unique values in that column.\n\nReturn unique values of Series object.\n\n"}, {"name": "pandas.Index.value_counts", "path": "reference/api/pandas.index.value_counts", "type": "Index Objects", "text": "\nReturn a Series containing counts of unique values.\n\nThe resulting object will be in descending order so that the first element is\nthe most frequently-occurring element. Excludes NA values by default.\n\nIf True then the object returned will contain the relative frequencies of the\nunique values.\n\nSort by frequencies.\n\nSort in ascending order.\n\nRather than count values, group them into half-open bins, a convenience for\n`pd.cut`, only works with numeric data.\n\nDon\u2019t include counts of NaN.\n\nSee also\n\nNumber of non-NA elements in a Series.\n\nNumber of non-NA elements in a DataFrame.\n\nEquivalent method on DataFrames.\n\nExamples\n\nWith normalize set to True, returns the relative frequency by dividing all\nvalues by the sum of values.\n\nbins\n\nBins can be useful for going from a continuous variable to a categorical\nvariable; instead of counting unique apparitions of values, divide the index\nin the specified number of half-open bins.\n\ndropna\n\nWith dropna set to False we can also see NaN index values.\n\n"}, {"name": "pandas.Index.values", "path": "reference/api/pandas.index.values", "type": "Index Objects", "text": "\nReturn an array representing the data in the Index.\n\nWarning\n\nWe recommend using `Index.array` or `Index.to_numpy()`, depending on whether\nyou need a reference to the underlying data or a NumPy array.\n\nSee also\n\nReference to the underlying data.\n\nA NumPy array representing the underlying data.\n\n"}, {"name": "pandas.Index.view", "path": "reference/api/pandas.index.view", "type": "Index Objects", "text": "\n\n"}, {"name": "pandas.Index.where", "path": "reference/api/pandas.index.where", "type": "Index Objects", "text": "\nReplace values where the condition is False.\n\nThe replacement is taken from other.\n\nCondition to select the values on.\n\nReplacement if the condition is False.\n\nA copy of self with values replaced from other where the condition is False.\n\nSee also\n\nSame method for Series.\n\nSame method for DataFrame.\n\nExamples\n\n"}, {"name": "pandas.IndexSlice", "path": "reference/api/pandas.indexslice", "type": "Index Objects", "text": "\nCreate an object to more easily perform multi-index slicing.\n\nSee also\n\nNew MultiIndex with no unused levels.\n\nNotes\n\nSee Defined Levels for further info on slicing a MultiIndex.\n\nExamples\n\nUsing the default slice command:\n\nUsing the IndexSlice class for a more intuitive command:\n\n"}, {"name": "pandas.infer_freq", "path": "reference/api/pandas.infer_freq", "type": "General functions", "text": "\nInfer the most likely frequency given the input index. If the frequency is\nuncertain, a warning will be printed.\n\nIf passed a Series will use the values of the series (NOT THE INDEX).\n\nNone if no discernible frequency.\n\nIf the index is not datetime-like.\n\nIf there are fewer than three values.\n\nExamples\n\n"}, {"name": "pandas.Int16Dtype", "path": "reference/api/pandas.int16dtype", "type": "Pandas arrays", "text": "\nAn ExtensionDtype for int16 integer data.\n\nChanged in version 1.0.0: Now uses `pandas.NA` as its missing value, rather\nthan `numpy.nan`.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.Int32Dtype", "path": "reference/api/pandas.int32dtype", "type": "Pandas arrays", "text": "\nAn ExtensionDtype for int32 integer data.\n\nChanged in version 1.0.0: Now uses `pandas.NA` as its missing value, rather\nthan `numpy.nan`.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.Int64Dtype", "path": "reference/api/pandas.int64dtype", "type": "Pandas arrays", "text": "\nAn ExtensionDtype for int64 integer data.\n\nChanged in version 1.0.0: Now uses `pandas.NA` as its missing value, rather\nthan `numpy.nan`.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.Int64Index", "path": "reference/api/pandas.int64index", "type": "Index Objects", "text": "\nImmutable sequence used for indexing and alignment. The basic object storing\naxis labels for all pandas objects. Int64Index is a special case of Index with\npurely integer labels. .\n\nDeprecated since version 1.4.0: In pandas v2.0 Int64Index will be removed and\n`NumericIndex` used instead. Int64Index will remain fully functional for the\nduration of pandas 1.x.\n\nMake a copy of input ndarray.\n\nName to be stored in the index.\n\nSee also\n\nThe base pandas Index type.\n\nIndex of numpy int/uint/float data.\n\nNotes\n\nAn Index instance can only contain hashable objects.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.Int8Dtype", "path": "reference/api/pandas.int8dtype", "type": "Pandas arrays", "text": "\nAn ExtensionDtype for int8 integer data.\n\nChanged in version 1.0.0: Now uses `pandas.NA` as its missing value, rather\nthan `numpy.nan`.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.Interval", "path": "reference/api/pandas.interval", "type": "Pandas arrays", "text": "\nImmutable object implementing an Interval, a bounded slice-like interval.\n\nLeft bound for the interval.\n\nRight bound for the interval.\n\nWhether the interval is closed on the left-side, right-side, both or neither.\nSee the Notes for more detailed explanation.\n\nSee also\n\nAn Index of Interval objects that are all closed on the same side.\n\nConvert continuous data into discrete bins (Categorical of Interval objects).\n\nConvert continuous data into bins (Categorical of Interval objects) based on\nquantiles.\n\nRepresents a period of time.\n\nNotes\n\nThe parameters left and right must be from the same type, you must be able to\ncompare them and they must satisfy `left <= right`.\n\nA closed interval (in mathematics denoted by square brackets) contains its\nendpoints, i.e. the closed interval `[0, 5]` is characterized by the\nconditions `0 <= x <= 5`. This is what `closed='both'` stands for. An open\ninterval (in mathematics denoted by parentheses) does not contain its\nendpoints, i.e. the open interval `(0, 5)` is characterized by the conditions\n`0 < x < 5`. This is what `closed='neither'` stands for. Intervals can also be\nhalf-open or half-closed, i.e. `[0, 5)` is described by `0 <= x < 5`\n(`closed='left'`) and `(0, 5]` is described by `0 < x <= 5`\n(`closed='right'`).\n\nExamples\n\nIt is possible to build Intervals of different types, like numeric ones:\n\nYou can check if an element belongs to it\n\nYou can test the bounds (`closed='right'`, so `0 < x <= 5`):\n\nCalculate its length\n\nYou can operate with + and * over an Interval and the operation is applied to\neach of its bounds, so the result depends on the type of the bound elements\n\nTo create a time interval you can use Timestamps as the bounds\n\nAttributes\n\n`closed`\n\nWhether the interval is closed on the left-side, right-side, both or neither.\n\n`closed_left`\n\nCheck if the interval is closed on the left side.\n\n`closed_right`\n\nCheck if the interval is closed on the right side.\n\n`is_empty`\n\nIndicates if an interval is empty, meaning it contains no points.\n\n`left`\n\nLeft bound for the interval.\n\n`length`\n\nReturn the length of the Interval.\n\n`mid`\n\nReturn the midpoint of the Interval.\n\n`open_left`\n\nCheck if the interval is open on the left side.\n\n`open_right`\n\nCheck if the interval is open on the right side.\n\n`right`\n\nRight bound for the interval.\n\nMethods\n\n`overlaps`\n\nCheck whether two Interval objects overlap.\n\n"}, {"name": "pandas.Interval.closed", "path": "reference/api/pandas.interval.closed", "type": "Pandas arrays", "text": "\nWhether the interval is closed on the left-side, right-side, both or neither.\n\n"}, {"name": "pandas.Interval.closed_left", "path": "reference/api/pandas.interval.closed_left", "type": "Pandas arrays", "text": "\nCheck if the interval is closed on the left side.\n\nFor the meaning of closed and open see `Interval`.\n\nTrue if the Interval is closed on the left-side.\n\n"}, {"name": "pandas.Interval.closed_right", "path": "reference/api/pandas.interval.closed_right", "type": "Pandas arrays", "text": "\nCheck if the interval is closed on the right side.\n\nFor the meaning of closed and open see `Interval`.\n\nTrue if the Interval is closed on the left-side.\n\n"}, {"name": "pandas.Interval.is_empty", "path": "reference/api/pandas.interval.is_empty", "type": "Pandas arrays", "text": "\nIndicates if an interval is empty, meaning it contains no points.\n\nNew in version 0.25.0.\n\nA boolean indicating if a scalar `Interval` is empty, or a boolean `ndarray`\npositionally indicating if an `Interval` in an `IntervalArray` or\n`IntervalIndex` is empty.\n\nExamples\n\nAn `Interval` that contains points is not empty:\n\nAn `Interval` that does not contain any points is empty:\n\nAn `Interval` that contains a single point is not empty:\n\nAn `IntervalArray` or `IntervalIndex` returns a boolean `ndarray` positionally\nindicating if an `Interval` is empty:\n\nMissing values are not considered empty:\n\n"}, {"name": "pandas.Interval.left", "path": "reference/api/pandas.interval.left", "type": "Pandas arrays", "text": "\nLeft bound for the interval.\n\n"}, {"name": "pandas.Interval.length", "path": "reference/api/pandas.interval.length", "type": "Pandas arrays", "text": "\nReturn the length of the Interval.\n\n"}, {"name": "pandas.Interval.mid", "path": "reference/api/pandas.interval.mid", "type": "Pandas arrays", "text": "\nReturn the midpoint of the Interval.\n\n"}, {"name": "pandas.Interval.open_left", "path": "reference/api/pandas.interval.open_left", "type": "Pandas arrays", "text": "\nCheck if the interval is open on the left side.\n\nFor the meaning of closed and open see `Interval`.\n\nTrue if the Interval is closed on the left-side.\n\n"}, {"name": "pandas.Interval.open_right", "path": "reference/api/pandas.interval.open_right", "type": "Pandas arrays", "text": "\nCheck if the interval is open on the right side.\n\nFor the meaning of closed and open see `Interval`.\n\nTrue if the Interval is closed on the left-side.\n\n"}, {"name": "pandas.Interval.overlaps", "path": "reference/api/pandas.interval.overlaps", "type": "Pandas arrays", "text": "\nCheck whether two Interval objects overlap.\n\nTwo intervals overlap if they share a common point, including closed\nendpoints. Intervals that only have an open endpoint in common do not overlap.\n\nInterval to check against for an overlap.\n\nTrue if the two intervals overlap.\n\nSee also\n\nThe corresponding method for IntervalArray.\n\nThe corresponding method for IntervalIndex.\n\nExamples\n\nIntervals that share closed endpoints overlap:\n\nIntervals that only have an open endpoint in common do not overlap:\n\n"}, {"name": "pandas.Interval.right", "path": "reference/api/pandas.interval.right", "type": "Pandas arrays", "text": "\nRight bound for the interval.\n\n"}, {"name": "pandas.interval_range", "path": "reference/api/pandas.interval_range", "type": "General functions", "text": "\nReturn a fixed frequency IntervalIndex.\n\nLeft bound for generating intervals.\n\nRight bound for generating intervals.\n\nNumber of periods to generate.\n\nThe length of each interval. Must be consistent with the type of start and\nend, e.g. 2 for numeric, or \u20185H\u2019 for datetime-like. Default is 1 for numeric\nand \u2018D\u2019 for datetime-like.\n\nName of the resulting IntervalIndex.\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\nSee also\n\nAn Index of intervals that are all closed on the same side.\n\nNotes\n\nOf the four parameters `start`, `end`, `periods`, and `freq`, exactly three\nmust be specified. If `freq` is omitted, the resulting `IntervalIndex` will\nhave `periods` linearly spaced elements between `start` and `end`,\ninclusively.\n\nTo learn more about datetime-like frequency strings, please see this link.\n\nExamples\n\nNumeric `start` and `end` is supported.\n\nAdditionally, datetime-like input is also supported.\n\nThe `freq` parameter specifies the frequency between the left and right.\nendpoints of the individual intervals within the `IntervalIndex`. For numeric\n`start` and `end`, the frequency must also be numeric.\n\nSimilarly, for datetime-like `start` and `end`, the frequency must be\nconvertible to a DateOffset.\n\nSpecify `start`, `end`, and `periods`; the frequency is generated\nautomatically (linearly spaced).\n\nThe `closed` parameter specifies which endpoints of the individual intervals\nwithin the `IntervalIndex` are closed.\n\n"}, {"name": "pandas.IntervalDtype", "path": "reference/api/pandas.intervaldtype", "type": "Pandas arrays", "text": "\nAn ExtensionDtype for Interval data.\n\nThis is not an actual numpy dtype, but a duck type.\n\nThe dtype of the Interval bounds.\n\nExamples\n\nAttributes\n\n`subtype`\n\nThe dtype of the Interval bounds.\n\nMethods\n\nNone\n\n"}, {"name": "pandas.IntervalDtype.subtype", "path": "reference/api/pandas.intervaldtype.subtype", "type": "Pandas arrays", "text": "\nThe dtype of the Interval bounds.\n\n"}, {"name": "pandas.IntervalIndex", "path": "reference/api/pandas.intervalindex", "type": "Index Objects", "text": "\nImmutable index of intervals that are closed on the same side.\n\nNew in version 0.20.0.\n\nArray-like containing Interval objects from which to build the IntervalIndex.\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\nIf None, dtype will be inferred.\n\nCopy the input data.\n\nName to be stored in the index.\n\nVerify that the IntervalIndex is valid.\n\nSee also\n\nThe base pandas Index type.\n\nA bounded slice-like interval; the elements of an IntervalIndex.\n\nFunction to create a fixed frequency IntervalIndex.\n\nBin values into discrete Intervals.\n\nBin values into equal-sized Intervals based on rank or sample quantiles.\n\nNotes\n\nSee the user guide for more.\n\nExamples\n\nA new `IntervalIndex` is typically constructed using `interval_range()`:\n\nIt may also be constructed using one of the constructor methods:\n`IntervalIndex.from_arrays()`, `IntervalIndex.from_breaks()`, and\n`IntervalIndex.from_tuples()`.\n\nSee further examples in the doc strings of `interval_range` and the mentioned\nconstructor methods.\n\nAttributes\n\n`closed`\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\n`is_empty`\n\nIndicates if an interval is empty, meaning it contains no points.\n\n`is_non_overlapping_monotonic`\n\nReturn True if the IntervalArray is non-overlapping (no Intervals share\npoints) and is either monotonic increasing or monotonic decreasing, else\nFalse.\n\n`is_overlapping`\n\nReturn True if the IntervalIndex has overlapping intervals, else False.\n\n`values`\n\nReturn an array representing the data in the Index.\n\nleft\n\nright\n\nmid\n\nlength\n\nMethods\n\n`from_arrays`(left, right[, closed, name, ...])\n\nConstruct from two arrays defining the left and right bounds.\n\n`from_tuples`(data[, closed, name, copy, dtype])\n\nConstruct an IntervalIndex from an array-like of tuples.\n\n`from_breaks`(breaks[, closed, name, copy, dtype])\n\nConstruct an IntervalIndex from an array of splits.\n\n`contains`(*args, **kwargs)\n\nCheck elementwise if the Intervals contain the value.\n\n`overlaps`(*args, **kwargs)\n\nCheck elementwise if an Interval overlaps the values in the IntervalArray.\n\n`set_closed`(*args, **kwargs)\n\nReturn an IntervalArray identical to the current one, but closed on the\nspecified side.\n\n`to_tuples`(*args, **kwargs)\n\nReturn an ndarray of tuples of the form (left, right).\n\n"}, {"name": "pandas.IntervalIndex.closed", "path": "reference/api/pandas.intervalindex.closed", "type": "Index Objects", "text": "\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\n"}, {"name": "pandas.IntervalIndex.contains", "path": "reference/api/pandas.intervalindex.contains", "type": "Index Objects", "text": "\nCheck elementwise if the Intervals contain the value.\n\nReturn a boolean mask whether the value is contained in the Intervals of the\nIntervalArray.\n\nNew in version 0.25.0.\n\nThe value to check whether it is contained in the Intervals.\n\nSee also\n\nCheck whether Interval object contains value.\n\nCheck if an Interval overlaps the values in the IntervalArray.\n\nExamples\n\n"}, {"name": "pandas.IntervalIndex.from_arrays", "path": "reference/api/pandas.intervalindex.from_arrays", "type": "Index Objects", "text": "\nConstruct from two arrays defining the left and right bounds.\n\nLeft bounds for each interval.\n\nRight bounds for each interval.\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\nCopy the data.\n\nIf None, dtype will be inferred.\n\nWhen a value is missing in only one of left or right. When a value in left is\ngreater than the corresponding value in right.\n\nSee also\n\nFunction to create a fixed frequency IntervalIndex.\n\nConstruct an IntervalIndex from an array of splits.\n\nConstruct an IntervalIndex from an array-like of tuples.\n\nNotes\n\nEach element of left must be less than or equal to the right element at the\nsame position. If an element is missing, it must be missing in both left and\nright. A TypeError is raised when using an unsupported type for left or right.\nAt the moment, \u2018category\u2019, \u2018object\u2019, and \u2018string\u2019 subtypes are not supported.\n\nExamples\n\n"}, {"name": "pandas.IntervalIndex.from_breaks", "path": "reference/api/pandas.intervalindex.from_breaks", "type": "Index Objects", "text": "\nConstruct an IntervalIndex from an array of splits.\n\nLeft and right bounds for each interval.\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\nCopy the data.\n\nIf None, dtype will be inferred.\n\nSee also\n\nFunction to create a fixed frequency IntervalIndex.\n\nConstruct from a left and right array.\n\nConstruct from a sequence of tuples.\n\nExamples\n\n"}, {"name": "pandas.IntervalIndex.from_tuples", "path": "reference/api/pandas.intervalindex.from_tuples", "type": "Index Objects", "text": "\nConstruct an IntervalIndex from an array-like of tuples.\n\nArray of tuples.\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\nBy-default copy the data, this is compat only and ignored.\n\nIf None, dtype will be inferred.\n\nSee also\n\nFunction to create a fixed frequency IntervalIndex.\n\nConstruct an IntervalIndex from a left and right array.\n\nConstruct an IntervalIndex from an array of splits.\n\nExamples\n\n"}, {"name": "pandas.IntervalIndex.get_indexer", "path": "reference/api/pandas.intervalindex.get_indexer", "type": "Index Objects", "text": "\nCompute indexer and mask for new index given the current index. The indexer\nshould be then used as an input to ndarray.take to align the current data to\nthe new index.\n\ndefault: exact matches only.\n\npad / ffill: find the PREVIOUS index value if no exact match.\n\nbackfill / bfill: use NEXT index value if no exact match\n\nnearest: use the NEAREST index value if no exact match. Tied distances are\nbroken by preferring the larger index value.\n\nMaximum number of consecutive labels in `target` to match for inexact matches.\n\nMaximum distance between original and new labels for inexact matches. The\nvalues of the index at the matching locations must satisfy the equation\n`abs(index[indexer] - target) <= tolerance`.\n\nTolerance may be a scalar value, which applies the same tolerance to all\nvalues, or list-like, which applies variable tolerance per element. List-like\nincludes list, tuple, array, Series, and must be the same size as the index\nand its dtype must exactly match the index\u2019s type.\n\nIntegers from 0 to n - 1 indicating that the index at these positions matches\nthe corresponding target values. Missing values in the target are marked by\n-1.\n\nNotes\n\nReturns -1 for unmatched values, for further explanation see the example\nbelow.\n\nExamples\n\nNotice that the return value is an array of locations in `index` and `x` is\nmarked by -1, as it is not in `index`.\n\n"}, {"name": "pandas.IntervalIndex.get_loc", "path": "reference/api/pandas.intervalindex.get_loc", "type": "Index Objects", "text": "\nGet integer location, slice or boolean mask for requested label.\n\ndefault: matches where the label is within an interval only.\n\nExamples\n\nYou can also supply a point inside an interval.\n\nIf a label is in several intervals, you get the locations of all the relevant\nintervals.\n\nOnly exact matches will be returned if an interval is provided.\n\n"}, {"name": "pandas.IntervalIndex.is_empty", "path": "reference/api/pandas.intervalindex.is_empty", "type": "Index Objects", "text": "\nIndicates if an interval is empty, meaning it contains no points.\n\nNew in version 0.25.0.\n\nA boolean indicating if a scalar `Interval` is empty, or a boolean `ndarray`\npositionally indicating if an `Interval` in an `IntervalArray` or\n`IntervalIndex` is empty.\n\nExamples\n\nAn `Interval` that contains points is not empty:\n\nAn `Interval` that does not contain any points is empty:\n\nAn `Interval` that contains a single point is not empty:\n\nAn `IntervalArray` or `IntervalIndex` returns a boolean `ndarray` positionally\nindicating if an `Interval` is empty:\n\nMissing values are not considered empty:\n\n"}, {"name": "pandas.IntervalIndex.is_non_overlapping_monotonic", "path": "reference/api/pandas.intervalindex.is_non_overlapping_monotonic", "type": "Index Objects", "text": "\nReturn True if the IntervalArray is non-overlapping (no Intervals share\npoints) and is either monotonic increasing or monotonic decreasing, else\nFalse.\n\n"}, {"name": "pandas.IntervalIndex.is_overlapping", "path": "reference/api/pandas.intervalindex.is_overlapping", "type": "Index Objects", "text": "\nReturn True if the IntervalIndex has overlapping intervals, else False.\n\nTwo intervals overlap if they share a common point, including closed\nendpoints. Intervals that only have an open endpoint in common do not overlap.\n\nBoolean indicating if the IntervalIndex has overlapping intervals.\n\nSee also\n\nCheck whether two Interval objects overlap.\n\nCheck an IntervalIndex elementwise for overlaps.\n\nExamples\n\nIntervals that share closed endpoints overlap:\n\nIntervals that only have an open endpoint in common do not overlap:\n\n"}, {"name": "pandas.IntervalIndex.left", "path": "reference/api/pandas.intervalindex.left", "type": "Index Objects", "text": "\n\n"}, {"name": "pandas.IntervalIndex.length", "path": "reference/api/pandas.intervalindex.length", "type": "Index Objects", "text": "\n\n"}, {"name": "pandas.IntervalIndex.mid", "path": "reference/api/pandas.intervalindex.mid", "type": "Index Objects", "text": "\n\n"}, {"name": "pandas.IntervalIndex.overlaps", "path": "reference/api/pandas.intervalindex.overlaps", "type": "Index Objects", "text": "\nCheck elementwise if an Interval overlaps the values in the IntervalArray.\n\nTwo intervals overlap if they share a common point, including closed\nendpoints. Intervals that only have an open endpoint in common do not overlap.\n\nInterval to check against for an overlap.\n\nBoolean array positionally indicating where an overlap occurs.\n\nSee also\n\nCheck whether two Interval objects overlap.\n\nExamples\n\nIntervals that share closed endpoints overlap:\n\nIntervals that only have an open endpoint in common do not overlap:\n\n"}, {"name": "pandas.IntervalIndex.right", "path": "reference/api/pandas.intervalindex.right", "type": "Index Objects", "text": "\n\n"}, {"name": "pandas.IntervalIndex.set_closed", "path": "reference/api/pandas.intervalindex.set_closed", "type": "Index Objects", "text": "\nReturn an IntervalArray identical to the current one, but closed on the\nspecified side.\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\nExamples\n\n"}, {"name": "pandas.IntervalIndex.to_tuples", "path": "reference/api/pandas.intervalindex.to_tuples", "type": "Index Objects", "text": "\nReturn an ndarray of tuples of the form (left, right).\n\nReturns NA as a tuple if True, `(nan, nan)`, or just as the NA value itself if\nFalse, `nan`.\n\n"}, {"name": "pandas.IntervalIndex.values", "path": "reference/api/pandas.intervalindex.values", "type": "Index Objects", "text": "\nReturn an array representing the data in the Index.\n\nWarning\n\nWe recommend using `Index.array` or `Index.to_numpy()`, depending on whether\nyou need a reference to the underlying data or a NumPy array.\n\nSee also\n\nReference to the underlying data.\n\nA NumPy array representing the underlying data.\n\n"}, {"name": "pandas.io.formats.style.Styler", "path": "reference/api/pandas.io.formats.style.styler", "type": "Style", "text": "\nHelps style a DataFrame or Series according to the data with HTML and CSS.\n\nData to be styled - either a Series or DataFrame.\n\nPrecision to round floats to. If not given defaults to\n`pandas.options.styler.format.precision`.\n\nChanged in version 1.4.0.\n\nList of {selector: (attr, value)} dicts; see Notes.\n\nA unique identifier to avoid CSS collisions; generated automatically.\n\nString caption to attach to the table. Tuple only used for LaTeX dual\ncaptions.\n\nItems that show up in the opening `<table>` tag in addition to automatic (by\ndefault) id.\n\nIf True, each cell will have an `id` attribute in their HTML tag. The `id`\ntakes the form `T_<uuid>_row<num_row>_col<num_col>` where `<uuid>` is the\nunique identifier, `<num_row>` is the row number and `<num_col>` is the column\nnumber.\n\nRepresentation for missing values. If `na_rep` is None, no special formatting\nis applied, and falls back to `pandas.options.styler.format.na_rep`.\n\nNew in version 1.0.0.\n\nIf `uuid` is not specified, the length of the `uuid` to randomly generate\nexpressed in hex characters, in range [0, 32].\n\nNew in version 1.2.0.\n\nCharacter used as decimal separator for floats, complex and integers. If not\ngiven uses `pandas.options.styler.format.decimal`.\n\nNew in version 1.3.0.\n\nCharacter used as thousands separator for floats, complex and integers. If not\ngiven uses `pandas.options.styler.format.thousands`.\n\nNew in version 1.3.0.\n\nUse \u2018html\u2019 to replace the characters `&`, `<`, `>`, `'`, and `\"` in cell\ndisplay string with HTML-safe sequences. Use \u2018latex\u2019 to replace the characters\n`&`, `%`, `$`, `#`, `_`, `{`, `}`, `~`, `^`, and `\\` in the cell display\nstring with LaTeX-safe sequences. If not given uses\n`pandas.options.styler.format.escape`.\n\nNew in version 1.3.0.\n\nObject to define how values are displayed. See `Styler.format`. If not given\nuses `pandas.options.styler.format.formatter`.\n\nNew in version 1.4.0.\n\nSee also\n\nReturn a Styler object containing methods for building a styled HTML\nrepresentation for the DataFrame.\n\nNotes\n\nMost styling will be done by passing style functions into `Styler.apply` or\n`Styler.applymap`. Style functions should return values with strings\ncontaining CSS `'attr: value'` that will be applied to the indicated cells.\n\nIf using in the Jupyter notebook, Styler has defined a `_repr_html_` to\nautomatically render itself. Otherwise call Styler.to_html to get the\ngenerated HTML.\n\nCSS classes are attached to the generated HTML\n\nIndex and Column names include `index_name` and `level<k>` where k is its\nlevel in a MultiIndex\n\nIndex label cells include\n\n`row_heading`\n\n`row<n>` where n is the numeric position of the row\n\n`level<k>` where k is the level in a MultiIndex\n\nColumn label cells include * `col_heading` * `col<n>` where n is the numeric\nposition of the column * `level<k>` where k is the level in a MultiIndex\n\nBlank cells include `blank`\n\nData cells include `data`\n\nTrimmed cells include `col_trim` or `row_trim`.\n\nAny, or all, or these classes can be renamed by using the `css_class_names`\nargument in `Styler.set_table_classes`, giving a value such as {\u201crow\u201d:\n\u201cMY_ROW_CLASS\u201d, \u201ccol_trim\u201d: \u201c\u201d, \u201crow_trim\u201d: \u201c\u201d}.\n\nAttributes\n\nenv\n\n(Jinja2 jinja2.Environment)\n\ntemplate_html\n\n(Jinja2 Template)\n\ntemplate_html_table\n\n(Jinja2 Template)\n\ntemplate_html_style\n\n(Jinja2 Template)\n\ntemplate_latex\n\n(Jinja2 Template)\n\nloader\n\n(Jinja2 Loader)\n\nMethods\n\n`apply`(func[, axis, subset])\n\nApply a CSS-styling function column-wise, row-wise, or table-wise.\n\n`apply_index`(func[, axis, level])\n\nApply a CSS-styling function to the index or column headers, level-wise.\n\n`applymap`(func[, subset])\n\nApply a CSS-styling function elementwise.\n\n`applymap_index`(func[, axis, level])\n\nApply a CSS-styling function to the index or column headers, elementwise.\n\n`background_gradient`([cmap, low, high, axis, ...])\n\nColor the background in a gradient style.\n\n`bar`([subset, axis, color, cmap, width, ...])\n\nDraw bar chart in the cell backgrounds.\n\n`clear`()\n\nReset the `Styler`, removing any previously applied styles.\n\n`export`()\n\nExport the styles applied to the current Styler.\n\n`format`([formatter, subset, na_rep, ...])\n\nFormat the text display value of cells.\n\n`format_index`([formatter, axis, level, ...])\n\nFormat the text display value of index labels or column headers.\n\n`from_custom_template`(searchpath[, ...])\n\nFactory function for creating a subclass of `Styler`.\n\n`hide`([subset, axis, level, names])\n\nHide the entire index / column headers, or specific rows / columns from\ndisplay.\n\n`hide_columns`([subset, level, names])\n\nHide the column headers or specific keys in the columns from rendering.\n\n`hide_index`([subset, level, names])\n\n(DEPRECATED) Hide the entire index, or specific keys in the index from\nrendering.\n\n`highlight_between`([subset, color, axis, ...])\n\nHighlight a defined range with a style.\n\n`highlight_max`([subset, color, axis, props])\n\nHighlight the maximum with a style.\n\n`highlight_min`([subset, color, axis, props])\n\nHighlight the minimum with a style.\n\n`highlight_null`([null_color, subset, props])\n\nHighlight missing values with a style.\n\n`highlight_quantile`([subset, color, axis, ...])\n\nHighlight values defined by a quantile with a style.\n\n`pipe`(func, *args, **kwargs)\n\nApply `func(self, *args, **kwargs)`, and return the result.\n\n`render`([sparse_index, sparse_columns])\n\n(DEPRECATED) Render the `Styler` including all applied styles to HTML.\n\n`set_caption`(caption)\n\nSet the text added to a `<caption>` HTML element.\n\n`set_na_rep`(na_rep)\n\n(DEPRECATED) Set the missing data representation on a `Styler`.\n\n`set_precision`(precision)\n\n(DEPRECATED) Set the precision used to display values.\n\n`set_properties`([subset])\n\nSet defined CSS-properties to each `<td>` HTML element within the given\nsubset.\n\n`set_sticky`([axis, pixel_size, levels])\n\nAdd CSS to permanently display the index or column headers in a scrolling\nframe.\n\n`set_table_attributes`(attributes)\n\nSet the table attributes added to the `<table>` HTML element.\n\n`set_table_styles`([table_styles, axis, ...])\n\nSet the table styles included within the `<style>` HTML element.\n\n`set_td_classes`(classes)\n\nSet the DataFrame of strings added to the `class` attribute of `<td>` HTML\nelements.\n\n`set_tooltips`(ttips[, props, css_class])\n\nSet the DataFrame of strings on `Styler` generating `:hover` tooltips.\n\n`set_uuid`(uuid)\n\nSet the uuid applied to `id` attributes of HTML elements.\n\n`text_gradient`([cmap, low, high, axis, ...])\n\nColor the text in a gradient style.\n\n`to_excel`(excel_writer[, sheet_name, na_rep, ...])\n\nWrite Styler to an Excel sheet.\n\n`to_html`([buf, table_uuid, table_attributes, ...])\n\nWrite Styler to a file, buffer or string in HTML-CSS format.\n\n`to_latex`([buf, column_format, position, ...])\n\nWrite Styler to a file, buffer or string in LaTeX format.\n\n`use`(styles)\n\nSet the styles on the current Styler.\n\n`where`(cond, value[, other, subset])\n\n(DEPRECATED) Apply CSS-styles based on a conditional function elementwise.\n\n"}, {"name": "pandas.io.formats.style.Styler.apply", "path": "reference/api/pandas.io.formats.style.styler.apply", "type": "Style", "text": "\nApply a CSS-styling function column-wise, row-wise, or table-wise.\n\nUpdates the HTML representation with the result.\n\n`func` should take a Series if `axis` in [0,1] and return a list-like object\nof same length, or a Series, not necessarily of same length, with valid index\nlabels considering `subset`. `func` should take a DataFrame if `axis` is\n`None` and return either an ndarray with the same shape or a DataFrame, not\nnecessarily of the same shape, with valid index and columns labels considering\n`subset`.\n\nChanged in version 1.3.0.\n\nChanged in version 1.4.0.\n\nApply to each column (`axis=0` or `'index'`), to each row (`axis=1` or\n`'columns'`), or to the entire DataFrame at once with `axis=None`.\n\nA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or\nsingle key, to DataFrame.loc[:, <subset>] where the columns are prioritised,\nto limit `data` to before applying the function.\n\nPass along to `func`.\n\nSee also\n\nApply a CSS-styling function to headers elementwise.\n\nApply a CSS-styling function to headers level-wise.\n\nApply a CSS-styling function elementwise.\n\nNotes\n\nThe elements of the output of `func` should be CSS styles as strings, in the\nformat \u2018attribute: value; attribute2: value2; \u2026\u2019 or, if nothing is to be\napplied to that element, an empty string or `None`.\n\nThis is similar to `DataFrame.apply`, except that `axis=None` applies the\nfunction to the entire DataFrame at once, rather than column-wise or row-wise.\n\nExamples\n\nUsing `subset` to restrict application to a single column or multiple columns\n\nUsing a 2d input to `subset` to select rows in addition to columns\n\nUsing a function which returns a Series / DataFrame of unequal length but\ncontaining valid index labels\n\nSee Table Visualization user guide for more details.\n\n"}, {"name": "pandas.io.formats.style.Styler.apply_index", "path": "reference/api/pandas.io.formats.style.styler.apply_index", "type": "Style", "text": "\nApply a CSS-styling function to the index or column headers, level-wise.\n\nUpdates the HTML representation with the result.\n\nNew in version 1.4.0.\n\n`func` should take a Series and return a string array of the same length.\n\nThe headers over which to apply the function.\n\nIf index is MultiIndex the level(s) over which to apply the function.\n\nPass along to `func`.\n\nSee also\n\nApply a CSS-styling function to headers elementwise.\n\nApply a CSS-styling function column-wise, row-wise, or table-wise.\n\nApply a CSS-styling function elementwise.\n\nNotes\n\nEach input to `func` will be the index as a Series, if an Index, or a level of\na MultiIndex. The output of `func` should be an identically sized array of CSS\nstyles as strings, in the format \u2018attribute: value; attribute2: value2; \u2026\u2019 or,\nif nothing is to be applied to that element, an empty string or `None`.\n\nExamples\n\nBasic usage to conditionally highlight values in the index.\n\nSelectively applying to specific levels of MultiIndex columns.\n\n"}, {"name": "pandas.io.formats.style.Styler.applymap", "path": "reference/api/pandas.io.formats.style.styler.applymap", "type": "Style", "text": "\nApply a CSS-styling function elementwise.\n\nUpdates the HTML representation with the result.\n\n`func` should take a scalar and return a string.\n\nA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or\nsingle key, to DataFrame.loc[:, <subset>] where the columns are prioritised,\nto limit `data` to before applying the function.\n\nPass along to `func`.\n\nSee also\n\nApply a CSS-styling function to headers elementwise.\n\nApply a CSS-styling function to headers level-wise.\n\nApply a CSS-styling function column-wise, row-wise, or table-wise.\n\nNotes\n\nThe elements of the output of `func` should be CSS styles as strings, in the\nformat \u2018attribute: value; attribute2: value2; \u2026\u2019 or, if nothing is to be\napplied to that element, an empty string or `None`.\n\nExamples\n\nUsing `subset` to restrict application to a single column or multiple columns\n\nUsing a 2d input to `subset` to select rows in addition to columns\n\nSee Table Visualization user guide for more details.\n\n"}, {"name": "pandas.io.formats.style.Styler.applymap_index", "path": "reference/api/pandas.io.formats.style.styler.applymap_index", "type": "Style", "text": "\nApply a CSS-styling function to the index or column headers, elementwise.\n\nUpdates the HTML representation with the result.\n\nNew in version 1.4.0.\n\n`func` should take a scalar and return a string.\n\nThe headers over which to apply the function.\n\nIf index is MultiIndex the level(s) over which to apply the function.\n\nPass along to `func`.\n\nSee also\n\nApply a CSS-styling function to headers level-wise.\n\nApply a CSS-styling function column-wise, row-wise, or table-wise.\n\nApply a CSS-styling function elementwise.\n\nNotes\n\nEach input to `func` will be an index value, if an Index, or a level value of\na MultiIndex. The output of `func` should be CSS styles as a string, in the\nformat \u2018attribute: value; attribute2: value2; \u2026\u2019 or, if nothing is to be\napplied to that element, an empty string or `None`.\n\nExamples\n\nBasic usage to conditionally highlight values in the index.\n\nSelectively applying to specific levels of MultiIndex columns.\n\n"}, {"name": "pandas.io.formats.style.Styler.background_gradient", "path": "reference/api/pandas.io.formats.style.styler.background_gradient", "type": "Style", "text": "\nColor the background in a gradient style.\n\nThe background color is determined according to the data in each column, row\nor frame, or by a given gradient map. Requires matplotlib.\n\nMatplotlib colormap.\n\nCompress the color range at the low end. This is a multiple of the data range\nto extend below the minimum; good values usually in [0, 1], defaults to 0.\n\nCompress the color range at the high end. This is a multiple of the data range\nto extend above the maximum; good values usually in [0, 1], defaults to 0.\n\nApply to each column (`axis=0` or `'index'`), to each row (`axis=1` or\n`'columns'`), or to the entire DataFrame at once with `axis=None`.\n\nA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or\nsingle key, to DataFrame.loc[:, <subset>] where the columns are prioritised,\nto limit `data` to before applying the function.\n\nLuminance threshold for determining text color in [0, 1]. Facilitates text\nvisibility across varying background colors. All text is dark if 0, and light\nif 1, defaults to 0.408.\n\nMinimum data value that corresponds to colormap minimum value. If not\nspecified the minimum value of the data (or gmap) will be used.\n\nNew in version 1.0.0.\n\nMaximum data value that corresponds to colormap maximum value. If not\nspecified the maximum value of the data (or gmap) will be used.\n\nNew in version 1.0.0.\n\nGradient map for determining the background colors. If not supplied will use\nthe underlying data from rows, columns or frame. If given as an ndarray or\nlist-like must be an identical shape to the underlying data considering `axis`\nand `subset`. If given as DataFrame or Series must have same index and column\nlabels considering `axis` and `subset`. If supplied, `vmin` and `vmax` should\nbe given relative to this gradient map.\n\nNew in version 1.3.0.\n\nSee also\n\nColor the text in a gradient style.\n\nNotes\n\nWhen using `low` and `high` the range of the gradient, given by the data if\n`gmap` is not given or by `gmap`, is extended at the low end effectively by\nmap.min - low * map.range and at the high end by map.max + high * map.range\nbefore the colors are normalized and determined.\n\nIf combining with `vmin` and `vmax` the map.min, map.max and map.range are\nreplaced by values according to the values derived from `vmin` and `vmax`.\n\nThis method will preselect numeric columns and ignore non-numeric columns\nunless a `gmap` is supplied in which case no preselection occurs.\n\nExamples\n\nShading the values column-wise, with `axis=0`, preselecting numeric columns\n\nShading all values collectively using `axis=None`\n\nCompress the color map from the both `low` and `high` ends\n\nManually setting `vmin` and `vmax` gradient thresholds\n\nSetting a `gmap` and applying to all columns with another `cmap`\n\nSetting the gradient map for a dataframe (i.e. `axis=None`), we need to\nexplicitly state `subset` to match the `gmap` shape\n\n"}, {"name": "pandas.io.formats.style.Styler.bar", "path": "reference/api/pandas.io.formats.style.styler.bar", "type": "Style", "text": "\nDraw bar chart in the cell backgrounds.\n\nChanged in version 1.4.0.\n\nA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or\nsingle key, to DataFrame.loc[:, <subset>] where the columns are prioritised,\nto limit `data` to before applying the function.\n\nApply to each column (`axis=0` or `'index'`), to each row (`axis=1` or\n`'columns'`), or to the entire DataFrame at once with `axis=None`.\n\nIf a str is passed, the color is the same for both negative and positive\nnumbers. If 2-tuple/list is used, the first element is the color_negative and\nthe second is the color_positive (eg: [\u2018#d65f5f\u2019, \u2018#5fba7d\u2019]).\n\nA string name of a matplotlib Colormap, or a Colormap object. Cannot be used\ntogether with `color`.\n\nNew in version 1.4.0.\n\nThe percentage of the cell, measured from the left, in which to draw the bars,\nin [0, 100].\n\nThe percentage height of the bar in the cell, centrally aligned, in [0,100].\n\nNew in version 1.4.0.\n\nHow to align the bars within the cells relative to a width adjusted center. If\nstring must be one of:\n\n\u2018left\u2019 : bars are drawn rightwards from the minimum data value.\n\n\u2018right\u2019 : bars are drawn leftwards from the maximum data value.\n\n\u2018zero\u2019 : a value of zero is located at the center of the cell.\n\n\u2018mid\u2019 : a value of (max-min)/2 is located at the center of the cell, or if all\nvalues are negative (positive) the zero is aligned at the right (left) of the\ncell.\n\n\u2018mean\u2019 : the mean value of the data is located at the center of the cell.\n\nIf a float or integer is given this will indicate the center of the cell.\n\nIf a callable should take a 1d or 2d array and return a scalar.\n\nChanged in version 1.4.0.\n\nMinimum bar value, defining the left hand limit of the bar drawing range,\nlower values are clipped to vmin. When None (default): the minimum value of\nthe data will be used.\n\nMaximum bar value, defining the right hand limit of the bar drawing range,\nhigher values are clipped to vmax. When None (default): the maximum value of\nthe data will be used.\n\nThe base CSS of the cell that is extended to add the bar chart. Defaults to\n\u201cwidth: 10em;\u201d.\n\nNew in version 1.4.0.\n\nNotes\n\nThis section of the user guide: Table Visualization gives a number of examples\nfor different settings and color coordination.\n\n"}, {"name": "pandas.io.formats.style.Styler.clear", "path": "reference/api/pandas.io.formats.style.styler.clear", "type": "Style", "text": "\nReset the `Styler`, removing any previously applied styles.\n\nReturns None.\n\n"}, {"name": "pandas.io.formats.style.Styler.env", "path": "reference/api/pandas.io.formats.style.styler.env", "type": "Style", "text": "\n\n"}, {"name": "pandas.io.formats.style.Styler.export", "path": "reference/api/pandas.io.formats.style.styler.export", "type": "Style", "text": "\nExport the styles applied to the current Styler.\n\nCan be applied to a second Styler with `Styler.use`.\n\nSee also\n\nSet the styles on the current Styler.\n\nCreate a copy of the current Styler.\n\nNotes\n\nThis method is designed to copy non-data dependent attributes of one Styler to\nanother. It differs from `Styler.copy` where data and data dependent\nattributes are also copied.\n\nThe following items are exported since they are not generally data dependent:\n\nStyling functions added by the `apply` and `applymap`\n\nWhether axes and names are hidden from the display, if unambiguous.\n\nTable attributes\n\nTable styles\n\nThe following attributes are considered data dependent and therefore not\nexported:\n\nCaption\n\nUUID\n\nTooltips\n\nAny hidden rows or columns identified by Index labels\n\nAny formatting applied using `Styler.format`\n\nAny CSS classes added using `Styler.set_td_classes`\n\nExamples\n\n"}, {"name": "pandas.io.formats.style.Styler.format", "path": "reference/api/pandas.io.formats.style.styler.format", "type": "Style", "text": "\nFormat the text display value of cells.\n\nObject to define how values are displayed. See notes.\n\nA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or\nsingle key, to DataFrame.loc[:, <subset>] where the columns are prioritised,\nto limit `data` to before applying the function.\n\nRepresentation for missing values. If `na_rep` is None, no special formatting\nis applied.\n\nNew in version 1.0.0.\n\nFloating point precision to use for display purposes, if not determined by the\nspecified `formatter`.\n\nNew in version 1.3.0.\n\nCharacter used as decimal separator for floats, complex and integers.\n\nNew in version 1.3.0.\n\nCharacter used as thousands separator for floats, complex and integers.\n\nNew in version 1.3.0.\n\nUse \u2018html\u2019 to replace the characters `&`, `<`, `>`, `'`, and `\"` in cell\ndisplay string with HTML-safe sequences. Use \u2018latex\u2019 to replace the characters\n`&`, `%`, `$`, `#`, `_`, `{`, `}`, `~`, `^`, and `\\` in the cell display\nstring with LaTeX-safe sequences. Escaping is done before `formatter`.\n\nNew in version 1.3.0.\n\nConvert string patterns containing https://, http://, ftp:// or www. to HTML\n<a> tags as clickable URL hyperlinks if \u201chtml\u201d, or LaTeX href commands if\n\u201clatex\u201d.\n\nNew in version 1.4.0.\n\nNotes\n\nThis method assigns a formatting function, `formatter`, to each cell in the\nDataFrame. If `formatter` is `None`, then the default formatter is used. If a\ncallable then that function should take a data value as input and return a\ndisplayable representation, such as a string. If `formatter` is given as a\nstring this is assumed to be a valid Python format specification and is\nwrapped to a callable as `string.format(x)`. If a `dict` is given, keys should\ncorrespond to column names, and values should be string or callable, as above.\n\nThe default formatter currently expresses floats and complex numbers with the\npandas display precision unless using the `precision` argument here. The\ndefault formatter does not adjust the representation of missing values unless\nthe `na_rep` argument is used.\n\nThe `subset` argument defines which region to apply the formatting function\nto. If the `formatter` argument is given in dict form but does not include all\ncolumns within the subset then these columns will have the default formatter\napplied. Any columns in the formatter dict excluded from the subset will be\nignored.\n\nWhen using a `formatter` string the dtypes must be compatible, otherwise a\nValueError will be raised.\n\nWhen instantiating a Styler, default formatting can be applied be setting the\n`pandas.options`:\n\n`styler.format.formatter`: default None.\n\n`styler.format.na_rep`: default None.\n\n`styler.format.precision`: default 6.\n\n`styler.format.decimal`: default \u201c.\u201d.\n\n`styler.format.thousands`: default None.\n\n`styler.format.escape`: default None.\n\nExamples\n\nUsing `na_rep` and `precision` with the default `formatter`\n\nUsing a `formatter` specification on consistent column dtypes\n\nUsing the default `formatter` for unspecified columns\n\nMultiple `na_rep` or `precision` specifications under the default `formatter`.\n\nUsing a callable `formatter` function.\n\nUsing a `formatter` with HTML `escape` and `na_rep`.\n\nUsing a `formatter` with LaTeX `escape`.\n\n"}, {"name": "pandas.io.formats.style.Styler.format_index", "path": "reference/api/pandas.io.formats.style.styler.format_index", "type": "Style", "text": "\nFormat the text display value of index labels or column headers.\n\nNew in version 1.4.0.\n\nObject to define how values are displayed. See notes.\n\nWhether to apply the formatter to the index or column headers.\n\nThe level(s) over which to apply the generic formatter.\n\nRepresentation for missing values. If `na_rep` is None, no special formatting\nis applied.\n\nFloating point precision to use for display purposes, if not determined by the\nspecified `formatter`.\n\nCharacter used as decimal separator for floats, complex and integers.\n\nCharacter used as thousands separator for floats, complex and integers.\n\nUse \u2018html\u2019 to replace the characters `&`, `<`, `>`, `'`, and `\"` in cell\ndisplay string with HTML-safe sequences. Use \u2018latex\u2019 to replace the characters\n`&`, `%`, `$`, `#`, `_`, `{`, `}`, `~`, `^`, and `\\` in the cell display\nstring with LaTeX-safe sequences. Escaping is done before `formatter`.\n\nConvert string patterns containing https://, http://, ftp:// or www. to HTML\n<a> tags as clickable URL hyperlinks if \u201chtml\u201d, or LaTeX href commands if\n\u201clatex\u201d.\n\nNotes\n\nThis method assigns a formatting function, `formatter`, to each level label in\nthe DataFrame\u2019s index or column headers. If `formatter` is `None`, then the\ndefault formatter is used. If a callable then that function should take a\nlabel value as input and return a displayable representation, such as a\nstring. If `formatter` is given as a string this is assumed to be a valid\nPython format specification and is wrapped to a callable as\n`string.format(x)`. If a `dict` is given, keys should correspond to MultiIndex\nlevel numbers or names, and values should be string or callable, as above.\n\nThe default formatter currently expresses floats and complex numbers with the\npandas display precision unless using the `precision` argument here. The\ndefault formatter does not adjust the representation of missing values unless\nthe `na_rep` argument is used.\n\nThe `level` argument defines which levels of a MultiIndex to apply the method\nto. If the `formatter` argument is given in dict form but does not include all\nlevels within the level argument then these unspecified levels will have the\ndefault formatter applied. Any levels in the formatter dict specifically\nexcluded from the level argument will be ignored.\n\nWhen using a `formatter` string the dtypes must be compatible, otherwise a\nValueError will be raised.\n\nExamples\n\nUsing `na_rep` and `precision` with the default `formatter`\n\nUsing a `formatter` specification on consistent dtypes in a level\n\nUsing the default `formatter` for unspecified levels\n\nUsing a callable `formatter` function.\n\nUsing a `formatter` with HTML `escape` and `na_rep`.\n\nUsing a `formatter` with LaTeX `escape`.\n\n"}, {"name": "pandas.io.formats.style.Styler.from_custom_template", "path": "reference/api/pandas.io.formats.style.styler.from_custom_template", "type": "Style", "text": "\nFactory function for creating a subclass of `Styler`.\n\nUses custom templates and Jinja environment.\n\nChanged in version 1.3.0.\n\nPath or paths of directories containing the templates.\n\nName of your custom template to replace the html_table template.\n\nNew in version 1.3.0.\n\nName of your custom template to replace the html_style template.\n\nNew in version 1.3.0.\n\nHas the correct `env`,``template_html``, `template_html_table` and\n`template_html_style` class attributes set.\n\n"}, {"name": "pandas.io.formats.style.Styler.hide", "path": "reference/api/pandas.io.formats.style.styler.hide", "type": "Style", "text": "\nHide the entire index / column headers, or specific rows / columns from\ndisplay.\n\nNew in version 1.4.0.\n\nA valid 1d input or single key along the axis within DataFrame.loc[<subset>,\n:] or DataFrame.loc[:, <subset>] depending upon `axis`, to limit `data` to\nselect hidden rows / columns.\n\nApply to the index or columns.\n\nThe level(s) to hide in a MultiIndex if hiding the entire index / column\nheaders. Cannot be used simultaneously with `subset`.\n\nWhether to hide the level name(s) of the index / columns headers in the case\nit (or at least one the levels) remains visible.\n\nNotes\n\nThis method has multiple functionality depending upon the combination of the\n`subset`, `level` and `names` arguments (see examples). The `axis` argument is\nused only to control whether the method is applied to row or column headers:\n\n`subset`\n\n`level`\n\n`names`\n\nEffect\n\nNone\n\nNone\n\nFalse\n\nThe axis-Index is hidden entirely.\n\nNone\n\nNone\n\nTrue\n\nOnly the axis-Index names are hidden.\n\nNone\n\nInt, Str, List\n\nFalse\n\nSpecified axis-MultiIndex levels are hidden entirely.\n\nNone\n\nInt, Str, List\n\nTrue\n\nSpecified axis-MultiIndex levels are hidden entirely and the names of\nremaining axis-MultiIndex levels.\n\nSubset\n\nNone\n\nFalse\n\nThe specified data rows/columns are hidden, but the axis-Index itself, and\nnames, remain unchanged.\n\nSubset\n\nNone\n\nTrue\n\nThe specified data rows/columns and axis-Index names are hidden, but the axis-\nIndex itself remains unchanged.\n\nSubset\n\nInt, Str, List\n\nBoolean\n\nValueError: cannot supply `subset` and `level` simultaneously.\n\nNote this method only hides the identifed elements so can be chained to hide\nmultiple elements in sequence.\n\nExamples\n\nSimple application hiding specific rows:\n\nHide the index and retain the data values:\n\nHide specific rows in a MultiIndex but retain the index:\n\nHide specific rows and the index through chaining:\n\nHide a specific level:\n\nHiding just the index level names:\n\nExamples all produce equivalently transposed effects with `axis=\"columns\"`.\n\n"}, {"name": "pandas.io.formats.style.Styler.hide_columns", "path": "reference/api/pandas.io.formats.style.styler.hide_columns", "type": "Style", "text": "\nHide the column headers or specific keys in the columns from rendering.\n\nThis method has dual functionality:\n\nif `subset` is `None` then the entire column headers row, or specific levels,\nwill be hidden whilst the data-values remain visible.\n\nif a `subset` is given then those specific columns, including the data-values\nwill be hidden, whilst the column headers row remains visible.\n\nChanged in version 1.3.0.\n\nThis method should be replaced by `hide(axis=\"columns\", **kwargs)`\n\nA valid 1d input or single key along the columns axis within DataFrame.loc[:,\n<subset>], to limit `data` to before applying the function.\n\nThe level(s) to hide in a MultiIndex if hiding the entire column headers row.\nCannot be used simultaneously with `subset`.\n\nNew in version 1.4.0.\n\nWhether to hide the column index name(s), in the case all column headers, or\nsome levels, are visible.\n\nNew in version 1.4.0.\n\nSee also\n\nHide the entire index / columns, or specific rows / columns.\n\n"}, {"name": "pandas.io.formats.style.Styler.hide_index", "path": "reference/api/pandas.io.formats.style.styler.hide_index", "type": "Style", "text": "\nHide the entire index, or specific keys in the index from rendering.\n\nThis method has dual functionality:\n\nif `subset` is `None` then the entire index, or specified levels, will be\nhidden whilst displaying all data-rows.\n\nif a `subset` is given then those specific rows will be hidden whilst the\nindex itself remains visible.\n\nChanged in version 1.3.0.\n\nDeprecated since version 1.4.0: This method should be replaced by\n`hide(axis=\"index\", **kwargs)`\n\nA valid 1d input or single key along the index axis within\nDataFrame.loc[<subset>, :], to limit `data` to before applying the function.\n\nThe level(s) to hide in a MultiIndex if hiding the entire index. Cannot be\nused simultaneously with `subset`.\n\nNew in version 1.4.0.\n\nWhether to hide the index name(s), in the case the index or part of it remains\nvisible.\n\nNew in version 1.4.0.\n\nSee also\n\nHide the entire index / columns, or specific rows / columns.\n\n"}, {"name": "pandas.io.formats.style.Styler.highlight_between", "path": "reference/api/pandas.io.formats.style.styler.highlight_between", "type": "Style", "text": "\nHighlight a defined range with a style.\n\nNew in version 1.3.0.\n\nA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or\nsingle key, to DataFrame.loc[:, <subset>] where the columns are prioritised,\nto limit `data` to before applying the function.\n\nBackground color to use for highlighting.\n\nIf `left` or `right` given as sequence, axis along which to apply those\nboundaries. See examples.\n\nLeft bound for defining the range.\n\nRight bound for defining the range.\n\nIdentify whether bounds are closed or open.\n\nCSS properties to use for highlighting. If `props` is given, `color` is not\nused.\n\nSee also\n\nHighlight missing values with a style.\n\nHighlight the maximum with a style.\n\nHighlight the minimum with a style.\n\nHighlight values defined by a quantile with a style.\n\nNotes\n\nIf `left` is `None` only the right bound is applied. If `right` is `None` only\nthe left bound is applied. If both are `None` all values are highlighted.\n\n`axis` is only needed if `left` or `right` are provided as a sequence or an\narray-like object for aligning the shapes. If `left` and `right` are both\nscalars then all `axis` inputs will give the same result.\n\nThis function only works with compatible `dtypes`. For example a datetime-like\nregion can only use equivalent datetime-like `left` and `right` arguments. Use\n`subset` to control regions which have multiple `dtypes`.\n\nExamples\n\nBasic usage\n\nUsing a range input sequnce along an `axis`, in this case setting a `left` and\n`right` for each column individually\n\nUsing `axis=None` and providing the `left` argument as an array that matches\nthe input DataFrame, with a constant `right`\n\nUsing `props` instead of default background coloring\n\n"}, {"name": "pandas.io.formats.style.Styler.highlight_max", "path": "reference/api/pandas.io.formats.style.styler.highlight_max", "type": "Style", "text": "\nHighlight the maximum with a style.\n\nA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or\nsingle key, to DataFrame.loc[:, <subset>] where the columns are prioritised,\nto limit `data` to before applying the function.\n\nBackground color to use for highlighting.\n\nApply to each column (`axis=0` or `'index'`), to each row (`axis=1` or\n`'columns'`), or to the entire DataFrame at once with `axis=None`.\n\nCSS properties to use for highlighting. If `props` is given, `color` is not\nused.\n\nNew in version 1.3.0.\n\nSee also\n\nHighlight missing values with a style.\n\nHighlight the minimum with a style.\n\nHighlight a defined range with a style.\n\nHighlight values defined by a quantile with a style.\n\n"}, {"name": "pandas.io.formats.style.Styler.highlight_min", "path": "reference/api/pandas.io.formats.style.styler.highlight_min", "type": "Style", "text": "\nHighlight the minimum with a style.\n\nA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or\nsingle key, to DataFrame.loc[:, <subset>] where the columns are prioritised,\nto limit `data` to before applying the function.\n\nBackground color to use for highlighting.\n\nApply to each column (`axis=0` or `'index'`), to each row (`axis=1` or\n`'columns'`), or to the entire DataFrame at once with `axis=None`.\n\nCSS properties to use for highlighting. If `props` is given, `color` is not\nused.\n\nNew in version 1.3.0.\n\nSee also\n\nHighlight missing values with a style.\n\nHighlight the maximum with a style.\n\nHighlight a defined range with a style.\n\nHighlight values defined by a quantile with a style.\n\n"}, {"name": "pandas.io.formats.style.Styler.highlight_null", "path": "reference/api/pandas.io.formats.style.styler.highlight_null", "type": "Style", "text": "\nHighlight missing values with a style.\n\nA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or\nsingle key, to DataFrame.loc[:, <subset>] where the columns are prioritised,\nto limit `data` to before applying the function.\n\nNew in version 1.1.0.\n\nCSS properties to use for highlighting. If `props` is given, `color` is not\nused.\n\nNew in version 1.3.0.\n\nSee also\n\nHighlight the maximum with a style.\n\nHighlight the minimum with a style.\n\nHighlight a defined range with a style.\n\nHighlight values defined by a quantile with a style.\n\n"}, {"name": "pandas.io.formats.style.Styler.highlight_quantile", "path": "reference/api/pandas.io.formats.style.styler.highlight_quantile", "type": "Style", "text": "\nHighlight values defined by a quantile with a style.\n\nNew in version 1.3.0.\n\nA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or\nsingle key, to DataFrame.loc[:, <subset>] where the columns are prioritised,\nto limit `data` to before applying the function.\n\nBackground color to use for highlighting.\n\nAxis along which to determine and highlight quantiles. If `None` quantiles are\nmeasured over the entire DataFrame. See examples.\n\nLeft bound, in [0, q_right), for the target quantile range.\n\nRight bound, in (q_left, 1], for the target quantile range.\n\nArgument passed to `Series.quantile` or `DataFrame.quantile` for quantile\nestimation.\n\nIdentify whether quantile bounds are closed or open.\n\nCSS properties to use for highlighting. If `props` is given, `color` is not\nused.\n\nSee also\n\nHighlight missing values with a style.\n\nHighlight the maximum with a style.\n\nHighlight the minimum with a style.\n\nHighlight a defined range with a style.\n\nNotes\n\nThis function does not work with `str` dtypes.\n\nExamples\n\nUsing `axis=None` and apply a quantile to all collective data\n\nOr highlight quantiles row-wise or column-wise, in this case by row-wise\n\nUse `props` instead of default background coloring\n\n"}, {"name": "pandas.io.formats.style.Styler.loader", "path": "reference/api/pandas.io.formats.style.styler.loader", "type": "Style", "text": "\n\n"}, {"name": "pandas.io.formats.style.Styler.pipe", "path": "reference/api/pandas.io.formats.style.styler.pipe", "type": "Style", "text": "\nApply `func(self, *args, **kwargs)`, and return the result.\n\nFunction to apply to the Styler. Alternatively, a `(callable, keyword)` tuple\nwhere `keyword` is a string indicating the keyword of `callable` that expects\nthe Styler.\n\nArguments passed to func.\n\nA dictionary of keyword arguments passed into `func`.\n\nThe value returned by `func`.\n\nSee also\n\nAnalogous method for DataFrame.\n\nApply a CSS-styling function column-wise, row-wise, or table-wise.\n\nNotes\n\nLike `DataFrame.pipe()`, this method can simplify the application of several\nuser-defined functions to a styler. Instead of writing:\n\nusers can write:\n\nIn particular, this allows users to define functions that take a styler\nobject, along with other parameters, and return the styler after making\nstyling changes (such as calling `Styler.apply()` or\n`Styler.set_properties()`). Using `.pipe`, these user-defined style\n\u201ctransformations\u201d can be interleaved with calls to the built-in Styler\ninterface.\n\nExamples\n\nThe user-defined `format_conversion` function above can be called within a\nsequence of other style modifications:\n\n"}, {"name": "pandas.io.formats.style.Styler.render", "path": "reference/api/pandas.io.formats.style.styler.render", "type": "Style", "text": "\nRender the `Styler` including all applied styles to HTML.\n\nDeprecated since version 1.4.0.\n\nWhether to sparsify the display of a hierarchical index. Setting to False will\ndisplay each explicit level element in a hierarchical key for each row.\nDefaults to `pandas.options.styler.sparse.index` value.\n\nWhether to sparsify the display of a hierarchical index. Setting to False will\ndisplay each explicit level element in a hierarchical key for each row.\nDefaults to `pandas.options.styler.sparse.columns` value.\n\nAny additional keyword arguments are passed through to `self.template.render`.\nThis is useful when you need to provide additional variables for a custom\ntemplate.\n\nThe rendered HTML.\n\nNotes\n\nThis method is deprecated in favour of `Styler.to_html`.\n\nStyler objects have defined the `_repr_html_` method which automatically calls\n`self.to_html()` when it\u2019s the last item in a Notebook cell.\n\nWhen calling `Styler.render()` directly, wrap the result in\n`IPython.display.HTML` to view the rendered HTML in the notebook.\n\nPandas uses the following keys in render. Arguments passed in `**kwargs` take\nprecedence, so think carefully if you want to override them:\n\nhead\n\ncellstyle\n\nbody\n\nuuid\n\ntable_styles\n\ncaption\n\ntable_attributes\n\n"}, {"name": "pandas.io.formats.style.Styler.set_caption", "path": "reference/api/pandas.io.formats.style.styler.set_caption", "type": "Style", "text": "\nSet the text added to a `<caption>` HTML element.\n\nFor HTML output either the string input is used or the first element of the\ntuple. For LaTeX the string input provides a caption and the additional tuple\ninput allows for full captions and short captions, in that order.\n\n"}, {"name": "pandas.io.formats.style.Styler.set_na_rep", "path": "reference/api/pandas.io.formats.style.styler.set_na_rep", "type": "Style", "text": "\nSet the missing data representation on a `Styler`.\n\nNew in version 1.0.0.\n\nDeprecated since version 1.3.0.\n\nNotes\n\nThis method is deprecated. See Styler.format()\n\n"}, {"name": "pandas.io.formats.style.Styler.set_precision", "path": "reference/api/pandas.io.formats.style.styler.set_precision", "type": "Style", "text": "\nSet the precision used to display values.\n\nDeprecated since version 1.3.0.\n\nNotes\n\nThis method is deprecated see Styler.format.\n\n"}, {"name": "pandas.io.formats.style.Styler.set_properties", "path": "reference/api/pandas.io.formats.style.styler.set_properties", "type": "Style", "text": "\nSet defined CSS-properties to each `<td>` HTML element within the given\nsubset.\n\nA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or\nsingle key, to DataFrame.loc[:, <subset>] where the columns are prioritised,\nto limit `data` to before applying the function.\n\nA dictionary of property, value pairs to be set for each cell.\n\nNotes\n\nThis is a convenience methods which wraps the `Styler.applymap()` calling a\nfunction returning the CSS-properties independently of the data.\n\nExamples\n\nSee Table Visualization user guide for more details.\n\n"}, {"name": "pandas.io.formats.style.Styler.set_sticky", "path": "reference/api/pandas.io.formats.style.styler.set_sticky", "type": "Style", "text": "\nAdd CSS to permanently display the index or column headers in a scrolling\nframe.\n\nWhether to make the index or column headers sticky.\n\nRequired to configure the width of index cells or the height of column header\ncells when sticking a MultiIndex (or with a named Index). Defaults to 75 and\n25 respectively.\n\nIf `axis` is a MultiIndex the specific levels to stick. If `None` will stick\nall levels.\n\nNotes\n\nThis method uses the CSS \u2018position: sticky;\u2019 property to display. It is\ndesigned to work with visible axes, therefore both:\n\nstyler.set_sticky(axis=\u201dindex\u201d).hide(axis=\u201dindex\u201d)\n\nstyler.set_sticky(axis=\u201dcolumns\u201d).hide(axis=\u201dcolumns\u201d)\n\nmay produce strange behaviour due to CSS controls with missing elements.\n\n"}, {"name": "pandas.io.formats.style.Styler.set_table_attributes", "path": "reference/api/pandas.io.formats.style.styler.set_table_attributes", "type": "Style", "text": "\nSet the table attributes added to the `<table>` HTML element.\n\nThese are items in addition to automatic (by default) `id` attribute.\n\nSee also\n\nSet the table styles included within the `<style>` HTML element.\n\nSet the DataFrame of strings added to the `class` attribute of `<td>` HTML\nelements.\n\nExamples\n\n"}, {"name": "pandas.io.formats.style.Styler.set_table_styles", "path": "reference/api/pandas.io.formats.style.styler.set_table_styles", "type": "Style", "text": "\nSet the table styles included within the `<style>` HTML element.\n\nThis function can be used to style the entire table, columns, rows or specific\nHTML selectors.\n\nIf supplying a list, each individual table_style should be a dictionary with\n`selector` and `props` keys. `selector` should be a CSS selector that the\nstyle will be applied to (automatically prefixed by the table\u2019s UUID) and\n`props` should be a list of tuples with `(attribute, value)`. If supplying a\ndict, the dict keys should correspond to column names or index values,\ndepending upon the specified axis argument. These will be mapped to row or col\nCSS selectors. MultiIndex values as dict keys should be in their respective\ntuple form. The dict values should be a list as specified in the form with CSS\nselectors and props that will be applied to the specified row or column.\n\nChanged in version 1.2.0.\n\nApply to each column (`axis=0` or `'index'`), to each row (`axis=1` or\n`'columns'`). Only used if table_styles is dict.\n\nNew in version 1.2.0.\n\nStyles are replaced if True, or extended if False. CSS rules are preserved so\nmost recent styles set will dominate if selectors intersect.\n\nNew in version 1.2.0.\n\nA dict of strings used to replace the default CSS classes described below.\n\nNew in version 1.4.0.\n\nSee also\n\nSet the DataFrame of strings added to the `class` attribute of `<td>` HTML\nelements.\n\nSet the table attributes added to the `<table>` HTML element.\n\nNotes\n\nThe default CSS classes dict, whose values can be replaced is as follows:\n\nExamples\n\nOr with CSS strings\n\nAdding column styling by name\n\nAdding row styling\n\nSee Table Visualization user guide for more details.\n\n"}, {"name": "pandas.io.formats.style.Styler.set_td_classes", "path": "reference/api/pandas.io.formats.style.styler.set_td_classes", "type": "Style", "text": "\nSet the DataFrame of strings added to the `class` attribute of `<td>` HTML\nelements.\n\nDataFrame containing strings that will be translated to CSS classes, mapped by\nidentical column and index key values that must exist on the underlying Styler\ndata. None, NaN values, and empty strings will be ignored and not affect the\nrendered HTML.\n\nSee also\n\nSet the table styles included within the `<style>` HTML element.\n\nSet the table attributes added to the `<table>` HTML element.\n\nNotes\n\nCan be used in combination with `Styler.set_table_styles` to define an\ninternal CSS solution without reference to external CSS files.\n\nExamples\n\nUsing MultiIndex columns and a classes DataFrame as a subset of the\nunderlying,\n\nForm of the output with new additional css classes,\n\n"}, {"name": "pandas.io.formats.style.Styler.set_tooltips", "path": "reference/api/pandas.io.formats.style.styler.set_tooltips", "type": "Style", "text": "\nSet the DataFrame of strings on `Styler` generating `:hover` tooltips.\n\nThese string based tooltips are only applicable to `<td>` HTML elements, and\ncannot be used for column or index headers.\n\nNew in version 1.3.0.\n\nDataFrame containing strings that will be translated to tooltips, mapped by\nidentical column and index values that must exist on the underlying Styler\ndata. None, NaN values, and empty strings will be ignored and not affect the\nrendered HTML.\n\nList of (attr, value) tuples or a valid CSS string. If `None` adopts the\ninternal default values described in notes.\n\nName of the tooltip class used in CSS, should conform to HTML standards. Only\nuseful if integrating tooltips with external CSS. If `None` uses the internal\ndefault value \u2018pd-t\u2019.\n\nNotes\n\nTooltips are created by adding <span class=\u201dpd-t\u201d></span> to each data cell\nand then manipulating the table level CSS to attach pseudo hover and pseudo\nafter selectors to produce the required the results.\n\nThe default properties for the tooltip CSS class are:\n\nvisibility: hidden\n\nposition: absolute\n\nz-index: 1\n\nbackground-color: black\n\ncolor: white\n\ntransform: translate(-20px, -20px)\n\nThe property \u2018visibility: hidden;\u2019 is a key prerequisite to the hover\nfunctionality, and should always be included in any manual properties\nspecification, using the `props` argument.\n\nTooltips are not designed to be efficient, and can add large amounts of\nadditional HTML for larger tables, since they also require that `cell_ids` is\nforced to True.\n\nExamples\n\nBasic application\n\nOptionally controlling the tooltip visual display\n\n"}, {"name": "pandas.io.formats.style.Styler.set_uuid", "path": "reference/api/pandas.io.formats.style.styler.set_uuid", "type": "Style", "text": "\nSet the uuid applied to `id` attributes of HTML elements.\n\nNotes\n\nAlmost all HTML elements within the table, and including the `<table>` element\nare assigned `id` attributes. The format is `T_uuid_<extra>` where `<extra>`\nis typically a more specific identifier, such as `row1_col2`.\n\n"}, {"name": "pandas.io.formats.style.Styler.template_html", "path": "reference/api/pandas.io.formats.style.styler.template_html", "type": "Style", "text": "\n\n"}, {"name": "pandas.io.formats.style.Styler.template_html_style", "path": "reference/api/pandas.io.formats.style.styler.template_html_style", "type": "Style", "text": "\n\n"}, {"name": "pandas.io.formats.style.Styler.template_html_table", "path": "reference/api/pandas.io.formats.style.styler.template_html_table", "type": "Style", "text": "\n\n"}, {"name": "pandas.io.formats.style.Styler.template_latex", "path": "reference/api/pandas.io.formats.style.styler.template_latex", "type": "Style", "text": "\n\n"}, {"name": "pandas.io.formats.style.Styler.text_gradient", "path": "reference/api/pandas.io.formats.style.styler.text_gradient", "type": "Style", "text": "\nColor the text in a gradient style.\n\nThe text color is determined according to the data in each column, row or\nframe, or by a given gradient map. Requires matplotlib.\n\nMatplotlib colormap.\n\nCompress the color range at the low end. This is a multiple of the data range\nto extend below the minimum; good values usually in [0, 1], defaults to 0.\n\nCompress the color range at the high end. This is a multiple of the data range\nto extend above the maximum; good values usually in [0, 1], defaults to 0.\n\nApply to each column (`axis=0` or `'index'`), to each row (`axis=1` or\n`'columns'`), or to the entire DataFrame at once with `axis=None`.\n\nA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or\nsingle key, to DataFrame.loc[:, <subset>] where the columns are prioritised,\nto limit `data` to before applying the function.\n\nThis argument is ignored (only used in background_gradient). Luminance\nthreshold for determining text color in [0, 1]. Facilitates text visibility\nacross varying background colors. All text is dark if 0, and light if 1,\ndefaults to 0.408.\n\nMinimum data value that corresponds to colormap minimum value. If not\nspecified the minimum value of the data (or gmap) will be used.\n\nNew in version 1.0.0.\n\nMaximum data value that corresponds to colormap maximum value. If not\nspecified the maximum value of the data (or gmap) will be used.\n\nNew in version 1.0.0.\n\nGradient map for determining the text colors. If not supplied will use the\nunderlying data from rows, columns or frame. If given as an ndarray or list-\nlike must be an identical shape to the underlying data considering `axis` and\n`subset`. If given as DataFrame or Series must have same index and column\nlabels considering `axis` and `subset`. If supplied, `vmin` and `vmax` should\nbe given relative to this gradient map.\n\nNew in version 1.3.0.\n\nSee also\n\nColor the background in a gradient style.\n\nNotes\n\nWhen using `low` and `high` the range of the gradient, given by the data if\n`gmap` is not given or by `gmap`, is extended at the low end effectively by\nmap.min - low * map.range and at the high end by map.max + high * map.range\nbefore the colors are normalized and determined.\n\nIf combining with `vmin` and `vmax` the map.min, map.max and map.range are\nreplaced by values according to the values derived from `vmin` and `vmax`.\n\nThis method will preselect numeric columns and ignore non-numeric columns\nunless a `gmap` is supplied in which case no preselection occurs.\n\nExamples\n\nShading the values column-wise, with `axis=0`, preselecting numeric columns\n\nShading all values collectively using `axis=None`\n\nCompress the color map from the both `low` and `high` ends\n\nManually setting `vmin` and `vmax` gradient thresholds\n\nSetting a `gmap` and applying to all columns with another `cmap`\n\nSetting the gradient map for a dataframe (i.e. `axis=None`), we need to\nexplicitly state `subset` to match the `gmap` shape\n\n"}, {"name": "pandas.io.formats.style.Styler.to_excel", "path": "reference/api/pandas.io.formats.style.styler.to_excel", "type": "Style", "text": "\nWrite Styler to an Excel sheet.\n\nTo write a single Styler to an Excel .xlsx file it is only necessary to\nspecify a target file name. To write to multiple sheets it is necessary to\ncreate an ExcelWriter object with a target file name, and specify a sheet in\nthe file to write to.\n\nMultiple sheets may be written to by specifying unique sheet_name. With all\ndata written to the file it is necessary to save the changes. Note that\ncreating an ExcelWriter object with a file name that already exists will\nresult in the contents of the existing file being erased.\n\nFile path or existing ExcelWriter.\n\nName of sheet which will contain DataFrame.\n\nMissing data representation.\n\nFormat string for floating point numbers. For example `float_format=\"%.2f\"`\nwill format 0.1234 to 0.12.\n\nColumns to write.\n\nWrite out the column names. If a list of string is given it is assumed to be\naliases for the column names.\n\nWrite row names (index).\n\nColumn label for index column(s) if desired. If not specified, and header and\nindex are True, then the index names are used. A sequence should be given if\nthe DataFrame uses MultiIndex.\n\nUpper left cell row to dump data frame.\n\nUpper left cell column to dump data frame.\n\nWrite engine to use, \u2018openpyxl\u2019 or \u2018xlsxwriter\u2019. You can also set this via the\noptions `io.excel.xlsx.writer`, `io.excel.xls.writer`, and\n`io.excel.xlsm.writer`.\n\nDeprecated since version 1.2.0: As the xlwt package is no longer maintained,\nthe `xlwt` engine will be removed in a future version of pandas.\n\nWrite MultiIndex and Hierarchical Rows as merged cells.\n\nEncoding of the resulting excel file. Only necessary for xlwt, other writers\nsupport unicode natively.\n\nRepresentation for infinity (there is no native representation for infinity in\nExcel).\n\nDisplay more information in the error logs.\n\nSpecifies the one-based bottommost row and rightmost column that is to be\nfrozen.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nSee also\n\nWrite DataFrame to a comma-separated values (csv) file.\n\nClass for writing DataFrame objects into excel sheets.\n\nRead an Excel file into a pandas DataFrame.\n\nRead a comma-separated values (csv) file into DataFrame.\n\nNotes\n\nFor compatibility with `to_csv()`, to_excel serializes lists and dicts to\nstrings before writing.\n\nOnce a workbook has been saved it is not possible to write further data\nwithout rewriting the whole workbook.\n\nExamples\n\nCreate, write to and save a workbook:\n\nTo specify the sheet name:\n\nIf you wish to write to more than one sheet in the workbook, it is necessary\nto specify an ExcelWriter object:\n\nExcelWriter can also be used to append to an existing Excel file:\n\nTo set the library that is used to write the Excel file, you can pass the\nengine keyword (the default engine is automatically chosen depending on the\nfile extension):\n\n"}, {"name": "pandas.io.formats.style.Styler.to_html", "path": "reference/api/pandas.io.formats.style.styler.to_html", "type": "Style", "text": "\nWrite Styler to a file, buffer or string in HTML-CSS format.\n\nNew in version 1.3.0.\n\nString, path object (implementing `os.PathLike[str]`), or file-like object\nimplementing a string `write()` function. If None, the result is returned as a\nstring.\n\nId attribute assigned to the <table> HTML element in the format:\n\n`<table id=\"T_<table_uuid>\" ..>`\n\nIf not given uses Styler\u2019s initially assigned value.\n\nAttributes to assign within the <table> HTML element in the format:\n\n`<table .. <table_attributes> >`\n\nIf not given defaults to Styler\u2019s preexisting value.\n\nWhether to sparsify the display of a hierarchical index. Setting to False will\ndisplay each explicit level element in a hierarchical key for each row.\nDefaults to `pandas.options.styler.sparse.index` value.\n\nNew in version 1.4.0.\n\nWhether to sparsify the display of a hierarchical index. Setting to False will\ndisplay each explicit level element in a hierarchical key for each column.\nDefaults to `pandas.options.styler.sparse.columns` value.\n\nNew in version 1.4.0.\n\nAdds \u201cfont-weight: bold;\u201d as a CSS property to table style header cells.\n\nNew in version 1.4.0.\n\nSet, or overwrite, the caption on Styler before rendering.\n\nNew in version 1.4.0.\n\nThe maximum number of rows that will be rendered. Defaults to\n`pandas.options.styler.render.max_rows/max_columns`.\n\nNew in version 1.4.0.\n\nThe maximum number of columns that will be rendered. Defaults to\n`pandas.options.styler.render.max_columns`, which is None.\n\nRows and columns may be reduced if the number of total elements is large. This\nvalue is set to `pandas.options.styler.render.max_elements`, which is 262144\n(18 bit browser rendering).\n\nNew in version 1.4.0.\n\nCharacter encoding setting for file output, and HTML meta tags. Defaults to\n`pandas.options.styler.render.encoding` value of \u201cutf-8\u201d.\n\nWhether to output a fully structured HTML file including all HTML elements, or\njust the core `<style>` and `<table>` elements.\n\nWhether to include the `<style>` element and all associated element `class`\nand `id` identifiers, or solely the `<table>` element without styling\nidentifiers.\n\nAny additional keyword arguments are passed through to the jinja2\n`self.template.render` process. This is useful when you need to provide\nadditional variables for a custom template.\n\nIf buf is None, returns the result as a string. Otherwise returns None.\n\nSee also\n\nWrite a DataFrame to a file, buffer or string in HTML format.\n\n"}, {"name": "pandas.io.formats.style.Styler.to_latex", "path": "reference/api/pandas.io.formats.style.styler.to_latex", "type": "Style", "text": "\nWrite Styler to a file, buffer or string in LaTeX format.\n\nNew in version 1.3.0.\n\nString, path object (implementing `os.PathLike[str]`), or file-like object\nimplementing a string `write()` function. If None, the result is returned as a\nstring.\n\nThe LaTeX column specification placed in location:\n\n\\begin{tabular}{<column_format>}\n\nDefaults to \u2018l\u2019 for index and non-numeric data columns, and, for numeric data\ncolumns, to \u2018r\u2019 by default, or \u2018S\u2019 if `siunitx` is `True`.\n\nThe LaTeX positional argument (e.g. \u2018h!\u2019) for tables, placed in location:\n\n`\\\\begin{table}[<position>]`.\n\nThe LaTeX float command placed in location:\n\n\\begin{table}[<position>]\n\n\\<position_float>\n\nCannot be used if `environment` is \u201clongtable\u201d.\n\nSet to True to add \\toprule, \\midrule and \\bottomrule from the {booktabs}\nLaTeX package. Defaults to `pandas.options.styler.latex.hrules`, which is\nFalse.\n\nChanged in version 1.4.0.\n\nUse to control adding \\cline commands for the index labels separation.\nPossible values are:\n\nNone: no cline commands are added (default).\n\n\u201call;data\u201d: a cline is added for every index value extending the width of the\ntable, including data entries.\n\n\u201call;index\u201d: as above with lines extending only the width of the index\nentries.\n\n\u201cskip-last;data\u201d: a cline is added for each index value except the last level\n(which is never sparsified), extending the widtn of the table.\n\n\u201cskip-last;index\u201d: as above with lines extending only the width of the index\nentries.\n\nNew in version 1.4.0.\n\nThe LaTeX label included as: \\label{<label>}. This is used with \\ref{<label>}\nin the main .tex file.\n\nIf string, the LaTeX table caption included as: \\caption{<caption>}. If tuple,\ni.e (\u201cfull caption\u201d, \u201cshort caption\u201d), the caption included as:\n\\caption[<caption[1]>]{<caption[0]>}.\n\nWhether to sparsify the display of a hierarchical index. Setting to False will\ndisplay each explicit level element in a hierarchical key for each row.\nDefaults to `pandas.options.styler.sparse.index`, which is True.\n\nWhether to sparsify the display of a hierarchical index. Setting to False will\ndisplay each explicit level element in a hierarchical key for each column.\nDefaults to `pandas.options.styler.sparse.columns`, which is True.\n\nIf sparsifying hierarchical MultiIndexes whether to align text centrally, at\nthe top or bottom using the multirow package. If not given defaults to\n`pandas.options.styler.latex.multirow_align`, which is \u201cc\u201d. If \u201cnaive\u201d is\ngiven renders without multirow.\n\nChanged in version 1.4.0.\n\nIf sparsifying hierarchical MultiIndex columns whether to align text at the\nleft, centrally, or at the right. If not given defaults to\n`pandas.options.styler.latex.multicol_align`, which is \u201cr\u201d. If a naive option\nis given renders without multicol. Pipe decorators can also be added to non-\nnaive values to draw vertical rules, e.g. \u201c|r\u201d will draw a rule on the left\nside of right aligned merged cells.\n\nChanged in version 1.4.0.\n\nSet to `True` to structure LaTeX compatible with the {siunitx} package.\n\nIf given, the environment that will replace \u2018table\u2019 in `\\\\begin{table}`. If\n\u2018longtable\u2019 is specified then a more suitable template is rendered. If not\ngiven defaults to `pandas.options.styler.latex.environment`, which is None.\n\nNew in version 1.4.0.\n\nCharacter encoding setting. Defaults to\n`pandas.options.styler.render.encoding`, which is \u201cutf-8\u201d.\n\nConvert simple cell-styles from CSS to LaTeX format. Any CSS not found in\nconversion table is dropped. A style can be forced by adding option \u2013latex.\nSee notes.\n\nIf buf is None, returns the result as a string. Otherwise returns None.\n\nSee also\n\nFormat the text display value of cells.\n\nNotes\n\nLatex Packages\n\nFor the following features we recommend the following LaTeX inclusions:\n\nFeature\n\nInclusion\n\nsparse columns\n\nnone: included within default {tabular} environment\n\nsparse rows\n\n\\usepackage{multirow}\n\nhrules\n\n\\usepackage{booktabs}\n\ncolors\n\n\\usepackage[table]{xcolor}\n\nsiunitx\n\n\\usepackage{siunitx}\n\nbold (with siunitx)\n\nitalic (with siunitx)\n\nenvironment\n\n\\usepackage{longtable} if arg is \u201clongtable\u201d | or any other relevant\nenvironment package\n\nhyperlinks\n\n\\usepackage{hyperref}\n\nCell Styles\n\nLaTeX styling can only be rendered if the accompanying styling functions have\nbeen constructed with appropriate LaTeX commands. All styling functionality is\nbuilt around the concept of a CSS `(<attribute>, <value>)` pair (see Table\nVisualization), and this should be replaced by a LaTeX `(<command>,\n<options>)` approach. Each cell will be styled individually using nested LaTeX\ncommands with their accompanied options.\n\nFor example the following code will highlight and bold a cell in HTML-CSS:\n\nThe equivalent using LaTeX only commands is the following:\n\nInternally these structured LaTeX `(<command>, <options>)` pairs are\ntranslated to the `display_value` with the default structure:\n`\\<command><options> <display_value>`. Where there are multiple commands the\nlatter is nested recursively, so that the above example highlighed cell is\nrendered as `\\cellcolor{red} \\bfseries 4`.\n\nOccasionally this format does not suit the applied command, or combination of\nLaTeX packages that is in use, so additional flags can be added to the\n`<options>`, within the tuple, to result in different positions of required\nbraces (the default being the same as `--nowrap`):\n\nTuple Format\n\nOutput Structure\n\n(<command>,<options>)\n\n\\<command><options> <display_value>\n\n(<command>,<options> `--nowrap`)\n\n\\<command><options> <display_value>\n\n(<command>,<options> `--rwrap`)\n\n\\<command><options>{<display_value>}\n\n(<command>,<options> `--wrap`)\n\n{\\<command><options> <display_value>}\n\n(<command>,<options> `--lwrap`)\n\n{\\<command><options>} <display_value>\n\n(<command>,<options> `--dwrap`)\n\n{\\<command><options>}{<display_value>}\n\nFor example the textbf command for font-weight should always be used with\n\u2013rwrap so `('textbf', '--rwrap')` will render a working cell, wrapped with\nbraces, as `\\textbf{<display_value>}`.\n\nA more comprehensive example is as follows:\n\nTable Styles\n\nInternally Styler uses its `table_styles` object to parse the `column_format`,\n`position`, `position_float`, and `label` input arguments. These arguments are\nadded to table styles in the format:\n\nException is made for the `hrules` argument which, in fact, controls all three\ncommands: `toprule`, `bottomrule` and `midrule` simultaneously. Instead of\nsetting `hrules` to `True`, it is also possible to set each individual rule\ndefinition, by manually setting the `table_styles`, for example below we set a\nregular `toprule`, set an `hline` for `bottomrule` and exclude the `midrule`:\n\nIf other `commands` are added to table styles they will be detected, and\npositioned immediately above the \u2018\\begin{tabular}\u2019 command. For example to add\nodd and even row coloring, from the {colortbl} package, in format\n`\\rowcolors{1}{pink}{red}`, use:\n\nA more comprehensive example using these arguments is as follows:\n\nFormatting\n\nTo format values `Styler.format()` should be used prior to calling\nStyler.to_latex, as well as other methods such as `Styler.hide()` for example:\n\nCSS Conversion\n\nThis method can convert a Styler constructured with HTML-CSS to LaTeX using\nthe following limited conversions.\n\nCSS Attribute\n\nCSS value\n\nLaTeX Command\n\nLaTeX Options\n\nfont-weight\n\nfont-style\n\nbackground-color\n\ncellcolor\n\ncolor\n\ncolor\n\nIt is also possible to add user-defined LaTeX only styles to a HTML-CSS Styler\nusing the `--latex` flag, and to add LaTeX parsing options that the converter\nwill detect within a CSS-comment.\n\nExamples\n\nBelow we give a complete step by step example adding some advanced features\nand noting some common gotchas.\n\nFirst we create the DataFrame and Styler as usual, including MultiIndex rows\nand columns, which allow for more advanced formatting options:\n\nSecond we will format the display and, since our table is quite wide, will\nhide the repeated level-0 of the index:\n\nNote that one of the string entries of the index and column headers is \u201cH&M\u201d.\nWithout applying the escape=\u201dlatex\u201d option to the format_index method the\nresultant LaTeX will fail to render, and the error returned is quite difficult\nto debug. Using the appropriate escape the \u201c&\u201d is converted to \u201c\\&\u201d.\n\nThirdly we will apply some (CSS-HTML) styles to our object. We will use a\nbuiltin method and also define our own method to highlight the stock\nrecommendation:\n\nAll the above styles will work with HTML (see below) and LaTeX upon\nconversion:\n\nHowever, we finally want to add one LaTeX only style (from the {graphicx}\npackage), that is not easy to convert from CSS and pandas does not support it.\nNotice the \u2013latex flag used here, as well as \u2013rwrap to ensure this is\nformatted correctly and not ignored upon conversion.\n\nFinally we render our LaTeX adding in other options as required:\n\n"}, {"name": "pandas.io.formats.style.Styler.use", "path": "reference/api/pandas.io.formats.style.styler.use", "type": "Style", "text": "\nSet the styles on the current Styler.\n\nPossibly uses styles from `Styler.export`.\n\n\u201capply\u201d: list of styler functions, typically added with `apply` or `applymap`.\n\n\u201ctable_attributes\u201d: HTML attributes, typically added with\n`set_table_attributes`.\n\n\u201ctable_styles\u201d: CSS selectors and properties, typically added with\n`set_table_styles`.\n\n\u201chide_index\u201d: whether the index is hidden, typically added with `hide_index`,\nor a boolean list for hidden levels.\n\n\u201chide_columns\u201d: whether column headers are hidden, typically added with\n`hide_columns`, or a boolean list for hidden levels.\n\n\u201chide_index_names\u201d: whether index names are hidden.\n\n\u201chide_column_names\u201d: whether column header names are hidden.\n\n\u201ccss\u201d: the css class names used.\n\nSee also\n\nExport the non data dependent attributes to the current Styler.\n\nExamples\n\n"}, {"name": "pandas.io.formats.style.Styler.where", "path": "reference/api/pandas.io.formats.style.styler.where", "type": "Style", "text": "\nApply CSS-styles based on a conditional function elementwise.\n\nDeprecated since version 1.3.0.\n\nUpdates the HTML representation with a style which is selected in accordance\nwith the return value of a function.\n\n`cond` should take a scalar, and optional keyword arguments, and return a\nboolean.\n\nApplied when `cond` returns true.\n\nApplied when `cond` returns false.\n\nA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or\nsingle key, to DataFrame.loc[:, <subset>] where the columns are prioritised,\nto limit `data` to before applying the function.\n\nPass along to `cond`.\n\nSee also\n\nApply a CSS-styling function elementwise.\n\nApply a CSS-styling function column-wise, row-wise, or table-wise.\n\nNotes\n\nThis method is deprecated.\n\nThis method is a convenience wrapper for `Styler.applymap()`, which we\nrecommend using instead.\n\nThe example:\n\nshould be refactored to:\n\n"}, {"name": "pandas.io.json.build_table_schema", "path": "reference/api/pandas.io.json.build_table_schema", "type": "Input/output", "text": "\nCreate a Table schema from `data`.\n\nWhether to include `data.index` in the schema.\n\nColumn names to designate as the primary key. The default None will set\n\u2018primaryKey\u2019 to the index level or levels if the index is unique.\n\nWhether to include a field pandas_version with the version of pandas that last\nrevised the table schema. This version can be different from the installed\npandas version.\n\nNotes\n\nSee Table Schema for conversion types. Timedeltas as converted to ISO8601\nduration format with 9 decimal places after the seconds field for nanosecond\nprecision.\n\nCategoricals are converted to the any dtype, and use the enum field constraint\nto list the allowed values. The ordered attribute is included in an ordered\nfield.\n\nExamples\n\n"}, {"name": "pandas.io.stata.StataReader.data_label", "path": "reference/api/pandas.io.stata.statareader.data_label", "type": "Input/output", "text": "\nReturn data label of Stata file.\n\n"}, {"name": "pandas.io.stata.StataReader.value_labels", "path": "reference/api/pandas.io.stata.statareader.value_labels", "type": "Input/output", "text": "\nReturn a dict, associating each variable name a dict, associating each value\nits corresponding label.\n\n"}, {"name": "pandas.io.stata.StataReader.variable_labels", "path": "reference/api/pandas.io.stata.statareader.variable_labels", "type": "Input/output", "text": "\nReturn variable labels as a dict, associating each variable name with\ncorresponding label.\n\n"}, {"name": "pandas.io.stata.StataWriter.write_file", "path": "reference/api/pandas.io.stata.statawriter.write_file", "type": "Input/output", "text": "\nExport DataFrame object to Stata dta format.\n\n"}, {"name": "pandas.isna", "path": "reference/api/pandas.isna", "type": "General functions", "text": "\nDetect missing values for an array-like object.\n\nThis function takes a scalar or array-like object and indicates whether values\nare missing (`NaN` in numeric arrays, `None` or `NaN` in object arrays, `NaT`\nin datetimelike).\n\nObject to check for null or missing values.\n\nFor scalar input, returns a scalar boolean. For array input, returns an array\nof boolean indicating whether each corresponding element is missing.\n\nSee also\n\nBoolean inverse of pandas.isna.\n\nDetect missing values in a Series.\n\nDetect missing values in a DataFrame.\n\nDetect missing values in an Index.\n\nExamples\n\nScalar arguments (including strings) result in a scalar boolean.\n\nndarrays result in an ndarray of booleans.\n\nFor indexes, an ndarray of booleans is returned.\n\nFor Series and DataFrame, the same type is returned, containing booleans.\n\n"}, {"name": "pandas.isnull", "path": "reference/api/pandas.isnull", "type": "General functions", "text": "\nDetect missing values for an array-like object.\n\nThis function takes a scalar or array-like object and indicates whether values\nare missing (`NaN` in numeric arrays, `None` or `NaN` in object arrays, `NaT`\nin datetimelike).\n\nObject to check for null or missing values.\n\nFor scalar input, returns a scalar boolean. For array input, returns an array\nof boolean indicating whether each corresponding element is missing.\n\nSee also\n\nBoolean inverse of pandas.isna.\n\nDetect missing values in a Series.\n\nDetect missing values in a DataFrame.\n\nDetect missing values in an Index.\n\nExamples\n\nScalar arguments (including strings) result in a scalar boolean.\n\nndarrays result in an ndarray of booleans.\n\nFor indexes, an ndarray of booleans is returned.\n\nFor Series and DataFrame, the same type is returned, containing booleans.\n\n"}, {"name": "pandas.json_normalize", "path": "reference/api/pandas.json_normalize", "type": "General functions", "text": "\nNormalize semi-structured JSON data into a flat table.\n\nUnserialized JSON objects.\n\nPath in each object to list of records. If not passed, data will be assumed to\nbe an array of records.\n\nFields to use as metadata for each record in resulting table.\n\nIf True, prefix records with dotted (?) path, e.g. foo.bar.field if meta is\n[\u2018foo\u2019, \u2018bar\u2019].\n\nIf True, prefix records with dotted (?) path, e.g. foo.bar.field if path to\nrecords is [\u2018foo\u2019, \u2018bar\u2019].\n\nConfigures error handling.\n\n\u2018ignore\u2019 : will ignore KeyError if keys listed in meta are not always present.\n\n\u2018raise\u2019 : will raise KeyError if keys listed in meta are not always present.\n\nNested records will generate names separated by sep. e.g., for sep=\u2019.\u2019,\n{\u2018foo\u2019: {\u2018bar\u2019: 0}} -> foo.bar.\n\nMax number of levels(depth of dict) to normalize. if None, normalizes all\nlevels.\n\nNew in version 0.25.0.\n\nExamples\n\nNormalizes nested data up to level 1.\n\nReturns normalized data with columns prefixed with the given string.\n\n"}, {"name": "pandas.melt", "path": "reference/api/pandas.melt", "type": "General functions", "text": "\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers\nset.\n\nThis function is useful to massage a DataFrame into a format where one or more\ncolumns are identifier variables (id_vars), while all other columns,\nconsidered measured variables (value_vars), are \u201cunpivoted\u201d to the row axis,\nleaving just two non-identifier columns, \u2018variable\u2019 and \u2018value\u2019.\n\nColumn(s) to use as identifier variables.\n\nColumn(s) to unpivot. If not specified, uses all columns that are not set as\nid_vars.\n\nName to use for the \u2018variable\u2019 column. If None it uses `frame.columns.name` or\n\u2018variable\u2019.\n\nName to use for the \u2018value\u2019 column.\n\nIf columns are a MultiIndex then use this level to melt.\n\nIf True, original index is ignored. If False, the original index is retained.\nIndex labels will be repeated as necessary.\n\nNew in version 1.1.0.\n\nUnpivoted DataFrame.\n\nSee also\n\nIdentical method.\n\nCreate a spreadsheet-style pivot table as a DataFrame.\n\nReturn reshaped DataFrame organized by given index / column values.\n\nExplode a DataFrame from list-like columns to long format.\n\nExamples\n\nThe names of \u2018variable\u2019 and \u2018value\u2019 columns can be customized:\n\nOriginal index values can be kept around:\n\nIf you have multi-index columns:\n\n"}, {"name": "pandas.merge", "path": "reference/api/pandas.merge", "type": "General functions", "text": "\nMerge DataFrame or named Series objects with a database-style join.\n\nA named Series object is treated as a DataFrame with a single named column.\n\nThe join is done on columns or indexes. If joining columns on columns, the\nDataFrame indexes will be ignored. Otherwise if joining indexes on indexes or\nindexes on a column or columns, the index will be passed on. When performing a\ncross merge, no column specifications to merge on are allowed.\n\nWarning\n\nIf both key columns contain rows where the key is a null value, those rows\nwill be matched against each other. This is different from usual SQL join\nbehaviour and can lead to unexpected results.\n\nObject to merge with.\n\nType of merge to be performed.\n\nleft: use only keys from left frame, similar to a SQL left outer join;\npreserve key order.\n\nright: use only keys from right frame, similar to a SQL right outer join;\npreserve key order.\n\nouter: use union of keys from both frames, similar to a SQL full outer join;\nsort keys lexicographically.\n\ninner: use intersection of keys from both frames, similar to a SQL inner join;\npreserve the order of the left keys.\n\ncross: creates the cartesian product from both frames, preserves the order of\nthe left keys.\n\nNew in version 1.2.0.\n\nColumn or index level names to join on. These must be found in both\nDataFrames. If on is None and not merging on indexes then this defaults to the\nintersection of the columns in both DataFrames.\n\nColumn or index level names to join on in the left DataFrame. Can also be an\narray or list of arrays of the length of the left DataFrame. These arrays are\ntreated as if they are columns.\n\nColumn or index level names to join on in the right DataFrame. Can also be an\narray or list of arrays of the length of the right DataFrame. These arrays are\ntreated as if they are columns.\n\nUse the index from the left DataFrame as the join key(s). If it is a\nMultiIndex, the number of keys in the other DataFrame (either the index or a\nnumber of columns) must match the number of levels.\n\nUse the index from the right DataFrame as the join key. Same caveats as\nleft_index.\n\nSort the join keys lexicographically in the result DataFrame. If False, the\norder of the join keys depends on the join type (how keyword).\n\nA length-2 sequence where each element is optionally a string indicating the\nsuffix to add to overlapping column names in left and right respectively. Pass\na value of None instead of a string to indicate that the column name from left\nor right should be left as-is, with no suffix. At least one of the values must\nnot be None.\n\nIf False, avoid copy if possible.\n\nIf True, adds a column to the output DataFrame called \u201c_merge\u201d with\ninformation on the source of each row. The column can be given a different\nname by providing a string argument. The column will have a Categorical type\nwith the value of \u201cleft_only\u201d for observations whose merge key only appears in\nthe left DataFrame, \u201cright_only\u201d for observations whose merge key only appears\nin the right DataFrame, and \u201cboth\u201d if the observation\u2019s merge key is found in\nboth DataFrames.\n\nIf specified, checks if merge is of specified type.\n\n\u201cone_to_one\u201d or \u201c1:1\u201d: check if merge keys are unique in both left and right\ndatasets.\n\n\u201cone_to_many\u201d or \u201c1:m\u201d: check if merge keys are unique in left dataset.\n\n\u201cmany_to_one\u201d or \u201cm:1\u201d: check if merge keys are unique in right dataset.\n\n\u201cmany_to_many\u201d or \u201cm:m\u201d: allowed, but does not result in checks.\n\nA DataFrame of the two merged objects.\n\nSee also\n\nMerge with optional filling/interpolation.\n\nMerge on nearest keys.\n\nSimilar method using indices.\n\nNotes\n\nSupport for specifying index levels as the on, left_on, and right_on\nparameters was added in version 0.23.0 Support for merging named Series\nobjects was added in version 0.24.0\n\nExamples\n\nMerge df1 and df2 on the lkey and rkey columns. The value columns have the\ndefault suffixes, _x and _y, appended.\n\nMerge DataFrames df1 and df2 with specified left and right suffixes appended\nto any overlapping columns.\n\nMerge DataFrames df1 and df2, but raise an exception if the DataFrames have\nany overlapping columns.\n\n"}, {"name": "pandas.merge_asof", "path": "reference/api/pandas.merge_asof", "type": "General functions", "text": "\nPerform a merge by key distance.\n\nThis is similar to a left-join except that we match on nearest key rather than\nequal keys. Both DataFrames must be sorted by the key.\n\nFor each row in the left DataFrame:\n\nA \u201cbackward\u201d search selects the last row in the right DataFrame whose \u2018on\u2019 key\nis less than or equal to the left\u2019s key.\n\nA \u201cforward\u201d search selects the first row in the right DataFrame whose \u2018on\u2019 key\nis greater than or equal to the left\u2019s key.\n\nA \u201cnearest\u201d search selects the row in the right DataFrame whose \u2018on\u2019 key is\nclosest in absolute distance to the left\u2019s key.\n\nThe default is \u201cbackward\u201d and is compatible in versions below 0.20.0. The\ndirection parameter was added in version 0.20.0 and introduces \u201cforward\u201d and\n\u201cnearest\u201d.\n\nOptionally match on equivalent keys with \u2018by\u2019 before searching with \u2018on\u2019.\n\nField name to join on. Must be found in both DataFrames. The data MUST be\nordered. Furthermore this must be a numeric column, such as datetimelike,\ninteger, or float. On or left_on/right_on must be given.\n\nField name to join on in left DataFrame.\n\nField name to join on in right DataFrame.\n\nUse the index of the left DataFrame as the join key.\n\nUse the index of the right DataFrame as the join key.\n\nMatch on these columns before performing merge operation.\n\nField names to match on in the left DataFrame.\n\nField names to match on in the right DataFrame.\n\nSuffix to apply to overlapping column names in the left and right side,\nrespectively.\n\nSelect asof tolerance within this range; must be compatible with the merge\nindex.\n\nIf True, allow matching with the same \u2018on\u2019 value (i.e. less-than-or-equal-to /\ngreater-than-or-equal-to)\n\nIf False, don\u2019t match the same \u2018on\u2019 value (i.e., strictly less-than / strictly\ngreater-than).\n\nWhether to search for prior, subsequent, or closest matches.\n\nSee also\n\nMerge with a database-style join.\n\nMerge with optional filling/interpolation.\n\nExamples\n\nWe can use indexed DataFrames as well.\n\nHere is a real-world times-series example\n\nBy default we are taking the asof of the quotes\n\nWe only asof within 2ms between the quote time and the trade time\n\nWe only asof within 10ms between the quote time and the trade time and we\nexclude exact matches on time. However prior data will propagate forward\n\n"}, {"name": "pandas.merge_ordered", "path": "reference/api/pandas.merge_ordered", "type": "General functions", "text": "\nPerform a merge for ordered data with optional filling/interpolation.\n\nDesigned for ordered data like time series data. Optionally perform group-wise\nmerge (see examples).\n\nField names to join on. Must be found in both DataFrames.\n\nField names to join on in left DataFrame. Can be a vector or list of vectors\nof the length of the DataFrame to use a particular vector as the join key\ninstead of columns.\n\nField names to join on in right DataFrame or vector/list of vectors per\nleft_on docs.\n\nGroup left DataFrame by group columns and merge piece by piece with right\nDataFrame.\n\nGroup right DataFrame by group columns and merge piece by piece with left\nDataFrame.\n\nInterpolation method for data.\n\nA length-2 sequence where each element is optionally a string indicating the\nsuffix to add to overlapping column names in left and right respectively. Pass\na value of None instead of a string to indicate that the column name from left\nor right should be left as-is, with no suffix. At least one of the values must\nnot be None.\n\nChanged in version 0.25.0.\n\nleft: use only keys from left frame (SQL: left outer join)\n\nright: use only keys from right frame (SQL: right outer join)\n\nouter: use union of keys from both frames (SQL: full outer join)\n\ninner: use intersection of keys from both frames (SQL: inner join).\n\nThe merged DataFrame output type will the be same as \u2018left\u2019, if it is a\nsubclass of DataFrame.\n\nSee also\n\nMerge with a database-style join.\n\nMerge on nearest keys.\n\nExamples\n\n"}, {"name": "pandas.MultiIndex", "path": "reference/api/pandas.multiindex", "type": "Index Objects", "text": "\nA multi-level, or hierarchical, index object for pandas objects.\n\nThe unique labels for each level.\n\nIntegers for each level designating which label at each location.\n\nLevel of sortedness (must be lexicographically sorted by that level).\n\nNames for each of the index levels. (name is accepted for compat).\n\nCopy the meta-data.\n\nCheck that the levels/codes are consistent and valid.\n\nSee also\n\nConvert list of arrays to MultiIndex.\n\nCreate a MultiIndex from the cartesian product of iterables.\n\nConvert list of tuples to a MultiIndex.\n\nMake a MultiIndex from a DataFrame.\n\nThe base pandas Index type.\n\nNotes\n\nSee the user guide for more.\n\nExamples\n\nA new `MultiIndex` is typically constructed using one of the helper methods\n`MultiIndex.from_arrays()`, `MultiIndex.from_product()` and\n`MultiIndex.from_tuples()`. For example (using `.from_arrays`):\n\nSee further examples for how to construct a MultiIndex in the doc strings of\nthe mentioned helper methods.\n\nAttributes\n\n`names`\n\nNames of levels in MultiIndex.\n\n`nlevels`\n\nInteger number of levels in this MultiIndex.\n\n`levshape`\n\nA tuple with the length of each level.\n\nlevels\n\ncodes\n\nMethods\n\n`from_arrays`(arrays[, sortorder, names])\n\nConvert arrays to MultiIndex.\n\n`from_tuples`(tuples[, sortorder, names])\n\nConvert list of tuples to MultiIndex.\n\n`from_product`(iterables[, sortorder, names])\n\nMake a MultiIndex from the cartesian product of multiple iterables.\n\n`from_frame`(df[, sortorder, names])\n\nMake a MultiIndex from a DataFrame.\n\n`set_levels`(levels[, level, inplace, ...])\n\nSet new levels on MultiIndex.\n\n`set_codes`(codes[, level, inplace, ...])\n\nSet new codes on MultiIndex.\n\n`to_frame`([index, name])\n\nCreate a DataFrame with the levels of the MultiIndex as columns.\n\n`to_flat_index`()\n\nConvert a MultiIndex to an Index of Tuples containing the level values.\n\n`sortlevel`([level, ascending, sort_remaining])\n\nSort MultiIndex at the requested level.\n\n`droplevel`([level])\n\nReturn index with requested level(s) removed.\n\n`swaplevel`([i, j])\n\nSwap level i with level j.\n\n`reorder_levels`(order)\n\nRearrange levels using input order.\n\n`remove_unused_levels`()\n\nCreate new MultiIndex from current that removes unused levels.\n\n`get_locs`(seq)\n\nGet location for a sequence of labels.\n\n"}, {"name": "pandas.MultiIndex.codes", "path": "reference/api/pandas.multiindex.codes", "type": "Index Objects", "text": "\n\n"}, {"name": "pandas.MultiIndex.droplevel", "path": "reference/api/pandas.multiindex.droplevel", "type": "Index Objects", "text": "\nReturn index with requested level(s) removed.\n\nIf resulting index has only 1 level left, the result will be of Index type,\nnot MultiIndex.\n\nIf a string is given, must be the name of a level If list-like, elements must\nbe names or indexes of levels.\n\nExamples\n\n"}, {"name": "pandas.MultiIndex.dtypes", "path": "reference/api/pandas.multiindex.dtypes", "type": "General utility functions", "text": "\nReturn the dtypes as a Series for the underlying MultiIndex.\n\n"}, {"name": "pandas.MultiIndex.from_arrays", "path": "reference/api/pandas.multiindex.from_arrays", "type": "Index Objects", "text": "\nConvert arrays to MultiIndex.\n\nEach array-like gives one level\u2019s value for each data point. len(arrays) is\nthe number of levels.\n\nLevel of sortedness (must be lexicographically sorted by that level).\n\nNames for the levels in the index.\n\nSee also\n\nConvert list of tuples to MultiIndex.\n\nMake a MultiIndex from cartesian product of iterables.\n\nMake a MultiIndex from a DataFrame.\n\nExamples\n\n"}, {"name": "pandas.MultiIndex.from_frame", "path": "reference/api/pandas.multiindex.from_frame", "type": "DataFrame", "text": "\nMake a MultiIndex from a DataFrame.\n\nDataFrame to be converted to MultiIndex.\n\nLevel of sortedness (must be lexicographically sorted by that level).\n\nIf no names are provided, use the column names, or tuple of column names if\nthe columns is a MultiIndex. If a sequence, overwrite names with the given\nsequence.\n\nThe MultiIndex representation of the given DataFrame.\n\nSee also\n\nConvert list of arrays to MultiIndex.\n\nConvert list of tuples to MultiIndex.\n\nMake a MultiIndex from cartesian product of iterables.\n\nExamples\n\nUsing explicit names, instead of the column names\n\n"}, {"name": "pandas.MultiIndex.from_product", "path": "reference/api/pandas.multiindex.from_product", "type": "Index Objects", "text": "\nMake a MultiIndex from the cartesian product of multiple iterables.\n\nEach iterable has unique labels for each level of the index.\n\nLevel of sortedness (must be lexicographically sorted by that level).\n\nNames for the levels in the index.\n\nChanged in version 1.0.0: If not explicitly provided, names will be inferred\nfrom the elements of iterables if an element has a name attribute\n\nSee also\n\nConvert list of arrays to MultiIndex.\n\nConvert list of tuples to MultiIndex.\n\nMake a MultiIndex from a DataFrame.\n\nExamples\n\n"}, {"name": "pandas.MultiIndex.from_tuples", "path": "reference/api/pandas.multiindex.from_tuples", "type": "Index Objects", "text": "\nConvert list of tuples to MultiIndex.\n\nEach tuple is the index of one row/column.\n\nLevel of sortedness (must be lexicographically sorted by that level).\n\nNames for the levels in the index.\n\nSee also\n\nConvert list of arrays to MultiIndex.\n\nMake a MultiIndex from cartesian product of iterables.\n\nMake a MultiIndex from a DataFrame.\n\nExamples\n\n"}, {"name": "pandas.MultiIndex.get_indexer", "path": "reference/api/pandas.multiindex.get_indexer", "type": "Index Objects", "text": "\nCompute indexer and mask for new index given the current index. The indexer\nshould be then used as an input to ndarray.take to align the current data to\nthe new index.\n\ndefault: exact matches only.\n\npad / ffill: find the PREVIOUS index value if no exact match.\n\nbackfill / bfill: use NEXT index value if no exact match\n\nnearest: use the NEAREST index value if no exact match. Tied distances are\nbroken by preferring the larger index value.\n\nMaximum number of consecutive labels in `target` to match for inexact matches.\n\nMaximum distance between original and new labels for inexact matches. The\nvalues of the index at the matching locations must satisfy the equation\n`abs(index[indexer] - target) <= tolerance`.\n\nTolerance may be a scalar value, which applies the same tolerance to all\nvalues, or list-like, which applies variable tolerance per element. List-like\nincludes list, tuple, array, Series, and must be the same size as the index\nand its dtype must exactly match the index\u2019s type.\n\nIntegers from 0 to n - 1 indicating that the index at these positions matches\nthe corresponding target values. Missing values in the target are marked by\n-1.\n\nNotes\n\nReturns -1 for unmatched values, for further explanation see the example\nbelow.\n\nExamples\n\nNotice that the return value is an array of locations in `index` and `x` is\nmarked by -1, as it is not in `index`.\n\n"}, {"name": "pandas.MultiIndex.get_level_values", "path": "reference/api/pandas.multiindex.get_level_values", "type": "Index Objects", "text": "\nReturn vector of label values for requested level.\n\nLength of returned vector is equal to the length of the index.\n\n`level` is either the integer position of the level in the MultiIndex, or the\nname of the level.\n\nValues is a level of this MultiIndex converted to a single `Index` (or\nsubclass thereof).\n\nNotes\n\nIf the level contains missing values, the result may be casted to `float` with\nmissing values specified as `NaN`. This is because the level is converted to a\nregular `Index`.\n\nExamples\n\nCreate a MultiIndex:\n\nGet level values by supplying level as either integer or name:\n\nIf a level contains missing values, the return type of the level maybe casted\nto `float`.\n\n"}, {"name": "pandas.MultiIndex.get_loc", "path": "reference/api/pandas.multiindex.get_loc", "type": "Index Objects", "text": "\nGet location for a label or a tuple of labels.\n\nThe location is returned as an integer/slice or boolean mask.\n\nIf the key is past the lexsort depth, the return may be a boolean mask array,\notherwise it is always a slice or int.\n\nSee also\n\nThe get_loc method for (single-level) index.\n\nGet slice location given start label(s) and end label(s).\n\nGet location for a label/slice/list/mask or a sequence of such.\n\nNotes\n\nThe key cannot be a slice, list of same-level labels, a boolean mask, or a\nsequence of such. If you want to use those, use `MultiIndex.get_locs()`\ninstead.\n\nExamples\n\n"}, {"name": "pandas.MultiIndex.get_loc_level", "path": "reference/api/pandas.multiindex.get_loc_level", "type": "Index Objects", "text": "\nGet location and sliced index for requested label(s)/level(s).\n\nIf `False`, the resulting index will not drop any level.\n\nElement 0: int, slice object or boolean array Element 1: The resulting sliced\nmultiindex/index. If the key contains all levels, this will be `None`.\n\nSee also\n\nGet location for a label or a tuple of labels.\n\nGet location for a label/slice/list/mask or a sequence of such.\n\nExamples\n\n"}, {"name": "pandas.MultiIndex.get_locs", "path": "reference/api/pandas.multiindex.get_locs", "type": "Index Objects", "text": "\nGet location for a sequence of labels.\n\nYou should use one of the above for each level. If a level should not be used,\nset it to `slice(None)`.\n\nNumPy array of integers suitable for passing to iloc.\n\nSee also\n\nGet location for a label or a tuple of labels.\n\nGet slice location given start label(s) and end label(s).\n\nExamples\n\n"}, {"name": "pandas.MultiIndex.levels", "path": "reference/api/pandas.multiindex.levels", "type": "Index Objects", "text": "\n\n"}, {"name": "pandas.MultiIndex.levshape", "path": "reference/api/pandas.multiindex.levshape", "type": "Index Objects", "text": "\nA tuple with the length of each level.\n\nExamples\n\n"}, {"name": "pandas.MultiIndex.names", "path": "reference/api/pandas.multiindex.names", "type": "Index Objects", "text": "\nNames of levels in MultiIndex.\n\nExamples\n\n"}, {"name": "pandas.MultiIndex.nlevels", "path": "reference/api/pandas.multiindex.nlevels", "type": "Index Objects", "text": "\nInteger number of levels in this MultiIndex.\n\nExamples\n\n"}, {"name": "pandas.MultiIndex.remove_unused_levels", "path": "reference/api/pandas.multiindex.remove_unused_levels", "type": "Index Objects", "text": "\nCreate new MultiIndex from current that removes unused levels.\n\nUnused level(s) means levels that are not expressed in the labels. The\nresulting MultiIndex will have the same outward appearance, meaning the same\n.values and ordering. It will also be .equals() to the original.\n\nExamples\n\nThe 0 from the first level is not represented and can be removed\n\n"}, {"name": "pandas.MultiIndex.reorder_levels", "path": "reference/api/pandas.multiindex.reorder_levels", "type": "Index Objects", "text": "\nRearrange levels using input order. May not drop or duplicate levels.\n\nList representing new level order. Reference level by number (position) or by\nkey (label).\n\nExamples\n\n"}, {"name": "pandas.MultiIndex.set_codes", "path": "reference/api/pandas.multiindex.set_codes", "type": "Index Objects", "text": "\nSet new codes on MultiIndex. Defaults to returning new index.\n\nNew codes to apply.\n\nLevel(s) to set (None for all levels).\n\nIf True, mutates in place.\n\nDeprecated since version 1.2.0.\n\nIf True, checks that levels and codes are compatible.\n\nThe same type as the caller or None if `inplace=True`.\n\nExamples\n\n"}, {"name": "pandas.MultiIndex.set_levels", "path": "reference/api/pandas.multiindex.set_levels", "type": "Index Objects", "text": "\nSet new levels on MultiIndex. Defaults to returning new index.\n\nNew level(s) to apply.\n\nLevel(s) to set (None for all levels).\n\nIf True, mutates in place.\n\nDeprecated since version 1.2.0.\n\nIf True, checks that levels and codes are compatible.\n\nThe same type as the caller or None if `inplace=True`.\n\nExamples\n\nIf any of the levels passed to `set_levels()` exceeds the existing length, all\nof the values from that argument will be stored in the MultiIndex levels,\nthough the values will be truncated in the MultiIndex output.\n\n"}, {"name": "pandas.MultiIndex.sortlevel", "path": "reference/api/pandas.multiindex.sortlevel", "type": "Index Objects", "text": "\nSort MultiIndex at the requested level.\n\nThe result will respect the original ordering of the associated factor at that\nlevel.\n\nIf a string is given, must be a name of the level. If list-like must be names\nor ints of levels.\n\nFalse to sort in descending order. Can also be a list to specify a directed\nordering.\n\nResulting index.\n\nIndices of output values in original index.\n\nExamples\n\n"}, {"name": "pandas.MultiIndex.swaplevel", "path": "reference/api/pandas.multiindex.swaplevel", "type": "Index Objects", "text": "\nSwap level i with level j.\n\nCalling this method does not change the ordering of the values.\n\nFirst level of index to be swapped. Can pass level name as string. Type of\nparameters can be mixed.\n\nSecond level of index to be swapped. Can pass level name as string. Type of\nparameters can be mixed.\n\nA new MultiIndex.\n\nSee also\n\nSwap levels i and j in a MultiIndex.\n\nSwap levels i and j in a MultiIndex on a particular axis.\n\nExamples\n\n"}, {"name": "pandas.MultiIndex.to_flat_index", "path": "reference/api/pandas.multiindex.to_flat_index", "type": "Index Objects", "text": "\nConvert a MultiIndex to an Index of Tuples containing the level values.\n\nIndex with the MultiIndex data represented in Tuples.\n\nSee also\n\nConvert flat index back to MultiIndex.\n\nNotes\n\nThis method will simply return the caller if called by anything other than a\nMultiIndex.\n\nExamples\n\n"}, {"name": "pandas.MultiIndex.to_frame", "path": "reference/api/pandas.multiindex.to_frame", "type": "DataFrame", "text": "\nCreate a DataFrame with the levels of the MultiIndex as columns.\n\nColumn ordering is determined by the DataFrame constructor with data as a\ndict.\n\nSet the index of the returned DataFrame as the original MultiIndex.\n\nThe passed names should substitute index level names.\n\nSee also\n\nTwo-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nExamples\n\n"}, {"name": "pandas.notna", "path": "reference/api/pandas.notna", "type": "General functions", "text": "\nDetect non-missing values for an array-like object.\n\nThis function takes a scalar or array-like object and indicates whether values\nare valid (not missing, which is `NaN` in numeric arrays, `None` or `NaN` in\nobject arrays, `NaT` in datetimelike).\n\nObject to check for not null or non-missing values.\n\nFor scalar input, returns a scalar boolean. For array input, returns an array\nof boolean indicating whether each corresponding element is valid.\n\nSee also\n\nBoolean inverse of pandas.notna.\n\nDetect valid values in a Series.\n\nDetect valid values in a DataFrame.\n\nDetect valid values in an Index.\n\nExamples\n\nScalar arguments (including strings) result in a scalar boolean.\n\nndarrays result in an ndarray of booleans.\n\nFor indexes, an ndarray of booleans is returned.\n\nFor Series and DataFrame, the same type is returned, containing booleans.\n\n"}, {"name": "pandas.notnull", "path": "reference/api/pandas.notnull", "type": "General functions", "text": "\nDetect non-missing values for an array-like object.\n\nThis function takes a scalar or array-like object and indicates whether values\nare valid (not missing, which is `NaN` in numeric arrays, `None` or `NaN` in\nobject arrays, `NaT` in datetimelike).\n\nObject to check for not null or non-missing values.\n\nFor scalar input, returns a scalar boolean. For array input, returns an array\nof boolean indicating whether each corresponding element is valid.\n\nSee also\n\nBoolean inverse of pandas.notna.\n\nDetect valid values in a Series.\n\nDetect valid values in a DataFrame.\n\nDetect valid values in an Index.\n\nExamples\n\nScalar arguments (including strings) result in a scalar boolean.\n\nndarrays result in an ndarray of booleans.\n\nFor indexes, an ndarray of booleans is returned.\n\nFor Series and DataFrame, the same type is returned, containing booleans.\n\n"}, {"name": "pandas.option_context", "path": "reference/api/pandas.option_context", "type": "General utility functions", "text": "\nContext manager to temporarily set options in the with statement context.\n\nYou need to invoke as `option_context(pat, val, [(pat, val), ...])`.\n\nExamples\n\nMethods\n\n`__call__`(func)\n\nCall self as a function.\n\n"}, {"name": "pandas.option_context.__call__", "path": "reference/api/pandas.option_context.__call__", "type": "General utility functions", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.Period", "path": "reference/api/pandas.period", "type": "Input/output", "text": "\nRepresents a period of time.\n\nThe time period represented (e.g., \u20184Q2005\u2019).\n\nOne of pandas period strings or corresponding objects.\n\nThe period offset from the proleptic Gregorian epoch.\n\nYear value of the period.\n\nMonth value of the period.\n\nQuarter value of the period.\n\nDay value of the period.\n\nHour value of the period.\n\nMinute value of the period.\n\nSecond value of the period.\n\nAttributes\n\n`day`\n\nGet day of the month that a Period falls on.\n\n`day_of_week`\n\nDay of the week the period lies in, with Monday=0 and Sunday=6.\n\n`day_of_year`\n\nReturn the day of the year.\n\n`dayofweek`\n\nDay of the week the period lies in, with Monday=0 and Sunday=6.\n\n`dayofyear`\n\nReturn the day of the year.\n\n`days_in_month`\n\nGet the total number of days in the month that this period falls on.\n\n`daysinmonth`\n\nGet the total number of days of the month that the Period falls in.\n\n`end_time`\n\nGet the Timestamp for the end of the period.\n\n`freqstr`\n\nReturn a string representation of the frequency.\n\n`hour`\n\nGet the hour of the day component of the Period.\n\n`is_leap_year`\n\nReturn True if the period's year is in a leap year.\n\n`minute`\n\nGet minute of the hour component of the Period.\n\n`month`\n\nReturn the month this Period falls on.\n\n`quarter`\n\nReturn the quarter this Period falls on.\n\n`qyear`\n\nFiscal year the Period lies in according to its starting-quarter.\n\n`second`\n\nGet the second component of the Period.\n\n`start_time`\n\nGet the Timestamp for the start of the period.\n\n`week`\n\nGet the week of the year on the given Period.\n\n`weekday`\n\nDay of the week the period lies in, with Monday=0 and Sunday=6.\n\n`weekofyear`\n\nGet the week of the year on the given Period.\n\n`year`\n\nReturn the year this Period falls on.\n\nfreq\n\nordinal\n\nMethods\n\n`asfreq`\n\nConvert Period to desired frequency, at the start or end of the interval.\n\n`now`\n\nReturn the period of now's date.\n\n`strftime`\n\nReturns the string representation of the `Period`, depending on the selected\n`fmt`.\n\n`to_timestamp`\n\nReturn the Timestamp representation of the Period.\n\n"}, {"name": "pandas.Period.asfreq", "path": "reference/api/pandas.period.asfreq", "type": "Input/output", "text": "\nConvert Period to desired frequency, at the start or end of the interval.\n\nThe desired frequency.\n\nStart or end of the timespan.\n\n"}, {"name": "pandas.Period.day", "path": "reference/api/pandas.period.day", "type": "Input/output", "text": "\nGet day of the month that a Period falls on.\n\nSee also\n\nGet the day of the week.\n\nGet the day of the year.\n\nExamples\n\n"}, {"name": "pandas.Period.day_of_week", "path": "reference/api/pandas.period.day_of_week", "type": "Input/output", "text": "\nDay of the week the period lies in, with Monday=0 and Sunday=6.\n\nIf the period frequency is lower than daily (e.g. hourly), and the period\nspans over multiple days, the day at the start of the period is used.\n\nIf the frequency is higher than daily (e.g. monthly), the last day of the\nperiod is used.\n\nDay of the week.\n\nSee also\n\nDay of the week the period lies in.\n\nAlias of Period.day_of_week.\n\nDay of the month.\n\nDay of the year.\n\nExamples\n\nFor periods that span over multiple days, the day at the beginning of the\nperiod is returned.\n\nFor periods with a frequency higher than days, the last day of the period is\nreturned.\n\n"}, {"name": "pandas.Period.day_of_year", "path": "reference/api/pandas.period.day_of_year", "type": "Input/output", "text": "\nReturn the day of the year.\n\nThis attribute returns the day of the year on which the particular date\noccurs. The return value ranges between 1 to 365 for regular years and 1 to\n366 for leap years.\n\nThe day of year.\n\nSee also\n\nReturn the day of the month.\n\nReturn the day of week.\n\nReturn the day of year of all indexes.\n\nExamples\n\n"}, {"name": "pandas.Period.dayofweek", "path": "reference/api/pandas.period.dayofweek", "type": "Input/output", "text": "\nDay of the week the period lies in, with Monday=0 and Sunday=6.\n\nIf the period frequency is lower than daily (e.g. hourly), and the period\nspans over multiple days, the day at the start of the period is used.\n\nIf the frequency is higher than daily (e.g. monthly), the last day of the\nperiod is used.\n\nDay of the week.\n\nSee also\n\nDay of the week the period lies in.\n\nAlias of Period.day_of_week.\n\nDay of the month.\n\nDay of the year.\n\nExamples\n\nFor periods that span over multiple days, the day at the beginning of the\nperiod is returned.\n\nFor periods with a frequency higher than days, the last day of the period is\nreturned.\n\n"}, {"name": "pandas.Period.dayofyear", "path": "reference/api/pandas.period.dayofyear", "type": "Input/output", "text": "\nReturn the day of the year.\n\nThis attribute returns the day of the year on which the particular date\noccurs. The return value ranges between 1 to 365 for regular years and 1 to\n366 for leap years.\n\nThe day of year.\n\nSee also\n\nReturn the day of the month.\n\nReturn the day of week.\n\nReturn the day of year of all indexes.\n\nExamples\n\n"}, {"name": "pandas.Period.days_in_month", "path": "reference/api/pandas.period.days_in_month", "type": "Input/output", "text": "\nGet the total number of days in the month that this period falls on.\n\nSee also\n\nGets the number of days in the month.\n\nGets the number of days in the month.\n\nReturns a tuple containing weekday (0-6 ~ Mon-Sun) and number of days (28-31).\n\nExamples\n\nHandles the leap year case as well:\n\n"}, {"name": "pandas.Period.daysinmonth", "path": "reference/api/pandas.period.daysinmonth", "type": "Input/output", "text": "\nGet the total number of days of the month that the Period falls in.\n\nSee also\n\nReturn the days of the month.\n\nReturn the day of the year.\n\nExamples\n\n"}, {"name": "pandas.Period.end_time", "path": "reference/api/pandas.period.end_time", "type": "Input/output", "text": "\nGet the Timestamp for the end of the period.\n\nSee also\n\nReturn the start Timestamp.\n\nReturn the day of year.\n\nReturn the days in that month.\n\nReturn the day of the week.\n\n"}, {"name": "pandas.Period.freq", "path": "reference/api/pandas.period.freq", "type": "Input/output", "text": "\n\n"}, {"name": "pandas.Period.freqstr", "path": "reference/api/pandas.period.freqstr", "type": "Input/output", "text": "\nReturn a string representation of the frequency.\n\n"}, {"name": "pandas.Period.hour", "path": "reference/api/pandas.period.hour", "type": "Input/output", "text": "\nGet the hour of the day component of the Period.\n\nThe hour as an integer, between 0 and 23.\n\nSee also\n\nGet the second component of the Period.\n\nGet the minute component of the Period.\n\nExamples\n\nPeriod longer than a day\n\n"}, {"name": "pandas.Period.is_leap_year", "path": "reference/api/pandas.period.is_leap_year", "type": "Input/output", "text": "\nReturn True if the period\u2019s year is in a leap year.\n\n"}, {"name": "pandas.Period.minute", "path": "reference/api/pandas.period.minute", "type": "Input/output", "text": "\nGet minute of the hour component of the Period.\n\nThe minute as an integer, between 0 and 59.\n\nSee also\n\nGet the hour component of the Period.\n\nGet the second component of the Period.\n\nExamples\n\n"}, {"name": "pandas.Period.month", "path": "reference/api/pandas.period.month", "type": "Input/output", "text": "\nReturn the month this Period falls on.\n\n"}, {"name": "pandas.Period.now", "path": "reference/api/pandas.period.now", "type": "Input/output", "text": "\nReturn the period of now\u2019s date.\n\n"}, {"name": "pandas.Period.ordinal", "path": "reference/api/pandas.period.ordinal", "type": "Input/output", "text": "\n\n"}, {"name": "pandas.Period.quarter", "path": "reference/api/pandas.period.quarter", "type": "Input/output", "text": "\nReturn the quarter this Period falls on.\n\n"}, {"name": "pandas.Period.qyear", "path": "reference/api/pandas.period.qyear", "type": "Input/output", "text": "\nFiscal year the Period lies in according to its starting-quarter.\n\nThe year and the qyear of the period will be the same if the fiscal and\ncalendar years are the same. When they are not, the fiscal year can be\ndifferent from the calendar year of the period.\n\nThe fiscal year of the period.\n\nSee also\n\nReturn the calendar year of the period.\n\nExamples\n\nIf the natural and fiscal year are the same, qyear and year will be the same.\n\nIf the fiscal year starts in April (Q-MAR), the first quarter of 2018 will\nstart in April 2017. year will then be 2018, but qyear will be the fiscal\nyear, 2018.\n\n"}, {"name": "pandas.Period.second", "path": "reference/api/pandas.period.second", "type": "Input/output", "text": "\nGet the second component of the Period.\n\nThe second of the Period (ranges from 0 to 59).\n\nSee also\n\nGet the hour component of the Period.\n\nGet the minute component of the Period.\n\nExamples\n\n"}, {"name": "pandas.Period.start_time", "path": "reference/api/pandas.period.start_time", "type": "Input/output", "text": "\nGet the Timestamp for the start of the period.\n\nSee also\n\nReturn the end Timestamp.\n\nReturn the day of year.\n\nReturn the days in that month.\n\nReturn the day of the week.\n\nExamples\n\n"}, {"name": "pandas.Period.strftime", "path": "reference/api/pandas.period.strftime", "type": "Input/output", "text": "\nReturns the string representation of the `Period`, depending on the selected\n`fmt`. `fmt` must be a string containing one or several directives. The method\nrecognizes the same directives as the `time.strftime()` function of the\nstandard Python distribution, as well as the specific additional directives\n`%f`, `%F`, `%q`. (formatting & docs originally from scikits.timeries).\n\nDirective\n\nMeaning\n\nNotes\n\n`%a`\n\nLocale\u2019s abbreviated weekday name.\n\n`%A`\n\nLocale\u2019s full weekday name.\n\n`%b`\n\nLocale\u2019s abbreviated month name.\n\n`%B`\n\nLocale\u2019s full month name.\n\n`%c`\n\nLocale\u2019s appropriate date and time representation.\n\n`%d`\n\nDay of the month as a decimal number [01,31].\n\n`%f`\n\n\u2018Fiscal\u2019 year without a century as a decimal number [00,99]\n\n(1)\n\n`%F`\n\n\u2018Fiscal\u2019 year with a century as a decimal number\n\n(2)\n\n`%H`\n\nHour (24-hour clock) as a decimal number [00,23].\n\n`%I`\n\nHour (12-hour clock) as a decimal number [01,12].\n\n`%j`\n\nDay of the year as a decimal number [001,366].\n\n`%m`\n\nMonth as a decimal number [01,12].\n\n`%M`\n\nMinute as a decimal number [00,59].\n\n`%p`\n\nLocale\u2019s equivalent of either AM or PM.\n\n(3)\n\n`%q`\n\nQuarter as a decimal number [01,04]\n\n`%S`\n\nSecond as a decimal number [00,61].\n\n(4)\n\n`%U`\n\nWeek number of the year (Sunday as the first day of the week) as a decimal\nnumber [00,53]. All days in a new year preceding the first Sunday are\nconsidered to be in week 0.\n\n(5)\n\n`%w`\n\nWeekday as a decimal number [0(Sunday),6].\n\n`%W`\n\nWeek number of the year (Monday as the first day of the week) as a decimal\nnumber [00,53]. All days in a new year preceding the first Monday are\nconsidered to be in week 0.\n\n(5)\n\n`%x`\n\nLocale\u2019s appropriate date representation.\n\n`%X`\n\nLocale\u2019s appropriate time representation.\n\n`%y`\n\nYear without century as a decimal number [00,99].\n\n`%Y`\n\nYear with century as a decimal number.\n\n`%Z`\n\nTime zone name (no characters if no time zone exists).\n\n`%%`\n\nA literal `'%'` character.\n\nNotes\n\nThe `%f` directive is the same as `%y` if the frequency is not quarterly.\nOtherwise, it corresponds to the \u2018fiscal\u2019 year, as defined by the `qyear`\nattribute.\n\nThe `%F` directive is the same as `%Y` if the frequency is not quarterly.\nOtherwise, it corresponds to the \u2018fiscal\u2019 year, as defined by the `qyear`\nattribute.\n\nThe `%p` directive only affects the output hour field if the `%I` directive is\nused to parse the hour.\n\nThe range really is `0` to `61`; this accounts for leap seconds and the (very\nrare) double leap seconds.\n\nThe `%U` and `%W` directives are only used in calculations when the day of the\nweek and the year are specified.\n\nExamples\n\n"}, {"name": "pandas.Period.to_timestamp", "path": "reference/api/pandas.period.to_timestamp", "type": "Input/output", "text": "\nReturn the Timestamp representation of the Period.\n\nUses the target frequency specified at the part of the period specified by\nhow, which is either Start or Finish.\n\nTarget frequency. Default is \u2018D\u2019 if self.freq is week or longer and \u2018S\u2019\notherwise.\n\nOne of \u2018S\u2019, \u2018E\u2019. Can be aliased as case insensitive \u2018Start\u2019, \u2018Finish\u2019,\n\u2018Begin\u2019, \u2018End\u2019.\n\n"}, {"name": "pandas.Period.week", "path": "reference/api/pandas.period.week", "type": "Input/output", "text": "\nGet the week of the year on the given Period.\n\nSee also\n\nGet the day component of the Period.\n\nGet the day component of the Period.\n\nExamples\n\n"}, {"name": "pandas.Period.weekday", "path": "reference/api/pandas.period.weekday", "type": "Input/output", "text": "\nDay of the week the period lies in, with Monday=0 and Sunday=6.\n\nIf the period frequency is lower than daily (e.g. hourly), and the period\nspans over multiple days, the day at the start of the period is used.\n\nIf the frequency is higher than daily (e.g. monthly), the last day of the\nperiod is used.\n\nDay of the week.\n\nSee also\n\nDay of the week the period lies in.\n\nAlias of Period.dayofweek.\n\nDay of the month.\n\nDay of the year.\n\nExamples\n\nFor periods that span over multiple days, the day at the beginning of the\nperiod is returned.\n\nFor periods with a frequency higher than days, the last day of the period is\nreturned.\n\n"}, {"name": "pandas.Period.weekofyear", "path": "reference/api/pandas.period.weekofyear", "type": "Input/output", "text": "\nGet the week of the year on the given Period.\n\nSee also\n\nGet the day component of the Period.\n\nGet the day component of the Period.\n\nExamples\n\n"}, {"name": "pandas.Period.year", "path": "reference/api/pandas.period.year", "type": "Input/output", "text": "\nReturn the year this Period falls on.\n\n"}, {"name": "pandas.period_range", "path": "reference/api/pandas.period_range", "type": "Input/output", "text": "\nReturn a fixed frequency PeriodIndex.\n\nThe day (calendar) is the default frequency.\n\nLeft bound for generating periods.\n\nRight bound for generating periods.\n\nNumber of periods to generate.\n\nFrequency alias. By default the freq is taken from start or end if those are\nPeriod objects. Otherwise, the default is `\"D\"` for daily frequency.\n\nName of the resulting PeriodIndex.\n\nNotes\n\nOf the three parameters: `start`, `end`, and `periods`, exactly two must be\nspecified.\n\nTo learn more about the frequency strings, please see this link.\n\nExamples\n\nIf `start` or `end` are `Period` objects, they will be used as anchor\nendpoints for a `PeriodIndex` with frequency matching that of the\n`period_range` constructor.\n\n"}, {"name": "pandas.PeriodDtype", "path": "reference/api/pandas.perioddtype", "type": "Input/output", "text": "\nAn ExtensionDtype for Period data.\n\nThis is not an actual numpy dtype, but a duck type.\n\nThe frequency of this PeriodDtype.\n\nExamples\n\nAttributes\n\n`freq`\n\nThe frequency object of this PeriodDtype.\n\nMethods\n\nNone\n\n"}, {"name": "pandas.PeriodDtype.freq", "path": "reference/api/pandas.perioddtype.freq", "type": "Input/output", "text": "\nThe frequency object of this PeriodDtype.\n\n"}, {"name": "pandas.PeriodIndex", "path": "reference/api/pandas.periodindex", "type": "Input/output", "text": "\nImmutable ndarray holding ordinal values indicating regular periods in time.\n\nIndex keys are boxed to Period objects which carries the metadata (eg,\nfrequency information).\n\nOptional period-like data to construct index with.\n\nMake a copy of input ndarray.\n\nOne of pandas period strings or corresponding objects.\n\nSee also\n\nThe base pandas Index type.\n\nRepresents a period of time.\n\nIndex with datetime64 data.\n\nIndex of timedelta64 data.\n\nCreate a fixed-frequency PeriodIndex.\n\nExamples\n\nAttributes\n\n`day`\n\nThe days of the period.\n\n`dayofweek`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`day_of_week`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`dayofyear`\n\nThe ordinal day of the year.\n\n`day_of_year`\n\nThe ordinal day of the year.\n\n`days_in_month`\n\nThe number of days in the month.\n\n`daysinmonth`\n\nThe number of days in the month.\n\n`freq`\n\nReturn the frequency object if it is set, otherwise None.\n\n`freqstr`\n\nReturn the frequency object as a string if its set, otherwise None.\n\n`hour`\n\nThe hour of the period.\n\n`is_leap_year`\n\nLogical indicating if the date belongs to a leap year.\n\n`minute`\n\nThe minute of the period.\n\n`month`\n\nThe month as January=1, December=12.\n\n`quarter`\n\nThe quarter of the date.\n\n`second`\n\nThe second of the period.\n\n`week`\n\nThe week ordinal of the year.\n\n`weekday`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`weekofyear`\n\nThe week ordinal of the year.\n\n`year`\n\nThe year of the period.\n\nend_time\n\nqyear\n\nstart_time\n\nMethods\n\n`asfreq`([freq, how])\n\nConvert the PeriodArray to the specified frequency freq.\n\n`strftime`(*args, **kwargs)\n\nConvert to Index using specified date_format.\n\n`to_timestamp`([freq, how])\n\nCast to DatetimeArray/Index.\n\n"}, {"name": "pandas.PeriodIndex.asfreq", "path": "reference/api/pandas.periodindex.asfreq", "type": "Input/output", "text": "\nConvert the PeriodArray to the specified frequency freq.\n\nEquivalent to applying `pandas.Period.asfreq()` with the given arguments to\neach `Period` in this PeriodArray.\n\nA frequency.\n\nWhether the elements should be aligned to the end or start within pa period.\n\n\u2018E\u2019, \u2018END\u2019, or \u2018FINISH\u2019 for end,\n\n\u2018S\u2019, \u2018START\u2019, or \u2018BEGIN\u2019 for start.\n\nJanuary 31st (\u2018END\u2019) vs. January 1st (\u2018START\u2019) for example.\n\nThe transformed PeriodArray with the new frequency.\n\nSee also\n\nConvert each Period in a PeriodArray to the given frequency.\n\nConvert a `Period` object to the given frequency.\n\nExamples\n\n"}, {"name": "pandas.PeriodIndex.day", "path": "reference/api/pandas.periodindex.day", "type": "Input/output", "text": "\nThe days of the period.\n\n"}, {"name": "pandas.PeriodIndex.day_of_week", "path": "reference/api/pandas.periodindex.day_of_week", "type": "Input/output", "text": "\nThe day of the week with Monday=0, Sunday=6.\n\n"}, {"name": "pandas.PeriodIndex.day_of_year", "path": "reference/api/pandas.periodindex.day_of_year", "type": "Input/output", "text": "\nThe ordinal day of the year.\n\n"}, {"name": "pandas.PeriodIndex.dayofweek", "path": "reference/api/pandas.periodindex.dayofweek", "type": "Input/output", "text": "\nThe day of the week with Monday=0, Sunday=6.\n\n"}, {"name": "pandas.PeriodIndex.dayofyear", "path": "reference/api/pandas.periodindex.dayofyear", "type": "Input/output", "text": "\nThe ordinal day of the year.\n\n"}, {"name": "pandas.PeriodIndex.days_in_month", "path": "reference/api/pandas.periodindex.days_in_month", "type": "Input/output", "text": "\nThe number of days in the month.\n\n"}, {"name": "pandas.PeriodIndex.daysinmonth", "path": "reference/api/pandas.periodindex.daysinmonth", "type": "Input/output", "text": "\nThe number of days in the month.\n\n"}, {"name": "pandas.PeriodIndex.end_time", "path": "reference/api/pandas.periodindex.end_time", "type": "Input/output", "text": "\n\n"}, {"name": "pandas.PeriodIndex.freq", "path": "reference/api/pandas.periodindex.freq", "type": "Input/output", "text": "\nReturn the frequency object if it is set, otherwise None.\n\n"}, {"name": "pandas.PeriodIndex.freqstr", "path": "reference/api/pandas.periodindex.freqstr", "type": "Input/output", "text": "\nReturn the frequency object as a string if its set, otherwise None.\n\n"}, {"name": "pandas.PeriodIndex.hour", "path": "reference/api/pandas.periodindex.hour", "type": "Input/output", "text": "\nThe hour of the period.\n\n"}, {"name": "pandas.PeriodIndex.is_leap_year", "path": "reference/api/pandas.periodindex.is_leap_year", "type": "Input/output", "text": "\nLogical indicating if the date belongs to a leap year.\n\n"}, {"name": "pandas.PeriodIndex.minute", "path": "reference/api/pandas.periodindex.minute", "type": "Input/output", "text": "\nThe minute of the period.\n\n"}, {"name": "pandas.PeriodIndex.month", "path": "reference/api/pandas.periodindex.month", "type": "Input/output", "text": "\nThe month as January=1, December=12.\n\n"}, {"name": "pandas.PeriodIndex.quarter", "path": "reference/api/pandas.periodindex.quarter", "type": "Input/output", "text": "\nThe quarter of the date.\n\n"}, {"name": "pandas.PeriodIndex.qyear", "path": "reference/api/pandas.periodindex.qyear", "type": "Input/output", "text": "\n\n"}, {"name": "pandas.PeriodIndex.second", "path": "reference/api/pandas.periodindex.second", "type": "Input/output", "text": "\nThe second of the period.\n\n"}, {"name": "pandas.PeriodIndex.start_time", "path": "reference/api/pandas.periodindex.start_time", "type": "Input/output", "text": "\n\n"}, {"name": "pandas.PeriodIndex.strftime", "path": "reference/api/pandas.periodindex.strftime", "type": "Input/output", "text": "\nConvert to Index using specified date_format.\n\nReturn an Index of formatted strings specified by date_format, which supports\nthe same string format as the python standard library. Details of the string\nformat can be found in python string format doc.\n\nDate format string (e.g. \u201c%Y-%m-%d\u201d).\n\nNumPy ndarray of formatted strings.\n\nSee also\n\nConvert the given argument to datetime.\n\nReturn DatetimeIndex with times to midnight.\n\nRound the DatetimeIndex to the specified freq.\n\nFloor the DatetimeIndex to the specified freq.\n\nExamples\n\n"}, {"name": "pandas.PeriodIndex.to_timestamp", "path": "reference/api/pandas.periodindex.to_timestamp", "type": "Input/output", "text": "\nCast to DatetimeArray/Index.\n\nTarget frequency. The default is \u2018D\u2019 for week or longer, \u2018S\u2019 otherwise.\n\nWhether to use the start or end of the time period being converted.\n\n"}, {"name": "pandas.PeriodIndex.week", "path": "reference/api/pandas.periodindex.week", "type": "Input/output", "text": "\nThe week ordinal of the year.\n\n"}, {"name": "pandas.PeriodIndex.weekday", "path": "reference/api/pandas.periodindex.weekday", "type": "Input/output", "text": "\nThe day of the week with Monday=0, Sunday=6.\n\n"}, {"name": "pandas.PeriodIndex.weekofyear", "path": "reference/api/pandas.periodindex.weekofyear", "type": "Input/output", "text": "\nThe week ordinal of the year.\n\n"}, {"name": "pandas.PeriodIndex.year", "path": "reference/api/pandas.periodindex.year", "type": "Input/output", "text": "\nThe year of the period.\n\n"}, {"name": "pandas.pivot", "path": "reference/api/pandas.pivot", "type": "General functions", "text": "\nReturn reshaped DataFrame organized by given index / column values.\n\nReshape data (produce a \u201cpivot\u201d table) based on column values. Uses unique\nvalues from specified index / columns to form axes of the resulting DataFrame.\nThis function does not support data aggregation, multiple values will result\nin a MultiIndex in the columns. See the User Guide for more on reshaping.\n\nColumn to use to make new frame\u2019s index. If None, uses existing index.\n\nChanged in version 1.1.0: Also accept list of index names.\n\nColumn to use to make new frame\u2019s columns.\n\nChanged in version 1.1.0: Also accept list of columns names.\n\nColumn(s) to use for populating new frame\u2019s values. If not specified, all\nremaining columns will be used and the result will have hierarchically indexed\ncolumns.\n\nReturns reshaped DataFrame.\n\nWhen there are any index, columns combinations with multiple values.\nDataFrame.pivot_table when you need to aggregate.\n\nSee also\n\nGeneralization of pivot that can handle duplicate values for one index/column\npair.\n\nPivot based on the index values instead of a column.\n\nWide panel to long format. Less flexible but more user-friendly than melt.\n\nNotes\n\nFor finer-tuned control, see hierarchical indexing documentation along with\nthe related stack/unstack methods.\n\nExamples\n\nYou could also assign a list of column names or a list of index names.\n\nA ValueError is raised if there are any duplicates.\n\nNotice that the first two rows are the same for our index and columns\narguments.\n\n"}, {"name": "pandas.pivot_table", "path": "reference/api/pandas.pivot_table", "type": "General functions", "text": "\nCreate a spreadsheet-style pivot table as a DataFrame.\n\nThe levels in the pivot table will be stored in MultiIndex objects\n(hierarchical indexes) on the index and columns of the result DataFrame.\n\nIf an array is passed, it must be the same length as the data. The list can\ncontain any of the other types (except list). Keys to group by on the pivot\ntable index. If an array is passed, it is being used as the same manner as\ncolumn values.\n\nIf an array is passed, it must be the same length as the data. The list can\ncontain any of the other types (except list). Keys to group by on the pivot\ntable column. If an array is passed, it is being used as the same manner as\ncolumn values.\n\nIf list of functions passed, the resulting pivot table will have hierarchical\ncolumns whose top level are the function names (inferred from the function\nobjects themselves) If dict is passed, the key is column to aggregate and\nvalue is function or list of functions.\n\nValue to replace missing values with (in the resulting pivot table, after\naggregation).\n\nAdd all row / columns (e.g. for subtotal / grand totals).\n\nDo not include columns whose entries are all NaN.\n\nName of the row / column that will contain the totals when margins is True.\n\nThis only applies if any of the groupers are Categoricals. If True: only show\nobserved values for categorical groupers. If False: show all values for\ncategorical groupers.\n\nChanged in version 0.25.0.\n\nSpecifies if the result should be sorted.\n\nNew in version 1.3.0.\n\nAn Excel style pivot table.\n\nSee also\n\nPivot without aggregation that can handle non-numeric data.\n\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers\nset.\n\nWide panel to long format. Less flexible but more user-friendly than melt.\n\nExamples\n\nThis first example aggregates values by taking the sum.\n\nWe can also fill missing values using the fill_value parameter.\n\nThe next example aggregates by taking the mean across multiple columns.\n\nWe can also calculate multiple types of aggregations for any given value\ncolumn.\n\n"}, {"name": "pandas.plotting.andrews_curves", "path": "reference/api/pandas.plotting.andrews_curves", "type": "Plotting", "text": "\nGenerate a matplotlib plot of Andrews curves, for visualising clusters of\nmultivariate data.\n\nAndrews curves have the functional form:\n\nx_4 sin(2t) + x_5 cos(2t) + \u2026\n\nWhere x coefficients correspond to the values of each dimension and t is\nlinearly spaced between -pi and +pi. Each row of frame then corresponds to a\nsingle curve.\n\nData to be plotted, preferably normalized to (0.0, 1.0).\n\nColors to use for the different classes.\n\nColormap to select colors from. If string, load colormap with that name from\nmatplotlib.\n\nOptions to pass to matplotlib plotting method.\n\nExamples\n\n"}, {"name": "pandas.plotting.autocorrelation_plot", "path": "reference/api/pandas.plotting.autocorrelation_plot", "type": "Input/output", "text": "\nAutocorrelation plot for time series.\n\nOptions to pass to matplotlib plotting method.\n\nExamples\n\nThe horizontal lines in the plot correspond to 95% and 99% confidence bands.\n\nThe dashed line is 99% confidence band.\n\n"}, {"name": "pandas.plotting.bootstrap_plot", "path": "reference/api/pandas.plotting.bootstrap_plot", "type": "Plotting", "text": "\nBootstrap plot on mean, median and mid-range statistics.\n\nThe bootstrap plot is used to estimate the uncertainty of a statistic by\nrelaying on random sampling with replacement [1]. This function will generate\nbootstrapping plots for mean, median and mid-range statistics for the given\nnumber of samples of the given size.\n\n\u201cBootstrapping (statistics)\u201d in\nhttps://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29\n\nSeries from where to get the samplings for the bootstrapping.\n\nIf given, it will use the fig reference for plotting instead of creating a new\none with default parameters.\n\nNumber of data points to consider during each sampling. It must be less than\nor equal to the length of the series.\n\nNumber of times the bootstrap procedure is performed.\n\nOptions to pass to matplotlib plotting method.\n\nMatplotlib figure.\n\nSee also\n\nBasic plotting for DataFrame objects.\n\nBasic plotting for Series objects.\n\nExamples\n\nThis example draws a basic bootstrap plot for a Series.\n\n"}, {"name": "pandas.plotting.boxplot", "path": "reference/api/pandas.plotting.boxplot", "type": "Plotting", "text": "\nMake a box plot from DataFrame columns.\n\nMake a box-and-whisker plot from DataFrame columns, optionally grouped by some\nother columns. A box plot is a method for graphically depicting groups of\nnumerical data through their quartiles. The box extends from the Q1 to Q3\nquartile values of the data, with a line at the median (Q2). The whiskers\nextend from the edges of box to show the range of the data. By default, they\nextend no more than 1.5 * IQR (IQR = Q3 - Q1) from the edges of the box,\nending at the farthest data point within that interval. Outliers are plotted\nas separate dots.\n\nFor further details see Wikipedia\u2019s entry for boxplot.\n\nColumn name or list of names, or vector. Can be any valid input to\n`pandas.DataFrame.groupby()`.\n\nColumn in the DataFrame to `pandas.DataFrame.groupby()`. One box-plot will be\ndone per value of columns in by.\n\nThe matplotlib axes to be used by boxplot.\n\nTick label font size in points or as a string (e.g., large).\n\nThe rotation angle of labels (in degrees) with respect to the screen\ncoordinate system.\n\nSetting this to True will show the grid.\n\nThe size of the figure to create in matplotlib.\n\nFor example, (3, 5) will display the subplots using 3 columns and 5 rows,\nstarting from the top-left.\n\nThe kind of object to return. The default is `axes`.\n\n\u2018axes\u2019 returns the matplotlib axes the boxplot is drawn on.\n\n\u2018dict\u2019 returns a dictionary whose values are the matplotlib Lines of the\nboxplot.\n\n\u2018both\u2019 returns a namedtuple with the axes and dict.\n\nwhen grouping with `by`, a Series mapping columns to `return_type` is\nreturned.\n\nIf `return_type` is None, a NumPy array of axes with the same shape as\n`layout` is returned.\n\nAll other plotting keyword arguments to be passed to\n`matplotlib.pyplot.boxplot()`.\n\nSee Notes.\n\nSee also\n\nMake a histogram.\n\nMatplotlib equivalent plot.\n\nNotes\n\nThe return type depends on the return_type parameter:\n\n\u2018axes\u2019 : object of class matplotlib.axes.Axes\n\n\u2018dict\u2019 : dict of matplotlib.lines.Line2D objects\n\n\u2018both\u2019 : a namedtuple with structure (ax, lines)\n\nFor data grouped with `by`, return a Series of the above or a numpy array:\n\n`Series`\n\n`array` (for `return_type = None`)\n\nUse `return_type='dict'` when you want to tweak the appearance of the lines\nafter plotting. In this case a dict containing the Lines making up the boxes,\ncaps, fliers, medians, and whiskers is returned.\n\nExamples\n\nBoxplots can be created for every column in the dataframe by `df.boxplot()` or\nindicating the columns to be used:\n\nBoxplots of variables distributions grouped by the values of a third variable\ncan be created using the option `by`. For instance:\n\nA list of strings (i.e. `['X', 'Y']`) can be passed to boxplot in order to\ngroup the data by combination of the variables in the x-axis:\n\nThe layout of boxplot can be adjusted giving a tuple to `layout`:\n\nAdditional formatting can be done to the boxplot, like suppressing the grid\n(`grid=False`), rotating the labels in the x-axis (i.e. `rot=45`) or changing\nthe fontsize (i.e. `fontsize=15`):\n\nThe parameter `return_type` can be used to select the type of element returned\nby boxplot. When `return_type='axes'` is selected, the matplotlib axes on\nwhich the boxplot is drawn are returned:\n\nWhen grouping with `by`, a Series mapping columns to `return_type` is\nreturned:\n\nIf `return_type` is None, a NumPy array of axes with the same shape as\n`layout` is returned:\n\n"}, {"name": "pandas.plotting.deregister_matplotlib_converters", "path": "reference/api/pandas.plotting.deregister_matplotlib_converters", "type": "Plotting", "text": "\nRemove pandas formatters and converters.\n\nRemoves the custom converters added by `register()`. This attempts to set the\nstate of the registry back to the state before pandas registered its own\nunits. Converters for pandas\u2019 own types like Timestamp and Period are removed\ncompletely. Converters for types pandas overwrites, like `datetime.datetime`,\nare restored to their original value.\n\nSee also\n\nRegister pandas formatters and converters with matplotlib.\n\n"}, {"name": "pandas.plotting.lag_plot", "path": "reference/api/pandas.plotting.lag_plot", "type": "Plotting", "text": "\nLag plot for time series.\n\nMatplotlib scatter method keyword arguments.\n\nExamples\n\nLag plots are most commonly used to look for patterns in time series data.\n\nGiven the following time series\n\nA lag plot with `lag=1` returns\n\n"}, {"name": "pandas.plotting.parallel_coordinates", "path": "reference/api/pandas.plotting.parallel_coordinates", "type": "Plotting", "text": "\nParallel coordinates plotting.\n\nColumn name containing class names.\n\nA list of column names to use.\n\nMatplotlib axis object.\n\nColors to use for the different classes.\n\nIf true, columns will be used as xticks.\n\nA list of values to use for xticks.\n\nColormap to use for line colors.\n\nIf true, vertical lines will be added at each xtick.\n\nOptions to be passed to axvline method for vertical lines.\n\nSort class_column labels, useful when assigning colors.\n\nOptions to pass to matplotlib plotting method.\n\nExamples\n\n"}, {"name": "pandas.plotting.plot_params", "path": "reference/api/pandas.plotting.plot_params", "type": "Plotting", "text": "\nStores pandas plotting options.\n\nAllows for parameter aliasing so you can just use parameter names that are the\nsame as the plot function parameters, but is stored in a canonical format that\nmakes it easy to breakdown into groups later.\n\n"}, {"name": "pandas.plotting.radviz", "path": "reference/api/pandas.plotting.radviz", "type": "Plotting", "text": "\nPlot a multidimensional dataset in 2D.\n\nEach Series in the DataFrame is represented as a evenly distributed slice on a\ncircle. Each data point is rendered in the circle according to the value on\neach Series. Highly correlated Series in the DataFrame are placed closer on\nthe unit circle.\n\nRadViz allow to project a N-dimensional data set into a 2D space where the\ninfluence of each dimension can be interpreted as a balance between the\ninfluence of all dimensions.\n\nMore info available at the original article describing RadViz.\n\nObject holding the data.\n\nColumn name containing the name of the data point category.\n\nA plot instance to which to add the information.\n\nAssign a color to each category. Example: [\u2018blue\u2019, \u2018green\u2019].\n\nColormap to select colors from. If string, load colormap with that name from\nmatplotlib.\n\nOptions to pass to matplotlib scatter plotting method.\n\nSee also\n\nPlot clustering visualization.\n\nExamples\n\n"}, {"name": "pandas.plotting.register_matplotlib_converters", "path": "reference/api/pandas.plotting.register_matplotlib_converters", "type": "Plotting", "text": "\nRegister pandas formatters and converters with matplotlib.\n\nThis function modifies the global `matplotlib.units.registry` dictionary.\npandas adds custom converters for\n\npd.Timestamp\n\npd.Period\n\nnp.datetime64\n\ndatetime.datetime\n\ndatetime.date\n\ndatetime.time\n\nSee also\n\nRemove pandas formatters and converters.\n\n"}, {"name": "pandas.plotting.scatter_matrix", "path": "reference/api/pandas.plotting.scatter_matrix", "type": "Plotting", "text": "\nDraw a matrix of scatter plots.\n\nAmount of transparency applied.\n\nA tuple (width, height) in inches.\n\nSetting this to True will show the grid.\n\nPick between \u2018kde\u2019 and \u2018hist\u2019 for either Kernel Density Estimation or\nHistogram plot in the diagonal.\n\nMatplotlib marker type, default \u2018.\u2019.\n\nKeyword arguments to be passed to kernel density estimate plot.\n\nKeyword arguments to be passed to hist function.\n\nRelative extension of axis range in x and y with respect to (x_max - x_min) or\n(y_max - y_min).\n\nKeyword arguments to be passed to scatter function.\n\nA matrix of scatter plots.\n\nExamples\n\n"}, {"name": "pandas.plotting.table", "path": "reference/api/pandas.plotting.table", "type": "Plotting", "text": "\nHelper function to convert DataFrame and Series to matplotlib.table.\n\nData for table contents.\n\nKeyword arguments to be passed to matplotlib.table.table. If rowLabels or\ncolLabels is not specified, data index or column name will be used.\n\n"}, {"name": "pandas.qcut", "path": "reference/api/pandas.qcut", "type": "General functions", "text": "\nQuantile-based discretization function.\n\nDiscretize variable into equal-sized buckets based on rank or based on sample\nquantiles. For example 1000 values for 10 quantiles would produce a\nCategorical object indicating quantile membership for each data point.\n\nNumber of quantiles. 10 for deciles, 4 for quartiles, etc. Alternately array\nof quantiles, e.g. [0, .25, .5, .75, 1.] for quartiles.\n\nUsed as labels for the resulting bins. Must be of the same length as the\nresulting bins. If False, return only integer indicators of the bins. If True,\nraises an error.\n\nWhether to return the (bins, labels) or not. Can be useful if bins is given as\na scalar.\n\nThe precision at which to store and display the bins labels.\n\nIf bin edges are not unique, raise ValueError or drop non-uniques.\n\nThe return type (Categorical or Series) depends on the input: a Series of type\ncategory if input is a Series else Categorical. Bins are represented as\ncategories when categorical data is returned.\n\nReturned only if retbins is True.\n\nNotes\n\nOut of bounds values will be NA in the resulting Categorical object\n\nExamples\n\n"}, {"name": "pandas.RangeIndex", "path": "reference/api/pandas.rangeindex", "type": "Index Objects", "text": "\nImmutable Index implementing a monotonic integer range.\n\nRangeIndex is a memory-saving special case of Int64Index limited to\nrepresenting monotonic ranges. Using RangeIndex may in some instances improve\ncomputing speed.\n\nThis is the default index type used by DataFrame and Series when no explicit\nindex is provided by the user.\n\nIf int and \u201cstop\u201d is not given, interpreted as \u201cstop\u201d instead.\n\nUnused, accepted for homogeneity with other index types.\n\nUnused, accepted for homogeneity with other index types.\n\nName to be stored in the index.\n\nSee also\n\nThe base pandas Index type.\n\nIndex of int64 data.\n\nAttributes\n\n`start`\n\nThe value of the start parameter (`0` if this was not supplied).\n\n`stop`\n\nThe value of the stop parameter.\n\n`step`\n\nThe value of the step parameter (`1` if this was not supplied).\n\nMethods\n\n`from_range`(data[, name, dtype])\n\nCreate RangeIndex from a range object.\n\n"}, {"name": "pandas.RangeIndex.from_range", "path": "reference/api/pandas.rangeindex.from_range", "type": "Index Objects", "text": "\nCreate RangeIndex from a range object.\n\n"}, {"name": "pandas.RangeIndex.start", "path": "reference/api/pandas.rangeindex.start", "type": "Index Objects", "text": "\nThe value of the start parameter (`0` if this was not supplied).\n\n"}, {"name": "pandas.RangeIndex.step", "path": "reference/api/pandas.rangeindex.step", "type": "Index Objects", "text": "\nThe value of the step parameter (`1` if this was not supplied).\n\n"}, {"name": "pandas.RangeIndex.stop", "path": "reference/api/pandas.rangeindex.stop", "type": "Index Objects", "text": "\nThe value of the stop parameter.\n\n"}, {"name": "pandas.read_clipboard", "path": "reference/api/pandas.read_clipboard", "type": "Input/output", "text": "\nRead text from clipboard and pass to read_csv.\n\nA string or regex delimiter. The default of \u2018s+\u2019 denotes one or more\nwhitespace characters.\n\nSee read_csv for the full argument list.\n\nA parsed DataFrame object.\n\n"}, {"name": "pandas.read_csv", "path": "reference/api/pandas.read_csv", "type": "Input/output", "text": "\nRead a comma-separated values (csv) file into DataFrame.\n\nAlso supports optionally iterating or breaking of the file into chunks.\n\nAdditional help can be found in the online docs for IO Tools.\n\nAny valid string path is acceptable. The string could be a URL. Valid URL\nschemes include http, ftp, s3, gs, and file. For file URLs, a host is\nexpected. A local file could be: file://localhost/path/to/table.csv.\n\nIf you want to pass in a path object, pandas accepts any `os.PathLike`.\n\nBy file-like object, we refer to objects with a `read()` method, such as a\nfile handle (e.g. via builtin `open` function) or `StringIO`.\n\nDelimiter to use. If sep is None, the C engine cannot automatically detect the\nseparator, but the Python parsing engine can, meaning the latter will be used\nand automatically detect the separator by Python\u2019s builtin sniffer tool,\n`csv.Sniffer`. In addition, separators longer than 1 character and different\nfrom `'\\s+'` will be interpreted as regular expressions and will also force\nthe use of the Python parsing engine. Note that regex delimiters are prone to\nignoring quoted data. Regex example: `'\\r\\t'`.\n\nAlias for sep.\n\nRow number(s) to use as the column names, and the start of the data. Default\nbehavior is to infer the column names: if no names are passed the behavior is\nidentical to `header=0` and column names are inferred from the first line of\nthe file, if column names are passed explicitly then the behavior is identical\nto `header=None`. Explicitly pass `header=0` to be able to replace existing\nnames. The header can be a list of integers that specify row locations for a\nmulti-index on the columns e.g. [0,1,3]. Intervening rows that are not\nspecified will be skipped (e.g. 2 in this example is skipped). Note that this\nparameter ignores commented lines and empty lines if `skip_blank_lines=True`,\nso `header=0` denotes the first line of data rather than the first line of the\nfile.\n\nList of column names to use. If the file contains a header row, then you\nshould explicitly pass `header=0` to override the column names. Duplicates in\nthis list are not allowed.\n\nColumn(s) to use as the row labels of the `DataFrame`, either given as string\nname or column index. If a sequence of int / str is given, a MultiIndex is\nused.\n\nNote: `index_col=False` can be used to force pandas to not use the first\ncolumn as the index, e.g. when you have a malformed file with delimiters at\nthe end of each line.\n\nReturn a subset of the columns. If list-like, all elements must either be\npositional (i.e. integer indices into the document columns) or strings that\ncorrespond to column names provided either by the user in names or inferred\nfrom the document header row(s). If `names` are given, the document header\nrow(s) are not taken into account. For example, a valid list-like usecols\nparameter would be `[0, 1, 2]` or `['foo', 'bar', 'baz']`. Element order is\nignored, so `usecols=[0, 1]` is the same as `[1, 0]`. To instantiate a\nDataFrame from `data` with element order preserved use `pd.read_csv(data,\nusecols=['foo', 'bar'])[['foo', 'bar']]` for columns in `['foo', 'bar']` order\nor `pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]` for `['bar',\n'foo']` order.\n\nIf callable, the callable function will be evaluated against the column names,\nreturning names where the callable function evaluates to True. An example of a\nvalid callable argument would be `lambda x: x.upper() in ['AAA', 'BBB',\n'DDD']`. Using this parameter results in much faster parsing time and lower\nmemory usage.\n\nIf the parsed data only contains one column then return a Series.\n\nDeprecated since version 1.4.0: Append `.squeeze(\"columns\")` to the call to\n`read_csv` to squeeze the data.\n\nPrefix to add to column numbers when no header, e.g. \u2018X\u2019 for X0, X1, \u2026\n\nDeprecated since version 1.4.0: Use a list comprehension on the DataFrame\u2019s\ncolumns after calling `read_csv`.\n\nDuplicate columns will be specified as \u2018X\u2019, \u2018X.1\u2019, \u2026\u2019X.N\u2019, rather than\n\u2018X\u2019\u2026\u2019X\u2019. Passing in False will cause data to be overwritten if there are\nduplicate names in the columns.\n\nData type for data or columns. E.g. {\u2018a\u2019: np.float64, \u2018b\u2019: np.int32, \u2018c\u2019:\n\u2018Int64\u2019} Use str or object together with suitable na_values settings to\npreserve and not interpret dtype. If converters are specified, they will be\napplied INSTEAD of dtype conversion.\n\nParser engine to use. The C and pyarrow engines are faster, while the python\nengine is currently more feature-complete. Multithreading is currently only\nsupported by the pyarrow engine.\n\nNew in version 1.4.0: The \u201cpyarrow\u201d engine was added as an experimental\nengine, and some features are unsupported, or may not work correctly, with\nthis engine.\n\nDict of functions for converting values in certain columns. Keys can either be\nintegers or column labels.\n\nValues to consider as True.\n\nValues to consider as False.\n\nSkip spaces after delimiter.\n\nLine numbers to skip (0-indexed) or number of lines to skip (int) at the start\nof the file.\n\nIf callable, the callable function will be evaluated against the row indices,\nreturning True if the row should be skipped and False otherwise. An example of\na valid callable argument would be `lambda x: x in [0, 2]`.\n\nNumber of lines at bottom of file to skip (Unsupported with engine=\u2019c\u2019).\n\nNumber of rows of file to read. Useful for reading pieces of large files.\n\nAdditional strings to recognize as NA/NaN. If dict passed, specific per-column\nNA values. By default the following values are interpreted as NaN: \u2018\u2019, \u2018#N/A\u2019,\n\u2018#N/A N/A\u2019, \u2018#NA\u2019, \u2018-1.#IND\u2019, \u2018-1.#QNAN\u2019, \u2018-NaN\u2019, \u2018-nan\u2019, \u20181.#IND\u2019, \u20181.#QNAN\u2019,\n\u2018<NA>\u2019, \u2018N/A\u2019, \u2018NA\u2019, \u2018NULL\u2019, \u2018NaN\u2019, \u2018n/a\u2019, \u2018nan\u2019, \u2018null\u2019.\n\nWhether or not to include the default NaN values when parsing the data.\nDepending on whether na_values is passed in, the behavior is as follows:\n\nIf keep_default_na is True, and na_values are specified, na_values is appended\nto the default NaN values used for parsing.\n\nIf keep_default_na is True, and na_values are not specified, only the default\nNaN values are used for parsing.\n\nIf keep_default_na is False, and na_values are specified, only the NaN values\nspecified na_values are used for parsing.\n\nIf keep_default_na is False, and na_values are not specified, no strings will\nbe parsed as NaN.\n\nNote that if na_filter is passed in as False, the keep_default_na and\nna_values parameters will be ignored.\n\nDetect missing value markers (empty strings and the value of na_values). In\ndata without any NAs, passing na_filter=False can improve the performance of\nreading a large file.\n\nIndicate number of NA values placed in non-numeric columns.\n\nIf True, skip over blank lines rather than interpreting as NaN values.\n\nThe behavior is as follows:\n\nboolean. If True -> try parsing the index.\n\nlist of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as\na separate date column.\n\nlist of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as a\nsingle date column.\n\ndict, e.g. {\u2018foo\u2019 : [1, 3]} -> parse columns 1, 3 as date and call result\n\u2018foo\u2019\n\nIf a column or index cannot be represented as an array of datetimes, say\nbecause of an unparsable value or a mixture of timezones, the column or index\nwill be returned unaltered as an object data type. For non-standard datetime\nparsing, use `pd.to_datetime` after `pd.read_csv`. To parse an index or column\nwith a mixture of timezones, specify `date_parser` to be a partially-applied\n`pandas.to_datetime()` with `utc=True`. See Parsing a CSV with mixed timezones\nfor more.\n\nNote: A fast-path exists for iso8601-formatted dates.\n\nIf True and parse_dates is enabled, pandas will attempt to infer the format of\nthe datetime strings in the columns, and if it can be inferred, switch to a\nfaster method of parsing them. In some cases this can increase the parsing\nspeed by 5-10x.\n\nIf True and parse_dates specifies combining multiple columns then keep the\noriginal columns.\n\nFunction to use for converting a sequence of string columns to an array of\ndatetime instances. The default uses `dateutil.parser.parser` to do the\nconversion. Pandas will try to call date_parser in three different ways,\nadvancing to the next if an exception occurs: 1) Pass one or more arrays (as\ndefined by parse_dates) as arguments; 2) concatenate (row-wise) the string\nvalues from the columns defined by parse_dates into a single array and pass\nthat; and 3) call date_parser once for each row using one or more strings\n(corresponding to the columns defined by parse_dates) as arguments.\n\nDD/MM format dates, international and European format.\n\nIf True, use a cache of unique, converted dates to apply the datetime\nconversion. May produce significant speed-up when parsing duplicate date\nstrings, especially ones with timezone offsets.\n\nNew in version 0.25.0.\n\nReturn TextFileReader object for iteration or getting chunks with\n`get_chunk()`.\n\nChanged in version 1.2: `TextFileReader` is a context manager.\n\nReturn TextFileReader object for iteration. See the IO Tools docs for more\ninformation on `iterator` and `chunksize`.\n\nChanged in version 1.2: `TextFileReader` is a context manager.\n\nFor on-the-fly decompression of on-disk data. If \u2018infer\u2019 and \u2018%s\u2019 is path-\nlike, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019,\n\u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). If using \u2018zip\u2019, the ZIP\nfile must contain only one data file to be read in. Set to `None` for no\ndecompression. Can also be a dict with key `'method'` set to one of {`'zip'`,\n`'gzip'`, `'bz2'`, `'zstd'`} and other key-value pairs are forwarded to\n`zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or\n`zstandard.ZstdDecompressor`, respectively. As an example, the following could\nbe passed for Zstandard decompression using a custom compression dictionary:\n`compression={'method': 'zstd', 'dict_data': my_compression_dict}`.\n\nChanged in version 1.4.0: Zstandard support.\n\nThousands separator.\n\nCharacter to recognize as decimal point (e.g. use \u2018,\u2019 for European data).\n\nCharacter to break file into lines. Only valid with C parser.\n\nThe character used to denote the start and end of a quoted item. Quoted items\ncan include the delimiter and it will be ignored.\n\nControl field quoting behavior per `csv.QUOTE_*` constants. Use one of\nQUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n\nWhen quotechar is specified and quoting is not `QUOTE_NONE`, indicate whether\nor not to interpret two consecutive quotechar elements INSIDE a field as a\nsingle `quotechar` element.\n\nOne-character string used to escape other characters.\n\nIndicates remainder of line should not be parsed. If found at the beginning of\na line, the line will be ignored altogether. This parameter must be a single\ncharacter. Like empty lines (as long as `skip_blank_lines=True`), fully\ncommented lines are ignored by the parameter header but not by skiprows. For\nexample, if `comment='#'`, parsing `#empty\\na,b,c\\n1,2,3` with `header=0` will\nresult in \u2018a,b,c\u2019 being treated as the header.\n\nEncoding to use for UTF when reading/writing (ex. \u2018utf-8\u2019). List of Python\nstandard encodings .\n\nChanged in version 1.2: When `encoding` is `None`, `errors=\"replace\"` is\npassed to `open()`. Otherwise, `errors=\"strict\"` is passed to `open()`. This\nbehavior was previously only the case for `engine=\"python\"`.\n\nChanged in version 1.3.0: `encoding_errors` is a new argument. `encoding` has\nno longer an influence on how encoding errors are handled.\n\nHow encoding errors are treated. List of possible values .\n\nNew in version 1.3.0.\n\nIf provided, this parameter will override values (default or not) for the\nfollowing parameters: delimiter, doublequote, escapechar, skipinitialspace,\nquotechar, and quoting. If it is necessary to override values, a ParserWarning\nwill be issued. See csv.Dialect documentation for more details.\n\nLines with too many fields (e.g. a csv line with too many commas) will by\ndefault cause an exception to be raised, and no DataFrame will be returned. If\nFalse, then these \u201cbad lines\u201d will be dropped from the DataFrame that is\nreturned.\n\nDeprecated since version 1.3.0: The `on_bad_lines` parameter should be used\ninstead to specify behavior upon encountering a bad line instead.\n\nIf error_bad_lines is False, and warn_bad_lines is True, a warning for each\n\u201cbad line\u201d will be output.\n\nDeprecated since version 1.3.0: The `on_bad_lines` parameter should be used\ninstead to specify behavior upon encountering a bad line instead.\n\nSpecifies what to do upon encountering a bad line (a line with too many\nfields). Allowed values are :\n\n\u2018error\u2019, raise an Exception when a bad line is encountered.\n\n\u2018warn\u2019, raise a warning when a bad line is encountered and skip that line.\n\n\u2018skip\u2019, skip bad lines without raising or warning when they are encountered.\n\nNew in version 1.3.0:\n\ncallable, function with signature `(bad_line: list[str]) -> list[str] | None`\nthat will process a single bad line. `bad_line` is a list of strings split by\nthe `sep`. If the function returns `None`, the bad line will be ignored. If\nthe function returns a new list of strings with more elements than expected, a\n``ParserWarning` will be emitted while dropping extra elements. Only supported\nwhen `engine=\"python\"`\n\nNew in version 1.4.0.\n\nSpecifies whether or not whitespace (e.g. `' '` or `' '`) will be used as the\nsep. Equivalent to setting `sep='\\s+'`. If this option is set to True, nothing\nshould be passed in for the `delimiter` parameter.\n\nInternally process the file in chunks, resulting in lower memory use while\nparsing, but possibly mixed type inference. To ensure no mixed types either\nset False, or specify the type with the dtype parameter. Note that the entire\nfile is read into a single DataFrame regardless, use the chunksize or iterator\nparameter to return the data in chunks. (Only valid with C parser).\n\nIf a filepath is provided for filepath_or_buffer, map the file object directly\nonto memory and access the data directly from there. Using this option can\nimprove performance because there is no longer any I/O overhead.\n\nSpecifies which converter the C engine should use for floating-point values.\nThe options are `None` or \u2018high\u2019 for the ordinary converter, \u2018legacy\u2019 for the\noriginal lower precision pandas converter, and \u2018round_trip\u2019 for the round-trip\nconverter.\n\nChanged in version 1.2.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.\n\nA comma-separated values (csv) file is returned as two-dimensional data\nstructure with labeled axes.\n\nSee also\n\nWrite DataFrame to a comma-separated values (csv) file.\n\nRead a comma-separated values (csv) file into DataFrame.\n\nRead a table of fixed-width formatted lines into DataFrame.\n\nExamples\n\n"}, {"name": "pandas.read_excel", "path": "reference/api/pandas.read_excel", "type": "Input/output", "text": "\nRead an Excel file into a pandas DataFrame.\n\nSupports xls, xlsx, xlsm, xlsb, odf, ods and odt file extensions read from a\nlocal filesystem or URL. Supports an option to read a single sheet or a list\nof sheets.\n\nAny valid string path is acceptable. The string could be a URL. Valid URL\nschemes include http, ftp, s3, and file. For file URLs, a host is expected. A\nlocal file could be: `file://localhost/path/to/table.xlsx`.\n\nIf you want to pass in a path object, pandas accepts any `os.PathLike`.\n\nBy file-like object, we refer to objects with a `read()` method, such as a\nfile handle (e.g. via builtin `open` function) or `StringIO`.\n\nStrings are used for sheet names. Integers are used in zero-indexed sheet\npositions (chart sheets do not count as a sheet position). Lists of\nstrings/integers are used to request multiple sheets. Specify None to get all\nworksheets.\n\nAvailable cases:\n\nDefaults to `0`: 1st sheet as a DataFrame\n\n`1`: 2nd sheet as a DataFrame\n\n`\"Sheet1\"`: Load sheet with name \u201cSheet1\u201d\n\n`[0, 1, \"Sheet5\"]`: Load first, second and sheet named \u201cSheet5\u201d as a dict of\nDataFrame\n\nNone: All worksheets.\n\nRow (0-indexed) to use for the column labels of the parsed DataFrame. If a\nlist of integers is passed those row positions will be combined into a\n`MultiIndex`. Use None if there is no header.\n\nList of column names to use. If file contains no header row, then you should\nexplicitly pass header=None.\n\nColumn (0-indexed) to use as the row labels of the DataFrame. Pass None if\nthere is no such column. If a list is passed, those columns will be combined\ninto a `MultiIndex`. If a subset of data is selected with `usecols`, index_col\nis based on the subset.\n\nIf None, then parse all columns.\n\nIf str, then indicates comma separated list of Excel column letters and column\nranges (e.g. \u201cA:E\u201d or \u201cA,C,E:F\u201d). Ranges are inclusive of both sides.\n\nIf list of int, then indicates list of column numbers to be parsed.\n\nIf list of string, then indicates list of column names to be parsed.\n\nIf callable, then evaluate each column name against it and parse the column if\nthe callable returns `True`.\n\nReturns a subset of the columns according to behavior above.\n\nIf the parsed data only contains one column then return a Series.\n\nDeprecated since version 1.4.0: Append `.squeeze(\"columns\")` to the call to\n`read_excel` to squeeze the data.\n\nData type for data or columns. E.g. {\u2018a\u2019: np.float64, \u2018b\u2019: np.int32} Use\nobject to preserve data as stored in Excel and not interpret dtype. If\nconverters are specified, they will be applied INSTEAD of dtype conversion.\n\nIf io is not a buffer or path, this must be set to identify io. Supported\nengines: \u201cxlrd\u201d, \u201copenpyxl\u201d, \u201codf\u201d, \u201cpyxlsb\u201d. Engine compatibility :\n\n\u201cxlrd\u201d supports old-style Excel files (.xls).\n\n\u201copenpyxl\u201d supports newer Excel file formats.\n\n\u201codf\u201d supports OpenDocument file formats (.odf, .ods, .odt).\n\n\u201cpyxlsb\u201d supports Binary Excel files.\n\nChanged in version 1.2.0: The engine xlrd now only supports old-style `.xls`\nfiles. When `engine=None`, the following logic will be used to determine the\nengine:\n\nIf `path_or_buffer` is an OpenDocument format (.odf, .ods, .odt), then odf\nwill be used.\n\nOtherwise if `path_or_buffer` is an xls format, `xlrd` will be used.\n\nOtherwise if `path_or_buffer` is in xlsb format, `pyxlsb` will be used.\n\nNew in version 1.3.0.\n\nOtherwise `openpyxl` will be used.\n\nChanged in version 1.3.0.\n\nDict of functions for converting values in certain columns. Keys can either be\nintegers or column labels, values are functions that take one input argument,\nthe Excel cell content, and return the transformed content.\n\nValues to consider as True.\n\nValues to consider as False.\n\nLine numbers to skip (0-indexed) or number of lines to skip (int) at the start\nof the file. If callable, the callable function will be evaluated against the\nrow indices, returning True if the row should be skipped and False otherwise.\nAn example of a valid callable argument would be `lambda x: x in [0, 2]`.\n\nNumber of rows to parse.\n\nAdditional strings to recognize as NA/NaN. If dict passed, specific per-column\nNA values. By default the following values are interpreted as NaN: \u2018\u2019, \u2018#N/A\u2019,\n\u2018#N/A N/A\u2019, \u2018#NA\u2019, \u2018-1.#IND\u2019, \u2018-1.#QNAN\u2019, \u2018-NaN\u2019, \u2018-nan\u2019, \u20181.#IND\u2019, \u20181.#QNAN\u2019,\n\u2018<NA>\u2019, \u2018N/A\u2019, \u2018NA\u2019, \u2018NULL\u2019, \u2018NaN\u2019, \u2018n/a\u2019, \u2018nan\u2019, \u2018null\u2019.\n\nWhether or not to include the default NaN values when parsing the data.\nDepending on whether na_values is passed in, the behavior is as follows:\n\nIf keep_default_na is True, and na_values are specified, na_values is appended\nto the default NaN values used for parsing.\n\nIf keep_default_na is True, and na_values are not specified, only the default\nNaN values are used for parsing.\n\nIf keep_default_na is False, and na_values are specified, only the NaN values\nspecified na_values are used for parsing.\n\nIf keep_default_na is False, and na_values are not specified, no strings will\nbe parsed as NaN.\n\nNote that if na_filter is passed in as False, the keep_default_na and\nna_values parameters will be ignored.\n\nDetect missing value markers (empty strings and the value of na_values). In\ndata without any NAs, passing na_filter=False can improve the performance of\nreading a large file.\n\nIndicate number of NA values placed in non-numeric columns.\n\nThe behavior is as follows:\n\nbool. If True -> try parsing the index.\n\nlist of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as\na separate date column.\n\nlist of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as a\nsingle date column.\n\ndict, e.g. {\u2018foo\u2019 : [1, 3]} -> parse columns 1, 3 as date and call result\n\u2018foo\u2019\n\nIf a column or index contains an unparsable date, the entire column or index\nwill be returned unaltered as an object data type. If you don`t want to parse\nsome cells as date just change their type in Excel to \u201cText\u201d. For non-standard\ndatetime parsing, use `pd.to_datetime` after `pd.read_excel`.\n\nNote: A fast-path exists for iso8601-formatted dates.\n\nFunction to use for converting a sequence of string columns to an array of\ndatetime instances. The default uses `dateutil.parser.parser` to do the\nconversion. Pandas will try to call date_parser in three different ways,\nadvancing to the next if an exception occurs: 1) Pass one or more arrays (as\ndefined by parse_dates) as arguments; 2) concatenate (row-wise) the string\nvalues from the columns defined by parse_dates into a single array and pass\nthat; and 3) call date_parser once for each row using one or more strings\n(corresponding to the columns defined by parse_dates) as arguments.\n\nThousands separator for parsing string columns to numeric. Note that this\nparameter is only necessary for columns stored as TEXT in Excel, any numeric\ncolumns will automatically be parsed, regardless of display format.\n\nCharacter to recognize as decimal point for parsing string columns to numeric.\nNote that this parameter is only necessary for columns stored as TEXT in\nExcel, any numeric columns will automatically be parsed, regardless of display\nformat.(e.g. use \u2018,\u2019 for European data).\n\nNew in version 1.4.0.\n\nComments out remainder of line. Pass a character or characters to this\nargument to indicate comments in the input file. Any data between the comment\nstring and the end of the current line is ignored.\n\nRows at the end to skip (0-indexed).\n\nConvert integral floats to int (i.e., 1.0 \u2013> 1). If False, all numeric data\nwill be read in as floats: Excel stores all numbers as floats internally.\n\nDeprecated since version 1.3.0: convert_float will be removed in a future\nversion\n\nDuplicate columns will be specified as \u2018X\u2019, \u2018X.1\u2019, \u2026\u2019X.N\u2019, rather than\n\u2018X\u2019\u2026\u2019X\u2019. Passing in False will cause data to be overwritten if there are\nduplicate names in the columns.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc., if using a URL that will be parsed by\n`fsspec`, e.g., starting \u201cs3://\u201d, \u201cgcs://\u201d. An error will be raised if\nproviding this argument with a local path or a file-like buffer. See the\nfsspec and backend storage implementation docs for the set of allowed keys and\nvalues.\n\nNew in version 1.2.0.\n\nDataFrame from the passed in Excel file. See notes in sheet_name argument for\nmore information on when a dict of DataFrames is returned.\n\nSee also\n\nWrite DataFrame to an Excel file.\n\nWrite DataFrame to a comma-separated values (csv) file.\n\nRead a comma-separated values (csv) file into DataFrame.\n\nRead a table of fixed-width formatted lines into DataFrame.\n\nExamples\n\nThe file can be read using the file name as string or an open file object:\n\nIndex and header can be specified via the index_col and header arguments\n\nColumn types are inferred but can be explicitly specified\n\nTrue, False, and NA values, and thousands separators have defaults, but can be\nexplicitly specified, too. Supply the values you would like as strings or\nlists of strings!\n\nComment lines in the excel input file can be skipped using the comment kwarg\n\n"}, {"name": "pandas.read_feather", "path": "reference/api/pandas.read_feather", "type": "Input/output", "text": "\nLoad a feather-format object from the file path.\n\nString, path object (implementing `os.PathLike[str]`), or file-like object\nimplementing a binary `read()` function. The string could be a URL. Valid URL\nschemes include http, ftp, s3, and file. For file URLs, a host is expected. A\nlocal file could be: `file://localhost/path/to/table.feather`.\n\nIf not provided, all columns are read.\n\nWhether to parallelize reading using multiple threads.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\n"}, {"name": "pandas.read_fwf", "path": "reference/api/pandas.read_fwf", "type": "Input/output", "text": "\nRead a table of fixed-width formatted lines into DataFrame.\n\nAlso supports optionally iterating or breaking of the file into chunks.\n\nAdditional help can be found in the online docs for IO Tools.\n\nString, path object (implementing `os.PathLike[str]`), or file-like object\nimplementing a text `read()` function.The string could be a URL. Valid URL\nschemes include http, ftp, s3, and file. For file URLs, a host is expected. A\nlocal file could be: `file://localhost/path/to/table.csv`.\n\nA list of tuples giving the extents of the fixed-width fields of each line as\nhalf-open intervals (i.e., [from, to[ ). String value \u2018infer\u2019 can be used to\ninstruct the parser to try detecting the column specifications from the first\n100 rows of the data which are not being skipped via skiprows\n(default=\u2019infer\u2019).\n\nA list of field widths which can be used instead of \u2018colspecs\u2019 if the\nintervals are contiguous.\n\nThe number of rows to consider when letting the parser determine the colspecs.\n\nOptional keyword arguments can be passed to `TextFileReader`.\n\nA comma-separated values (csv) file is returned as two-dimensional data\nstructure with labeled axes.\n\nSee also\n\nWrite DataFrame to a comma-separated values (csv) file.\n\nRead a comma-separated values (csv) file into DataFrame.\n\nExamples\n\n"}, {"name": "pandas.read_gbq", "path": "reference/api/pandas.read_gbq", "type": "Input/output", "text": "\nLoad data from Google BigQuery.\n\nThis function requires the pandas-gbq package.\n\nSee the How to authenticate with Google BigQuery guide for authentication\ninstructions.\n\nSQL-Like Query to return data values.\n\nGoogle BigQuery Account project ID. Optional when available from the\nenvironment.\n\nName of result column to use for index in results DataFrame.\n\nList of BigQuery column names in the desired order for results DataFrame.\n\nForce Google BigQuery to re-authenticate the user. This is useful if multiple\naccounts are used.\n\nUse the local webserver flow instead of the console flow when getting user\ncredentials.\n\nNew in version 0.2.0 of pandas-gbq.\n\nNote: The default value is changing to \u2018standard\u2019 in a future version.\n\nSQL syntax dialect to use. Value can be one of:\n\nUse BigQuery\u2019s legacy SQL dialect. For more information see BigQuery Legacy\nSQL Reference.\n\nUse BigQuery\u2019s standard SQL, which is compliant with the SQL 2011 standard.\nFor more information see BigQuery Standard SQL Reference.\n\nLocation where the query job should run. See the BigQuery locations\ndocumentation for a list of available locations. The location must match that\nof any datasets used in the query.\n\nNew in version 0.5.0 of pandas-gbq.\n\nQuery config parameters for job processing. For example:\n\nconfiguration = {\u2018query\u2019: {\u2018useQueryCache\u2019: False}}\n\nFor more information see BigQuery REST API Reference.\n\nCredentials for accessing Google APIs. Use this parameter to override default\ncredentials, such as to use Compute Engine\n`google.auth.compute_engine.Credentials` or Service Account\n`google.oauth2.service_account.Credentials` directly.\n\nNew in version 0.8.0 of pandas-gbq.\n\nUse the BigQuery Storage API to download query results quickly, but at an\nincreased cost. To use this API, first enable it in the Cloud Console. You\nmust also have the bigquery.readsessions.create permission on the project you\nare billing queries to.\n\nThis feature requires version 0.10.0 or later of the `pandas-gbq` package. It\nalso requires the `google-cloud-bigquery-storage` and `fastavro` packages.\n\nNew in version 0.25.0.\n\nIf set, limit the maximum number of rows to fetch from the query results.\n\nNew in version 0.12.0 of pandas-gbq.\n\nNew in version 1.1.0.\n\nIf set, use the tqdm library to display a progress bar while the data\ndownloads. Install the `tqdm` package to use this feature.\n\nPossible values of `progress_bar_type` include:\n\nNo progress bar.\n\nUse the `tqdm.tqdm()` function to print a progress bar to `sys.stderr`.\n\nUse the `tqdm.tqdm_notebook()` function to display a progress bar as a Jupyter\nnotebook widget.\n\nUse the `tqdm.tqdm_gui()` function to display a progress bar as a graphical\ndialog box.\n\nNote that this feature requires version 0.12.0 or later of the `pandas-gbq`\npackage. And it requires the `tqdm` package. Slightly different than `pandas-\ngbq`, here the default is `None`.\n\nNew in version 1.0.0.\n\nDataFrame representing results of query.\n\nSee also\n\nThis function in the pandas-gbq library.\n\nWrite a DataFrame to Google BigQuery.\n\n"}, {"name": "pandas.read_hdf", "path": "reference/api/pandas.read_hdf", "type": "Input/output", "text": "\nRead from the store, close it if we opened it.\n\nRetrieve pandas object stored in file, optionally based on where criteria.\n\nWarning\n\nPandas uses PyTables for reading and writing HDF5 files, which allows\nserializing object-dtype data with pickle when using the \u201cfixed\u201d format.\nLoading pickled data received from untrusted sources can be unsafe.\n\nSee: https://docs.python.org/3/library/pickle.html for more.\n\nAny valid string path is acceptable. Only supports the local file system,\nremote URLs and file-like objects are not supported.\n\nIf you want to pass in a path object, pandas accepts any `os.PathLike`.\n\nAlternatively, pandas accepts an open `pandas.HDFStore` object.\n\nThe group identifier in the store. Can be omitted if the HDF file contains a\nsingle pandas object.\n\nMode to use when opening the file. Ignored if path_or_buf is a\n`pandas.HDFStore`. Default is \u2018r\u2019.\n\nSpecifies how encoding and decoding errors are to be handled. See the errors\nargument for `open()` for a full list of options.\n\nA list of Term (or convertible) objects.\n\nRow number to start selection.\n\nRow number to stop selection.\n\nA list of columns names to return.\n\nReturn an iterator object.\n\nNumber of rows to include in an iteration when using an iterator.\n\nAdditional keyword arguments passed to HDFStore.\n\nThe selected object. Return type depends on the object stored.\n\nSee also\n\nWrite a HDF file from a DataFrame.\n\nLow-level access to HDF files.\n\nExamples\n\n"}, {"name": "pandas.read_html", "path": "reference/api/pandas.read_html", "type": "Input/output", "text": "\nRead HTML tables into a `list` of `DataFrame` objects.\n\nString, path object (implementing `os.PathLike[str]`), or file-like object\nimplementing a string `read()` function. The string can represent a URL or the\nHTML itself. Note that lxml only accepts the http, ftp and file url protocols.\nIf you have a URL that starts with `'https'` you might try removing the `'s'`.\n\nThe set of tables containing text matching this regex or string will be\nreturned. Unless the HTML is extremely simple you will probably need to pass a\nnon-empty string here. Defaults to \u2018.+\u2019 (match any non-empty string). The\ndefault value will return all tables contained on a page. This value is\nconverted to a regular expression so that there is consistent behavior between\nBeautiful Soup and lxml.\n\nThe parsing engine to use. \u2018bs4\u2019 and \u2018html5lib\u2019 are synonymous with each\nother, they are both there for backwards compatibility. The default of `None`\ntries to use `lxml` to parse and if that fails it falls back on `bs4` \\+\n`html5lib`.\n\nThe row (or list of rows for a `MultiIndex`) to use to make the columns\nheaders.\n\nThe column (or list of columns) to use to create the index.\n\nNumber of rows to skip after parsing the column integer. 0-based. If a\nsequence of integers or a slice is given, will skip the rows indexed by that\nsequence. Note that a single element sequence means \u2018skip the nth row\u2019 whereas\nan integer means \u2018skip n rows\u2019.\n\nThis is a dictionary of attributes that you can pass to use to identify the\ntable in the HTML. These are not checked for validity before being passed to\nlxml or Beautiful Soup. However, these attributes must be valid HTML table\nattributes to work correctly. For example,\n\nis a valid attribute dictionary because the \u2018id\u2019 HTML tag attribute is a valid\nHTML attribute for any HTML tag as per this document.\n\nis not a valid attribute dictionary because \u2018asdf\u2019 is not a valid HTML\nattribute even if it is a valid XML attribute. Valid HTML 4.01 table\nattributes can be found here. A working draft of the HTML 5 spec can be found\nhere. It contains the latest information on table attributes for the modern\nweb.\n\nSee `read_csv()` for more details.\n\nSeparator to use to parse thousands. Defaults to `','`.\n\nThe encoding used to decode the web page. Defaults to `None`.``None``\npreserves the previous encoding behavior, which depends on the underlying\nparser library (e.g., the parser library will try to use the encoding provided\nby the document).\n\nCharacter to recognize as decimal point (e.g. use \u2018,\u2019 for European data).\n\nDict of functions for converting values in certain columns. Keys can either be\nintegers or column labels, values are functions that take one input argument,\nthe cell (not column) content, and return the transformed content.\n\nCustom NA values.\n\nIf na_values are specified and keep_default_na is False the default NaN values\nare overridden, otherwise they\u2019re appended to.\n\nWhether elements with \u201cdisplay: none\u201d should be parsed.\n\nA list of DataFrames.\n\nSee also\n\nRead a comma-separated values (csv) file into DataFrame.\n\nNotes\n\nBefore using this function you should read the gotchas about the HTML parsing\nlibraries.\n\nExpect to do some cleanup after you call this function. For example, you might\nneed to manually assign column names if the column names are converted to NaN\nwhen you pass the header=0 argument. We try to assume as little as possible\nabout the structure of the table and push the idiosyncrasies of the HTML\ncontained in the table to the user.\n\nThis function searches for `<table>` elements and only for `<tr>` and `<th>`\nrows and `<td>` elements within each `<tr>` or `<th>` element in the table.\n`<td>` stands for \u201ctable data\u201d. This function attempts to properly handle\n`colspan` and `rowspan` attributes. If the function has a `<thead>` argument,\nit is used to construct the header, otherwise the function attempts to find\nthe header within the body (by putting rows with only `<th>` elements into the\nheader).\n\nSimilar to `read_csv()` the header argument is applied after skiprows is\napplied.\n\nThis function will always return a list of `DataFrame` or it will fail, e.g.,\nit will not return an empty list.\n\nExamples\n\nSee the read_html documentation in the IO section of the docs for some\nexamples of reading in HTML tables.\n\n"}, {"name": "pandas.read_json", "path": "reference/api/pandas.read_json", "type": "Input/output", "text": "\nConvert a JSON string to pandas object.\n\nAny valid string path is acceptable. The string could be a URL. Valid URL\nschemes include http, ftp, s3, and file. For file URLs, a host is expected. A\nlocal file could be: `file://localhost/path/to/table.json`.\n\nIf you want to pass in a path object, pandas accepts any `os.PathLike`.\n\nBy file-like object, we refer to objects with a `read()` method, such as a\nfile handle (e.g. via builtin `open` function) or `StringIO`.\n\nIndication of expected JSON string format. Compatible JSON strings can be\nproduced by `to_json()` with a corresponding orient value. The set of possible\norients is:\n\n`'split'` : dict like `{index -> [index], columns -> [columns], data ->\n[values]}`\n\n`'records'` : list like `[{column -> value}, ... , {column -> value}]`\n\n`'index'` : dict like `{index -> {column -> value}}`\n\n`'columns'` : dict like `{column -> {index -> value}}`\n\n`'values'` : just the values array\n\nThe allowed and default values depend on the value of the typ parameter.\n\nwhen `typ == 'series'`,\n\nallowed orients are `{'split','records','index'}`\n\ndefault is `'index'`\n\nThe Series index must be unique for orient `'index'`.\n\nwhen `typ == 'frame'`,\n\nallowed orients are `{'split','records','index', 'columns','values', 'table'}`\n\ndefault is `'columns'`\n\nThe DataFrame index must be unique for orients `'index'` and `'columns'`.\n\nThe DataFrame columns must be unique for orients `'index'`, `'columns'`, and\n`'records'`.\n\nThe type of object to recover.\n\nIf True, infer dtypes; if a dict of column to dtype, then use those; if False,\nthen don\u2019t infer dtypes at all, applies only to the data.\n\nFor all `orient` values except `'table'`, default is True.\n\nChanged in version 0.25.0: Not applicable for `orient='table'`.\n\nTry to convert the axes to the proper dtypes.\n\nFor all `orient` values except `'table'`, default is True.\n\nChanged in version 0.25.0: Not applicable for `orient='table'`.\n\nIf True then default datelike columns may be converted (depending on\nkeep_default_dates). If False, no dates will be converted. If a list of column\nnames, then those columns will be converted and default datelike columns may\nalso be converted (depending on keep_default_dates).\n\nIf parsing dates (convert_dates is not False), then try to parse the default\ndatelike columns. A column label is datelike if\n\nit ends with `'_at'`,\n\nit ends with `'_time'`,\n\nit begins with `'timestamp'`,\n\nit is `'modified'`, or\n\nit is `'date'`.\n\nDirect decoding to numpy arrays. Supports numeric data only, but non-numeric\ncolumn and index labels are supported. Note also that the JSON ordering MUST\nbe the same for each term if numpy=True.\n\nDeprecated since version 1.0.0.\n\nSet to enable usage of higher precision (strtod) function when decoding string\nto double values. Default (False) is to use fast but less precise builtin\nfunctionality.\n\nThe timestamp unit to detect if converting dates. The default behaviour is to\ntry and detect the correct precision, but if this is not desired then pass one\nof \u2018s\u2019, \u2018ms\u2019, \u2018us\u2019 or \u2018ns\u2019 to force parsing only seconds, milliseconds,\nmicroseconds or nanoseconds respectively.\n\nThe encoding to use to decode py3 bytes.\n\nHow encoding errors are treated. List of possible values .\n\nNew in version 1.3.0.\n\nRead the file as a json object per line.\n\nReturn JsonReader object for iteration. See the line-delimited json docs for\nmore information on `chunksize`. This can only be passed if lines=True. If\nthis is None, the file will be read into memory all at once.\n\nChanged in version 1.2: `JsonReader` is a context manager.\n\nFor on-the-fly decompression of on-disk data. If \u2018infer\u2019 and \u2018path_or_buf\u2019 is\npath-like, then detect compression from the following extensions: \u2018.gz\u2019,\n\u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). If using \u2018zip\u2019,\nthe ZIP file must contain only one data file to be read in. Set to `None` for\nno decompression. Can also be a dict with key `'method'` set to one of\n{`'zip'`, `'gzip'`, `'bz2'`, `'zstd'`} and other key-value pairs are forwarded\nto `zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or\n`zstandard.ZstdDecompressor`, respectively. As an example, the following could\nbe passed for Zstandard decompression using a custom compression dictionary:\n`compression={'method': 'zstd', 'dict_data': my_compression_dict}`.\n\nChanged in version 1.4.0: Zstandard support.\n\nThe number of lines from the line-delimited jsonfile that has to be read. This\ncan only be passed if lines=True. If this is None, all the rows will be\nreturned.\n\nNew in version 1.1.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nThe type returned depends on the value of typ.\n\nSee also\n\nConvert a DataFrame to a JSON string.\n\nConvert a Series to a JSON string.\n\nNormalize semi-structured JSON data into a flat table.\n\nNotes\n\nSpecific to `orient='table'`, if a `DataFrame` with a literal `Index` name of\nindex gets written with `to_json()`, the subsequent read operation will\nincorrectly set the `Index` name to `None`. This is because index is also used\nby `DataFrame.to_json()` to denote a missing `Index` name, and the subsequent\n`read_json()` operation cannot distinguish between the two. The same\nlimitation is encountered with a `MultiIndex` and any names beginning with\n`'level_'`.\n\nExamples\n\nEncoding/decoding a Dataframe using `'split'` formatted JSON:\n\nEncoding/decoding a Dataframe using `'index'` formatted JSON:\n\nEncoding/decoding a Dataframe using `'records'` formatted JSON. Note that\nindex labels are not preserved with this encoding.\n\nEncoding with Table Schema\n\n"}, {"name": "pandas.read_orc", "path": "reference/api/pandas.read_orc", "type": "Input/output", "text": "\nLoad an ORC object from the file path, returning a DataFrame.\n\nNew in version 1.0.0.\n\nString, path object (implementing `os.PathLike[str]`), or file-like object\nimplementing a binary `read()` function. The string could be a URL. Valid URL\nschemes include http, ftp, s3, and file. For file URLs, a host is expected. A\nlocal file could be: `file://localhost/path/to/table.orc`.\n\nIf not None, only these columns will be read from the file.\n\nAny additional kwargs are passed to pyarrow.\n\nNotes\n\nBefore using this function you should read the user guide about ORC and\ninstall optional dependencies.\n\n"}, {"name": "pandas.read_parquet", "path": "reference/api/pandas.read_parquet", "type": "Input/output", "text": "\nLoad a parquet object from the file path, returning a DataFrame.\n\nString, path object (implementing `os.PathLike[str]`), or file-like object\nimplementing a binary `read()` function. The string could be a URL. Valid URL\nschemes include http, ftp, s3, gs, and file. For file URLs, a host is\nexpected. A local file could be: `file://localhost/path/to/table.parquet`. A\nfile URL can also be a path to a directory that contains multiple partitioned\nparquet files. Both pyarrow and fastparquet support paths to directories as\nwell as file URLs. A directory path could be:\n`file://localhost/path/to/tables` or `s3://bucket/partition_dir`.\n\nParquet library to use. If \u2018auto\u2019, then the option `io.parquet.engine` is\nused. The default `io.parquet.engine` behavior is to try \u2018pyarrow\u2019, falling\nback to \u2018fastparquet\u2019 if \u2018pyarrow\u2019 is unavailable.\n\nIf not None, only these columns will be read from the file.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.3.0.\n\nIf True, use dtypes that use `pd.NA` as missing value indicator for the\nresulting DataFrame. (only applicable for the `pyarrow` engine) As new dtypes\nare added that support `pd.NA` in the future, the output with this option will\nchange to use those dtypes. Note: this is an experimental option, and\nbehaviour (e.g. additional support dtypes) may change without notice.\n\nNew in version 1.2.0.\n\nAny additional kwargs are passed to the engine.\n\n"}, {"name": "pandas.read_pickle", "path": "reference/api/pandas.read_pickle", "type": "Input/output", "text": "\nLoad pickled pandas object (or any object) from file.\n\nWarning\n\nLoading pickled data received from untrusted sources can be unsafe. See here.\n\nString, path object (implementing `os.PathLike[str]`), or file-like object\nimplementing a binary `readlines()` function.\n\nChanged in version 1.0.0: Accept URL. URL is not limited to S3 and GCS.\n\nFor on-the-fly decompression of on-disk data. If \u2018infer\u2019 and\n\u2018filepath_or_buffer\u2019 is path-like, then detect compression from the following\nextensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no\ncompression). If using \u2018zip\u2019, the ZIP file must contain only one data file to\nbe read in. Set to `None` for no decompression. Can also be a dict with key\n`'method'` set to one of {`'zip'`, `'gzip'`, `'bz2'`, `'zstd'`} and other key-\nvalue pairs are forwarded to `zipfile.ZipFile`, `gzip.GzipFile`,\n`bz2.BZ2File`, or `zstandard.ZstdDecompressor`, respectively. As an example,\nthe following could be passed for Zstandard decompression using a custom\ncompression dictionary: `compression={'method': 'zstd', 'dict_data':\nmy_compression_dict}`.\n\nChanged in version 1.4.0: Zstandard support.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nSee also\n\nPickle (serialize) DataFrame object to file.\n\nPickle (serialize) Series object to file.\n\nRead HDF5 file into a DataFrame.\n\nRead SQL query or database table into a DataFrame.\n\nLoad a parquet object, returning a DataFrame.\n\nNotes\n\nread_pickle is only guaranteed to be backwards compatible to pandas 0.20.3.\n\nExamples\n\n"}, {"name": "pandas.read_sas", "path": "reference/api/pandas.read_sas", "type": "Input/output", "text": "\nRead SAS files stored as either XPORT or SAS7BDAT format files.\n\nString, path object (implementing `os.PathLike[str]`), or file-like object\nimplementing a binary `read()` function. The string could be a URL. Valid URL\nschemes include http, ftp, s3, and file. For file URLs, a host is expected. A\nlocal file could be: `file://localhost/path/to/table.sas`.\n\nIf None, file format is inferred from file extension. If \u2018xport\u2019 or\n\u2018sas7bdat\u2019, uses the corresponding format.\n\nIdentifier of column that should be used as index of the DataFrame.\n\nEncoding for text data. If None, text data are stored as raw bytes.\n\nRead file chunksize lines at a time, returns iterator.\n\nChanged in version 1.2: `TextFileReader` is a context manager.\n\nIf True, returns an iterator for reading the file incrementally.\n\nChanged in version 1.2: `TextFileReader` is a context manager.\n\n"}, {"name": "pandas.read_spss", "path": "reference/api/pandas.read_spss", "type": "Input/output", "text": "\nLoad an SPSS file from the file path, returning a DataFrame.\n\nNew in version 0.25.0.\n\nFile path.\n\nReturn a subset of the columns. If None, return all columns.\n\nConvert categorical columns into pd.Categorical.\n\n"}, {"name": "pandas.read_sql", "path": "reference/api/pandas.read_sql", "type": "Input/output", "text": "\nRead SQL query or database table into a DataFrame.\n\nThis function is a convenience wrapper around `read_sql_table` and\n`read_sql_query` (for backward compatibility). It will delegate to the\nspecific function depending on the provided input. A SQL query will be routed\nto `read_sql_query`, while a database table name will be routed to\n`read_sql_table`. Note that the delegated function might have more specific\nnotes about their functionality not listed here.\n\nSQL query to be executed or a table name.\n\nUsing SQLAlchemy makes it possible to use any DB supported by that library. If\na DBAPI2 object, only sqlite3 is supported. The user is responsible for engine\ndisposal and connection closure for the SQLAlchemy connectable; str\nconnections are closed automatically. See here.\n\nColumn(s) to set as index(MultiIndex).\n\nAttempts to convert values of non-string, non-numeric objects (like\ndecimal.Decimal) to floating point, useful for SQL result sets.\n\nList of parameters to pass to execute method. The syntax used to pass\nparameters is database driver dependent. Check your database driver\ndocumentation for which of the five syntax styles, described in PEP 249\u2019s\nparamstyle, is supported. Eg. for psycopg2, uses %(name)s so use\nparams={\u2018name\u2019 : \u2018value\u2019}.\n\nList of column names to parse as dates.\n\nDict of `{column_name: format string}` where format string is strftime\ncompatible in case of parsing string times, or is one of (D, s, ns, ms, us) in\ncase of parsing integer timestamps.\n\nDict of `{column_name: arg dict}`, where the arg dict corresponds to the\nkeyword arguments of `pandas.to_datetime()` Especially useful with databases\nwithout native Datetime support, such as SQLite.\n\nList of column names to select from SQL table (only used when reading a\ntable).\n\nIf specified, return an iterator where chunksize is the number of rows to\ninclude in each chunk.\n\nSee also\n\nRead SQL database table into a DataFrame.\n\nRead SQL query into a DataFrame.\n\nExamples\n\nRead data from SQL via either a SQL query or a SQL tablename. When using a\nSQLite database only SQL queries are accepted, providing only the SQL\ntablename will result in an error.\n\nApply date parsing to columns through the `parse_dates` argument\n\nThe `parse_dates` argument calls `pd.to_datetime` on the provided columns.\nCustom argument values for applying `pd.to_datetime` on a column are specified\nvia a dictionary format: 1. Ignore errors while parsing the values of\n\u201cdate_column\u201d\n\nApply a dayfirst date parsing order on the values of \u201cdate_column\u201d\n\nApply custom formatting when date parsing the values of \u201cdate_column\u201d\n\n"}, {"name": "pandas.read_sql_query", "path": "reference/api/pandas.read_sql_query", "type": "Input/output", "text": "\nRead SQL query into a DataFrame.\n\nReturns a DataFrame corresponding to the result set of the query string.\nOptionally provide an index_col parameter to use one of the columns as the\nindex, otherwise default integer index will be used.\n\nSQL query to be executed.\n\nUsing SQLAlchemy makes it possible to use any DB supported by that library. If\na DBAPI2 object, only sqlite3 is supported.\n\nColumn(s) to set as index(MultiIndex).\n\nAttempts to convert values of non-string, non-numeric objects (like\ndecimal.Decimal) to floating point. Useful for SQL result sets.\n\nList of parameters to pass to execute method. The syntax used to pass\nparameters is database driver dependent. Check your database driver\ndocumentation for which of the five syntax styles, described in PEP 249\u2019s\nparamstyle, is supported. Eg. for psycopg2, uses %(name)s so use\nparams={\u2018name\u2019 : \u2018value\u2019}.\n\nList of column names to parse as dates.\n\nDict of `{column_name: format string}` where format string is strftime\ncompatible in case of parsing string times, or is one of (D, s, ns, ms, us) in\ncase of parsing integer timestamps.\n\nDict of `{column_name: arg dict}`, where the arg dict corresponds to the\nkeyword arguments of `pandas.to_datetime()` Especially useful with databases\nwithout native Datetime support, such as SQLite.\n\nIf specified, return an iterator where chunksize is the number of rows to\ninclude in each chunk.\n\nData type for data or columns. E.g. np.float64 or {\u2018a\u2019: np.float64, \u2018b\u2019:\nnp.int32, \u2018c\u2019: \u2018Int64\u2019}.\n\nNew in version 1.3.0.\n\nSee also\n\nRead SQL database table into a DataFrame.\n\nRead SQL query or database table into a DataFrame.\n\nNotes\n\nAny datetime values with time zone information parsed via the parse_dates\nparameter will be converted to UTC.\n\n"}, {"name": "pandas.read_sql_table", "path": "reference/api/pandas.read_sql_table", "type": "Input/output", "text": "\nRead SQL database table into a DataFrame.\n\nGiven a table name and a SQLAlchemy connectable, returns a DataFrame. This\nfunction does not support DBAPI connections.\n\nName of SQL table in database.\n\nA database URI could be provided as str. SQLite DBAPI connection mode not\nsupported.\n\nName of SQL schema in database to query (if database flavor supports this).\nUses default schema if None (default).\n\nColumn(s) to set as index(MultiIndex).\n\nAttempts to convert values of non-string, non-numeric objects (like\ndecimal.Decimal) to floating point. Can result in loss of Precision.\n\nList of column names to parse as dates.\n\nDict of `{column_name: format string}` where format string is strftime\ncompatible in case of parsing string times or is one of (D, s, ns, ms, us) in\ncase of parsing integer timestamps.\n\nDict of `{column_name: arg dict}`, where the arg dict corresponds to the\nkeyword arguments of `pandas.to_datetime()` Especially useful with databases\nwithout native Datetime support, such as SQLite.\n\nList of column names to select from SQL table.\n\nIf specified, returns an iterator where chunksize is the number of rows to\ninclude in each chunk.\n\nA SQL table is returned as two-dimensional data structure with labeled axes.\n\nSee also\n\nRead SQL query into a DataFrame.\n\nRead SQL query or database table into a DataFrame.\n\nNotes\n\nAny datetime values with time zone information will be converted to UTC.\n\nExamples\n\n"}, {"name": "pandas.read_stata", "path": "reference/api/pandas.read_stata", "type": "Input/output", "text": "\nRead Stata file into DataFrame.\n\nAny valid string path is acceptable. The string could be a URL. Valid URL\nschemes include http, ftp, s3, and file. For file URLs, a host is expected. A\nlocal file could be: `file://localhost/path/to/table.dta`.\n\nIf you want to pass in a path object, pandas accepts any `os.PathLike`.\n\nBy file-like object, we refer to objects with a `read()` method, such as a\nfile handle (e.g. via builtin `open` function) or `StringIO`.\n\nConvert date variables to DataFrame time values.\n\nRead value labels and convert columns to Categorical/Factor variables.\n\nColumn to set as index.\n\nFlag indicating whether to convert missing values to their Stata\nrepresentations. If False, missing values are replaced with nan. If True,\ncolumns containing missing values are returned with object data types and\nmissing values are represented by StataMissingValue objects.\n\nPreserve Stata datatypes. If False, numeric data are upcast to pandas default\ntypes for foreign data (float64 or int64).\n\nColumns to retain. Columns will be returned in the given order. None returns\nall columns.\n\nFlag indicating whether converted categorical data are ordered.\n\nReturn StataReader object for iterations, returns chunks with given number of\nlines.\n\nReturn StataReader object.\n\nFor on-the-fly decompression of on-disk data. If \u2018infer\u2019 and \u2018%s\u2019 is path-\nlike, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019,\n\u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). If using \u2018zip\u2019, the ZIP\nfile must contain only one data file to be read in. Set to `None` for no\ndecompression. Can also be a dict with key `'method'` set to one of {`'zip'`,\n`'gzip'`, `'bz2'`, `'zstd'`} and other key-value pairs are forwarded to\n`zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or\n`zstandard.ZstdDecompressor`, respectively. As an example, the following could\nbe passed for Zstandard decompression using a custom compression dictionary:\n`compression={'method': 'zstd', 'dict_data': my_compression_dict}`.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nSee also\n\nLow-level reader for Stata data files.\n\nExport Stata data files.\n\nNotes\n\nCategorical variables read through an iterator may not have the same\ncategories and dtype. This occurs when a variable stored in a DTA file is\nassociated to an incomplete set of value labels that only label a strict\nsubset of the values.\n\nExamples\n\nCreating a dummy stata for this example >>> df = pd.DataFrame({\u2018animal\u2019:\n[\u2018falcon\u2019, \u2018parrot\u2019, \u2018falcon\u2019, \u2026 \u2018parrot\u2019], \u2026 \u2018speed\u2019: [350, 18, 361, 15]}) #\ndoctest: +SKIP >>> df.to_stata(\u2018animals.dta\u2019) # doctest: +SKIP\n\nRead a Stata dta file:\n\nRead a Stata dta file in 10,000 line chunks: >>> values = np.random.randint(0,\n10, size=(20_000, 1), dtype=\u201duint8\u201d) # doctest: +SKIP >>> df =\npd.DataFrame(values, columns=[\u201ci\u201d]) # doctest: +SKIP >>>\ndf.to_stata(\u2018filename.dta\u2019) # doctest: +SKIP\n\n"}, {"name": "pandas.read_table", "path": "reference/api/pandas.read_table", "type": "Input/output", "text": "\nRead general delimited file into DataFrame.\n\nAlso supports optionally iterating or breaking of the file into chunks.\n\nAdditional help can be found in the online docs for IO Tools.\n\nAny valid string path is acceptable. The string could be a URL. Valid URL\nschemes include http, ftp, s3, gs, and file. For file URLs, a host is\nexpected. A local file could be: file://localhost/path/to/table.csv.\n\nIf you want to pass in a path object, pandas accepts any `os.PathLike`.\n\nBy file-like object, we refer to objects with a `read()` method, such as a\nfile handle (e.g. via builtin `open` function) or `StringIO`.\n\nDelimiter to use. If sep is None, the C engine cannot automatically detect the\nseparator, but the Python parsing engine can, meaning the latter will be used\nand automatically detect the separator by Python\u2019s builtin sniffer tool,\n`csv.Sniffer`. In addition, separators longer than 1 character and different\nfrom `'\\s+'` will be interpreted as regular expressions and will also force\nthe use of the Python parsing engine. Note that regex delimiters are prone to\nignoring quoted data. Regex example: `'\\r\\t'`.\n\nAlias for sep.\n\nRow number(s) to use as the column names, and the start of the data. Default\nbehavior is to infer the column names: if no names are passed the behavior is\nidentical to `header=0` and column names are inferred from the first line of\nthe file, if column names are passed explicitly then the behavior is identical\nto `header=None`. Explicitly pass `header=0` to be able to replace existing\nnames. The header can be a list of integers that specify row locations for a\nmulti-index on the columns e.g. [0,1,3]. Intervening rows that are not\nspecified will be skipped (e.g. 2 in this example is skipped). Note that this\nparameter ignores commented lines and empty lines if `skip_blank_lines=True`,\nso `header=0` denotes the first line of data rather than the first line of the\nfile.\n\nList of column names to use. If the file contains a header row, then you\nshould explicitly pass `header=0` to override the column names. Duplicates in\nthis list are not allowed.\n\nColumn(s) to use as the row labels of the `DataFrame`, either given as string\nname or column index. If a sequence of int / str is given, a MultiIndex is\nused.\n\nNote: `index_col=False` can be used to force pandas to not use the first\ncolumn as the index, e.g. when you have a malformed file with delimiters at\nthe end of each line.\n\nReturn a subset of the columns. If list-like, all elements must either be\npositional (i.e. integer indices into the document columns) or strings that\ncorrespond to column names provided either by the user in names or inferred\nfrom the document header row(s). If `names` are given, the document header\nrow(s) are not taken into account. For example, a valid list-like usecols\nparameter would be `[0, 1, 2]` or `['foo', 'bar', 'baz']`. Element order is\nignored, so `usecols=[0, 1]` is the same as `[1, 0]`. To instantiate a\nDataFrame from `data` with element order preserved use `pd.read_csv(data,\nusecols=['foo', 'bar'])[['foo', 'bar']]` for columns in `['foo', 'bar']` order\nor `pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]` for `['bar',\n'foo']` order.\n\nIf callable, the callable function will be evaluated against the column names,\nreturning names where the callable function evaluates to True. An example of a\nvalid callable argument would be `lambda x: x.upper() in ['AAA', 'BBB',\n'DDD']`. Using this parameter results in much faster parsing time and lower\nmemory usage.\n\nIf the parsed data only contains one column then return a Series.\n\nDeprecated since version 1.4.0: Append `.squeeze(\"columns\")` to the call to\n`read_table` to squeeze the data.\n\nPrefix to add to column numbers when no header, e.g. \u2018X\u2019 for X0, X1, \u2026\n\nDeprecated since version 1.4.0: Use a list comprehension on the DataFrame\u2019s\ncolumns after calling `read_csv`.\n\nDuplicate columns will be specified as \u2018X\u2019, \u2018X.1\u2019, \u2026\u2019X.N\u2019, rather than\n\u2018X\u2019\u2026\u2019X\u2019. Passing in False will cause data to be overwritten if there are\nduplicate names in the columns.\n\nData type for data or columns. E.g. {\u2018a\u2019: np.float64, \u2018b\u2019: np.int32, \u2018c\u2019:\n\u2018Int64\u2019} Use str or object together with suitable na_values settings to\npreserve and not interpret dtype. If converters are specified, they will be\napplied INSTEAD of dtype conversion.\n\nParser engine to use. The C and pyarrow engines are faster, while the python\nengine is currently more feature-complete. Multithreading is currently only\nsupported by the pyarrow engine.\n\nNew in version 1.4.0: The \u201cpyarrow\u201d engine was added as an experimental\nengine, and some features are unsupported, or may not work correctly, with\nthis engine.\n\nDict of functions for converting values in certain columns. Keys can either be\nintegers or column labels.\n\nValues to consider as True.\n\nValues to consider as False.\n\nSkip spaces after delimiter.\n\nLine numbers to skip (0-indexed) or number of lines to skip (int) at the start\nof the file.\n\nIf callable, the callable function will be evaluated against the row indices,\nreturning True if the row should be skipped and False otherwise. An example of\na valid callable argument would be `lambda x: x in [0, 2]`.\n\nNumber of lines at bottom of file to skip (Unsupported with engine=\u2019c\u2019).\n\nNumber of rows of file to read. Useful for reading pieces of large files.\n\nAdditional strings to recognize as NA/NaN. If dict passed, specific per-column\nNA values. By default the following values are interpreted as NaN: \u2018\u2019, \u2018#N/A\u2019,\n\u2018#N/A N/A\u2019, \u2018#NA\u2019, \u2018-1.#IND\u2019, \u2018-1.#QNAN\u2019, \u2018-NaN\u2019, \u2018-nan\u2019, \u20181.#IND\u2019, \u20181.#QNAN\u2019,\n\u2018<NA>\u2019, \u2018N/A\u2019, \u2018NA\u2019, \u2018NULL\u2019, \u2018NaN\u2019, \u2018n/a\u2019, \u2018nan\u2019, \u2018null\u2019.\n\nWhether or not to include the default NaN values when parsing the data.\nDepending on whether na_values is passed in, the behavior is as follows:\n\nIf keep_default_na is True, and na_values are specified, na_values is appended\nto the default NaN values used for parsing.\n\nIf keep_default_na is True, and na_values are not specified, only the default\nNaN values are used for parsing.\n\nIf keep_default_na is False, and na_values are specified, only the NaN values\nspecified na_values are used for parsing.\n\nIf keep_default_na is False, and na_values are not specified, no strings will\nbe parsed as NaN.\n\nNote that if na_filter is passed in as False, the keep_default_na and\nna_values parameters will be ignored.\n\nDetect missing value markers (empty strings and the value of na_values). In\ndata without any NAs, passing na_filter=False can improve the performance of\nreading a large file.\n\nIndicate number of NA values placed in non-numeric columns.\n\nIf True, skip over blank lines rather than interpreting as NaN values.\n\nThe behavior is as follows:\n\nboolean. If True -> try parsing the index.\n\nlist of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as\na separate date column.\n\nlist of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as a\nsingle date column.\n\ndict, e.g. {\u2018foo\u2019 : [1, 3]} -> parse columns 1, 3 as date and call result\n\u2018foo\u2019\n\nIf a column or index cannot be represented as an array of datetimes, say\nbecause of an unparsable value or a mixture of timezones, the column or index\nwill be returned unaltered as an object data type. For non-standard datetime\nparsing, use `pd.to_datetime` after `pd.read_csv`. To parse an index or column\nwith a mixture of timezones, specify `date_parser` to be a partially-applied\n`pandas.to_datetime()` with `utc=True`. See Parsing a CSV with mixed timezones\nfor more.\n\nNote: A fast-path exists for iso8601-formatted dates.\n\nIf True and parse_dates is enabled, pandas will attempt to infer the format of\nthe datetime strings in the columns, and if it can be inferred, switch to a\nfaster method of parsing them. In some cases this can increase the parsing\nspeed by 5-10x.\n\nIf True and parse_dates specifies combining multiple columns then keep the\noriginal columns.\n\nFunction to use for converting a sequence of string columns to an array of\ndatetime instances. The default uses `dateutil.parser.parser` to do the\nconversion. Pandas will try to call date_parser in three different ways,\nadvancing to the next if an exception occurs: 1) Pass one or more arrays (as\ndefined by parse_dates) as arguments; 2) concatenate (row-wise) the string\nvalues from the columns defined by parse_dates into a single array and pass\nthat; and 3) call date_parser once for each row using one or more strings\n(corresponding to the columns defined by parse_dates) as arguments.\n\nDD/MM format dates, international and European format.\n\nIf True, use a cache of unique, converted dates to apply the datetime\nconversion. May produce significant speed-up when parsing duplicate date\nstrings, especially ones with timezone offsets.\n\nNew in version 0.25.0.\n\nReturn TextFileReader object for iteration or getting chunks with\n`get_chunk()`.\n\nChanged in version 1.2: `TextFileReader` is a context manager.\n\nReturn TextFileReader object for iteration. See the IO Tools docs for more\ninformation on `iterator` and `chunksize`.\n\nChanged in version 1.2: `TextFileReader` is a context manager.\n\nFor on-the-fly decompression of on-disk data. If \u2018infer\u2019 and \u2018%s\u2019 is path-\nlike, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019,\n\u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). If using \u2018zip\u2019, the ZIP\nfile must contain only one data file to be read in. Set to `None` for no\ndecompression. Can also be a dict with key `'method'` set to one of {`'zip'`,\n`'gzip'`, `'bz2'`, `'zstd'`} and other key-value pairs are forwarded to\n`zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or\n`zstandard.ZstdDecompressor`, respectively. As an example, the following could\nbe passed for Zstandard decompression using a custom compression dictionary:\n`compression={'method': 'zstd', 'dict_data': my_compression_dict}`.\n\nChanged in version 1.4.0: Zstandard support.\n\nThousands separator.\n\nCharacter to recognize as decimal point (e.g. use \u2018,\u2019 for European data).\n\nCharacter to break file into lines. Only valid with C parser.\n\nThe character used to denote the start and end of a quoted item. Quoted items\ncan include the delimiter and it will be ignored.\n\nControl field quoting behavior per `csv.QUOTE_*` constants. Use one of\nQUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n\nWhen quotechar is specified and quoting is not `QUOTE_NONE`, indicate whether\nor not to interpret two consecutive quotechar elements INSIDE a field as a\nsingle `quotechar` element.\n\nOne-character string used to escape other characters.\n\nIndicates remainder of line should not be parsed. If found at the beginning of\na line, the line will be ignored altogether. This parameter must be a single\ncharacter. Like empty lines (as long as `skip_blank_lines=True`), fully\ncommented lines are ignored by the parameter header but not by skiprows. For\nexample, if `comment='#'`, parsing `#empty\\na,b,c\\n1,2,3` with `header=0` will\nresult in \u2018a,b,c\u2019 being treated as the header.\n\nEncoding to use for UTF when reading/writing (ex. \u2018utf-8\u2019). List of Python\nstandard encodings .\n\nChanged in version 1.2: When `encoding` is `None`, `errors=\"replace\"` is\npassed to `open()`. Otherwise, `errors=\"strict\"` is passed to `open()`. This\nbehavior was previously only the case for `engine=\"python\"`.\n\nChanged in version 1.3.0: `encoding_errors` is a new argument. `encoding` has\nno longer an influence on how encoding errors are handled.\n\nHow encoding errors are treated. List of possible values .\n\nNew in version 1.3.0.\n\nIf provided, this parameter will override values (default or not) for the\nfollowing parameters: delimiter, doublequote, escapechar, skipinitialspace,\nquotechar, and quoting. If it is necessary to override values, a ParserWarning\nwill be issued. See csv.Dialect documentation for more details.\n\nLines with too many fields (e.g. a csv line with too many commas) will by\ndefault cause an exception to be raised, and no DataFrame will be returned. If\nFalse, then these \u201cbad lines\u201d will be dropped from the DataFrame that is\nreturned.\n\nDeprecated since version 1.3.0: The `on_bad_lines` parameter should be used\ninstead to specify behavior upon encountering a bad line instead.\n\nIf error_bad_lines is False, and warn_bad_lines is True, a warning for each\n\u201cbad line\u201d will be output.\n\nDeprecated since version 1.3.0: The `on_bad_lines` parameter should be used\ninstead to specify behavior upon encountering a bad line instead.\n\nSpecifies what to do upon encountering a bad line (a line with too many\nfields). Allowed values are :\n\n\u2018error\u2019, raise an Exception when a bad line is encountered.\n\n\u2018warn\u2019, raise a warning when a bad line is encountered and skip that line.\n\n\u2018skip\u2019, skip bad lines without raising or warning when they are encountered.\n\nNew in version 1.3.0:\n\ncallable, function with signature `(bad_line: list[str]) -> list[str] | None`\nthat will process a single bad line. `bad_line` is a list of strings split by\nthe `sep`. If the function returns `None`, the bad line will be ignored. If\nthe function returns a new list of strings with more elements than expected, a\n``ParserWarning` will be emitted while dropping extra elements. Only supported\nwhen `engine=\"python\"`\n\nNew in version 1.4.0.\n\nSpecifies whether or not whitespace (e.g. `' '` or `' '`) will be used as the\nsep. Equivalent to setting `sep='\\s+'`. If this option is set to True, nothing\nshould be passed in for the `delimiter` parameter.\n\nInternally process the file in chunks, resulting in lower memory use while\nparsing, but possibly mixed type inference. To ensure no mixed types either\nset False, or specify the type with the dtype parameter. Note that the entire\nfile is read into a single DataFrame regardless, use the chunksize or iterator\nparameter to return the data in chunks. (Only valid with C parser).\n\nIf a filepath is provided for filepath_or_buffer, map the file object directly\nonto memory and access the data directly from there. Using this option can\nimprove performance because there is no longer any I/O overhead.\n\nSpecifies which converter the C engine should use for floating-point values.\nThe options are `None` or \u2018high\u2019 for the ordinary converter, \u2018legacy\u2019 for the\noriginal lower precision pandas converter, and \u2018round_trip\u2019 for the round-trip\nconverter.\n\nChanged in version 1.2.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.\n\nA comma-separated values (csv) file is returned as two-dimensional data\nstructure with labeled axes.\n\nSee also\n\nWrite DataFrame to a comma-separated values (csv) file.\n\nRead a comma-separated values (csv) file into DataFrame.\n\nRead a table of fixed-width formatted lines into DataFrame.\n\nExamples\n\n"}, {"name": "pandas.read_xml", "path": "reference/api/pandas.read_xml", "type": "Input/output", "text": "\nRead XML document into a `DataFrame` object.\n\nNew in version 1.3.0.\n\nString, path object (implementing `os.PathLike[str]`), or file-like object\nimplementing a `read()` function. The string can be any valid XML string or a\npath. The string can further be a URL. Valid URL schemes include http, ftp,\ns3, and file.\n\nThe XPath to parse required set of nodes for migration to DataFrame. XPath\nshould return a collection of elements and not a single element. Note: The\n`etree` parser supports limited XPath expressions. For more complex XPath, use\n`lxml` which requires installation.\n\nThe namespaces defined in XML document as dicts with key being namespace\nprefix and value the URI. There is no need to include all namespaces in XML,\nonly the ones used in `xpath` expression. Note: if XML document uses default\nnamespace denoted as xmlns=\u2019<URI>\u2019 without a prefix, you must assign any\ntemporary namespace prefix such as \u2018doc\u2019 to the URI in order to parse\nunderlying nodes and/or attributes. For example,\n\nParse only the child elements at the specified `xpath`. By default, all child\nelements and non-empty text nodes are returned.\n\nParse only the attributes at the specified `xpath`. By default, all attributes\nare returned.\n\nColumn names for DataFrame of parsed XML data. Use this parameter to rename\noriginal element names and distinguish same named elements.\n\nEncoding of XML document.\n\nParser module to use for retrieval of data. Only \u2018lxml\u2019 and \u2018etree\u2019 are\nsupported. With \u2018lxml\u2019 more complex XPath searches and ability to use XSLT\nstylesheet are supported.\n\nA URL, file-like object, or a raw string containing an XSLT script. This\nstylesheet should flatten complex, deeply nested XML documents for easier\nparsing. To use this feature you must have `lxml` module installed and specify\n\u2018lxml\u2019 as `parser`. The `xpath` must reference nodes of transformed XML\ndocument generated after XSLT transformation and not the original XML\ndocument. Only XSLT 1.0 scripts and not later versions is currently supported.\n\nFor on-the-fly decompression of on-disk data. If \u2018infer\u2019 and \u2018path_or_buffer\u2019\nis path-like, then detect compression from the following extensions: \u2018.gz\u2019,\n\u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). If using \u2018zip\u2019,\nthe ZIP file must contain only one data file to be read in. Set to `None` for\nno decompression. Can also be a dict with key `'method'` set to one of\n{`'zip'`, `'gzip'`, `'bz2'`, `'zstd'`} and other key-value pairs are forwarded\nto `zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or\n`zstandard.ZstdDecompressor`, respectively. As an example, the following could\nbe passed for Zstandard decompression using a custom compression dictionary:\n`compression={'method': 'zstd', 'dict_data': my_compression_dict}`.\n\nChanged in version 1.4.0: Zstandard support.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nA DataFrame.\n\nSee also\n\nConvert a JSON string to pandas object.\n\nRead HTML tables into a list of DataFrame objects.\n\nNotes\n\nThis method is best designed to import shallow XML documents in following\nformat which is the ideal fit for the two-dimensions of a `DataFrame` (row by\ncolumn).\n\nAs a file format, XML documents can be designed any way including layout of\nelements and attributes as long as it conforms to W3C specifications.\nTherefore, this method is a convenience handler for a specific flatter design\nand not all possible XML structures.\n\nHowever, for more complex XML documents, `stylesheet` allows you to\ntemporarily redesign original document with XSLT (a special purpose language)\nfor a flatter version for migration to a DataFrame.\n\nThis function will always return a single `DataFrame` or raise exceptions due\nto issues with XML document, `xpath`, or other parameters.\n\nExamples\n\n"}, {"name": "pandas.reset_option", "path": "reference/api/pandas.reset_option", "type": "General utility functions", "text": "\nReset one or more options to their default value.\n\nPass \u201call\u201d as argument to reset all options.\n\nAvailable options:\n\ncompute.[use_bottleneck, use_numba, use_numexpr]\n\ndisplay.[chop_threshold, colheader_justify, column_space, date_dayfirst,\ndate_yearfirst, encoding, expand_frame_repr, float_format]\n\ndisplay.html.[border, table_schema, use_mathjax]\n\ndisplay.[large_repr]\n\ndisplay.latex.[escape, longtable, multicolumn, multicolumn_format, multirow,\nrepr]\n\ndisplay.[max_categories, max_columns, max_colwidth, max_dir_items,\nmax_info_columns, max_info_rows, max_rows, max_seq_items, memory_usage,\nmin_rows, multi_sparse, notebook_repr_html, pprint_nest_depth, precision,\nshow_dimensions]\n\ndisplay.unicode.[ambiguous_as_wide, east_asian_width]\n\ndisplay.[width]\n\nio.excel.ods.[reader, writer]\n\nio.excel.xls.[reader, writer]\n\nio.excel.xlsb.[reader]\n\nio.excel.xlsm.[reader, writer]\n\nio.excel.xlsx.[reader, writer]\n\nio.hdf.[default_format, dropna_table]\n\nio.parquet.[engine]\n\nio.sql.[engine]\n\nmode.[chained_assignment, data_manager, sim_interactive, string_storage,\nuse_inf_as_na, use_inf_as_null]\n\nplotting.[backend]\n\nplotting.matplotlib.[register_converters]\n\nstyler.format.[decimal, escape, formatter, na_rep, precision, thousands]\n\nstyler.html.[mathjax]\n\nstyler.latex.[environment, hrules, multicol_align, multirow_align]\n\nstyler.render.[encoding, max_columns, max_elements, max_rows, repr]\n\nstyler.sparse.[columns, index]\n\nIf specified only options matching prefix* will be reset. Note: partial\nmatches are supported for convenience, but unless you use the full option name\n(e.g. x.y.z.option_name), your code may break in future versions if new\noptions with similar names are introduced.\n\nNotes\n\nThe available options with its descriptions:\n\nUse the bottleneck library to accelerate if it is installed, the default is\nTrue Valid values: False,True [default: True] [currently: True]\n\nUse the numba engine option for select operations if it is installed, the\ndefault is False Valid values: False,True [default: False] [currently: False]\n\nUse the numexpr library to accelerate computation if it is installed, the\ndefault is True Valid values: False,True [default: True] [currently: True]\n\nif set to a float value, all float values smaller then the given threshold\nwill be displayed as exactly 0 by repr and friends. [default: None]\n[currently: None]\n\nControls the justification of column headers. used by DataFrameFormatter.\n[default: right] [currently: right]\n\n[default: 12] [currently: 12]\n\nWhen True, prints and parses dates with the day first, eg 20/01/2005 [default:\nFalse] [currently: False]\n\nWhen True, prints and parses dates with the year first, eg 2005/01/20\n[default: False] [currently: False]\n\nDefaults to the detected encoding of the console. Specifies the encoding to be\nused for strings returned by to_string, these are generally strings meant to\nbe displayed on the console. [default: utf-8] [currently: utf-8]\n\nWhether to print out the full DataFrame repr for wide DataFrames across\nmultiple lines, max_columns is still respected, but the output will wrap-\naround across multiple \u201cpages\u201d if its width exceeds display.width. [default:\nTrue] [currently: True]\n\nThe callable should accept a floating point number and return a string with\nthe desired format of the number. This is used in some places like\nSeriesFormatter. See formats.format.EngFormatter for an example. [default:\nNone] [currently: None]\n\nA `border=value` attribute is inserted in the `<table>` tag for the DataFrame\nHTML repr. [default: 1] [currently: 1]\n\nWhether to publish a Table Schema representation for frontends that support\nit. (default: False) [default: False] [currently: False]\n\nWhen True, Jupyter notebook will process table contents using MathJax,\nrendering mathematical expressions enclosed by the dollar symbol. (default:\nTrue) [default: True] [currently: True]\n\nFor DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show\na truncated table (the default from 0.13), or switch to the view from\ndf.info() (the behaviour in earlier versions of pandas). [default: truncate]\n[currently: truncate]\n\nThis specifies if the to_latex method of a Dataframe uses escapes special\ncharacters. Valid values: False,True [default: True] [currently: True]\n\nThis specifies if the to_latex method of a Dataframe uses the longtable\nformat. Valid values: False,True [default: False] [currently: False]\n\nThis specifies if the to_latex method of a Dataframe uses multicolumns to\npretty-print MultiIndex columns. Valid values: False,True [default: True]\n[currently: True]\n\nThis specifies if the to_latex method of a Dataframe uses multicolumns to\npretty-print MultiIndex columns. Valid values: False,True [default: l]\n[currently: l]\n\nThis specifies if the to_latex method of a Dataframe uses multirows to pretty-\nprint MultiIndex rows. Valid values: False,True [default: False] [currently:\nFalse]\n\nWhether to produce a latex DataFrame representation for jupyter environments\nthat support it. (default: False) [default: False] [currently: False]\n\nThis sets the maximum number of categories pandas should output when printing\nout a Categorical or a Series of dtype \u201ccategory\u201d. [default: 8] [currently: 8]\n\nIf max_cols is exceeded, switch to truncate view. Depending on large_repr,\nobjects are either centrally truncated or printed as a summary view. \u2018None\u2019\nvalue means unlimited.\n\nIn case python/IPython is running in a terminal and large_repr equals\n\u2018truncate\u2019 this can be set to 0 and pandas will auto-detect the width of the\nterminal and print a truncated object which fits the screen width. The IPython\nnotebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is\nnot possible to do correct auto-detection. [default: 0] [currently: 0]\n\nThe maximum width in characters of a column in the repr of a pandas data\nstructure. When the column overflows, a \u201c\u2026\u201d placeholder is embedded in the\noutput. A \u2018None\u2019 value means unlimited. [default: 50] [currently: 50]\n\nThe number of items that will be added to dir(\u2026). \u2018None\u2019 value means\nunlimited. Because dir is cached, changing this option will not immediately\naffect already existing dataframes until a column is deleted or added.\n\nThis is for instance used to suggest columns from a dataframe to tab\ncompletion. [default: 100] [currently: 100]\n\nmax_info_columns is used in DataFrame.info method to decide if per column\ninformation will be printed. [default: 100] [currently: 100]\n\ndf.info() will usually show null-counts for each column. For large frames this\ncan be quite slow. max_info_rows and max_info_cols limit this null check only\nto frames with smaller dimensions than specified. [default: 1690785]\n[currently: 1690785]\n\nIf max_rows is exceeded, switch to truncate view. Depending on large_repr,\nobjects are either centrally truncated or printed as a summary view. \u2018None\u2019\nvalue means unlimited.\n\nIn case python/IPython is running in a terminal and large_repr equals\n\u2018truncate\u2019 this can be set to 0 and pandas will auto-detect the height of the\nterminal and print a truncated object which fits the screen height. The\nIPython notebook, IPython qtconsole, or IDLE do not run in a terminal and\nhence it is not possible to do correct auto-detection. [default: 60]\n[currently: 60]\n\nWhen pretty-printing a long sequence, no more then max_seq_items will be\nprinted. If items are omitted, they will be denoted by the addition of \u201c\u2026\u201d to\nthe resulting string.\n\nIf set to None, the number of items to be printed is unlimited. [default: 100]\n[currently: 100]\n\nThis specifies if the memory usage of a DataFrame should be displayed when\ndf.info() is called. Valid values True,False,\u2019deep\u2019 [default: True]\n[currently: True]\n\nThe numbers of rows to show in a truncated view (when max_rows is exceeded).\nIgnored when max_rows is set to None or 0. When set to None, follows the value\nof max_rows. [default: 10] [currently: 10]\n\n\u201csparsify\u201d MultiIndex display (don\u2019t display repeated elements in outer levels\nwithin groups) [default: True] [currently: True]\n\nWhen True, IPython notebook will use html representation for pandas objects\n(if it is available). [default: True] [currently: True]\n\nControls the number of nested levels to process when pretty-printing [default:\n3] [currently: 3]\n\nFloating point output precision in terms of number of places after the\ndecimal, for regular formatting as well as scientific notation. Similar to\n`precision` in `numpy.set_printoptions()`. [default: 6] [currently: 6]\n\nWhether to print out dimensions at the end of DataFrame repr. If \u2018truncate\u2019 is\nspecified, only print out the dimensions if the frame is truncated (e.g. not\ndisplay all rows and/or columns) [default: truncate] [currently: truncate]\n\nWhether to use the Unicode East Asian Width to calculate the display text\nwidth. Enabling this may affect to the performance (default: False) [default:\nFalse] [currently: False]\n\nWhether to use the Unicode East Asian Width to calculate the display text\nwidth. Enabling this may affect to the performance (default: False) [default:\nFalse] [currently: False]\n\nWidth of the display in characters. In case python/IPython is running in a\nterminal this can be set to None and pandas will correctly auto-detect the\nwidth. Note that the IPython notebook, IPython qtconsole, or IDLE do not run\nin a terminal and hence it is not possible to correctly detect the width.\n[default: 80] [currently: 80]\n\nThe default Excel reader engine for \u2018ods\u2019 files. Available options: auto, odf.\n[default: auto] [currently: auto]\n\nThe default Excel writer engine for \u2018ods\u2019 files. Available options: auto, odf.\n[default: auto] [currently: auto]\n\nThe default Excel reader engine for \u2018xls\u2019 files. Available options: auto,\nxlrd. [default: auto] [currently: auto]\n\nThe default Excel writer engine for \u2018xls\u2019 files. Available options: auto,\nxlwt. [default: auto] [currently: auto] (Deprecated, use `` instead.)\n\nThe default Excel reader engine for \u2018xlsb\u2019 files. Available options: auto,\npyxlsb. [default: auto] [currently: auto]\n\nThe default Excel reader engine for \u2018xlsm\u2019 files. Available options: auto,\nxlrd, openpyxl. [default: auto] [currently: auto]\n\nThe default Excel writer engine for \u2018xlsm\u2019 files. Available options: auto,\nopenpyxl. [default: auto] [currently: auto]\n\nThe default Excel reader engine for \u2018xlsx\u2019 files. Available options: auto,\nxlrd, openpyxl. [default: auto] [currently: auto]\n\nThe default Excel writer engine for \u2018xlsx\u2019 files. Available options: auto,\nopenpyxl, xlsxwriter. [default: auto] [currently: auto]\n\ndefault format writing format, if None, then put will default to \u2018fixed\u2019 and\nappend will default to \u2018table\u2019 [default: None] [currently: None]\n\ndrop ALL nan rows when appending to a table [default: False] [currently:\nFalse]\n\nThe default parquet reader/writer engine. Available options: \u2018auto\u2019,\n\u2018pyarrow\u2019, \u2018fastparquet\u2019, the default is \u2018auto\u2019 [default: auto] [currently:\nauto]\n\nThe default sql reader/writer engine. Available options: \u2018auto\u2019, \u2018sqlalchemy\u2019,\nthe default is \u2018auto\u2019 [default: auto] [currently: auto]\n\nRaise an exception, warn, or no action if trying to use chained assignment,\nThe default is warn [default: warn] [currently: warn]\n\nInternal data manager type; can be \u201cblock\u201d or \u201carray\u201d. Defaults to \u201cblock\u201d,\nunless overridden by the \u2018PANDAS_DATA_MANAGER\u2019 environment variable (needs to\nbe set before pandas is imported). [default: block] [currently: block]\n\nWhether to simulate interactive mode for purposes of testing [default: False]\n[currently: False]\n\nThe default storage for StringDtype. [default: python] [currently: python]\n\nTrue means treat None, NaN, INF, -INF as NA (old way), False means None and\nNaN are null, but INF, -INF are not NA (new way). [default: False] [currently:\nFalse]\n\nuse_inf_as_null had been deprecated and will be removed in a future version.\nUse use_inf_as_na instead. [default: False] [currently: False] (Deprecated,\nuse mode.use_inf_as_na instead.)\n\nThe plotting backend to use. The default value is \u201cmatplotlib\u201d, the backend\nprovided with pandas. Other backends can be specified by providing the name of\nthe module that implements the backend. [default: matplotlib] [currently:\nmatplotlib]\n\nWhether to register converters with matplotlib\u2019s units registry for dates,\ntimes, datetimes, and Periods. Toggling to False will remove the converters,\nrestoring any converters that pandas overwrote. [default: auto] [currently:\nauto]\n\nThe character representation for the decimal separator for floats and complex.\n[default: .] [currently: .]\n\nWhether to escape certain characters according to the given context; html or\nlatex. [default: None] [currently: None]\n\nA formatter object to be used as default within `Styler.format`. [default:\nNone] [currently: None]\n\nThe string representation for values identified as missing. [default: None]\n[currently: None]\n\nThe precision for floats and complex numbers. [default: 6] [currently: 6]\n\nThe character representation for thousands separator for floats, int and\ncomplex. [default: None] [currently: None]\n\nIf False will render special CSS classes to table attributes that indicate\nMathjax will not be used in Jupyter Notebook. [default: True] [currently:\nTrue]\n\nThe environment to replace `\\begin{table}`. If \u201clongtable\u201d is used results in\na specific longtable environment format. [default: None] [currently: None]\n\nWhether to add horizontal rules on top and bottom and below the headers.\n[default: False] [currently: False]\n\nThe specifier for horizontal alignment of sparsified LaTeX multicolumns. Pipe\ndecorators can also be added to non-naive values to draw vertical rules, e.g.\n\u201c|r\u201d will draw a rule on the left side of right aligned merged cells.\n[default: r] [currently: r]\n\nThe specifier for vertical alignment of sparsified LaTeX multirows. [default:\nc] [currently: c]\n\nThe encoding used for output HTML and LaTeX files. [default: utf-8]\n[currently: utf-8]\n\nThe maximum number of columns that will be rendered. May still be reduced to\nsatsify `max_elements`, which takes precedence. [default: None] [currently:\nNone]\n\nThe maximum number of data-cell (<td>) elements that will be rendered before\ntrimming will occur over columns, rows or both if needed. [default: 262144]\n[currently: 262144]\n\nThe maximum number of rows that will be rendered. May still be reduced to\nsatsify `max_elements`, which takes precedence. [default: None] [currently:\nNone]\n\nDetermine which output to use in Jupyter Notebook in {\u201chtml\u201d, \u201clatex\u201d}.\n[default: html] [currently: html]\n\nWhether to sparsify the display of hierarchical columns. Setting to False will\ndisplay each explicit level element in a hierarchical key for each column.\n[default: True] [currently: True]\n\nWhether to sparsify the display of a hierarchical index. Setting to False will\ndisplay each explicit level element in a hierarchical key for each row.\n[default: True] [currently: True]\n\n"}, {"name": "pandas.Series", "path": "reference/api/pandas.series", "type": "Series", "text": "\nOne-dimensional ndarray with axis labels (including time series).\n\nLabels need not be unique but must be a hashable type. The object supports\nboth integer- and label-based indexing and provides a host of methods for\nperforming operations involving the index. Statistical methods from ndarray\nhave been overridden to automatically exclude missing data (currently\nrepresented as NaN).\n\nOperations between Series (+, -, /, *, **) align values based on their\nassociated index values\u2013 they need not be the same length. The result index\nwill be the sorted union of the two indexes.\n\nContains data stored in Series. If data is a dict, argument order is\nmaintained.\n\nValues must be hashable and have the same length as data. Non-unique index\nvalues are allowed. Will default to RangeIndex (0, 1, 2, \u2026, n) if not\nprovided. If data is dict-like and index is None, then the keys in the data\nare used as the index. If the index is not None, the resulting Series is\nreindexed with the index values.\n\nData type for the output Series. If not specified, this will be inferred from\ndata. See the user guide for more usages.\n\nThe name to give to the Series.\n\nCopy input data. Only affects Series or 1d ndarray input. See examples.\n\nExamples\n\nConstructing Series from a dictionary with an Index specified\n\nThe keys of the dictionary match with the Index values, hence the Index values\nhave no effect.\n\nNote that the Index is first build with the keys from the dictionary. After\nthis the Series is reindexed with the given Index values, hence we get all NaN\nas a result.\n\nConstructing Series from a list with copy=False.\n\nDue to input data type the Series has a copy of the original data even though\ncopy=False, so the data is unchanged.\n\nConstructing Series from a 1d ndarray with copy=False.\n\nDue to input data type the Series has a view on the original data, so the data\nis changed as well.\n\nAttributes\n\n`T`\n\nReturn the transpose, which is by definition self.\n\n`array`\n\nThe ExtensionArray of the data backing this Series or Index.\n\n`at`\n\nAccess a single value for a row/column label pair.\n\n`attrs`\n\nDictionary of global attributes of this dataset.\n\n`axes`\n\nReturn a list of the row axis labels.\n\n`dtype`\n\nReturn the dtype object of the underlying data.\n\n`dtypes`\n\nReturn the dtype object of the underlying data.\n\n`flags`\n\nGet the properties associated with this pandas object.\n\n`hasnans`\n\nReturn True if there are any NaNs.\n\n`iat`\n\nAccess a single value for a row/column pair by integer position.\n\n`iloc`\n\nPurely integer-location based indexing for selection by position.\n\n`index`\n\nThe index (axis labels) of the Series.\n\n`is_monotonic`\n\nReturn boolean if values in the object are monotonic_increasing.\n\n`is_monotonic_decreasing`\n\nReturn boolean if values in the object are monotonic_decreasing.\n\n`is_monotonic_increasing`\n\nAlias for is_monotonic.\n\n`is_unique`\n\nReturn boolean if values in the object are unique.\n\n`loc`\n\nAccess a group of rows and columns by label(s) or a boolean array.\n\n`name`\n\nReturn the name of the Series.\n\n`nbytes`\n\nReturn the number of bytes in the underlying data.\n\n`ndim`\n\nNumber of dimensions of the underlying data, by definition 1.\n\n`shape`\n\nReturn a tuple of the shape of the underlying data.\n\n`size`\n\nReturn the number of elements in the underlying data.\n\n`values`\n\nReturn Series as ndarray or ndarray-like depending on the dtype.\n\nempty\n\nMethods\n\n`abs`()\n\nReturn a Series/DataFrame with absolute numeric value of each element.\n\n`add`(other[, level, fill_value, axis])\n\nReturn Addition of series and other, element-wise (binary operator add).\n\n`add_prefix`(prefix)\n\nPrefix labels with string prefix.\n\n`add_suffix`(suffix)\n\nSuffix labels with string suffix.\n\n`agg`([func, axis])\n\nAggregate using one or more operations over the specified axis.\n\n`aggregate`([func, axis])\n\nAggregate using one or more operations over the specified axis.\n\n`align`(other[, join, axis, level, copy, ...])\n\nAlign two objects on their axes with the specified join method.\n\n`all`([axis, bool_only, skipna, level])\n\nReturn whether all elements are True, potentially over an axis.\n\n`any`([axis, bool_only, skipna, level])\n\nReturn whether any element is True, potentially over an axis.\n\n`append`(to_append[, ignore_index, ...])\n\nConcatenate two or more Series.\n\n`apply`(func[, convert_dtype, args])\n\nInvoke function on values of Series.\n\n`argmax`([axis, skipna])\n\nReturn int position of the largest value in the Series.\n\n`argmin`([axis, skipna])\n\nReturn int position of the smallest value in the Series.\n\n`argsort`([axis, kind, order])\n\nReturn the integer indices that would sort the Series values.\n\n`asfreq`(freq[, method, how, normalize, ...])\n\nConvert time series to specified frequency.\n\n`asof`(where[, subset])\n\nReturn the last row(s) without any NaNs before where.\n\n`astype`(dtype[, copy, errors])\n\nCast a pandas object to a specified dtype `dtype`.\n\n`at_time`(time[, asof, axis])\n\nSelect values at particular time of day (e.g., 9:30AM).\n\n`autocorr`([lag])\n\nCompute the lag-N autocorrelation.\n\n`backfill`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\n`between`(left, right[, inclusive])\n\nReturn boolean Series equivalent to left <= series <= right.\n\n`between_time`(start_time, end_time[, ...])\n\nSelect values between particular times of the day (e.g., 9:00-9:30 AM).\n\n`bfill`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\n`bool`()\n\nReturn the bool of a single element Series or DataFrame.\n\n`cat`\n\nalias of `pandas.core.arrays.categorical.CategoricalAccessor`\n\n`clip`([lower, upper, axis, inplace])\n\nTrim values at input threshold(s).\n\n`combine`(other, func[, fill_value])\n\nCombine the Series with a Series or scalar according to func.\n\n`combine_first`(other)\n\nUpdate null elements with value in the same location in 'other'.\n\n`compare`(other[, align_axis, keep_shape, ...])\n\nCompare to another Series and show the differences.\n\n`convert_dtypes`([infer_objects, ...])\n\nConvert columns to best possible dtypes using dtypes supporting `pd.NA`.\n\n`copy`([deep])\n\nMake a copy of this object's indices and data.\n\n`corr`(other[, method, min_periods])\n\nCompute correlation with other Series, excluding missing values.\n\n`count`([level])\n\nReturn number of non-NA/null observations in the Series.\n\n`cov`(other[, min_periods, ddof])\n\nCompute covariance with Series, excluding missing values.\n\n`cummax`([axis, skipna])\n\nReturn cumulative maximum over a DataFrame or Series axis.\n\n`cummin`([axis, skipna])\n\nReturn cumulative minimum over a DataFrame or Series axis.\n\n`cumprod`([axis, skipna])\n\nReturn cumulative product over a DataFrame or Series axis.\n\n`cumsum`([axis, skipna])\n\nReturn cumulative sum over a DataFrame or Series axis.\n\n`describe`([percentiles, include, exclude, ...])\n\nGenerate descriptive statistics.\n\n`diff`([periods])\n\nFirst discrete difference of element.\n\n`div`(other[, level, fill_value, axis])\n\nReturn Floating division of series and other, element-wise (binary operator\ntruediv).\n\n`divide`(other[, level, fill_value, axis])\n\nReturn Floating division of series and other, element-wise (binary operator\ntruediv).\n\n`divmod`(other[, level, fill_value, axis])\n\nReturn Integer division and modulo of series and other, element-wise (binary\noperator divmod).\n\n`dot`(other)\n\nCompute the dot product between the Series and the columns of other.\n\n`drop`([labels, axis, index, columns, level, ...])\n\nReturn Series with specified index labels removed.\n\n`drop_duplicates`([keep, inplace])\n\nReturn Series with duplicate values removed.\n\n`droplevel`(level[, axis])\n\nReturn Series/DataFrame with requested index / column level(s) removed.\n\n`dropna`([axis, inplace, how])\n\nReturn a new Series with missing values removed.\n\n`dt`\n\nalias of `pandas.core.indexes.accessors.CombinedDatetimelikeProperties`\n\n`duplicated`([keep])\n\nIndicate duplicate Series values.\n\n`eq`(other[, level, fill_value, axis])\n\nReturn Equal to of series and other, element-wise (binary operator eq).\n\n`equals`(other)\n\nTest whether two objects contain the same elements.\n\n`ewm`([com, span, halflife, alpha, ...])\n\nProvide exponentially weighted (EW) calculations.\n\n`expanding`([min_periods, center, axis, method])\n\nProvide expanding window calculations.\n\n`explode`([ignore_index])\n\nTransform each element of a list-like to a row.\n\n`factorize`([sort, na_sentinel])\n\nEncode the object as an enumerated type or categorical variable.\n\n`ffill`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\n`fillna`([value, method, axis, inplace, ...])\n\nFill NA/NaN values using the specified method.\n\n`filter`([items, like, regex, axis])\n\nSubset the dataframe rows or columns according to the specified index labels.\n\n`first`(offset)\n\nSelect initial periods of time series data based on a date offset.\n\n`first_valid_index`()\n\nReturn index for first non-NA value or None, if no NA value is found.\n\n`floordiv`(other[, level, fill_value, axis])\n\nReturn Integer division of series and other, element-wise (binary operator\nfloordiv).\n\n`ge`(other[, level, fill_value, axis])\n\nReturn Greater than or equal to of series and other, element-wise (binary\noperator ge).\n\n`get`(key[, default])\n\nGet item from object for given key (ex: DataFrame column).\n\n`groupby`([by, axis, level, as_index, sort, ...])\n\nGroup Series using a mapper or by a Series of columns.\n\n`gt`(other[, level, fill_value, axis])\n\nReturn Greater than of series and other, element-wise (binary operator gt).\n\n`head`([n])\n\nReturn the first n rows.\n\n`hist`([by, ax, grid, xlabelsize, xrot, ...])\n\nDraw histogram of the input series using matplotlib.\n\n`idxmax`([axis, skipna])\n\nReturn the row label of the maximum value.\n\n`idxmin`([axis, skipna])\n\nReturn the row label of the minimum value.\n\n`infer_objects`()\n\nAttempt to infer better dtypes for object columns.\n\n`info`([verbose, buf, max_cols, memory_usage, ...])\n\nPrint a concise summary of a Series.\n\n`interpolate`([method, axis, limit, inplace, ...])\n\nFill NaN values using an interpolation method.\n\n`isin`(values)\n\nWhether elements in Series are contained in values.\n\n`isna`()\n\nDetect missing values.\n\n`isnull`()\n\nSeries.isnull is an alias for Series.isna.\n\n`item`()\n\nReturn the first element of the underlying data as a Python scalar.\n\n`items`()\n\nLazily iterate over (index, value) tuples.\n\n`iteritems`()\n\nLazily iterate over (index, value) tuples.\n\n`keys`()\n\nReturn alias for index.\n\n`kurt`([axis, skipna, level, numeric_only])\n\nReturn unbiased kurtosis over requested axis.\n\n`kurtosis`([axis, skipna, level, numeric_only])\n\nReturn unbiased kurtosis over requested axis.\n\n`last`(offset)\n\nSelect final periods of time series data based on a date offset.\n\n`last_valid_index`()\n\nReturn index for last non-NA value or None, if no NA value is found.\n\n`le`(other[, level, fill_value, axis])\n\nReturn Less than or equal to of series and other, element-wise (binary\noperator le).\n\n`lt`(other[, level, fill_value, axis])\n\nReturn Less than of series and other, element-wise (binary operator lt).\n\n`mad`([axis, skipna, level])\n\nReturn the mean absolute deviation of the values over the requested axis.\n\n`map`(arg[, na_action])\n\nMap values of Series according to an input mapping or function.\n\n`mask`(cond[, other, inplace, axis, level, ...])\n\nReplace values where the condition is True.\n\n`max`([axis, skipna, level, numeric_only])\n\nReturn the maximum of the values over the requested axis.\n\n`mean`([axis, skipna, level, numeric_only])\n\nReturn the mean of the values over the requested axis.\n\n`median`([axis, skipna, level, numeric_only])\n\nReturn the median of the values over the requested axis.\n\n`memory_usage`([index, deep])\n\nReturn the memory usage of the Series.\n\n`min`([axis, skipna, level, numeric_only])\n\nReturn the minimum of the values over the requested axis.\n\n`mod`(other[, level, fill_value, axis])\n\nReturn Modulo of series and other, element-wise (binary operator mod).\n\n`mode`([dropna])\n\nReturn the mode(s) of the Series.\n\n`mul`(other[, level, fill_value, axis])\n\nReturn Multiplication of series and other, element-wise (binary operator mul).\n\n`multiply`(other[, level, fill_value, axis])\n\nReturn Multiplication of series and other, element-wise (binary operator mul).\n\n`ne`(other[, level, fill_value, axis])\n\nReturn Not equal to of series and other, element-wise (binary operator ne).\n\n`nlargest`([n, keep])\n\nReturn the largest n elements.\n\n`notna`()\n\nDetect existing (non-missing) values.\n\n`notnull`()\n\nSeries.notnull is an alias for Series.notna.\n\n`nsmallest`([n, keep])\n\nReturn the smallest n elements.\n\n`nunique`([dropna])\n\nReturn number of unique elements in the object.\n\n`pad`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\n`pct_change`([periods, fill_method, limit, freq])\n\nPercentage change between the current and a prior element.\n\n`pipe`(func, *args, **kwargs)\n\nApply chainable functions that expect Series or DataFrames.\n\n`plot`\n\nalias of `pandas.plotting._core.PlotAccessor`\n\n`pop`(item)\n\nReturn item and drops from series.\n\n`pow`(other[, level, fill_value, axis])\n\nReturn Exponential power of series and other, element-wise (binary operator\npow).\n\n`prod`([axis, skipna, level, numeric_only, ...])\n\nReturn the product of the values over the requested axis.\n\n`product`([axis, skipna, level, numeric_only, ...])\n\nReturn the product of the values over the requested axis.\n\n`quantile`([q, interpolation])\n\nReturn value at the given quantile.\n\n`radd`(other[, level, fill_value, axis])\n\nReturn Addition of series and other, element-wise (binary operator radd).\n\n`rank`([axis, method, numeric_only, ...])\n\nCompute numerical data ranks (1 through n) along axis.\n\n`ravel`([order])\n\nReturn the flattened underlying data as an ndarray.\n\n`rdiv`(other[, level, fill_value, axis])\n\nReturn Floating division of series and other, element-wise (binary operator\nrtruediv).\n\n`rdivmod`(other[, level, fill_value, axis])\n\nReturn Integer division and modulo of series and other, element-wise (binary\noperator rdivmod).\n\n`reindex`(*args, **kwargs)\n\nConform Series to new index with optional filling logic.\n\n`reindex_like`(other[, method, copy, limit, ...])\n\nReturn an object with matching indices as other object.\n\n`rename`([index, axis, copy, inplace, level, ...])\n\nAlter Series index labels or name.\n\n`rename_axis`([mapper, index, columns, axis, ...])\n\nSet the name of the axis for the index or columns.\n\n`reorder_levels`(order)\n\nRearrange index levels using input order.\n\n`repeat`(repeats[, axis])\n\nRepeat elements of a Series.\n\n`replace`([to_replace, value, inplace, limit, ...])\n\nReplace values given in to_replace with value.\n\n`resample`(rule[, axis, closed, label, ...])\n\nResample time-series data.\n\n`reset_index`([level, drop, name, inplace])\n\nGenerate a new DataFrame or Series with the index reset.\n\n`rfloordiv`(other[, level, fill_value, axis])\n\nReturn Integer division of series and other, element-wise (binary operator\nrfloordiv).\n\n`rmod`(other[, level, fill_value, axis])\n\nReturn Modulo of series and other, element-wise (binary operator rmod).\n\n`rmul`(other[, level, fill_value, axis])\n\nReturn Multiplication of series and other, element-wise (binary operator\nrmul).\n\n`rolling`(window[, min_periods, center, ...])\n\nProvide rolling window calculations.\n\n`round`([decimals])\n\nRound each value in a Series to the given number of decimals.\n\n`rpow`(other[, level, fill_value, axis])\n\nReturn Exponential power of series and other, element-wise (binary operator\nrpow).\n\n`rsub`(other[, level, fill_value, axis])\n\nReturn Subtraction of series and other, element-wise (binary operator rsub).\n\n`rtruediv`(other[, level, fill_value, axis])\n\nReturn Floating division of series and other, element-wise (binary operator\nrtruediv).\n\n`sample`([n, frac, replace, weights, ...])\n\nReturn a random sample of items from an axis of object.\n\n`searchsorted`(value[, side, sorter])\n\nFind indices where elements should be inserted to maintain order.\n\n`sem`([axis, skipna, level, ddof, numeric_only])\n\nReturn unbiased standard error of the mean over requested axis.\n\n`set_axis`(labels[, axis, inplace])\n\nAssign desired index to given axis.\n\n`set_flags`(*[, copy, allows_duplicate_labels])\n\nReturn a new object with updated flags.\n\n`shift`([periods, freq, axis, fill_value])\n\nShift index by desired number of periods with an optional time freq.\n\n`skew`([axis, skipna, level, numeric_only])\n\nReturn unbiased skew over requested axis.\n\n`slice_shift`([periods, axis])\n\n(DEPRECATED) Equivalent to shift without copying data.\n\n`sort_index`([axis, level, ascending, ...])\n\nSort Series by index labels.\n\n`sort_values`([axis, ascending, inplace, ...])\n\nSort by the values.\n\n`sparse`\n\nalias of `pandas.core.arrays.sparse.accessor.SparseAccessor`\n\n`squeeze`([axis])\n\nSqueeze 1 dimensional axis objects into scalars.\n\n`std`([axis, skipna, level, ddof, numeric_only])\n\nReturn sample standard deviation over requested axis.\n\n`str`\n\nalias of `pandas.core.strings.accessor.StringMethods`\n\n`sub`(other[, level, fill_value, axis])\n\nReturn Subtraction of series and other, element-wise (binary operator sub).\n\n`subtract`(other[, level, fill_value, axis])\n\nReturn Subtraction of series and other, element-wise (binary operator sub).\n\n`sum`([axis, skipna, level, numeric_only, ...])\n\nReturn the sum of the values over the requested axis.\n\n`swapaxes`(axis1, axis2[, copy])\n\nInterchange axes and swap values axes appropriately.\n\n`swaplevel`([i, j, copy])\n\nSwap levels i and j in a `MultiIndex`.\n\n`tail`([n])\n\nReturn the last n rows.\n\n`take`(indices[, axis, is_copy])\n\nReturn the elements in the given positional indices along an axis.\n\n`to_clipboard`([excel, sep])\n\nCopy object to the system clipboard.\n\n`to_csv`([path_or_buf, sep, na_rep, ...])\n\nWrite object to a comma-separated values (csv) file.\n\n`to_dict`([into])\n\nConvert Series to {label -> value} dict or dict-like object.\n\n`to_excel`(excel_writer[, sheet_name, na_rep, ...])\n\nWrite object to an Excel sheet.\n\n`to_frame`([name])\n\nConvert Series to DataFrame.\n\n`to_hdf`(path_or_buf, key[, mode, complevel, ...])\n\nWrite the contained data to an HDF5 file using HDFStore.\n\n`to_json`([path_or_buf, orient, date_format, ...])\n\nConvert the object to a JSON string.\n\n`to_latex`([buf, columns, col_space, header, ...])\n\nRender object to a LaTeX tabular, longtable, or nested table.\n\n`to_list`()\n\nReturn a list of the values.\n\n`to_markdown`([buf, mode, index, storage_options])\n\nPrint Series in Markdown-friendly format.\n\n`to_numpy`([dtype, copy, na_value])\n\nA NumPy ndarray representing the values in this Series or Index.\n\n`to_period`([freq, copy])\n\nConvert Series from DatetimeIndex to PeriodIndex.\n\n`to_pickle`(path[, compression, protocol, ...])\n\nPickle (serialize) object to file.\n\n`to_sql`(name, con[, schema, if_exists, ...])\n\nWrite records stored in a DataFrame to a SQL database.\n\n`to_string`([buf, na_rep, float_format, ...])\n\nRender a string representation of the Series.\n\n`to_timestamp`([freq, how, copy])\n\nCast to DatetimeIndex of Timestamps, at beginning of period.\n\n`to_xarray`()\n\nReturn an xarray object from the pandas object.\n\n`tolist`()\n\nReturn a list of the values.\n\n`transform`(func[, axis])\n\nCall `func` on self producing a Series with the same axis shape as self.\n\n`transpose`(*args, **kwargs)\n\nReturn the transpose, which is by definition self.\n\n`truediv`(other[, level, fill_value, axis])\n\nReturn Floating division of series and other, element-wise (binary operator\ntruediv).\n\n`truncate`([before, after, axis, copy])\n\nTruncate a Series or DataFrame before and after some index value.\n\n`tshift`([periods, freq, axis])\n\n(DEPRECATED) Shift the time index, using the index's frequency if available.\n\n`tz_convert`(tz[, axis, level, copy])\n\nConvert tz-aware axis to target time zone.\n\n`tz_localize`(tz[, axis, level, copy, ...])\n\nLocalize tz-naive index of a Series or DataFrame to target time zone.\n\n`unique`()\n\nReturn unique values of Series object.\n\n`unstack`([level, fill_value])\n\nUnstack, also known as pivot, Series with MultiIndex to produce DataFrame.\n\n`update`(other)\n\nModify Series in place using values from passed Series.\n\n`value_counts`([normalize, sort, ascending, ...])\n\nReturn a Series containing counts of unique values.\n\n`var`([axis, skipna, level, ddof, numeric_only])\n\nReturn unbiased variance over requested axis.\n\n`view`([dtype])\n\nCreate a new view of the Series.\n\n`where`(cond[, other, inplace, axis, level, ...])\n\nReplace values where the condition is False.\n\n`xs`(key[, axis, level, drop_level])\n\nReturn cross-section from the Series/DataFrame.\n\n"}, {"name": "pandas.Series.__array__", "path": "reference/api/pandas.series.__array__", "type": "Series", "text": "\nReturn the values as a NumPy array.\n\nUsers should not call this directly. Rather, it is invoked by `numpy.array()`\nand `numpy.asarray()`.\n\nThe dtype to use for the resulting NumPy array. By default, the dtype is\ninferred from the data.\n\nThe values in the series converted to a `numpy.ndarray` with the specified\ndtype.\n\nSee also\n\nCreate a new array from data.\n\nZero-copy view to the array backing the Series.\n\nSeries method for similar behavior.\n\nExamples\n\nFor timezone-aware data, the timezones may be retained with `dtype='object'`\n\nOr the values may be localized to UTC and the tzinfo discarded with\n`dtype='datetime64[ns]'`\n\n"}, {"name": "pandas.Series.__iter__", "path": "reference/api/pandas.series.__iter__", "type": "Series", "text": "\nReturn an iterator of the values.\n\nThese are each a scalar type, which is a Python scalar (for str, int, float)\nor a pandas scalar (for Timestamp/Timedelta/Interval/Period)\n\n"}, {"name": "pandas.Series.abs", "path": "reference/api/pandas.series.abs", "type": "Series", "text": "\nReturn a Series/DataFrame with absolute numeric value of each element.\n\nThis function only applies to elements that are all numeric.\n\nSeries/DataFrame containing the absolute value of each element.\n\nSee also\n\nCalculate the absolute value element-wise.\n\nNotes\n\nFor `complex` inputs, `1.2 + 1j`, the absolute value is \\\\(\\sqrt{ a^2 + b^2\n}\\\\).\n\nExamples\n\nAbsolute numeric values in a Series.\n\nAbsolute numeric values in a Series with complex numbers.\n\nAbsolute numeric values in a Series with a Timedelta element.\n\nSelect rows with data closest to certain value using argsort (from\nStackOverflow).\n\n"}, {"name": "pandas.Series.add", "path": "reference/api/pandas.series.add", "type": "Series", "text": "\nReturn Addition of series and other, element-wise (binary operator add).\n\nEquivalent to `series + other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nReverse of the Addition operator, see Python documentation for more details.\n\nExamples\n\n"}, {"name": "pandas.Series.add_prefix", "path": "reference/api/pandas.series.add_prefix", "type": "Series", "text": "\nPrefix labels with string prefix.\n\nFor Series, the row labels are prefixed. For DataFrame, the column labels are\nprefixed.\n\nThe string to add before each label.\n\nNew Series or DataFrame with updated labels.\n\nSee also\n\nSuffix row labels with string suffix.\n\nSuffix column labels with string suffix.\n\nExamples\n\n"}, {"name": "pandas.Series.add_suffix", "path": "reference/api/pandas.series.add_suffix", "type": "Series", "text": "\nSuffix labels with string suffix.\n\nFor Series, the row labels are suffixed. For DataFrame, the column labels are\nsuffixed.\n\nThe string to add after each label.\n\nNew Series or DataFrame with updated labels.\n\nSee also\n\nPrefix row labels with string prefix.\n\nPrefix column labels with string prefix.\n\nExamples\n\n"}, {"name": "pandas.Series.agg", "path": "reference/api/pandas.series.agg", "type": "Series", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a Series or when passed to Series.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nParameter needed for compatibility with DataFrame.\n\nPositional arguments to pass to func.\n\nKeyword arguments to pass to func.\n\nThe return can be:\n\nscalar : when Series.agg is called with single function\n\nSeries : when DataFrame.agg is called with a single function\n\nDataFrame : when DataFrame.agg is called with several functions\n\nReturn scalar, Series or DataFrame.\n\nSee also\n\nInvoke function on a Series.\n\nTransform function producing a Series with like indexes.\n\nNotes\n\nagg is an alias for aggregate. Use the alias.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nA passed user-defined-function will be passed a Series for evaluation.\n\nExamples\n\n"}, {"name": "pandas.Series.aggregate", "path": "reference/api/pandas.series.aggregate", "type": "Series", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a Series or when passed to Series.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nParameter needed for compatibility with DataFrame.\n\nPositional arguments to pass to func.\n\nKeyword arguments to pass to func.\n\nThe return can be:\n\nscalar : when Series.agg is called with single function\n\nSeries : when DataFrame.agg is called with a single function\n\nDataFrame : when DataFrame.agg is called with several functions\n\nReturn scalar, Series or DataFrame.\n\nSee also\n\nInvoke function on a Series.\n\nTransform function producing a Series with like indexes.\n\nNotes\n\nagg is an alias for aggregate. Use the alias.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nA passed user-defined-function will be passed a Series for evaluation.\n\nExamples\n\n"}, {"name": "pandas.Series.align", "path": "reference/api/pandas.series.align", "type": "Series", "text": "\nAlign two objects on their axes with the specified join method.\n\nJoin method is specified for each axis Index.\n\nAlign on index (0), columns (1), or both (None).\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nAlways returns new objects. If copy=False and no reindexing is required then\noriginal objects are returned.\n\nValue to use for missing values. Defaults to NaN, but can be any \u201ccompatible\u201d\nvalue.\n\nMethod to use for filling holes in reindexed Series:\n\npad / ffill: propagate last valid observation forward to next valid.\n\nbackfill / bfill: use NEXT valid observation to fill gap.\n\nIf method is specified, this is the maximum number of consecutive NaN values\nto forward/backward fill. In other words, if there is a gap with more than\nthis number of consecutive NaNs, it will only be partially filled. If method\nis not specified, this is the maximum number of entries along the entire axis\nwhere NaNs will be filled. Must be greater than 0 if not None.\n\nFilling axis, method and limit.\n\nBroadcast values along this axis, if aligning two objects of different\ndimensions.\n\nAligned objects.\n\nExamples\n\nAlign on columns:\n\nWe can also align on the index:\n\nFinally, the default axis=None will align on both index and columns:\n\n"}, {"name": "pandas.Series.all", "path": "reference/api/pandas.series.all", "type": "Series", "text": "\nReturn whether all elements are True, potentially over an axis.\n\nReturns True unless there at least one element within a series or along a\nDataframe axis that is False or equivalent (e.g. zero or empty).\n\nIndicate which axis or axes should be reduced.\n\n0 / \u2018index\u2019 : reduce the index, return a Series whose index is the original\ncolumn labels.\n\n1 / \u2018columns\u2019 : reduce the columns, return a Series whose index is the\noriginal index.\n\nNone : reduce all axes, return a scalar.\n\nInclude only boolean columns. If None, will attempt to use everything, then\nuse only boolean data. Not implemented for Series.\n\nExclude NA/null values. If the entire row/column is NA and skipna is True,\nthen the result will be True, as for an empty row/column. If skipna is False,\nthen NA are treated as True, because these are not equal to zero.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a scalar.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nIf level is specified, then, Series is returned; otherwise, scalar is\nreturned.\n\nSee also\n\nReturn True if all elements are True.\n\nReturn True if one (or more) elements are True.\n\nExamples\n\nSeries\n\nDataFrames\n\nCreate a dataframe from a dictionary.\n\nDefault behaviour checks if column-wise values all return True.\n\nSpecify `axis='columns'` to check if row-wise values all return True.\n\nOr `axis=None` for whether every value is True.\n\n"}, {"name": "pandas.Series.any", "path": "reference/api/pandas.series.any", "type": "Series", "text": "\nReturn whether any element is True, potentially over an axis.\n\nReturns False unless there is at least one element within a series or along a\nDataframe axis that is True or equivalent (e.g. non-zero or non-empty).\n\nIndicate which axis or axes should be reduced.\n\n0 / \u2018index\u2019 : reduce the index, return a Series whose index is the original\ncolumn labels.\n\n1 / \u2018columns\u2019 : reduce the columns, return a Series whose index is the\noriginal index.\n\nNone : reduce all axes, return a scalar.\n\nInclude only boolean columns. If None, will attempt to use everything, then\nuse only boolean data. Not implemented for Series.\n\nExclude NA/null values. If the entire row/column is NA and skipna is True,\nthen the result will be False, as for an empty row/column. If skipna is False,\nthen NA are treated as True, because these are not equal to zero.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a scalar.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nIf level is specified, then, Series is returned; otherwise, scalar is\nreturned.\n\nSee also\n\nNumpy version of this method.\n\nReturn whether any element is True.\n\nReturn whether all elements are True.\n\nReturn whether any element is True over requested axis.\n\nReturn whether all elements are True over requested axis.\n\nExamples\n\nSeries\n\nFor Series input, the output is a scalar indicating whether any element is\nTrue.\n\nDataFrame\n\nWhether each column contains at least one True element (the default).\n\nAggregating over the columns.\n\nAggregating over the entire DataFrame with `axis=None`.\n\nany for an empty DataFrame is an empty Series.\n\n"}, {"name": "pandas.Series.append", "path": "reference/api/pandas.series.append", "type": "Series", "text": "\nConcatenate two or more Series.\n\nSeries to append with self.\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nIf True, raise Exception on creating index with duplicates.\n\nConcatenated Series.\n\nSee also\n\nGeneral function to concatenate DataFrame or Series objects.\n\nNotes\n\nIteratively appending to a Series can be more computationally intensive than a\nsingle concatenate. A better solution is to append values to a list and then\nconcatenate the list with the original Series all at once.\n\nExamples\n\nWith ignore_index set to True:\n\nWith verify_integrity set to True:\n\n"}, {"name": "pandas.Series.apply", "path": "reference/api/pandas.series.apply", "type": "Series", "text": "\nInvoke function on values of Series.\n\nCan be ufunc (a NumPy function that applies to the entire Series) or a Python\nfunction that only works on single values.\n\nPython function or NumPy ufunc to apply.\n\nTry to find better dtype for elementwise function results. If False, leave as\ndtype=object. Note that the dtype is always preserved for some extension array\ndtypes, such as Categorical.\n\nPositional arguments passed to func after the series value.\n\nAdditional keyword arguments passed to func.\n\nIf func returns a Series object the result will be a DataFrame.\n\nSee also\n\nFor element-wise operations.\n\nOnly perform aggregating type operations.\n\nOnly perform transforming type operations.\n\nNotes\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nExamples\n\nCreate a series with typical summer temperatures for each city.\n\nSquare the values by defining a function and passing it as an argument to\n`apply()`.\n\nSquare the values by passing an anonymous function as an argument to\n`apply()`.\n\nDefine a custom function that needs additional positional arguments and pass\nthese additional arguments using the `args` keyword.\n\nDefine a custom function that takes keyword arguments and pass these arguments\nto `apply`.\n\nUse a function from the Numpy library.\n\n"}, {"name": "pandas.Series.argmax", "path": "reference/api/pandas.series.argmax", "type": "Series", "text": "\nReturn int position of the largest value in the Series.\n\nIf the maximum is achieved in multiple locations, the first row position is\nreturned.\n\nDummy argument for consistency with Series.\n\nExclude NA/null values when showing the result.\n\nAdditional arguments and keywords for compatibility with NumPy.\n\nRow position of the maximum value.\n\nSee also\n\nReturn position of the maximum value.\n\nReturn position of the minimum value.\n\nEquivalent method for numpy arrays.\n\nReturn index label of the maximum values.\n\nReturn index label of the minimum values.\n\nExamples\n\nConsider dataset containing cereal calories\n\nThe maximum cereal calories is the third element and the minimum cereal\ncalories is the first element, since series is zero-indexed.\n\n"}, {"name": "pandas.Series.argmin", "path": "reference/api/pandas.series.argmin", "type": "Series", "text": "\nReturn int position of the smallest value in the Series.\n\nIf the minimum is achieved in multiple locations, the first row position is\nreturned.\n\nDummy argument for consistency with Series.\n\nExclude NA/null values when showing the result.\n\nAdditional arguments and keywords for compatibility with NumPy.\n\nRow position of the minimum value.\n\nSee also\n\nReturn position of the minimum value.\n\nReturn position of the maximum value.\n\nEquivalent method for numpy arrays.\n\nReturn index label of the maximum values.\n\nReturn index label of the minimum values.\n\nExamples\n\nConsider dataset containing cereal calories\n\nThe maximum cereal calories is the third element and the minimum cereal\ncalories is the first element, since series is zero-indexed.\n\n"}, {"name": "pandas.Series.argsort", "path": "reference/api/pandas.series.argsort", "type": "Series", "text": "\nReturn the integer indices that would sort the Series values.\n\nOverride ndarray.argsort. Argsorts the value, omitting NA/null values, and\nplaces the result in the same locations as the non-NA values.\n\nHas no effect but is accepted for compatibility with numpy.\n\nChoice of sorting algorithm. See `numpy.sort()` for more information.\n\u2018mergesort\u2019 and \u2018stable\u2019 are the only stable algorithms.\n\nHas no effect but is accepted for compatibility with numpy.\n\nPositions of values within the sort order with -1 indicating nan values.\n\nSee also\n\nReturns the indices that would sort this array.\n\n"}, {"name": "pandas.Series.array", "path": "reference/api/pandas.series.array", "type": "Series", "text": "\nThe ExtensionArray of the data backing this Series or Index.\n\nAn ExtensionArray of the values stored within. For extension types, this is\nthe actual array. For NumPy native types, this is a thin (no copy) wrapper\naround `numpy.ndarray`.\n\n`.array` differs `.values` which may require converting the data to a\ndifferent form.\n\nSee also\n\nSimilar method that always returns a NumPy array.\n\nSimilar method that always returns a NumPy array.\n\nNotes\n\nThis table lays out the different array types for each extension dtype within\npandas.\n\ndtype\n\narray type\n\ncategory\n\nCategorical\n\nperiod\n\nPeriodArray\n\ninterval\n\nIntervalArray\n\nIntegerNA\n\nIntegerArray\n\nstring\n\nStringArray\n\nboolean\n\nBooleanArray\n\ndatetime64[ns, tz]\n\nDatetimeArray\n\nFor any 3rd-party extension types, the array type will be an ExtensionArray.\n\nFor all remaining dtypes `.array` will be a `arrays.NumpyExtensionArray`\nwrapping the actual ndarray stored within. If you absolutely need a NumPy\narray (possibly with copying / coercing data), then use `Series.to_numpy()`\ninstead.\n\nExamples\n\nFor regular NumPy types like int, and float, a PandasArray is returned.\n\nFor extension types, like Categorical, the actual ExtensionArray is returned\n\n"}, {"name": "pandas.Series.asfreq", "path": "reference/api/pandas.series.asfreq", "type": "Series", "text": "\nConvert time series to specified frequency.\n\nReturns the original data conformed to a new index with the specified\nfrequency.\n\nIf the index of this Series is a `PeriodIndex`, the new index is the result of\ntransforming the original index with `PeriodIndex.asfreq` (so the original\nindex will map one-to-one to the new index).\n\nOtherwise, the new index will be equivalent to `pd.date_range(start, end,\nfreq=freq)` where `start` and `end` are, respectively, the first and last\nentries in the original index (see `pandas.date_range()`). The values\ncorresponding to any timesteps in the new index which were not present in the\noriginal index will be null (`NaN`), unless a method for filling such unknowns\nis provided (see the `method` parameter below).\n\nThe `resample()` method is more appropriate if an operation on each group of\ntimesteps (such as an aggregate) is necessary to represent the data at the new\nfrequency.\n\nFrequency DateOffset or string.\n\nMethod to use for filling holes in reindexed Series (note this does not fill\nNaNs that already were present):\n\n\u2018pad\u2019 / \u2018ffill\u2019: propagate last valid observation forward to next valid\n\n\u2018backfill\u2019 / \u2018bfill\u2019: use NEXT valid observation to fill.\n\nFor PeriodIndex only (see PeriodIndex.asfreq).\n\nWhether to reset output index to midnight.\n\nValue to use for missing values, applied during upsampling (note this does not\nfill NaNs that already were present).\n\nSeries object reindexed to the specified frequency.\n\nSee also\n\nConform DataFrame to new index with optional filling logic.\n\nNotes\n\nTo learn more about the frequency strings, please see this link.\n\nExamples\n\nStart by creating a series with 4 one minute timestamps.\n\nUpsample the series into 30 second bins.\n\nUpsample again, providing a `fill value`.\n\nUpsample again, providing a `method`.\n\n"}, {"name": "pandas.Series.asof", "path": "reference/api/pandas.series.asof", "type": "Series", "text": "\nReturn the last row(s) without any NaNs before where.\n\nThe last row (for each element in where, if list) without any NaN is taken. In\ncase of a `DataFrame`, the last row without NaN considering only the subset of\ncolumns (if not None)\n\nIf there is no good value, NaN is returned for a Series or a Series of NaN\nvalues for a DataFrame\n\nDate(s) before which the last row(s) are returned.\n\nFor DataFrame, if not None, only use these columns to check for NaNs.\n\nThe return can be:\n\nscalar : when self is a Series and where is a scalar\n\nSeries: when self is a Series and where is an array-like, or when self is a\nDataFrame and where is a scalar\n\nDataFrame : when self is a DataFrame and where is an array-like\n\nReturn scalar, Series, or DataFrame.\n\nSee also\n\nPerform an asof merge. Similar to left join.\n\nNotes\n\nDates are assumed to be sorted. Raises if this is not the case.\n\nExamples\n\nA Series and a scalar where.\n\nFor a sequence where, a Series is returned. The first value is NaN, because\nthe first element of where is before the first index value.\n\nMissing values are not considered. The following is `2.0`, not NaN, even\nthough NaN is at the index location for `30`.\n\nTake all columns into consideration\n\nTake a single column into consideration\n\n"}, {"name": "pandas.Series.astype", "path": "reference/api/pandas.series.astype", "type": "Series", "text": "\nCast a pandas object to a specified dtype `dtype`.\n\nUse a numpy.dtype or Python type to cast entire pandas object to the same\ntype. Alternatively, use {col: dtype, \u2026}, where col is a column label and\ndtype is a numpy.dtype or Python type to cast one or more of the DataFrame\u2019s\ncolumns to column-specific types.\n\nReturn a copy when `copy=True` (be very careful setting `copy=False` as\nchanges to values then may propagate to other pandas objects).\n\nControl raising of exceptions on invalid data for provided dtype.\n\n`raise` : allow exceptions to be raised\n\n`ignore` : suppress exceptions. On error return original object.\n\nSee also\n\nConvert argument to datetime.\n\nConvert argument to timedelta.\n\nConvert argument to a numeric type.\n\nCast a numpy array to a specified type.\n\nNotes\n\nDeprecated since version 1.3.0: Using `astype` to convert from timezone-naive\ndtype to timezone-aware dtype is deprecated and will raise in a future\nversion. Use `Series.dt.tz_localize()` instead.\n\nExamples\n\nCreate a DataFrame:\n\nCast all columns to int32:\n\nCast col1 to int32 using a dictionary:\n\nCreate a series:\n\nConvert to categorical type:\n\nConvert to ordered categorical type with custom ordering:\n\nNote that using `copy=False` and changing data on a new pandas object may\npropagate changes:\n\nCreate a series of dates:\n\n"}, {"name": "pandas.Series.at", "path": "reference/api/pandas.series.at", "type": "Series", "text": "\nAccess a single value for a row/column label pair.\n\nSimilar to `loc`, in that both provide label-based lookups. Use `at` if you\nonly need to get or set a single value in a DataFrame or Series.\n\nIf \u2018label\u2019 does not exist in DataFrame.\n\nSee also\n\nAccess a single value for a row/column pair by integer position.\n\nAccess a group of rows and columns by label(s).\n\nAccess a single value using a label.\n\nExamples\n\nGet value at specified row/column pair\n\nSet value at specified row/column pair\n\nGet value within a Series\n\n"}, {"name": "pandas.Series.at_time", "path": "reference/api/pandas.series.at_time", "type": "Series", "text": "\nSelect values at particular time of day (e.g., 9:30AM).\n\nIf the index is not a `DatetimeIndex`\n\nSee also\n\nSelect values between particular times of the day.\n\nSelect initial periods of time series based on a date offset.\n\nSelect final periods of time series based on a date offset.\n\nGet just the index locations for values at particular time of the day.\n\nExamples\n\n"}, {"name": "pandas.Series.attrs", "path": "reference/api/pandas.series.attrs", "type": "Series", "text": "\nDictionary of global attributes of this dataset.\n\nWarning\n\nattrs is experimental and may change without warning.\n\nSee also\n\nGlobal flags applying to this object.\n\n"}, {"name": "pandas.Series.autocorr", "path": "reference/api/pandas.series.autocorr", "type": "Series", "text": "\nCompute the lag-N autocorrelation.\n\nThis method computes the Pearson correlation between the Series and its\nshifted self.\n\nNumber of lags to apply before performing autocorrelation.\n\nThe Pearson correlation between self and self.shift(lag).\n\nSee also\n\nCompute the correlation between two Series.\n\nShift index by desired number of periods.\n\nCompute pairwise correlation of columns.\n\nCompute pairwise correlation between rows or columns of two DataFrame objects.\n\nNotes\n\nIf the Pearson correlation is not well defined return \u2018NaN\u2019.\n\nExamples\n\nIf the Pearson correlation is not well defined, then \u2018NaN\u2019 is returned.\n\n"}, {"name": "pandas.Series.axes", "path": "reference/api/pandas.series.axes", "type": "Series", "text": "\nReturn a list of the row axis labels.\n\n"}, {"name": "pandas.Series.backfill", "path": "reference/api/pandas.series.backfill", "type": "Series", "text": "\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\nObject with missing values filled or None if `inplace=True`.\n\n"}, {"name": "pandas.Series.between", "path": "reference/api/pandas.series.between", "type": "Series", "text": "\nReturn boolean Series equivalent to left <= series <= right.\n\nThis function returns a boolean vector containing True wherever the\ncorresponding Series element is between the boundary values left and right. NA\nvalues are treated as False.\n\nLeft boundary.\n\nRight boundary.\n\nInclude boundaries. Whether to set each bound as closed or open.\n\nChanged in version 1.3.0.\n\nSeries representing whether each element is between left and right\n(inclusive).\n\nSee also\n\nGreater than of series and other.\n\nLess than of series and other.\n\nNotes\n\nThis function is equivalent to `(left <= ser) & (ser <= right)`\n\nExamples\n\nBoundary values are included by default:\n\nWith inclusive set to `\"neither\"` boundary values are excluded:\n\nleft and right can be any scalar value:\n\n"}, {"name": "pandas.Series.between_time", "path": "reference/api/pandas.series.between_time", "type": "Series", "text": "\nSelect values between particular times of the day (e.g., 9:00-9:30 AM).\n\nBy setting `start_time` to be later than `end_time`, you can get the times\nthat are not between the two times.\n\nInitial time as a time filter limit.\n\nEnd time as a time filter limit.\n\nWhether the start time needs to be included in the result.\n\nDeprecated since version 1.4.0: Arguments include_start and include_end have\nbeen deprecated to standardize boundary inputs. Use inclusive instead, to set\neach bound as closed or open.\n\nWhether the end time needs to be included in the result.\n\nDeprecated since version 1.4.0: Arguments include_start and include_end have\nbeen deprecated to standardize boundary inputs. Use inclusive instead, to set\neach bound as closed or open.\n\nInclude boundaries; whether to set each bound as closed or open.\n\nDetermine range time on index or columns value.\n\nData from the original object filtered to the specified dates range.\n\nIf the index is not a `DatetimeIndex`\n\nSee also\n\nSelect values at a particular time of the day.\n\nSelect initial periods of time series based on a date offset.\n\nSelect final periods of time series based on a date offset.\n\nGet just the index locations for values between particular times of the day.\n\nExamples\n\nYou get the times that are not between two times by setting `start_time` later\nthan `end_time`:\n\n"}, {"name": "pandas.Series.bfill", "path": "reference/api/pandas.series.bfill", "type": "Series", "text": "\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\nObject with missing values filled or None if `inplace=True`.\n\n"}, {"name": "pandas.Series.bool", "path": "reference/api/pandas.series.bool", "type": "Series", "text": "\nReturn the bool of a single element Series or DataFrame.\n\nThis must be a boolean scalar value, either True or False. It will raise a\nValueError if the Series or DataFrame does not have exactly 1 element, or that\nelement is not boolean (integer values 0 and 1 will also raise an exception).\n\nThe value in the Series or DataFrame.\n\nSee also\n\nChange the data type of a Series, including to boolean.\n\nChange the data type of a DataFrame, including to boolean.\n\nNumPy boolean data type, used by pandas for boolean values.\n\nExamples\n\nThe method will only work for single element objects with a boolean value:\n\n"}, {"name": "pandas.Series.cat", "path": "reference/api/pandas.series.cat", "type": "Series", "text": "\nAccessor object for categorical properties of the Series values.\n\nBe aware that assigning to categories is a inplace operation, while all\nmethods return new categorical data per default (but can be called with\ninplace=True).\n\nExamples\n\n"}, {"name": "pandas.Series.cat.add_categories", "path": "reference/api/pandas.series.cat.add_categories", "type": "Series", "text": "\nAdd new categories.\n\nnew_categories will be included at the last/highest place in the categories\nand will be unused directly after this call.\n\nThe new categories to be included.\n\nWhether or not to add the categories inplace or return a copy of this\ncategorical with added categories.\n\nDeprecated since version 1.3.0.\n\nCategorical with new categories added or None if `inplace=True`.\n\nIf the new categories include old categories or do not validate as categories\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\nExamples\n\n"}, {"name": "pandas.Series.cat.as_ordered", "path": "reference/api/pandas.series.cat.as_ordered", "type": "Series", "text": "\nSet the Categorical to be ordered.\n\nWhether or not to set the ordered attribute in-place or return a copy of this\ncategorical with ordered set to True.\n\nOrdered Categorical or None if `inplace=True`.\n\n"}, {"name": "pandas.Series.cat.as_unordered", "path": "reference/api/pandas.series.cat.as_unordered", "type": "Series", "text": "\nSet the Categorical to be unordered.\n\nWhether or not to set the ordered attribute in-place or return a copy of this\ncategorical with ordered set to False.\n\nUnordered Categorical or None if `inplace=True`.\n\n"}, {"name": "pandas.Series.cat.categories", "path": "reference/api/pandas.series.cat.categories", "type": "Series", "text": "\nThe categories of this categorical.\n\nSetting assigns new values to each category (effectively a rename of each\nindividual category).\n\nThe assigned value has to be a list-like object. All items must be unique and\nthe number of items in the new categories must be the same as the number of\nitems in the old categories.\n\nAssigning to categories is a inplace operation!\n\nIf the new categories do not validate as categories or if the number of new\ncategories is unequal the number of old categories\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\n"}, {"name": "pandas.Series.cat.codes", "path": "reference/api/pandas.series.cat.codes", "type": "Series", "text": "\nReturn Series of codes as well as the index.\n\n"}, {"name": "pandas.Series.cat.ordered", "path": "reference/api/pandas.series.cat.ordered", "type": "Series", "text": "\nWhether the categories have an ordered relationship.\n\n"}, {"name": "pandas.Series.cat.remove_categories", "path": "reference/api/pandas.series.cat.remove_categories", "type": "Series", "text": "\nRemove the specified categories.\n\nremovals must be included in the old categories. Values which were in the\nremoved categories will be set to NaN\n\nThe categories which should be removed.\n\nWhether or not to remove the categories inplace or return a copy of this\ncategorical with removed categories.\n\nDeprecated since version 1.3.0.\n\nCategorical with removed categories or None if `inplace=True`.\n\nIf the removals are not contained in the categories\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nAdd new categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\nExamples\n\n"}, {"name": "pandas.Series.cat.remove_unused_categories", "path": "reference/api/pandas.series.cat.remove_unused_categories", "type": "Series", "text": "\nRemove categories which are not used.\n\nWhether or not to drop unused categories inplace or return a copy of this\ncategorical with unused categories dropped.\n\nDeprecated since version 1.2.0.\n\nCategorical with unused categories dropped or None if `inplace=True`.\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nSet the categories to the specified ones.\n\nExamples\n\n"}, {"name": "pandas.Series.cat.rename_categories", "path": "reference/api/pandas.series.cat.rename_categories", "type": "Series", "text": "\nRename categories.\n\nNew categories which will replace old categories.\n\nlist-like: all items must be unique and the number of items in the new\ncategories must match the existing number of categories.\n\ndict-like: specifies a mapping from old categories to new. Categories not\ncontained in the mapping are passed through and extra categories in the\nmapping are ignored.\n\ncallable : a callable that is called on all items in the old categories and\nwhose return values comprise the new categories.\n\nWhether or not to rename the categories inplace or return a copy of this\ncategorical with renamed categories.\n\nDeprecated since version 1.3.0.\n\nCategorical with removed categories or None if `inplace=True`.\n\nIf new categories are list-like and do not have the same number of items than\nthe current categories or do not validate as categories\n\nSee also\n\nReorder categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\nExamples\n\nFor dict-like `new_categories`, extra keys are ignored and categories not in\nthe dictionary are passed through\n\nYou may also provide a callable to create the new categories\n\n"}, {"name": "pandas.Series.cat.reorder_categories", "path": "reference/api/pandas.series.cat.reorder_categories", "type": "Series", "text": "\nReorder categories as specified in new_categories.\n\nnew_categories need to include all old categories and no new category items.\n\nThe categories in new order.\n\nWhether or not the categorical is treated as a ordered categorical. If not\ngiven, do not change the ordered information.\n\nWhether or not to reorder the categories inplace or return a copy of this\ncategorical with reordered categories.\n\nDeprecated since version 1.3.0.\n\nCategorical with removed categories or None if `inplace=True`.\n\nIf the new categories do not contain all old category items or any new ones\n\nSee also\n\nRename categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\n"}, {"name": "pandas.Series.cat.set_categories", "path": "reference/api/pandas.series.cat.set_categories", "type": "Series", "text": "\nSet the categories to the specified new_categories.\n\nnew_categories can include new categories (which will result in unused\ncategories) or remove old categories (which results in values set to NaN). If\nrename==True, the categories will simple be renamed (less or more items than\nin old categories will result in values set to NaN or in unused categories\nrespectively).\n\nThis method can be used to perform more than one action of adding, removing,\nand reordering simultaneously and is therefore faster than performing the\nindividual steps via the more specialised methods.\n\nOn the other hand this methods does not do checks (e.g., whether the old\ncategories are included in the new categories on a reorder), which can result\nin surprising changes, for example when using special string dtypes, which\ndoes not considers a S1 string equal to a single char python string.\n\nThe categories in new order.\n\nWhether or not the categorical is treated as a ordered categorical. If not\ngiven, do not change the ordered information.\n\nWhether or not the new_categories should be considered as a rename of the old\ncategories or as reordered categories.\n\nWhether or not to reorder the categories in-place or return a copy of this\ncategorical with reordered categories.\n\nDeprecated since version 1.3.0.\n\nIf new_categories does not validate as categories\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\n"}, {"name": "pandas.Series.clip", "path": "reference/api/pandas.series.clip", "type": "Series", "text": "\nTrim values at input threshold(s).\n\nAssigns values outside boundary to boundary values. Thresholds can be singular\nvalues or array like, and in the latter case the clipping is performed\nelement-wise in the specified axis.\n\nMinimum threshold value. All values below this threshold will be set to it. A\nmissing threshold (e.g NA) will not clip the value.\n\nMaximum threshold value. All values above this threshold will be set to it. A\nmissing threshold (e.g NA) will not clip the value.\n\nAlign object with lower and upper along the given axis.\n\nWhether to perform the operation in place on the data.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith numpy.\n\nSame type as calling object with the values outside the clip boundaries\nreplaced or None if `inplace=True`.\n\nSee also\n\nTrim values at input threshold in series.\n\nTrim values at input threshold in dataframe.\n\nClip (limit) the values in an array.\n\nExamples\n\nClips per column using lower and upper thresholds:\n\nClips using specific lower and upper thresholds per column element:\n\nClips using specific lower threshold per column element, with missing values:\n\n"}, {"name": "pandas.Series.combine", "path": "reference/api/pandas.series.combine", "type": "Series", "text": "\nCombine the Series with a Series or scalar according to func.\n\nCombine the Series and other using func to perform elementwise selection for\ncombined Series. fill_value is assumed when value is missing at some index\nfrom one of the two objects being combined.\n\nThe value(s) to be combined with the Series.\n\nFunction that takes two scalars as inputs and returns an element.\n\nThe value to assume when an index is missing from one Series or the other. The\ndefault specifies to use the appropriate NaN value for the underlying dtype of\nthe Series.\n\nThe result of combining the Series with the other object.\n\nSee also\n\nCombine Series values, choosing the calling Series\u2019 values first.\n\nExamples\n\nConsider 2 Datasets `s1` and `s2` containing highest clocked speeds of\ndifferent birds.\n\nNow, to combine the two datasets and view the highest speeds of the birds\nacross the two datasets\n\nIn the previous example, the resulting value for duck is missing, because the\nmaximum of a NaN and a float is a NaN. So, in the example, we set\n`fill_value=0`, so the maximum value returned will be the value from some\ndataset.\n\n"}, {"name": "pandas.Series.combine_first", "path": "reference/api/pandas.series.combine_first", "type": "Series", "text": "\nUpdate null elements with value in the same location in \u2018other\u2019.\n\nCombine two Series objects by filling null values in one Series with non-null\nvalues from the other Series. Result index will be the union of the two\nindexes.\n\nThe value(s) to be used for filling null values.\n\nThe result of combining the provided Series with the other object.\n\nSee also\n\nPerform element-wise operation on two Series using a given function.\n\nExamples\n\nNull values still persist if the location of that null value does not exist in\nother\n\n"}, {"name": "pandas.Series.compare", "path": "reference/api/pandas.series.compare", "type": "Series", "text": "\nCompare to another Series and show the differences.\n\nNew in version 1.1.0.\n\nObject to compare with.\n\nDetermine which axis to align the comparison on.\n\nwith rows drawn alternately from self and other.\n\nwith columns drawn alternately from self and other.\n\nIf true, all rows and columns are kept. Otherwise, only the ones with\ndifferent values are kept.\n\nIf true, the result keeps values that are equal. Otherwise, equal values are\nshown as NaNs.\n\nIf axis is 0 or \u2018index\u2019 the result will be a Series. The resulting index will\nbe a MultiIndex with \u2018self\u2019 and \u2018other\u2019 stacked alternately at the inner\nlevel.\n\nIf axis is 1 or \u2018columns\u2019 the result will be a DataFrame. It will have two\ncolumns namely \u2018self\u2019 and \u2018other\u2019.\n\nSee also\n\nCompare with another DataFrame and show differences.\n\nNotes\n\nMatching NaNs will not appear as a difference.\n\nExamples\n\nAlign the differences on columns\n\nStack the differences on indices\n\nKeep all original rows\n\nKeep all original rows and also all original values\n\n"}, {"name": "pandas.Series.convert_dtypes", "path": "reference/api/pandas.series.convert_dtypes", "type": "General utility functions", "text": "\nConvert columns to best possible dtypes using dtypes supporting `pd.NA`.\n\nNew in version 1.0.0.\n\nWhether object dtypes should be converted to the best possible types.\n\nWhether object dtypes should be converted to `StringDtype()`.\n\nWhether, if possible, conversion can be done to integer extension types.\n\nWhether object dtypes should be converted to `BooleanDtypes()`.\n\nWhether, if possible, conversion can be done to floating extension types. If\nconvert_integer is also True, preference will be give to integer dtypes if the\nfloats can be faithfully casted to integers.\n\nNew in version 1.2.0.\n\nCopy of input object with new dtype.\n\nSee also\n\nInfer dtypes of objects.\n\nConvert argument to datetime.\n\nConvert argument to timedelta.\n\nConvert argument to a numeric type.\n\nNotes\n\nBy default, `convert_dtypes` will attempt to convert a Series (or each Series\nin a DataFrame) to dtypes that support `pd.NA`. By using the options\n`convert_string`, `convert_integer`, `convert_boolean` and `convert_boolean`,\nit is possible to turn off individual conversions to `StringDtype`, the\ninteger extension types, `BooleanDtype` or floating extension types,\nrespectively.\n\nFor object-dtyped columns, if `infer_objects` is `True`, use the inference\nrules as during normal Series/DataFrame construction. Then, if possible,\nconvert to `StringDtype`, `BooleanDtype` or an appropriate integer or floating\nextension type, otherwise leave as `object`.\n\nIf the dtype is integer, convert to an appropriate integer extension type.\n\nIf the dtype is numeric, and consists of all integers, convert to an\nappropriate integer extension type. Otherwise, convert to an appropriate\nfloating extension type.\n\nChanged in version 1.2: Starting with pandas 1.2, this method also converts\nfloat columns to the nullable floating extension type.\n\nIn the future, as new dtypes are added that support `pd.NA`, the results of\nthis method will change to support those new dtypes.\n\nExamples\n\nStart with a DataFrame with default dtypes.\n\nConvert the DataFrame to use best possible dtypes.\n\nStart with a Series of strings and missing data represented by `np.nan`.\n\nObtain a Series with dtype `StringDtype`.\n\n"}, {"name": "pandas.Series.copy", "path": "reference/api/pandas.series.copy", "type": "Series", "text": "\nMake a copy of this object\u2019s indices and data.\n\nWhen `deep=True` (default), a new object will be created with a copy of the\ncalling object\u2019s data and indices. Modifications to the data or indices of the\ncopy will not be reflected in the original object (see notes below).\n\nWhen `deep=False`, a new object will be created without copying the calling\nobject\u2019s data or index (only references to the data and index are copied). Any\nchanges to the data of the original will be reflected in the shallow copy (and\nvice versa).\n\nMake a deep copy, including a copy of the data and the indices. With\n`deep=False` neither the indices nor the data are copied.\n\nObject type matches caller.\n\nNotes\n\nWhen `deep=True`, data is copied but actual Python objects will not be copied\nrecursively, only the reference to the object. This is in contrast to\ncopy.deepcopy in the Standard Library, which recursively copies object data\n(see examples below).\n\nWhile `Index` objects are copied when `deep=True`, the underlying numpy array\nis not copied for performance reasons. Since `Index` is immutable, the\nunderlying data can be safely shared and a copy is not needed.\n\nExamples\n\nShallow copy versus default (deep) copy:\n\nShallow copy shares data and index with original.\n\nDeep copy has own copy of data and index.\n\nUpdates to the data shared by shallow copy and original is reflected in both;\ndeep copy remains unchanged.\n\nNote that when copying an object containing Python objects, a deep copy will\ncopy the data, but will not do so recursively. Updating a nested data object\nwill be reflected in the deep copy.\n\n"}, {"name": "pandas.Series.corr", "path": "reference/api/pandas.series.corr", "type": "Series", "text": "\nCompute correlation with other Series, excluding missing values.\n\nSeries with which to compute the correlation.\n\nMethod used to compute correlation:\n\npearson : Standard correlation coefficient\n\nkendall : Kendall Tau correlation coefficient\n\nspearman : Spearman rank correlation\n\ncallable: Callable with input two 1d ndarrays and returning a float.\n\nWarning\n\nNote that the returned matrix from corr will have 1 along the diagonals and\nwill be symmetric regardless of the callable\u2019s behavior.\n\nMinimum number of observations needed to have a valid result.\n\nCorrelation with other.\n\nSee also\n\nCompute pairwise correlation between columns.\n\nCompute pairwise correlation with another DataFrame or Series.\n\nExamples\n\n"}, {"name": "pandas.Series.count", "path": "reference/api/pandas.series.count", "type": "Series", "text": "\nReturn number of non-NA/null observations in the Series.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a smaller Series.\n\nNumber of non-null values in the Series.\n\nSee also\n\nCount non-NA cells for each column or row.\n\nExamples\n\n"}, {"name": "pandas.Series.cov", "path": "reference/api/pandas.series.cov", "type": "Series", "text": "\nCompute covariance with Series, excluding missing values.\n\nSeries with which to compute the covariance.\n\nMinimum number of observations needed to have a valid result.\n\nDelta degrees of freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nNew in version 1.1.0.\n\nCovariance between Series and other normalized by N-1 (unbiased estimator).\n\nSee also\n\nCompute pairwise covariance of columns.\n\nExamples\n\n"}, {"name": "pandas.Series.cummax", "path": "reference/api/pandas.series.cummax", "type": "Series", "text": "\nReturn cumulative maximum over a DataFrame or Series axis.\n\nReturns a DataFrame or Series of the same size containing the cumulative\nmaximum.\n\nThe index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nReturn cumulative maximum of scalar or Series.\n\nSee also\n\nSimilar functionality but ignores `NaN` values.\n\nReturn the maximum over Series axis.\n\nReturn cumulative maximum over Series axis.\n\nReturn cumulative minimum over Series axis.\n\nReturn cumulative sum over Series axis.\n\nReturn cumulative product over Series axis.\n\nExamples\n\nSeries\n\nBy default, NA values are ignored.\n\nTo include NA values in the operation, use `skipna=False`\n\nDataFrame\n\nBy default, iterates over rows and finds the maximum in each column. This is\nequivalent to `axis=None` or `axis='index'`.\n\nTo iterate over columns and find the maximum in each row, use `axis=1`\n\n"}, {"name": "pandas.Series.cummin", "path": "reference/api/pandas.series.cummin", "type": "Series", "text": "\nReturn cumulative minimum over a DataFrame or Series axis.\n\nReturns a DataFrame or Series of the same size containing the cumulative\nminimum.\n\nThe index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nReturn cumulative minimum of scalar or Series.\n\nSee also\n\nSimilar functionality but ignores `NaN` values.\n\nReturn the minimum over Series axis.\n\nReturn cumulative maximum over Series axis.\n\nReturn cumulative minimum over Series axis.\n\nReturn cumulative sum over Series axis.\n\nReturn cumulative product over Series axis.\n\nExamples\n\nSeries\n\nBy default, NA values are ignored.\n\nTo include NA values in the operation, use `skipna=False`\n\nDataFrame\n\nBy default, iterates over rows and finds the minimum in each column. This is\nequivalent to `axis=None` or `axis='index'`.\n\nTo iterate over columns and find the minimum in each row, use `axis=1`\n\n"}, {"name": "pandas.Series.cumprod", "path": "reference/api/pandas.series.cumprod", "type": "Series", "text": "\nReturn cumulative product over a DataFrame or Series axis.\n\nReturns a DataFrame or Series of the same size containing the cumulative\nproduct.\n\nThe index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nReturn cumulative product of scalar or Series.\n\nSee also\n\nSimilar functionality but ignores `NaN` values.\n\nReturn the product over Series axis.\n\nReturn cumulative maximum over Series axis.\n\nReturn cumulative minimum over Series axis.\n\nReturn cumulative sum over Series axis.\n\nReturn cumulative product over Series axis.\n\nExamples\n\nSeries\n\nBy default, NA values are ignored.\n\nTo include NA values in the operation, use `skipna=False`\n\nDataFrame\n\nBy default, iterates over rows and finds the product in each column. This is\nequivalent to `axis=None` or `axis='index'`.\n\nTo iterate over columns and find the product in each row, use `axis=1`\n\n"}, {"name": "pandas.Series.cumsum", "path": "reference/api/pandas.series.cumsum", "type": "Series", "text": "\nReturn cumulative sum over a DataFrame or Series axis.\n\nReturns a DataFrame or Series of the same size containing the cumulative sum.\n\nThe index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nReturn cumulative sum of scalar or Series.\n\nSee also\n\nSimilar functionality but ignores `NaN` values.\n\nReturn the sum over Series axis.\n\nReturn cumulative maximum over Series axis.\n\nReturn cumulative minimum over Series axis.\n\nReturn cumulative sum over Series axis.\n\nReturn cumulative product over Series axis.\n\nExamples\n\nSeries\n\nBy default, NA values are ignored.\n\nTo include NA values in the operation, use `skipna=False`\n\nDataFrame\n\nBy default, iterates over rows and finds the sum in each column. This is\nequivalent to `axis=None` or `axis='index'`.\n\nTo iterate over columns and find the sum in each row, use `axis=1`\n\n"}, {"name": "pandas.Series.describe", "path": "reference/api/pandas.series.describe", "type": "Series", "text": "\nGenerate descriptive statistics.\n\nDescriptive statistics include those that summarize the central tendency,\ndispersion and shape of a dataset\u2019s distribution, excluding `NaN` values.\n\nAnalyzes both numeric and object series, as well as `DataFrame` column sets of\nmixed data types. The output will vary depending on what is provided. Refer to\nthe notes below for more detail.\n\nThe percentiles to include in the output. All should fall between 0 and 1. The\ndefault is `[.25, .5, .75]`, which returns the 25th, 50th, and 75th\npercentiles.\n\nA white list of data types to include in the result. Ignored for `Series`.\nHere are the options:\n\n\u2018all\u2019 : All columns of the input will be included in the output.\n\nA list-like of dtypes : Limits the results to the provided data types. To\nlimit the result to numeric types submit `numpy.number`. To limit it instead\nto object columns submit the `numpy.object` data type. Strings can also be\nused in the style of `select_dtypes` (e.g. `df.describe(include=['O'])`). To\nselect pandas categorical columns, use `'category'`\n\nNone (default) : The result will include all numeric columns.\n\nA black list of data types to omit from the result. Ignored for `Series`. Here\nare the options:\n\nA list-like of dtypes : Excludes the provided data types from the result. To\nexclude numeric types submit `numpy.number`. To exclude object columns submit\nthe data type `numpy.object`. Strings can also be used in the style of\n`select_dtypes` (e.g. `df.describe(exclude=['O'])`). To exclude pandas\ncategorical columns, use `'category'`\n\nNone (default) : The result will exclude nothing.\n\nWhether to treat datetime dtypes as numeric. This affects statistics\ncalculated for the column. For DataFrame input, this also controls whether\ndatetime columns are included by default.\n\nNew in version 1.1.0.\n\nSummary statistics of the Series or Dataframe provided.\n\nSee also\n\nCount number of non-NA/null observations.\n\nMaximum of the values in the object.\n\nMinimum of the values in the object.\n\nMean of the values.\n\nStandard deviation of the observations.\n\nSubset of a DataFrame including/excluding columns based on their dtype.\n\nNotes\n\nFor numeric data, the result\u2019s index will include `count`, `mean`, `std`,\n`min`, `max` as well as lower, `50` and upper percentiles. By default the\nlower percentile is `25` and the upper percentile is `75`. The `50` percentile\nis the same as the median.\n\nFor object data (e.g. strings or timestamps), the result\u2019s index will include\n`count`, `unique`, `top`, and `freq`. The `top` is the most common value. The\n`freq` is the most common value\u2019s frequency. Timestamps also include the\n`first` and `last` items.\n\nIf multiple object values have the highest count, then the `count` and `top`\nresults will be arbitrarily chosen from among those with the highest count.\n\nFor mixed data types provided via a `DataFrame`, the default is to return only\nan analysis of numeric columns. If the dataframe consists only of object and\ncategorical data without any numeric columns, the default is to return an\nanalysis of both the object and categorical columns. If `include='all'` is\nprovided as an option, the result will include a union of attributes of each\ntype.\n\nThe include and exclude parameters can be used to limit which columns in a\n`DataFrame` are analyzed for the output. The parameters are ignored when\nanalyzing a `Series`.\n\nExamples\n\nDescribing a numeric `Series`.\n\nDescribing a categorical `Series`.\n\nDescribing a timestamp `Series`.\n\nDescribing a `DataFrame`. By default only numeric fields are returned.\n\nDescribing all columns of a `DataFrame` regardless of data type.\n\nDescribing a column from a `DataFrame` by accessing it as an attribute.\n\nIncluding only numeric columns in a `DataFrame` description.\n\nIncluding only string columns in a `DataFrame` description.\n\nIncluding only categorical columns from a `DataFrame` description.\n\nExcluding numeric columns from a `DataFrame` description.\n\nExcluding object columns from a `DataFrame` description.\n\n"}, {"name": "pandas.Series.diff", "path": "reference/api/pandas.series.diff", "type": "Series", "text": "\nFirst discrete difference of element.\n\nCalculates the difference of a Series element compared with another element in\nthe Series (default is element in previous row).\n\nPeriods to shift for calculating difference, accepts negative values.\n\nFirst differences of the Series.\n\nSee also\n\nPercent change over given number of periods.\n\nShift index by desired number of periods with an optional time freq.\n\nFirst discrete difference of object.\n\nNotes\n\nFor boolean dtypes, this uses `operator.xor()` rather than `operator.sub()`.\nThe result is calculated according to current dtype in Series, however dtype\nof the result is always float64.\n\nExamples\n\nDifference with previous row\n\nDifference with 3rd previous row\n\nDifference with following row\n\nOverflow in input dtype\n\n"}, {"name": "pandas.Series.div", "path": "reference/api/pandas.series.div", "type": "Series", "text": "\nReturn Floating division of series and other, element-wise (binary operator\ntruediv).\n\nEquivalent to `series / other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nReverse of the Floating division operator, see Python documentation for more\ndetails.\n\nExamples\n\n"}, {"name": "pandas.Series.divide", "path": "reference/api/pandas.series.divide", "type": "Series", "text": "\nReturn Floating division of series and other, element-wise (binary operator\ntruediv).\n\nEquivalent to `series / other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nReverse of the Floating division operator, see Python documentation for more\ndetails.\n\nExamples\n\n"}, {"name": "pandas.Series.divmod", "path": "reference/api/pandas.series.divmod", "type": "Series", "text": "\nReturn Integer division and modulo of series and other, element-wise (binary\noperator divmod).\n\nEquivalent to `divmod(series, other)`, but with support to substitute a\nfill_value for missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nReverse of the Integer division and modulo operator, see Python documentation\nfor more details.\n\nExamples\n\n"}, {"name": "pandas.Series.dot", "path": "reference/api/pandas.series.dot", "type": "Series", "text": "\nCompute the dot product between the Series and the columns of other.\n\nThis method computes the dot product between the Series and another one, or\nthe Series and each columns of a DataFrame, or the Series and each columns of\nan array.\n\nIt can also be called using self @ other in Python >= 3.5.\n\nThe other object to compute the dot product with its columns.\n\nReturn the dot product of the Series and other if other is a Series, the\nSeries of the dot product of Series and each rows of other if other is a\nDataFrame or a numpy.ndarray between the Series and each columns of the numpy\narray.\n\nSee also\n\nCompute the matrix product with the DataFrame.\n\nMultiplication of series and other, element-wise.\n\nNotes\n\nThe Series and other has to share the same index if other is a Series or a\nDataFrame.\n\nExamples\n\n"}, {"name": "pandas.Series.drop", "path": "reference/api/pandas.series.drop", "type": "Series", "text": "\nReturn Series with specified index labels removed.\n\nRemove elements of a Series based on specifying the index labels. When using a\nmulti-index, labels on different levels can be removed by specifying the\nlevel.\n\nIndex labels to drop.\n\nRedundant for application on Series.\n\nRedundant for application on Series, but \u2018index\u2019 can be used instead of\n\u2018labels\u2019.\n\nNo change is made to the Series; use \u2018index\u2019 or \u2018labels\u2019 instead.\n\nFor MultiIndex, level for which the labels will be removed.\n\nIf True, do operation inplace and return None.\n\nIf \u2018ignore\u2019, suppress error and only existing labels are dropped.\n\nSeries with specified index labels removed or None if `inplace=True`.\n\nIf none of the labels are found in the index.\n\nSee also\n\nReturn only specified index labels of Series.\n\nReturn series without null values.\n\nReturn Series with duplicate values removed.\n\nDrop specified labels from rows or columns.\n\nExamples\n\nDrop labels B en C\n\nDrop 2nd level label in MultiIndex Series\n\n"}, {"name": "pandas.Series.drop_duplicates", "path": "reference/api/pandas.series.drop_duplicates", "type": "Series", "text": "\nReturn Series with duplicate values removed.\n\nMethod to handle dropping duplicates:\n\n\u2018first\u2019 : Drop duplicates except for the first occurrence.\n\n\u2018last\u2019 : Drop duplicates except for the last occurrence.\n\n`False` : Drop all duplicates.\n\nIf `True`, performs operation inplace and returns None.\n\nSeries with duplicates dropped or None if `inplace=True`.\n\nSee also\n\nEquivalent method on Index.\n\nEquivalent method on DataFrame.\n\nRelated method on Series, indicating duplicate Series values.\n\nExamples\n\nGenerate a Series with duplicated entries.\n\nWith the \u2018keep\u2019 parameter, the selection behaviour of duplicated values can be\nchanged. The value \u2018first\u2019 keeps the first occurrence for each set of\nduplicated entries. The default value of keep is \u2018first\u2019.\n\nThe value \u2018last\u2019 for parameter \u2018keep\u2019 keeps the last occurrence for each set\nof duplicated entries.\n\nThe value `False` for parameter \u2018keep\u2019 discards all sets of duplicated\nentries. Setting the value of \u2018inplace\u2019 to `True` performs the operation\ninplace and returns `None`.\n\n"}, {"name": "pandas.Series.droplevel", "path": "reference/api/pandas.series.droplevel", "type": "Series", "text": "\nReturn Series/DataFrame with requested index / column level(s) removed.\n\nIf a string is given, must be the name of a level If list-like, elements must\nbe names or positional indexes of levels.\n\nAxis along which the level(s) is removed:\n\n0 or \u2018index\u2019: remove level(s) in column.\n\n1 or \u2018columns\u2019: remove level(s) in row.\n\nSeries/DataFrame with requested index / column level(s) removed.\n\nExamples\n\n"}, {"name": "pandas.Series.dropna", "path": "reference/api/pandas.series.dropna", "type": "Series", "text": "\nReturn a new Series with missing values removed.\n\nSee the User Guide for more on which values are considered missing, and how to\nwork with missing data.\n\nThere is only one axis to drop values from.\n\nIf True, do operation inplace and return None.\n\nNot in use. Kept for compatibility.\n\nSeries with NA entries dropped from it or None if `inplace=True`.\n\nSee also\n\nIndicate missing values.\n\nIndicate existing (non-missing) values.\n\nReplace missing values.\n\nDrop rows or columns which contain NA values.\n\nDrop missing indices.\n\nExamples\n\nDrop NA values from a Series.\n\nKeep the Series with valid entries in the same variable.\n\nEmpty strings are not considered NA values. `None` is considered an NA value.\n\n"}, {"name": "pandas.Series.dt", "path": "reference/api/pandas.series.dt", "type": "Series", "text": "\nAccessor object for datetimelike properties of the Series values.\n\nExamples\n\nReturns a Series indexed like the original Series. Raises TypeError if the\nSeries does not contain datetimelike values.\n\n"}, {"name": "pandas.Series.dt.ceil", "path": "reference/api/pandas.series.dt.ceil", "type": "Series", "text": "\nPerform ceil operation on the data to the specified freq.\n\nThe frequency level to ceil the index to. Must be a fixed frequency like \u2018S\u2019\n(second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible\nfreq values.\n\nOnly relevant for DatetimeIndex:\n\n\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order\n\nbool-ndarray where True signifies a DST time, False designates a non-DST time\n(note that this flag is only applicable for ambiguous times)\n\n\u2018NaT\u2019 will return NaT where there are ambiguous times\n\n\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST.\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times\n\ntimedelta objects will shift nonexistent times by the timedelta\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with\nthe same index for a Series.\n\nNotes\n\nIf the timestamps have a timezone, ceiling will take place relative to the\nlocal (\u201cwall\u201d) time and re-localized to the same timezone. When ceiling near\ndaylight savings time, use `nonexistent` and `ambiguous` to control the re-\nlocalization behavior.\n\nExamples\n\nDatetimeIndex\n\nSeries\n\nWhen rounding near a daylight savings time transition, use `ambiguous` or\n`nonexistent` to control how the timestamp should be re-localized.\n\n"}, {"name": "pandas.Series.dt.components", "path": "reference/api/pandas.series.dt.components", "type": "Series", "text": "\nReturn a Dataframe of the components of the Timedeltas.\n\nExamples\n\n"}, {"name": "pandas.Series.dt.date", "path": "reference/api/pandas.series.dt.date", "type": "Series", "text": "\nReturns numpy array of python `datetime.date` objects.\n\nNamely, the date part of Timestamps without time and timezone information.\n\n"}, {"name": "pandas.Series.dt.day", "path": "reference/api/pandas.series.dt.day", "type": "Series", "text": "\nThe day of the datetime.\n\nExamples\n\n"}, {"name": "pandas.Series.dt.day_name", "path": "reference/api/pandas.series.dt.day_name", "type": "Series", "text": "\nReturn the day names of the DateTimeIndex with specified locale.\n\nLocale determining the language in which to return the day name. Default is\nEnglish locale.\n\nIndex of day names.\n\nExamples\n\n"}, {"name": "pandas.Series.dt.day_of_week", "path": "reference/api/pandas.series.dt.day_of_week", "type": "Series", "text": "\nThe day of the week with Monday=0, Sunday=6.\n\nReturn the day of the week. It is assumed the week starts on Monday, which is\ndenoted by 0 and ends on Sunday which is denoted by 6. This method is\navailable on both Series with datetime values (using the dt accessor) or\nDatetimeIndex.\n\nContaining integers indicating the day number.\n\nSee also\n\nAlias.\n\nAlias.\n\nReturns the name of the day of the week.\n\nExamples\n\n"}, {"name": "pandas.Series.dt.day_of_year", "path": "reference/api/pandas.series.dt.day_of_year", "type": "Series", "text": "\nThe ordinal day of the year.\n\n"}, {"name": "pandas.Series.dt.dayofweek", "path": "reference/api/pandas.series.dt.dayofweek", "type": "Series", "text": "\nThe day of the week with Monday=0, Sunday=6.\n\nReturn the day of the week. It is assumed the week starts on Monday, which is\ndenoted by 0 and ends on Sunday which is denoted by 6. This method is\navailable on both Series with datetime values (using the dt accessor) or\nDatetimeIndex.\n\nContaining integers indicating the day number.\n\nSee also\n\nAlias.\n\nAlias.\n\nReturns the name of the day of the week.\n\nExamples\n\n"}, {"name": "pandas.Series.dt.dayofyear", "path": "reference/api/pandas.series.dt.dayofyear", "type": "Series", "text": "\nThe ordinal day of the year.\n\n"}, {"name": "pandas.Series.dt.days", "path": "reference/api/pandas.series.dt.days", "type": "Series", "text": "\nNumber of days for each element.\n\n"}, {"name": "pandas.Series.dt.days_in_month", "path": "reference/api/pandas.series.dt.days_in_month", "type": "Series", "text": "\nThe number of days in the month.\n\n"}, {"name": "pandas.Series.dt.daysinmonth", "path": "reference/api/pandas.series.dt.daysinmonth", "type": "Series", "text": "\nThe number of days in the month.\n\n"}, {"name": "pandas.Series.dt.end_time", "path": "reference/api/pandas.series.dt.end_time", "type": "Series", "text": "\n\n"}, {"name": "pandas.Series.dt.floor", "path": "reference/api/pandas.series.dt.floor", "type": "Series", "text": "\nPerform floor operation on the data to the specified freq.\n\nThe frequency level to floor the index to. Must be a fixed frequency like \u2018S\u2019\n(second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible\nfreq values.\n\nOnly relevant for DatetimeIndex:\n\n\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order\n\nbool-ndarray where True signifies a DST time, False designates a non-DST time\n(note that this flag is only applicable for ambiguous times)\n\n\u2018NaT\u2019 will return NaT where there are ambiguous times\n\n\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST.\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times\n\ntimedelta objects will shift nonexistent times by the timedelta\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with\nthe same index for a Series.\n\nNotes\n\nIf the timestamps have a timezone, flooring will take place relative to the\nlocal (\u201cwall\u201d) time and re-localized to the same timezone. When flooring near\ndaylight savings time, use `nonexistent` and `ambiguous` to control the re-\nlocalization behavior.\n\nExamples\n\nDatetimeIndex\n\nSeries\n\nWhen rounding near a daylight savings time transition, use `ambiguous` or\n`nonexistent` to control how the timestamp should be re-localized.\n\n"}, {"name": "pandas.Series.dt.freq", "path": "reference/api/pandas.series.dt.freq", "type": "Series", "text": "\n\n"}, {"name": "pandas.Series.dt.hour", "path": "reference/api/pandas.series.dt.hour", "type": "Series", "text": "\nThe hours of the datetime.\n\nExamples\n\n"}, {"name": "pandas.Series.dt.is_leap_year", "path": "reference/api/pandas.series.dt.is_leap_year", "type": "Series", "text": "\nBoolean indicator if the date belongs to a leap year.\n\nA leap year is a year, which has 366 days (instead of 365) including 29th of\nFebruary as an intercalary day. Leap years are years which are multiples of\nfour with the exception of years divisible by 100 but not by 400.\n\nBooleans indicating if dates belong to a leap year.\n\nExamples\n\nThis method is available on Series with datetime values under the `.dt`\naccessor, and directly on DatetimeIndex.\n\n"}, {"name": "pandas.Series.dt.is_month_end", "path": "reference/api/pandas.series.dt.is_month_end", "type": "Series", "text": "\nIndicates whether the date is the last day of the month.\n\nFor Series, returns a Series with boolean values. For DatetimeIndex, returns a\nboolean array.\n\nSee also\n\nReturn a boolean indicating whether the date is the first day of the month.\n\nReturn a boolean indicating whether the date is the last day of the month.\n\nExamples\n\nThis method is available on Series with datetime values under the `.dt`\naccessor, and directly on DatetimeIndex.\n\n"}, {"name": "pandas.Series.dt.is_month_start", "path": "reference/api/pandas.series.dt.is_month_start", "type": "Series", "text": "\nIndicates whether the date is the first day of the month.\n\nFor Series, returns a Series with boolean values. For DatetimeIndex, returns a\nboolean array.\n\nSee also\n\nReturn a boolean indicating whether the date is the first day of the month.\n\nReturn a boolean indicating whether the date is the last day of the month.\n\nExamples\n\nThis method is available on Series with datetime values under the `.dt`\naccessor, and directly on DatetimeIndex.\n\n"}, {"name": "pandas.Series.dt.is_quarter_end", "path": "reference/api/pandas.series.dt.is_quarter_end", "type": "Series", "text": "\nIndicator for whether the date is the last day of a quarter.\n\nThe same type as the original data with boolean values. Series will have the\nsame name and index. DatetimeIndex will have the same name.\n\nSee also\n\nReturn the quarter of the date.\n\nSimilar property indicating the quarter start.\n\nExamples\n\nThis method is available on Series with datetime values under the `.dt`\naccessor, and directly on DatetimeIndex.\n\n"}, {"name": "pandas.Series.dt.is_quarter_start", "path": "reference/api/pandas.series.dt.is_quarter_start", "type": "Series", "text": "\nIndicator for whether the date is the first day of a quarter.\n\nThe same type as the original data with boolean values. Series will have the\nsame name and index. DatetimeIndex will have the same name.\n\nSee also\n\nReturn the quarter of the date.\n\nSimilar property for indicating the quarter start.\n\nExamples\n\nThis method is available on Series with datetime values under the `.dt`\naccessor, and directly on DatetimeIndex.\n\n"}, {"name": "pandas.Series.dt.is_year_end", "path": "reference/api/pandas.series.dt.is_year_end", "type": "Series", "text": "\nIndicate whether the date is the last day of the year.\n\nThe same type as the original data with boolean values. Series will have the\nsame name and index. DatetimeIndex will have the same name.\n\nSee also\n\nSimilar property indicating the start of the year.\n\nExamples\n\nThis method is available on Series with datetime values under the `.dt`\naccessor, and directly on DatetimeIndex.\n\n"}, {"name": "pandas.Series.dt.is_year_start", "path": "reference/api/pandas.series.dt.is_year_start", "type": "Series", "text": "\nIndicate whether the date is the first day of a year.\n\nThe same type as the original data with boolean values. Series will have the\nsame name and index. DatetimeIndex will have the same name.\n\nSee also\n\nSimilar property indicating the last day of the year.\n\nExamples\n\nThis method is available on Series with datetime values under the `.dt`\naccessor, and directly on DatetimeIndex.\n\n"}, {"name": "pandas.Series.dt.microsecond", "path": "reference/api/pandas.series.dt.microsecond", "type": "Series", "text": "\nThe microseconds of the datetime.\n\nExamples\n\n"}, {"name": "pandas.Series.dt.microseconds", "path": "reference/api/pandas.series.dt.microseconds", "type": "Series", "text": "\nNumber of microseconds (>= 0 and less than 1 second) for each element.\n\n"}, {"name": "pandas.Series.dt.minute", "path": "reference/api/pandas.series.dt.minute", "type": "Series", "text": "\nThe minutes of the datetime.\n\nExamples\n\n"}, {"name": "pandas.Series.dt.month", "path": "reference/api/pandas.series.dt.month", "type": "Series", "text": "\nThe month as January=1, December=12.\n\nExamples\n\n"}, {"name": "pandas.Series.dt.month_name", "path": "reference/api/pandas.series.dt.month_name", "type": "Series", "text": "\nReturn the month names of the DateTimeIndex with specified locale.\n\nLocale determining the language in which to return the month name. Default is\nEnglish locale.\n\nIndex of month names.\n\nExamples\n\n"}, {"name": "pandas.Series.dt.nanosecond", "path": "reference/api/pandas.series.dt.nanosecond", "type": "Series", "text": "\nThe nanoseconds of the datetime.\n\nExamples\n\n"}, {"name": "pandas.Series.dt.nanoseconds", "path": "reference/api/pandas.series.dt.nanoseconds", "type": "Series", "text": "\nNumber of nanoseconds (>= 0 and less than 1 microsecond) for each element.\n\n"}, {"name": "pandas.Series.dt.normalize", "path": "reference/api/pandas.series.dt.normalize", "type": "Series", "text": "\nConvert times to midnight.\n\nThe time component of the date-time is converted to midnight i.e. 00:00:00.\nThis is useful in cases, when the time does not matter. Length is unaltered.\nThe timezones are unaffected.\n\nThis method is available on Series with datetime values under the `.dt`\naccessor, and directly on Datetime Array/Index.\n\nThe same type as the original data. Series will have the same name and index.\nDatetimeIndex will have the same name.\n\nSee also\n\nFloor the datetimes to the specified freq.\n\nCeil the datetimes to the specified freq.\n\nRound the datetimes to the specified freq.\n\nExamples\n\n"}, {"name": "pandas.Series.dt.quarter", "path": "reference/api/pandas.series.dt.quarter", "type": "Series", "text": "\nThe quarter of the date.\n\n"}, {"name": "pandas.Series.dt.qyear", "path": "reference/api/pandas.series.dt.qyear", "type": "Series", "text": "\n\n"}, {"name": "pandas.Series.dt.round", "path": "reference/api/pandas.series.dt.round", "type": "Series", "text": "\nPerform round operation on the data to the specified freq.\n\nThe frequency level to round the index to. Must be a fixed frequency like \u2018S\u2019\n(second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible\nfreq values.\n\nOnly relevant for DatetimeIndex:\n\n\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order\n\nbool-ndarray where True signifies a DST time, False designates a non-DST time\n(note that this flag is only applicable for ambiguous times)\n\n\u2018NaT\u2019 will return NaT where there are ambiguous times\n\n\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST.\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times\n\ntimedelta objects will shift nonexistent times by the timedelta\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with\nthe same index for a Series.\n\nNotes\n\nIf the timestamps have a timezone, rounding will take place relative to the\nlocal (\u201cwall\u201d) time and re-localized to the same timezone. When rounding near\ndaylight savings time, use `nonexistent` and `ambiguous` to control the re-\nlocalization behavior.\n\nExamples\n\nDatetimeIndex\n\nSeries\n\nWhen rounding near a daylight savings time transition, use `ambiguous` or\n`nonexistent` to control how the timestamp should be re-localized.\n\n"}, {"name": "pandas.Series.dt.second", "path": "reference/api/pandas.series.dt.second", "type": "Series", "text": "\nThe seconds of the datetime.\n\nExamples\n\n"}, {"name": "pandas.Series.dt.seconds", "path": "reference/api/pandas.series.dt.seconds", "type": "Series", "text": "\nNumber of seconds (>= 0 and less than 1 day) for each element.\n\n"}, {"name": "pandas.Series.dt.start_time", "path": "reference/api/pandas.series.dt.start_time", "type": "Series", "text": "\n\n"}, {"name": "pandas.Series.dt.strftime", "path": "reference/api/pandas.series.dt.strftime", "type": "Series", "text": "\nConvert to Index using specified date_format.\n\nReturn an Index of formatted strings specified by date_format, which supports\nthe same string format as the python standard library. Details of the string\nformat can be found in python string format doc.\n\nDate format string (e.g. \u201c%Y-%m-%d\u201d).\n\nNumPy ndarray of formatted strings.\n\nSee also\n\nConvert the given argument to datetime.\n\nReturn DatetimeIndex with times to midnight.\n\nRound the DatetimeIndex to the specified freq.\n\nFloor the DatetimeIndex to the specified freq.\n\nExamples\n\n"}, {"name": "pandas.Series.dt.time", "path": "reference/api/pandas.series.dt.time", "type": "Series", "text": "\nReturns numpy array of `datetime.time` objects.\n\nThe time part of the Timestamps.\n\n"}, {"name": "pandas.Series.dt.timetz", "path": "reference/api/pandas.series.dt.timetz", "type": "Series", "text": "\nReturns numpy array of `datetime.time` objects with timezone information.\n\nThe time part of the Timestamps.\n\n"}, {"name": "pandas.Series.dt.to_period", "path": "reference/api/pandas.series.dt.to_period", "type": "Input/output", "text": "\nCast to PeriodArray/Index at a particular frequency.\n\nConverts DatetimeArray/Index to PeriodArray/Index.\n\nOne of pandas\u2019 offset strings or an Offset object. Will be inferred by\ndefault.\n\nWhen converting a DatetimeArray/Index with non-regular values, so that a\nfrequency cannot be inferred.\n\nSee also\n\nImmutable ndarray holding ordinal values.\n\nReturn DatetimeIndex as object.\n\nExamples\n\nInfer the daily frequency\n\n"}, {"name": "pandas.Series.dt.to_pydatetime", "path": "reference/api/pandas.series.dt.to_pydatetime", "type": "Series", "text": "\nReturn the data as an array of `datetime.datetime` objects.\n\nTimezone information is retained if present.\n\nWarning\n\nPython\u2019s datetime uses microsecond resolution, which is lower than pandas\n(nanosecond). The values are truncated.\n\nObject dtype array containing native Python datetime objects.\n\nSee also\n\nStandard library value for a datetime.\n\nExamples\n\npandas\u2019 nanosecond precision is truncated to microseconds.\n\n"}, {"name": "pandas.Series.dt.to_pytimedelta", "path": "reference/api/pandas.series.dt.to_pytimedelta", "type": "Series", "text": "\nReturn an array of native `datetime.timedelta` objects.\n\nPython\u2019s standard datetime library uses a different representation\ntimedelta\u2019s. This method converts a Series of pandas Timedeltas to\ndatetime.timedelta format with the same length as the original Series.\n\nArray of 1D containing data with datetime.timedelta type.\n\nSee also\n\nA duration expressing the difference between two date, time, or datetime.\n\nExamples\n\n"}, {"name": "pandas.Series.dt.total_seconds", "path": "reference/api/pandas.series.dt.total_seconds", "type": "Series", "text": "\nReturn total duration of each element expressed in seconds.\n\nThis method is available directly on TimedeltaArray, TimedeltaIndex and on\nSeries containing timedelta values under the `.dt` namespace.\n\nWhen the calling object is a TimedeltaArray, the return type is ndarray. When\nthe calling object is a TimedeltaIndex, the return type is a Float64Index.\nWhen the calling object is a Series, the return type is Series of type float64\nwhose index is the same as the original.\n\nSee also\n\nStandard library version of this method.\n\nReturn a DataFrame with components of each Timedelta.\n\nExamples\n\nSeries\n\nTimedeltaIndex\n\n"}, {"name": "pandas.Series.dt.tz", "path": "reference/api/pandas.series.dt.tz", "type": "Series", "text": "\nReturn the timezone.\n\nReturns None when the array is tz-naive.\n\n"}, {"name": "pandas.Series.dt.tz_convert", "path": "reference/api/pandas.series.dt.tz_convert", "type": "Series", "text": "\nConvert tz-aware Datetime Array/Index from one time zone to another.\n\nTime zone for time. Corresponding timestamps would be converted to this time\nzone of the Datetime Array/Index. A tz of None will convert to UTC and remove\nthe timezone information.\n\nIf Datetime Array/Index is tz-naive.\n\nSee also\n\nA timezone that has a variable offset from UTC.\n\nLocalize tz-naive DatetimeIndex to a given time zone, or remove timezone from\na tz-aware DatetimeIndex.\n\nExamples\n\nWith the tz parameter, we can change the DatetimeIndex to other time zones:\n\nWith the `tz=None`, we can remove the timezone (after converting to UTC if\nnecessary):\n\n"}, {"name": "pandas.Series.dt.tz_localize", "path": "reference/api/pandas.series.dt.tz_localize", "type": "Series", "text": "\nLocalize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index.\n\nThis method takes a time zone (tz) naive Datetime Array/Index object and makes\nthis time zone aware. It does not move the time to another time zone.\n\nThis method can also be used to do the inverse \u2013 to create a time zone unaware\nobject from an aware object. To that end, pass tz=None.\n\nTime zone to convert timestamps to. Passing `None` will remove the time zone\ninformation preserving local time.\n\nWhen clocks moved backward due to DST, ambiguous times may arise. For example\nin Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST,\n02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a\nsituation, the ambiguous parameter dictates how ambiguous times should be\nhandled.\n\n\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order\n\nbool-ndarray where True signifies a DST time, False signifies a non-DST time\n(note that this flag is only applicable for ambiguous times)\n\n\u2018NaT\u2019 will return NaT where there are ambiguous times\n\n\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST.\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times\n\ntimedelta objects will shift nonexistent times by the timedelta\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nArray/Index converted to the specified time zone.\n\nIf the Datetime Array/Index is tz-aware and tz is not None.\n\nSee also\n\nConvert tz-aware DatetimeIndex from one time zone to another.\n\nExamples\n\nLocalize DatetimeIndex in US/Eastern time zone:\n\nWith the `tz=None`, we can remove the time zone information while keeping the\nlocal time (not converted to UTC):\n\nBe careful with DST changes. When there is sequential data, pandas can infer\nthe DST time:\n\nIn some cases, inferring the DST is impossible. In such cases, you can pass an\nndarray to the ambiguous parameter to set the DST explicitly\n\nIf the DST transition causes nonexistent times, you can shift these dates\nforward or backwards with a timedelta object or \u2018shift_forward\u2019 or\n\u2018shift_backwards\u2019.\n\n"}, {"name": "pandas.Series.dt.week", "path": "reference/api/pandas.series.dt.week", "type": "Series", "text": "\nThe week ordinal of the year.\n\nDeprecated since version 1.1.0.\n\nSeries.dt.weekofyear and Series.dt.week have been deprecated. Please use\nSeries.dt.isocalendar().week instead.\n\n"}, {"name": "pandas.Series.dt.weekday", "path": "reference/api/pandas.series.dt.weekday", "type": "Series", "text": "\nThe day of the week with Monday=0, Sunday=6.\n\nReturn the day of the week. It is assumed the week starts on Monday, which is\ndenoted by 0 and ends on Sunday which is denoted by 6. This method is\navailable on both Series with datetime values (using the dt accessor) or\nDatetimeIndex.\n\nContaining integers indicating the day number.\n\nSee also\n\nAlias.\n\nAlias.\n\nReturns the name of the day of the week.\n\nExamples\n\n"}, {"name": "pandas.Series.dt.weekofyear", "path": "reference/api/pandas.series.dt.weekofyear", "type": "Series", "text": "\nThe week ordinal of the year.\n\nDeprecated since version 1.1.0.\n\nSeries.dt.weekofyear and Series.dt.week have been deprecated. Please use\nSeries.dt.isocalendar().week instead.\n\n"}, {"name": "pandas.Series.dt.year", "path": "reference/api/pandas.series.dt.year", "type": "Series", "text": "\nThe year of the datetime.\n\nExamples\n\n"}, {"name": "pandas.Series.dtype", "path": "reference/api/pandas.series.dtype", "type": "Series", "text": "\nReturn the dtype object of the underlying data.\n\n"}, {"name": "pandas.Series.dtypes", "path": "reference/api/pandas.series.dtypes", "type": "General utility functions", "text": "\nReturn the dtype object of the underlying data.\n\n"}, {"name": "pandas.Series.duplicated", "path": "reference/api/pandas.series.duplicated", "type": "Series", "text": "\nIndicate duplicate Series values.\n\nDuplicated values are indicated as `True` values in the resulting Series.\nEither all duplicates, all except the first or all except the last occurrence\nof duplicates can be indicated.\n\nMethod to handle dropping duplicates:\n\n\u2018first\u2019 : Mark duplicates as `True` except for the first occurrence.\n\n\u2018last\u2019 : Mark duplicates as `True` except for the last occurrence.\n\n`False` : Mark all duplicates as `True`.\n\nSeries indicating whether each value has occurred in the preceding values.\n\nSee also\n\nEquivalent method on pandas.Index.\n\nEquivalent method on pandas.DataFrame.\n\nRemove duplicate values from Series.\n\nExamples\n\nBy default, for each set of duplicated values, the first occurrence is set on\nFalse and all others on True:\n\nwhich is equivalent to\n\nBy using \u2018last\u2019, the last occurrence of each set of duplicated values is set\non False and all others on True:\n\nBy setting keep on `False`, all duplicates are True:\n\n"}, {"name": "pandas.Series.empty", "path": "reference/api/pandas.series.empty", "type": "Series", "text": "\nIndicator whether Series/DataFrame is empty.\n\nTrue if Series/DataFrame is entirely empty (no items), meaning any of the axes\nare of length 0.\n\nIf Series/DataFrame is empty, return True, if not return False.\n\nSee also\n\nReturn series without null values.\n\nReturn DataFrame with labels on given axis omitted where (all or any) data are\nmissing.\n\nNotes\n\nIf Series/DataFrame contains only NaNs, it is still not considered empty. See\nthe example below.\n\nExamples\n\nAn example of an actual empty DataFrame. Notice the index is empty:\n\nIf we only have NaNs in our DataFrame, it is not considered empty! We will\nneed to drop the NaNs to make the DataFrame empty:\n\n"}, {"name": "pandas.Series.eq", "path": "reference/api/pandas.series.eq", "type": "Series", "text": "\nReturn Equal to of series and other, element-wise (binary operator eq).\n\nEquivalent to `series == other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nExamples\n\n"}, {"name": "pandas.Series.equals", "path": "reference/api/pandas.series.equals", "type": "Series", "text": "\nTest whether two objects contain the same elements.\n\nThis function allows two Series or DataFrames to be compared against each\nother to see if they have the same shape and elements. NaNs in the same\nlocation are considered equal.\n\nThe row/column index do not need to have the same type, as long as the values\nare considered equal. Corresponding columns must be of the same dtype.\n\nThe other Series or DataFrame to be compared with the first.\n\nTrue if all elements are the same in both objects, False otherwise.\n\nSee also\n\nCompare two Series objects of the same length and return a Series where each\nelement is True if the element in each Series is equal, False otherwise.\n\nCompare two DataFrame objects of the same shape and return a DataFrame where\neach element is True if the respective element in each DataFrame is equal,\nFalse otherwise.\n\nRaises an AssertionError if left and right are not equal. Provides an easy\ninterface to ignore inequality in dtypes, indexes and precision among others.\n\nLike assert_series_equal, but targets DataFrames.\n\nReturn True if two arrays have the same shape and elements, False otherwise.\n\nExamples\n\nDataFrames df and exactly_equal have the same types and values for their\nelements and column labels, which will return True.\n\nDataFrames df and different_column_type have the same element types and\nvalues, but have different types for the column labels, which will still\nreturn True.\n\nDataFrames df and different_data_type have different types for the same values\nfor their elements, and will return False even though their column labels are\nthe same values and types.\n\n"}, {"name": "pandas.Series.ewm", "path": "reference/api/pandas.series.ewm", "type": "Series", "text": "\nProvide exponentially weighted (EW) calculations.\n\nExactly one parameter: `com`, `span`, `halflife`, or `alpha` must be provided.\n\nSpecify decay in terms of center of mass\n\n\\\\(\\alpha = 1 / (1 + com)\\\\), for \\\\(com \\geq 0\\\\).\n\nSpecify decay in terms of span\n\n\\\\(\\alpha = 2 / (span + 1)\\\\), for \\\\(span \\geq 1\\\\).\n\nSpecify decay in terms of half-life\n\n\\\\(\\alpha = 1 - \\exp\\left(-\\ln(2) / halflife\\right)\\\\), for \\\\(halflife >\n0\\\\).\n\nIf `times` is specified, the time unit (str or timedelta) over which an\nobservation decays to half its value. Only applicable to `mean()`, and\nhalflife value will not apply to the other functions.\n\nNew in version 1.1.0.\n\nSpecify smoothing factor \\\\(\\alpha\\\\) directly\n\n\\\\(0 < \\alpha \\leq 1\\\\).\n\nMinimum number of observations in window required to have a value; otherwise,\nresult is `np.nan`.\n\nDivide by decaying adjustment factor in beginning periods to account for\nimbalance in relative weightings (viewing EWMA as a moving average).\n\nWhen `adjust=True` (default), the EW function is calculated using weights\n\\\\(w_i = (1 - \\alpha)^i\\\\). For example, the EW moving average of the series\n[\\\\(x_0, x_1, ..., x_t\\\\)] would be:\n\nWhen `adjust=False`, the exponentially weighted function is calculated\nrecursively:\n\nIgnore missing values when calculating weights.\n\nWhen `ignore_na=False` (default), weights are based on absolute positions. For\nexample, the weights of \\\\(x_0\\\\) and \\\\(x_2\\\\) used in calculating the final\nweighted average of [\\\\(x_0\\\\), None, \\\\(x_2\\\\)] are \\\\((1-\\alpha)^2\\\\) and\n\\\\(1\\\\) if `adjust=True`, and \\\\((1-\\alpha)^2\\\\) and \\\\(\\alpha\\\\) if\n`adjust=False`.\n\nWhen `ignore_na=True`, weights are based on relative positions. For example,\nthe weights of \\\\(x_0\\\\) and \\\\(x_2\\\\) used in calculating the final weighted\naverage of [\\\\(x_0\\\\), None, \\\\(x_2\\\\)] are \\\\(1-\\alpha\\\\) and \\\\(1\\\\) if\n`adjust=True`, and \\\\(1-\\alpha\\\\) and \\\\(\\alpha\\\\) if `adjust=False`.\n\nIf `0` or `'index'`, calculate across the rows.\n\nIf `1` or `'columns'`, calculate across the columns.\n\nNew in version 1.1.0.\n\nOnly applicable to `mean()`.\n\nTimes corresponding to the observations. Must be monotonically increasing and\n`datetime64[ns]` dtype.\n\nIf 1-D array like, a sequence with the same shape as the observations.\n\nDeprecated since version 1.4.0: If str, the name of the column in the\nDataFrame representing the times.\n\nNew in version 1.4.0.\n\nExecute the rolling operation per single column or row (`'single'`) or over\nthe entire object (`'table'`).\n\nThis argument is only implemented when specifying `engine='numba'` in the\nmethod call.\n\nOnly applicable to `mean()`\n\nSee also\n\nProvides rolling window calculations.\n\nProvides expanding transformations.\n\nNotes\n\nSee Windowing Operations for further usage details and examples.\n\nExamples\n\nadjust\n\nignore_na\n\ntimes\n\nExponentially weighted mean with weights calculated with a timedelta\n`halflife` relative to `times`.\n\n"}, {"name": "pandas.Series.expanding", "path": "reference/api/pandas.series.expanding", "type": "Series", "text": "\nProvide expanding window calculations.\n\nMinimum number of observations in window required to have a value; otherwise,\nresult is `np.nan`.\n\nIf False, set the window labels as the right edge of the window index.\n\nIf True, set the window labels as the center of the window index.\n\nDeprecated since version 1.1.0.\n\nIf `0` or `'index'`, roll across the rows.\n\nIf `1` or `'columns'`, roll across the columns.\n\nExecute the rolling operation per single column or row (`'single'`) or over\nthe entire object (`'table'`).\n\nThis argument is only implemented when specifying `engine='numba'` in the\nmethod call.\n\nNew in version 1.3.0.\n\nSee also\n\nProvides rolling window calculations.\n\nProvides exponential weighted functions.\n\nNotes\n\nSee Windowing Operations for further usage details and examples.\n\nExamples\n\nmin_periods\n\nExpanding sum with 1 vs 3 observations needed to calculate a value.\n\n"}, {"name": "pandas.Series.explode", "path": "reference/api/pandas.series.explode", "type": "Series", "text": "\nTransform each element of a list-like to a row.\n\nNew in version 0.25.0.\n\nIf True, the resulting index will be labeled 0, 1, \u2026, n - 1.\n\nNew in version 1.1.0.\n\nExploded lists to rows; index will be duplicated for these rows.\n\nSee also\n\nSplit string values on specified separator.\n\nUnstack, a.k.a. pivot, Series with MultiIndex to produce DataFrame.\n\nUnpivot a DataFrame from wide format to long format.\n\nExplode a DataFrame from list-like columns to long format.\n\nNotes\n\nThis routine will explode list-likes including lists, tuples, sets, Series,\nand np.ndarray. The result dtype of the subset rows will be object. Scalars\nwill be returned unchanged, and empty list-likes will result in a np.nan for\nthat row. In addition, the ordering of elements in the output will be non-\ndeterministic when exploding sets.\n\nExamples\n\n"}, {"name": "pandas.Series.factorize", "path": "reference/api/pandas.series.factorize", "type": "Series", "text": "\nEncode the object as an enumerated type or categorical variable.\n\nThis method is useful for obtaining a numeric representation of an array when\nall that matters is identifying distinct values. factorize is available as\nboth a top-level function `pandas.factorize()`, and as a method\n`Series.factorize()` and `Index.factorize()`.\n\nSort uniques and shuffle codes to maintain the relationship.\n\nValue to mark \u201cnot found\u201d. If None, will not drop the NaN from the uniques of\nthe values.\n\nChanged in version 1.1.2.\n\nAn integer ndarray that\u2019s an indexer into uniques. `uniques.take(codes)` will\nhave the same values as values.\n\nThe unique valid values. When values is Categorical, uniques is a Categorical.\nWhen values is some other pandas object, an Index is returned. Otherwise, a\n1-D ndarray is returned.\n\nNote\n\nEven if there\u2019s a missing value in values, uniques will not contain an entry\nfor it.\n\nSee also\n\nDiscretize continuous-valued array.\n\nFind the unique value in an array.\n\nExamples\n\nThese examples all show factorize as a top-level method like\n`pd.factorize(values)`. The results are identical for methods like\n`Series.factorize()`.\n\nWith `sort=True`, the uniques will be sorted, and codes will be shuffled so\nthat the relationship is the maintained.\n\nMissing values are indicated in codes with na_sentinel (`-1` by default). Note\nthat missing values are never included in uniques.\n\nThus far, we\u2019ve only factorized lists (which are internally coerced to NumPy\narrays). When factorizing pandas objects, the type of uniques will differ. For\nCategoricals, a Categorical is returned.\n\nNotice that `'b'` is in `uniques.categories`, despite not being present in\n`cat.values`.\n\nFor all other pandas objects, an Index of the appropriate type is returned.\n\nIf NaN is in the values, and we want to include NaN in the uniques of the\nvalues, it can be achieved by setting `na_sentinel=None`.\n\n"}, {"name": "pandas.Series.ffill", "path": "reference/api/pandas.series.ffill", "type": "Series", "text": "\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\nObject with missing values filled or None if `inplace=True`.\n\n"}, {"name": "pandas.Series.fillna", "path": "reference/api/pandas.series.fillna", "type": "Series", "text": "\nFill NA/NaN values using the specified method.\n\nValue to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of\nvalues specifying which value to use for each index (for a Series) or column\n(for a DataFrame). Values not in the dict/Series/DataFrame will not be filled.\nThis value cannot be a list.\n\nMethod to use for filling holes in reindexed Series pad / ffill: propagate\nlast valid observation forward to next valid backfill / bfill: use next valid\nobservation to fill gap.\n\nAxis along which to fill missing values.\n\nIf True, fill in-place. Note: this will modify any other views on this object\n(e.g., a no-copy slice for a column in a DataFrame).\n\nIf method is specified, this is the maximum number of consecutive NaN values\nto forward/backward fill. In other words, if there is a gap with more than\nthis number of consecutive NaNs, it will only be partially filled. If method\nis not specified, this is the maximum number of entries along the entire axis\nwhere NaNs will be filled. Must be greater than 0 if not None.\n\nA dict of item->dtype of what to downcast if possible, or the string \u2018infer\u2019\nwhich will try to downcast to an appropriate equal type (e.g. float64 to int64\nif possible).\n\nObject with missing values filled or None if `inplace=True`.\n\nSee also\n\nFill NaN values using interpolation.\n\nConform object to new index.\n\nConvert TimeSeries to specified frequency.\n\nExamples\n\nReplace all NaN elements with 0s.\n\nWe can also propagate non-null values forward or backward.\n\nReplace all NaN elements in column \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019, with 0, 1, 2, and 3\nrespectively.\n\nOnly replace the first NaN element.\n\nWhen filling using a DataFrame, replacement happens along the same column\nnames and same indices\n\nNote that column D is not affected since it is not present in df2.\n\n"}, {"name": "pandas.Series.filter", "path": "reference/api/pandas.series.filter", "type": "Series", "text": "\nSubset the dataframe rows or columns according to the specified index labels.\n\nNote that this routine does not filter a dataframe on its contents. The filter\nis applied to the labels of the index.\n\nKeep labels from axis which are in items.\n\nKeep labels from axis for which \u201clike in label == True\u201d.\n\nKeep labels from axis for which re.search(regex, label) == True.\n\nThe axis to filter on, expressed either as an index (int) or axis name (str).\nBy default this is the info axis, \u2018index\u2019 for Series, \u2018columns\u2019 for DataFrame.\n\nSee also\n\nAccess a group of rows and columns by label(s) or a boolean array.\n\nNotes\n\nThe `items`, `like`, and `regex` parameters are enforced to be mutually\nexclusive.\n\n`axis` defaults to the info axis that is used when indexing with `[]`.\n\nExamples\n\n"}, {"name": "pandas.Series.first", "path": "reference/api/pandas.series.first", "type": "Series", "text": "\nSelect initial periods of time series data based on a date offset.\n\nWhen having a DataFrame with dates as index, this function can select the\nfirst few rows based on a date offset.\n\nThe offset length of the data that will be selected. For instance, \u20181M\u2019 will\ndisplay all the rows having their index within the first month.\n\nA subset of the caller.\n\nIf the index is not a `DatetimeIndex`\n\nSee also\n\nSelect final periods of time series based on a date offset.\n\nSelect values at a particular time of the day.\n\nSelect values between particular times of the day.\n\nExamples\n\nGet the rows for the first 3 days:\n\nNotice the data for 3 first calendar days were returned, not the first 3 days\nobserved in the dataset, and therefore data for 2018-04-13 was not returned.\n\n"}, {"name": "pandas.Series.first_valid_index", "path": "reference/api/pandas.series.first_valid_index", "type": "Series", "text": "\nReturn index for first non-NA value or None, if no NA value is found.\n\nNotes\n\nIf all elements are non-NA/null, returns None. Also returns None for empty\nSeries/DataFrame.\n\n"}, {"name": "pandas.Series.flags", "path": "reference/api/pandas.series.flags", "type": "Series", "text": "\nGet the properties associated with this pandas object.\n\nThe available flags are\n\n`Flags.allows_duplicate_labels`\n\nSee also\n\nFlags that apply to pandas objects.\n\nGlobal metadata applying to this dataset.\n\nNotes\n\n\u201cFlags\u201d differ from \u201cmetadata\u201d. Flags reflect properties of the pandas object\n(the Series or DataFrame). Metadata refer to properties of the dataset, and\nshould be stored in `DataFrame.attrs`.\n\nExamples\n\nFlags can be get or set using `.`\n\nOr by slicing with a key\n\n"}, {"name": "pandas.Series.floordiv", "path": "reference/api/pandas.series.floordiv", "type": "Series", "text": "\nReturn Integer division of series and other, element-wise (binary operator\nfloordiv).\n\nEquivalent to `series // other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nReverse of the Integer division operator, see Python documentation for more\ndetails.\n\nExamples\n\n"}, {"name": "pandas.Series.ge", "path": "reference/api/pandas.series.ge", "type": "Series", "text": "\nReturn Greater than or equal to of series and other, element-wise (binary\noperator ge).\n\nEquivalent to `series >= other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nExamples\n\n"}, {"name": "pandas.Series.get", "path": "reference/api/pandas.series.get", "type": "Series", "text": "\nGet item from object for given key (ex: DataFrame column).\n\nReturns default value if not found.\n\nExamples\n\nIf the key isn\u2019t found, the default value will be used.\n\n"}, {"name": "pandas.Series.groupby", "path": "reference/api/pandas.series.groupby", "type": "Series", "text": "\nGroup Series using a mapper or by a Series of columns.\n\nA groupby operation involves some combination of splitting the object,\napplying a function, and combining the results. This can be used to group\nlarge amounts of data and compute operations on these groups.\n\nUsed to determine the groups for the groupby. If `by` is a function, it\u2019s\ncalled on each value of the object\u2019s index. If a dict or Series is passed, the\nSeries or dict VALUES will be used to determine the groups (the Series\u2019 values\nare first aligned; see `.align()` method). If a list or ndarray of length\nequal to the selected axis is passed (see the groupby user guide), the values\nare used as-is to determine the groups. A label or list of labels may be\npassed to group by the columns in `self`. Notice that a tuple is interpreted\nas a (single) key.\n\nSplit along rows (0) or columns (1).\n\nIf the axis is a MultiIndex (hierarchical), group by a particular level or\nlevels.\n\nFor aggregated output, return object with group labels as the index. Only\nrelevant for DataFrame input. as_index=False is effectively \u201cSQL-style\u201d\ngrouped output.\n\nSort group keys. Get better performance by turning this off. Note this does\nnot influence the order of observations within each group. Groupby preserves\nthe order of rows within each group.\n\nWhen calling apply, add group keys to index to identify pieces.\n\nReduce the dimensionality of the return type if possible, otherwise return a\nconsistent type.\n\nDeprecated since version 1.1.0.\n\nThis only applies if any of the groupers are Categoricals. If True: only show\nobserved values for categorical groupers. If False: show all values for\ncategorical groupers.\n\nIf True, and if group keys contain NA values, NA values together with\nrow/column will be dropped. If False, NA values will also be treated as the\nkey in groups.\n\nNew in version 1.1.0.\n\nReturns a groupby object that contains information about the groups.\n\nSee also\n\nConvenience method for frequency conversion and resampling of time series.\n\nNotes\n\nSee the user guide for more detailed usage and examples, including splitting\nan object into groups, iterating through groups, selecting a group,\naggregation, and more.\n\nExamples\n\nGrouping by Indexes\n\nWe can groupby different levels of a hierarchical index using the level\nparameter:\n\nWe can also choose to include NA in group keys or not by defining dropna\nparameter, the default setting is True.\n\n"}, {"name": "pandas.Series.gt", "path": "reference/api/pandas.series.gt", "type": "Series", "text": "\nReturn Greater than of series and other, element-wise (binary operator gt).\n\nEquivalent to `series > other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nExamples\n\n"}, {"name": "pandas.Series.hasnans", "path": "reference/api/pandas.series.hasnans", "type": "Series", "text": "\nReturn True if there are any NaNs.\n\nEnables various performance speedups.\n\n"}, {"name": "pandas.Series.head", "path": "reference/api/pandas.series.head", "type": "Series", "text": "\nReturn the first n rows.\n\nThis function returns the first n rows for the object based on position. It is\nuseful for quickly testing if your object has the right type of data in it.\n\nFor negative values of n, this function returns all rows except the last n\nrows, equivalent to `df[:-n]`.\n\nNumber of rows to select.\n\nThe first n rows of the caller object.\n\nSee also\n\nReturns the last n rows.\n\nExamples\n\nViewing the first 5 lines\n\nViewing the first n lines (three in this case)\n\nFor negative values of n\n\n"}, {"name": "pandas.Series.hist", "path": "reference/api/pandas.series.hist", "type": "Series", "text": "\nDraw histogram of the input series using matplotlib.\n\nIf passed, then used to form histograms for separate groups.\n\nIf not passed, uses gca().\n\nWhether to show axis grid lines.\n\nIf specified changes the x-axis label size.\n\nRotation of x axis labels.\n\nIf specified changes the y-axis label size.\n\nRotation of y axis labels.\n\nFigure size in inches by default.\n\nNumber of histogram bins to be used. If an integer is given, bins + 1 bin\nedges are calculated and returned. If bins is a sequence, gives bin edges,\nincluding left edge of first bin and right edge of last bin. In this case,\nbins is returned unmodified.\n\nBackend to use instead of the backend specified in the option\n`plotting.backend`. For instance, \u2018matplotlib\u2019. Alternatively, to specify the\n`plotting.backend` for the whole session, set `pd.options.plotting.backend`.\n\nNew in version 1.0.0.\n\nWhether to show the legend.\n\nNew in version 1.1.0.\n\nTo be passed to the actual plotting function.\n\nA histogram plot.\n\nSee also\n\nPlot a histogram using matplotlib.\n\n"}, {"name": "pandas.Series.iat", "path": "reference/api/pandas.series.iat", "type": "Series", "text": "\nAccess a single value for a row/column pair by integer position.\n\nSimilar to `iloc`, in that both provide integer-based lookups. Use `iat` if\nyou only need to get or set a single value in a DataFrame or Series.\n\nWhen integer position is out of bounds.\n\nSee also\n\nAccess a single value for a row/column label pair.\n\nAccess a group of rows and columns by label(s).\n\nAccess a group of rows and columns by integer position(s).\n\nExamples\n\nGet value at specified row/column pair\n\nSet value at specified row/column pair\n\nGet value within a series\n\n"}, {"name": "pandas.Series.idxmax", "path": "reference/api/pandas.series.idxmax", "type": "Series", "text": "\nReturn the row label of the maximum value.\n\nIf multiple values equal the maximum, the first row label with that value is\nreturned.\n\nFor compatibility with DataFrame.idxmax. Redundant for application on Series.\n\nExclude NA/null values. If the entire Series is NA, the result will be NA.\n\nAdditional arguments and keywords have no effect but might be accepted for\ncompatibility with NumPy.\n\nLabel of the maximum value.\n\nIf the Series is empty.\n\nSee also\n\nReturn indices of the maximum values along the given axis.\n\nReturn index of first occurrence of maximum over requested axis.\n\nReturn index label of the first occurrence of minimum of values.\n\nNotes\n\nThis method is the Series version of `ndarray.argmax`. This method returns the\nlabel of the maximum, while `ndarray.argmax` returns the position. To get the\nposition, use `series.values.argmax()`.\n\nExamples\n\nIf skipna is False and there is an NA value in the data, the function returns\n`nan`.\n\n"}, {"name": "pandas.Series.idxmin", "path": "reference/api/pandas.series.idxmin", "type": "Series", "text": "\nReturn the row label of the minimum value.\n\nIf multiple values equal the minimum, the first row label with that value is\nreturned.\n\nFor compatibility with DataFrame.idxmin. Redundant for application on Series.\n\nExclude NA/null values. If the entire Series is NA, the result will be NA.\n\nAdditional arguments and keywords have no effect but might be accepted for\ncompatibility with NumPy.\n\nLabel of the minimum value.\n\nIf the Series is empty.\n\nSee also\n\nReturn indices of the minimum values along the given axis.\n\nReturn index of first occurrence of minimum over requested axis.\n\nReturn index label of the first occurrence of maximum of values.\n\nNotes\n\nThis method is the Series version of `ndarray.argmin`. This method returns the\nlabel of the minimum, while `ndarray.argmin` returns the position. To get the\nposition, use `series.values.argmin()`.\n\nExamples\n\nIf skipna is False and there is an NA value in the data, the function returns\n`nan`.\n\n"}, {"name": "pandas.Series.iloc", "path": "reference/api/pandas.series.iloc", "type": "Series", "text": "\nPurely integer-location based indexing for selection by position.\n\n`.iloc[]` is primarily integer position based (from `0` to `length-1` of the\naxis), but may also be used with a boolean array.\n\nAllowed inputs are:\n\nAn integer, e.g. `5`.\n\nA list or array of integers, e.g. `[4, 3, 0]`.\n\nA slice object with ints, e.g. `1:7`.\n\nA boolean array.\n\nA `callable` function with one argument (the calling Series or DataFrame) and\nthat returns valid output for indexing (one of the above). This is useful in\nmethod chains, when you don\u2019t have a reference to the calling object, but\nwould like to base your selection on some value.\n\n`.iloc` will raise `IndexError` if a requested indexer is out-of-bounds,\nexcept slice indexers which allow out-of-bounds indexing (this conforms with\npython/numpy slice semantics).\n\nSee more at Selection by Position.\n\nSee also\n\nFast integer location scalar accessor.\n\nPurely label-location based indexer for selection by label.\n\nPurely integer-location based indexing for selection by position.\n\nExamples\n\nIndexing just the rows\n\nWith a scalar integer.\n\nWith a list of integers.\n\nWith a slice object.\n\nWith a boolean mask the same length as the index.\n\nWith a callable, useful in method chains. The x passed to the `lambda` is the\nDataFrame being sliced. This selects the rows whose index label even.\n\nIndexing both axes\n\nYou can mix the indexer types for the index and columns. Use `:` to select the\nentire axis.\n\nWith scalar integers.\n\nWith lists of integers.\n\nWith slice objects.\n\nWith a boolean array whose length matches the columns.\n\nWith a callable function that expects the Series or DataFrame.\n\n"}, {"name": "pandas.Series.index", "path": "reference/api/pandas.series.index", "type": "Series", "text": "\nThe index (axis labels) of the Series.\n\n"}, {"name": "pandas.Series.infer_objects", "path": "reference/api/pandas.series.infer_objects", "type": "Series", "text": "\nAttempt to infer better dtypes for object columns.\n\nAttempts soft conversion of object-dtyped columns, leaving non-object and\nunconvertible columns unchanged. The inference rules are the same as during\nnormal Series/DataFrame construction.\n\nSee also\n\nConvert argument to datetime.\n\nConvert argument to timedelta.\n\nConvert argument to numeric type.\n\nConvert argument to best possible dtype.\n\nExamples\n\n"}, {"name": "pandas.Series.info", "path": "reference/api/pandas.series.info", "type": "Series", "text": "\nPrint a concise summary of a Series.\n\nThis method prints information about a Series including the index dtype, non-\nnull values and memory usage.\n\nNew in version 1.4.0.\n\nSeries to print information about.\n\nWhether to print the full summary. By default, the setting in\n`pandas.options.display.max_info_columns` is followed.\n\nWhere to send the output. By default, the output is printed to sys.stdout.\nPass a writable buffer if you need to further process the output.\n\nSpecifies whether total memory usage of the Series elements (including the\nindex) should be displayed. By default, this follows the\n`pandas.options.display.memory_usage` setting.\n\nTrue always show memory usage. False never shows memory usage. A value of\n\u2018deep\u2019 is equivalent to \u201cTrue with deep introspection\u201d. Memory usage is shown\nin human-readable units (base-2 representation). Without deep introspection a\nmemory estimation is made based in column dtype and number of rows assuming\nvalues consume the same memory amount for corresponding dtypes. With deep\nmemory introspection, a real memory usage calculation is performed at the cost\nof computational resources.\n\nWhether to show the non-null counts. By default, this is shown only if the\nDataFrame is smaller than `pandas.options.display.max_info_rows` and\n`pandas.options.display.max_info_columns`. A value of True always shows the\ncounts, and False never shows the counts.\n\nThis method prints a summary of a Series and returns None.\n\nSee also\n\nGenerate descriptive statistics of Series.\n\nMemory usage of Series.\n\nExamples\n\nPrints a summary excluding information about its values:\n\nPipe output of Series.info to buffer instead of sys.stdout, get buffer content\nand writes to a text file:\n\nThe memory_usage parameter allows deep introspection mode, specially useful\nfor big Series and fine-tune memory optimization:\n\n"}, {"name": "pandas.Series.interpolate", "path": "reference/api/pandas.series.interpolate", "type": "Series", "text": "\nFill NaN values using an interpolation method.\n\nPlease note that only `method='linear'` is supported for DataFrame/Series with\na MultiIndex.\n\nInterpolation technique to use. One of:\n\n\u2018linear\u2019: Ignore the index and treat the values as equally spaced. This is the\nonly method supported on MultiIndexes.\n\n\u2018time\u2019: Works on daily and higher resolution data to interpolate given length\nof interval.\n\n\u2018index\u2019, \u2018values\u2019: use the actual numerical values of the index.\n\n\u2018pad\u2019: Fill in NaNs using existing values.\n\n\u2018nearest\u2019, \u2018zero\u2019, \u2018slinear\u2019, \u2018quadratic\u2019, \u2018cubic\u2019, \u2018spline\u2019, \u2018barycentric\u2019,\n\u2018polynomial\u2019: Passed to scipy.interpolate.interp1d. These methods use the\nnumerical values of the index. Both \u2018polynomial\u2019 and \u2018spline\u2019 require that you\nalso specify an order (int), e.g. `df.interpolate(method='polynomial',\norder=5)`.\n\n\u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019, \u2018akima\u2019, \u2018cubicspline\u2019:\nWrappers around the SciPy interpolation methods of similar names. See Notes.\n\n\u2018from_derivatives\u2019: Refers to scipy.interpolate.BPoly.from_derivatives which\nreplaces \u2018piecewise_polynomial\u2019 interpolation method in scipy 0.18.\n\nAxis to interpolate along.\n\nMaximum number of consecutive NaNs to fill. Must be greater than 0.\n\nUpdate the data in place if possible.\n\nConsecutive NaNs will be filled in this direction.\n\nIf \u2018method\u2019 is \u2018pad\u2019 or \u2018ffill\u2019, \u2018limit_direction\u2019 must be \u2018forward\u2019.\n\nIf \u2018method\u2019 is \u2018backfill\u2019 or \u2018bfill\u2019, \u2018limit_direction\u2019 must be \u2018backwards\u2019.\n\nIf \u2018method\u2019 is \u2018backfill\u2019 or \u2018bfill\u2019, the default is \u2018backward\u2019\n\nelse the default is \u2018forward\u2019\n\nChanged in version 1.1.0: raises ValueError if limit_direction is \u2018forward\u2019 or\n\u2018both\u2019 and method is \u2018backfill\u2019 or \u2018bfill\u2019. raises ValueError if\nlimit_direction is \u2018backward\u2019 or \u2018both\u2019 and method is \u2018pad\u2019 or \u2018ffill\u2019.\n\nIf limit is specified, consecutive NaNs will be filled with this restriction.\n\n`None`: No fill restriction.\n\n\u2018inside\u2019: Only fill NaNs surrounded by valid values (interpolate).\n\n\u2018outside\u2019: Only fill NaNs outside valid values (extrapolate).\n\nDowncast dtypes if possible.\n\nKeyword arguments to pass on to the interpolating function.\n\nReturns the same object type as the caller, interpolated at some or all `NaN`\nvalues or None if `inplace=True`.\n\nSee also\n\nFill missing values using different methods.\n\nPiecewise cubic polynomials (Akima interpolator).\n\nPiecewise polynomial in the Bernstein basis.\n\nInterpolate a 1-D function.\n\nInterpolate polynomial (Krogh interpolator).\n\nPCHIP 1-d monotonic cubic interpolation.\n\nCubic spline data interpolator.\n\nNotes\n\nThe \u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019 and \u2018akima\u2019 methods are\nwrappers around the respective SciPy implementations of similar names. These\nuse the actual numerical values of the index. For more information on their\nbehavior, see the SciPy documentation and SciPy tutorial.\n\nExamples\n\nFilling in `NaN` in a `Series` via linear interpolation.\n\nFilling in `NaN` in a Series by padding, but filling at most two consecutive\n`NaN` at a time.\n\nFilling in `NaN` in a Series via polynomial interpolation or splines: Both\n\u2018polynomial\u2019 and \u2018spline\u2019 methods require that you also specify an `order`\n(int).\n\nFill the DataFrame forward (that is, going down) along each column using\nlinear interpolation.\n\nNote how the last entry in column \u2018a\u2019 is interpolated differently, because\nthere is no entry after it to use for interpolation. Note how the first entry\nin column \u2018b\u2019 remains `NaN`, because there is no entry before it to use for\ninterpolation.\n\nUsing polynomial interpolation.\n\n"}, {"name": "pandas.Series.is_monotonic", "path": "reference/api/pandas.series.is_monotonic", "type": "Series", "text": "\nReturn boolean if values in the object are monotonic_increasing.\n\n"}, {"name": "pandas.Series.is_monotonic_decreasing", "path": "reference/api/pandas.series.is_monotonic_decreasing", "type": "Series", "text": "\nReturn boolean if values in the object are monotonic_decreasing.\n\n"}, {"name": "pandas.Series.is_monotonic_increasing", "path": "reference/api/pandas.series.is_monotonic_increasing", "type": "Series", "text": "\nAlias for is_monotonic.\n\n"}, {"name": "pandas.Series.is_unique", "path": "reference/api/pandas.series.is_unique", "type": "Series", "text": "\nReturn boolean if values in the object are unique.\n\n"}, {"name": "pandas.Series.isin", "path": "reference/api/pandas.series.isin", "type": "Series", "text": "\nWhether elements in Series are contained in values.\n\nReturn a boolean Series showing whether each element in the Series matches an\nelement in the passed sequence of values exactly.\n\nThe sequence of values to test. Passing in a single string will raise a\n`TypeError`. Instead, turn a single string into a list of one element.\n\nSeries of booleans indicating if each element is in values.\n\nIf values is a string\n\nSee also\n\nEquivalent method on DataFrame.\n\nExamples\n\nTo invert the boolean values, use the `~` operator:\n\nPassing a single string as `s.isin('lama')` will raise an error. Use a list of\none element instead:\n\nStrings and integers are distinct and are therefore not comparable:\n\n"}, {"name": "pandas.Series.isna", "path": "reference/api/pandas.series.isna", "type": "Series", "text": "\nDetect missing values.\n\nReturn a boolean same-sized object indicating if the values are NA. NA values,\nsuch as None or `numpy.NaN`, gets mapped to True values. Everything else gets\nmapped to False values. Characters such as empty strings `''` or `numpy.inf`\nare not considered NA values (unless you set\n`pandas.options.mode.use_inf_as_na = True`).\n\nMask of bool values for each element in Series that indicates whether an\nelement is an NA value.\n\nSee also\n\nAlias of isna.\n\nBoolean inverse of isna.\n\nOmit axes labels with missing values.\n\nTop-level isna.\n\nExamples\n\nShow which entries in a DataFrame are NA.\n\nShow which entries in a Series are NA.\n\n"}, {"name": "pandas.Series.isnull", "path": "reference/api/pandas.series.isnull", "type": "Series", "text": "\nSeries.isnull is an alias for Series.isna.\n\nDetect missing values.\n\nReturn a boolean same-sized object indicating if the values are NA. NA values,\nsuch as None or `numpy.NaN`, gets mapped to True values. Everything else gets\nmapped to False values. Characters such as empty strings `''` or `numpy.inf`\nare not considered NA values (unless you set\n`pandas.options.mode.use_inf_as_na = True`).\n\nMask of bool values for each element in Series that indicates whether an\nelement is an NA value.\n\nSee also\n\nAlias of isna.\n\nBoolean inverse of isna.\n\nOmit axes labels with missing values.\n\nTop-level isna.\n\nExamples\n\nShow which entries in a DataFrame are NA.\n\nShow which entries in a Series are NA.\n\n"}, {"name": "pandas.Series.item", "path": "reference/api/pandas.series.item", "type": "Series", "text": "\nReturn the first element of the underlying data as a Python scalar.\n\nThe first element of %(klass)s.\n\nIf the data is not length-1.\n\n"}, {"name": "pandas.Series.items", "path": "reference/api/pandas.series.items", "type": "Series", "text": "\nLazily iterate over (index, value) tuples.\n\nThis method returns an iterable tuple (index, value). This is convenient if\nyou want to create a lazy iterator.\n\nIterable of tuples containing the (index, value) pairs from a Series.\n\nSee also\n\nIterate over (column name, Series) pairs.\n\nIterate over DataFrame rows as (index, Series) pairs.\n\nExamples\n\n"}, {"name": "pandas.Series.iteritems", "path": "reference/api/pandas.series.iteritems", "type": "Series", "text": "\nLazily iterate over (index, value) tuples.\n\nThis method returns an iterable tuple (index, value). This is convenient if\nyou want to create a lazy iterator.\n\nIterable of tuples containing the (index, value) pairs from a Series.\n\nSee also\n\nIterate over (column name, Series) pairs.\n\nIterate over DataFrame rows as (index, Series) pairs.\n\nExamples\n\n"}, {"name": "pandas.Series.keys", "path": "reference/api/pandas.series.keys", "type": "Series", "text": "\nReturn alias for index.\n\nIndex of the Series.\n\n"}, {"name": "pandas.Series.kurt", "path": "reference/api/pandas.series.kurt", "type": "Series", "text": "\nReturn unbiased kurtosis over requested axis.\n\nKurtosis obtained using Fisher\u2019s definition of kurtosis (kurtosis of normal ==\n0.0). Normalized by N-1.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a scalar.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.Series.kurtosis", "path": "reference/api/pandas.series.kurtosis", "type": "Series", "text": "\nReturn unbiased kurtosis over requested axis.\n\nKurtosis obtained using Fisher\u2019s definition of kurtosis (kurtosis of normal ==\n0.0). Normalized by N-1.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a scalar.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.Series.last", "path": "reference/api/pandas.series.last", "type": "Series", "text": "\nSelect final periods of time series data based on a date offset.\n\nFor a DataFrame with a sorted DatetimeIndex, this function selects the last\nfew rows based on a date offset.\n\nThe offset length of the data that will be selected. For instance, \u20183D\u2019 will\ndisplay all the rows having their index within the last 3 days.\n\nA subset of the caller.\n\nIf the index is not a `DatetimeIndex`\n\nSee also\n\nSelect initial periods of time series based on a date offset.\n\nSelect values at a particular time of the day.\n\nSelect values between particular times of the day.\n\nExamples\n\nGet the rows for the last 3 days:\n\nNotice the data for 3 last calendar days were returned, not the last 3\nobserved days in the dataset, and therefore data for 2018-04-11 was not\nreturned.\n\n"}, {"name": "pandas.Series.last_valid_index", "path": "reference/api/pandas.series.last_valid_index", "type": "Series", "text": "\nReturn index for last non-NA value or None, if no NA value is found.\n\nNotes\n\nIf all elements are non-NA/null, returns None. Also returns None for empty\nSeries/DataFrame.\n\n"}, {"name": "pandas.Series.le", "path": "reference/api/pandas.series.le", "type": "Series", "text": "\nReturn Less than or equal to of series and other, element-wise (binary\noperator le).\n\nEquivalent to `series <= other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nExamples\n\n"}, {"name": "pandas.Series.loc", "path": "reference/api/pandas.series.loc", "type": "Series", "text": "\nAccess a group of rows and columns by label(s) or a boolean array.\n\n`.loc[]` is primarily label based, but may also be used with a boolean array.\n\nAllowed inputs are:\n\nA single label, e.g. `5` or `'a'`, (note that `5` is interpreted as a label of\nthe index, and never as an integer position along the index).\n\nA list or array of labels, e.g. `['a', 'b', 'c']`.\n\nA slice object with labels, e.g. `'a':'f'`.\n\nWarning\n\nNote that contrary to usual python slices, both the start and the stop are\nincluded\n\nA boolean array of the same length as the axis being sliced, e.g. `[True,\nFalse, True]`.\n\nAn alignable boolean Series. The index of the key will be aligned before\nmasking.\n\nAn alignable Index. The Index of the returned selection will be the input.\n\nA `callable` function with one argument (the calling Series or DataFrame) and\nthat returns valid output for indexing (one of the above)\n\nSee more at Selection by Label.\n\nIf any items are not found.\n\nIf an indexed key is passed and its index is unalignable to the frame index.\n\nSee also\n\nAccess a single value for a row/column label pair.\n\nAccess group of rows and columns by integer position(s).\n\nReturns a cross-section (row(s) or column(s)) from the Series/DataFrame.\n\nAccess group of values using labels.\n\nExamples\n\nGetting values\n\nSingle label. Note this returns the row as a Series.\n\nList of labels. Note using `[[]]` returns a DataFrame.\n\nSingle label for row and column\n\nSlice with labels for row and single label for column. As mentioned above,\nnote that both the start and stop of the slice are included.\n\nBoolean list with the same length as the row axis\n\nAlignable boolean Series:\n\nIndex (same behavior as `df.reindex`)\n\nConditional that returns a boolean Series\n\nConditional that returns a boolean Series with column labels specified\n\nCallable that returns a boolean Series\n\nSetting values\n\nSet value for all items matching the list of labels\n\nSet value for an entire row\n\nSet value for an entire column\n\nSet value for rows matching callable condition\n\nGetting values on a DataFrame with an index that has integer labels\n\nAnother example using integers for the index\n\nSlice with integer labels for rows. As mentioned above, note that both the\nstart and stop of the slice are included.\n\nGetting values with a MultiIndex\n\nA number of examples using a DataFrame with a MultiIndex\n\nSingle label. Note this returns a DataFrame with a single index.\n\nSingle index tuple. Note this returns a Series.\n\nSingle label for row and column. Similar to passing in a tuple, this returns a\nSeries.\n\nSingle tuple. Note using `[[]]` returns a DataFrame.\n\nSingle tuple for the index with a single label for the column\n\nSlice from index tuple to single label\n\nSlice from index tuple to index tuple\n\n"}, {"name": "pandas.Series.lt", "path": "reference/api/pandas.series.lt", "type": "Series", "text": "\nReturn Less than of series and other, element-wise (binary operator lt).\n\nEquivalent to `series < other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nExamples\n\n"}, {"name": "pandas.Series.mad", "path": "reference/api/pandas.series.mad", "type": "Series", "text": "\nReturn the mean absolute deviation of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a scalar.\n\n"}, {"name": "pandas.Series.map", "path": "reference/api/pandas.series.map", "type": "Series", "text": "\nMap values of Series according to an input mapping or function.\n\nUsed for substituting each value in a Series with another value, that may be\nderived from a function, a `dict` or a `Series`.\n\nMapping correspondence.\n\nIf \u2018ignore\u2019, propagate NaN values, without passing them to the mapping\ncorrespondence.\n\nSame index as caller.\n\nSee also\n\nFor applying more complex functions on a Series.\n\nApply a function row-/column-wise.\n\nApply a function elementwise on a whole DataFrame.\n\nNotes\n\nWhen `arg` is a dictionary, values in Series that are not in the dictionary\n(as keys) are converted to `NaN`. However, if the dictionary is a `dict`\nsubclass that defines `__missing__` (i.e. provides a method for default\nvalues), then this default is used rather than `NaN`.\n\nExamples\n\n`map` accepts a `dict` or a `Series`. Values that are not found in the `dict`\nare converted to `NaN`, unless the dict has a default value (e.g.\n`defaultdict`):\n\nIt also accepts a function:\n\nTo avoid applying the function to missing values (and keep them as `NaN`)\n`na_action='ignore'` can be used:\n\n"}, {"name": "pandas.Series.mask", "path": "reference/api/pandas.series.mask", "type": "Series", "text": "\nReplace values where the condition is True.\n\nWhere cond is False, keep the original value. Where True, replace with\ncorresponding value from other. If cond is callable, it is computed on the\nSeries/DataFrame and should return boolean Series/DataFrame or array. The\ncallable must not change input Series/DataFrame (though pandas doesn\u2019t check\nit).\n\nEntries where cond is True are replaced with corresponding value from other.\nIf other is callable, it is computed on the Series/DataFrame and should return\nscalar or Series/DataFrame. The callable must not change input\nSeries/DataFrame (though pandas doesn\u2019t check it).\n\nWhether to perform the operation in place on the data.\n\nAlignment axis if needed.\n\nAlignment level if needed.\n\nNote that currently this parameter won\u2019t affect the results and will always\ncoerce to a suitable dtype.\n\n\u2018raise\u2019 : allow exceptions to be raised.\n\n\u2018ignore\u2019 : suppress exceptions. On error return original object.\n\nTry to cast the result back to the input type (if possible).\n\nDeprecated since version 1.3.0: Manually cast back if necessary.\n\nSee also\n\nReturn an object of same shape as self.\n\nNotes\n\nThe mask method is an application of the if-then idiom. For each element in\nthe calling DataFrame, if `cond` is `False` the element is used; otherwise the\ncorresponding element from the DataFrame `other` is used.\n\nThe signature for `DataFrame.where()` differs from `numpy.where()`. Roughly\n`df1.where(m, df2)` is equivalent to `np.where(m, df1, df2)`.\n\nFor further details and examples see the `mask` documentation in indexing.\n\nExamples\n\n"}, {"name": "pandas.Series.max", "path": "reference/api/pandas.series.max", "type": "Series", "text": "\nReturn the maximum of the values over the requested axis.\n\nIf you want the index of the maximum, use `idxmax`. This is the equivalent of\nthe `numpy.ndarray` method `argmax`.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a scalar.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\nSee also\n\nReturn the sum.\n\nReturn the minimum.\n\nReturn the maximum.\n\nReturn the index of the minimum.\n\nReturn the index of the maximum.\n\nReturn the sum over the requested axis.\n\nReturn the minimum over the requested axis.\n\nReturn the maximum over the requested axis.\n\nReturn the index of the minimum over the requested axis.\n\nReturn the index of the maximum over the requested axis.\n\nExamples\n\n"}, {"name": "pandas.Series.mean", "path": "reference/api/pandas.series.mean", "type": "Series", "text": "\nReturn the mean of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a scalar.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.Series.median", "path": "reference/api/pandas.series.median", "type": "Series", "text": "\nReturn the median of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a scalar.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.Series.memory_usage", "path": "reference/api/pandas.series.memory_usage", "type": "Series", "text": "\nReturn the memory usage of the Series.\n\nThe memory usage can optionally include the contribution of the index and of\nelements of object dtype.\n\nSpecifies whether to include the memory usage of the Series index.\n\nIf True, introspect the data deeply by interrogating object dtypes for system-\nlevel memory consumption, and include it in the returned value.\n\nBytes of memory consumed.\n\nSee also\n\nTotal bytes consumed by the elements of the array.\n\nBytes consumed by a DataFrame.\n\nExamples\n\nNot including the index gives the size of the rest of the data, which is\nnecessarily smaller:\n\nThe memory footprint of object values is ignored by default:\n\n"}, {"name": "pandas.Series.min", "path": "reference/api/pandas.series.min", "type": "Series", "text": "\nReturn the minimum of the values over the requested axis.\n\nIf you want the index of the minimum, use `idxmin`. This is the equivalent of\nthe `numpy.ndarray` method `argmin`.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a scalar.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\nSee also\n\nReturn the sum.\n\nReturn the minimum.\n\nReturn the maximum.\n\nReturn the index of the minimum.\n\nReturn the index of the maximum.\n\nReturn the sum over the requested axis.\n\nReturn the minimum over the requested axis.\n\nReturn the maximum over the requested axis.\n\nReturn the index of the minimum over the requested axis.\n\nReturn the index of the maximum over the requested axis.\n\nExamples\n\n"}, {"name": "pandas.Series.mod", "path": "reference/api/pandas.series.mod", "type": "Series", "text": "\nReturn Modulo of series and other, element-wise (binary operator mod).\n\nEquivalent to `series % other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nReverse of the Modulo operator, see Python documentation for more details.\n\nExamples\n\n"}, {"name": "pandas.Series.mode", "path": "reference/api/pandas.series.mode", "type": "Series", "text": "\nReturn the mode(s) of the Series.\n\nThe mode is the value that appears most often. There can be multiple modes.\n\nAlways returns Series even if only one value is returned.\n\nDon\u2019t consider counts of NaN/NaT.\n\nModes of the Series in sorted order.\n\n"}, {"name": "pandas.Series.mul", "path": "reference/api/pandas.series.mul", "type": "Series", "text": "\nReturn Multiplication of series and other, element-wise (binary operator mul).\n\nEquivalent to `series * other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nReverse of the Multiplication operator, see Python documentation for more\ndetails.\n\nExamples\n\n"}, {"name": "pandas.Series.multiply", "path": "reference/api/pandas.series.multiply", "type": "Series", "text": "\nReturn Multiplication of series and other, element-wise (binary operator mul).\n\nEquivalent to `series * other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nReverse of the Multiplication operator, see Python documentation for more\ndetails.\n\nExamples\n\n"}, {"name": "pandas.Series.name", "path": "reference/api/pandas.series.name", "type": "Series", "text": "\nReturn the name of the Series.\n\nThe name of a Series becomes its index or column name if it is used to form a\nDataFrame. It is also used whenever displaying the Series using the\ninterpreter.\n\nThe name of the Series, also the column name if part of a DataFrame.\n\nSee also\n\nSets the Series name when given a scalar input.\n\nCorresponding Index property.\n\nExamples\n\nThe Series name can be set initially when calling the constructor.\n\nThe name of a Series within a DataFrame is its column name.\n\n"}, {"name": "pandas.Series.nbytes", "path": "reference/api/pandas.series.nbytes", "type": "Series", "text": "\nReturn the number of bytes in the underlying data.\n\n"}, {"name": "pandas.Series.ndim", "path": "reference/api/pandas.series.ndim", "type": "Series", "text": "\nNumber of dimensions of the underlying data, by definition 1.\n\n"}, {"name": "pandas.Series.ne", "path": "reference/api/pandas.series.ne", "type": "Series", "text": "\nReturn Not equal to of series and other, element-wise (binary operator ne).\n\nEquivalent to `series != other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nExamples\n\n"}, {"name": "pandas.Series.nlargest", "path": "reference/api/pandas.series.nlargest", "type": "Series", "text": "\nReturn the largest n elements.\n\nReturn this many descending sorted values.\n\nWhen there are duplicate values that cannot all fit in a Series of n elements:\n\n`first` : return the first n occurrences in order of appearance.\n\n`last` : return the last n occurrences in reverse order of appearance.\n\n`all` : keep all occurrences. This can result in a Series of size larger than\nn.\n\nThe n largest values in the Series, sorted in decreasing order.\n\nSee also\n\nGet the n smallest elements.\n\nSort Series by values.\n\nReturn the first n rows.\n\nNotes\n\nFaster than `.sort_values(ascending=False).head(n)` for small n relative to\nthe size of the `Series` object.\n\nExamples\n\nThe n largest elements where `n=5` by default.\n\nThe n largest elements where `n=3`. Default keep value is \u2018first\u2019 so Malta\nwill be kept.\n\nThe n largest elements where `n=3` and keeping the last duplicates. Brunei\nwill be kept since it is the last with value 434000 based on the index order.\n\nThe n largest elements where `n=3` with all duplicates kept. Note that the\nreturned Series has five elements due to the three duplicates.\n\n"}, {"name": "pandas.Series.notna", "path": "reference/api/pandas.series.notna", "type": "Series", "text": "\nDetect existing (non-missing) values.\n\nReturn a boolean same-sized object indicating if the values are not NA. Non-\nmissing values get mapped to True. Characters such as empty strings `''` or\n`numpy.inf` are not considered NA values (unless you set\n`pandas.options.mode.use_inf_as_na = True`). NA values, such as None or\n`numpy.NaN`, get mapped to False values.\n\nMask of bool values for each element in Series that indicates whether an\nelement is not an NA value.\n\nSee also\n\nAlias of notna.\n\nBoolean inverse of notna.\n\nOmit axes labels with missing values.\n\nTop-level notna.\n\nExamples\n\nShow which entries in a DataFrame are not NA.\n\nShow which entries in a Series are not NA.\n\n"}, {"name": "pandas.Series.notnull", "path": "reference/api/pandas.series.notnull", "type": "Series", "text": "\nSeries.notnull is an alias for Series.notna.\n\nDetect existing (non-missing) values.\n\nReturn a boolean same-sized object indicating if the values are not NA. Non-\nmissing values get mapped to True. Characters such as empty strings `''` or\n`numpy.inf` are not considered NA values (unless you set\n`pandas.options.mode.use_inf_as_na = True`). NA values, such as None or\n`numpy.NaN`, get mapped to False values.\n\nMask of bool values for each element in Series that indicates whether an\nelement is not an NA value.\n\nSee also\n\nAlias of notna.\n\nBoolean inverse of notna.\n\nOmit axes labels with missing values.\n\nTop-level notna.\n\nExamples\n\nShow which entries in a DataFrame are not NA.\n\nShow which entries in a Series are not NA.\n\n"}, {"name": "pandas.Series.nsmallest", "path": "reference/api/pandas.series.nsmallest", "type": "Series", "text": "\nReturn the smallest n elements.\n\nReturn this many ascending sorted values.\n\nWhen there are duplicate values that cannot all fit in a Series of n elements:\n\n`first` : return the first n occurrences in order of appearance.\n\n`last` : return the last n occurrences in reverse order of appearance.\n\n`all` : keep all occurrences. This can result in a Series of size larger than\nn.\n\nThe n smallest values in the Series, sorted in increasing order.\n\nSee also\n\nGet the n largest elements.\n\nSort Series by values.\n\nReturn the first n rows.\n\nNotes\n\nFaster than `.sort_values().head(n)` for small n relative to the size of the\n`Series` object.\n\nExamples\n\nThe n smallest elements where `n=5` by default.\n\nThe n smallest elements where `n=3`. Default keep value is \u2018first\u2019 so Nauru\nand Tuvalu will be kept.\n\nThe n smallest elements where `n=3` and keeping the last duplicates. Anguilla\nand Tuvalu will be kept since they are the last with value 11300 based on the\nindex order.\n\nThe n smallest elements where `n=3` with all duplicates kept. Note that the\nreturned Series has four elements due to the three duplicates.\n\n"}, {"name": "pandas.Series.nunique", "path": "reference/api/pandas.series.nunique", "type": "Series", "text": "\nReturn number of unique elements in the object.\n\nExcludes NA values by default.\n\nDon\u2019t include NaN in the count.\n\nSee also\n\nMethod nunique for DataFrame.\n\nCount non-NA/null observations in the Series.\n\nExamples\n\n"}, {"name": "pandas.Series.pad", "path": "reference/api/pandas.series.pad", "type": "Series", "text": "\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\nObject with missing values filled or None if `inplace=True`.\n\n"}, {"name": "pandas.Series.pct_change", "path": "reference/api/pandas.series.pct_change", "type": "Series", "text": "\nPercentage change between the current and a prior element.\n\nComputes the percentage change from the immediately previous row by default.\nThis is useful in comparing the percentage of change in a time series of\nelements.\n\nPeriods to shift for forming percent change.\n\nHow to handle NAs before computing percent changes.\n\nThe number of consecutive NAs to fill before stopping.\n\nIncrement to use from time series API (e.g. \u2018M\u2019 or BDay()).\n\nAdditional keyword arguments are passed into DataFrame.shift or Series.shift.\n\nThe same type as the calling object.\n\nSee also\n\nCompute the difference of two elements in a Series.\n\nCompute the difference of two elements in a DataFrame.\n\nShift the index by some number of periods.\n\nShift the index by some number of periods.\n\nExamples\n\nSeries\n\nSee the percentage change in a Series where filling NAs with last valid\nobservation forward to next valid.\n\nDataFrame\n\nPercentage change in French franc, Deutsche Mark, and Italian lira from\n1980-01-01 to 1980-03-01.\n\nPercentage of change in GOOG and APPL stock volume. Shows computing the\npercentage change between columns.\n\n"}, {"name": "pandas.Series.pipe", "path": "reference/api/pandas.series.pipe", "type": "Series", "text": "\nApply chainable functions that expect Series or DataFrames.\n\nFunction to apply to the Series/DataFrame. `args`, and `kwargs` are passed\ninto `func`. Alternatively a `(callable, data_keyword)` tuple where\n`data_keyword` is a string indicating the keyword of `callable` that expects\nthe Series/DataFrame.\n\nPositional arguments passed into `func`.\n\nA dictionary of keyword arguments passed into `func`.\n\nSee also\n\nApply a function along input axis of DataFrame.\n\nApply a function elementwise on a whole DataFrame.\n\nApply a mapping correspondence on a `Series`.\n\nNotes\n\nUse `.pipe` when chaining together functions that expect Series, DataFrames or\nGroupBy objects. Instead of writing\n\nYou can write\n\nIf you have a function that takes the data as (say) the second argument, pass\na tuple indicating which keyword expects the data. For example, suppose `f`\ntakes its data as `arg2`:\n\n"}, {"name": "pandas.Series.plot", "path": "reference/api/pandas.series.plot", "type": "Series", "text": "\nMake plots of Series or DataFrame.\n\nUses the backend specified by the option `plotting.backend`. By default,\nmatplotlib is used.\n\nThe object for which the method is called.\n\nOnly used if data is a DataFrame.\n\nAllows plotting of one column versus another. Only used if data is a\nDataFrame.\n\nThe kind of plot to produce:\n\n\u2018line\u2019 : line plot (default)\n\n\u2018bar\u2019 : vertical bar plot\n\n\u2018barh\u2019 : horizontal bar plot\n\n\u2018hist\u2019 : histogram\n\n\u2018box\u2019 : boxplot\n\n\u2018kde\u2019 : Kernel Density Estimation plot\n\n\u2018density\u2019 : same as \u2018kde\u2019\n\n\u2018area\u2019 : area plot\n\n\u2018pie\u2019 : pie plot\n\n\u2018scatter\u2019 : scatter plot (DataFrame only)\n\n\u2018hexbin\u2019 : hexbin plot (DataFrame only)\n\nAn axes of the current figure.\n\nMake separate subplots for each column.\n\nIn case `subplots=True`, share x axis and set some x axis labels to invisible;\ndefaults to True if ax is None otherwise False if an ax is passed in; Be\naware, that passing in both an ax and `sharex=True` will alter all x axis\nlabels for all axis in a figure.\n\nIn case `subplots=True`, share y axis and set some y axis labels to invisible.\n\n(rows, columns) for the layout of subplots.\n\nSize of a figure object.\n\nUse index as ticks for x axis.\n\nTitle to use for the plot. If a string is passed, print the string at the top\nof the figure. If a list is passed and subplots is True, print each item in\nthe list above the corresponding subplot.\n\nAxis grid lines.\n\nPlace legend on axis subplots.\n\nThe matplotlib line style per column.\n\nUse log scaling or symlog scaling on x axis. .. versionchanged:: 0.25.0\n\nUse log scaling or symlog scaling on y axis. .. versionchanged:: 0.25.0\n\nUse log scaling or symlog scaling on both x and y axes. .. versionchanged::\n0.25.0\n\nValues to use for the xticks.\n\nValues to use for the yticks.\n\nSet the x limits of the current axes.\n\nSet the y limits of the current axes.\n\nName to use for the xlabel on x-axis. Default uses index name as xlabel, or\nthe x-column name for planar plots.\n\nNew in version 1.1.0.\n\nChanged in version 1.2.0: Now applicable to planar plots (scatter, hexbin).\n\nName to use for the ylabel on y-axis. Default will show no ylabel, or the\ny-column name for planar plots.\n\nNew in version 1.1.0.\n\nChanged in version 1.2.0: Now applicable to planar plots (scatter, hexbin).\n\nRotation for ticks (xticks for vertical, yticks for horizontal plots).\n\nFont size for xticks and yticks.\n\nColormap to select colors from. If string, load colormap with that name from\nmatplotlib.\n\nIf True, plot colorbar (only relevant for \u2018scatter\u2019 and \u2018hexbin\u2019 plots).\n\nSpecify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1\n(right/top-end). Default is 0.5 (center).\n\nIf True, draw a table using the data in the DataFrame and the data will be\ntransposed to meet matplotlib\u2019s default layout. If a Series or DataFrame is\npassed, use passed data to draw a table.\n\nSee Plotting with Error Bars for detail.\n\nEquivalent to yerr.\n\nIf True, create stacked plot.\n\nSort column names to determine plot ordering.\n\nWhether to plot on the secondary y-axis if a list/tuple, which columns to plot\non secondary y-axis.\n\nWhen using a secondary_y axis, automatically mark the column labels with\n\u201c(right)\u201d in the legend.\n\nIf True, boolean values can be plotted.\n\nBackend to use instead of the backend specified in the option\n`plotting.backend`. For instance, \u2018matplotlib\u2019. Alternatively, to specify the\n`plotting.backend` for the whole session, set `pd.options.plotting.backend`.\n\nNew in version 1.0.0.\n\nOptions to pass to matplotlib plotting method.\n\nIf the backend is not the default matplotlib one, the return value will be the\nobject returned by the backend.\n\nNotes\n\nSee matplotlib documentation online for more on this subject\n\nIf kind = \u2018bar\u2019 or \u2018barh\u2019, you can specify relative alignments for bar plot\nlayout by position keyword. From 0 (left/bottom-end) to 1 (right/top-end).\nDefault is 0.5 (center)\n\n"}, {"name": "pandas.Series.plot.area", "path": "reference/api/pandas.series.plot.area", "type": "Series", "text": "\nDraw a stacked area plot.\n\nAn area plot displays quantitative data visually. This function wraps the\nmatplotlib area function.\n\nCoordinates for the X axis. By default uses the index.\n\nColumn to plot. By default uses all columns.\n\nArea plots are stacked by default. Set to False to create a unstacked plot.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nArea plot, or array of area plots if subplots is True.\n\nSee also\n\nMake plots of DataFrame using matplotlib / pylab.\n\nExamples\n\nDraw an area plot based on basic business metrics:\n\nArea plots are stacked by default. To produce an unstacked plot, pass\n`stacked=False`:\n\nDraw an area plot for a single column:\n\nDraw with a different x:\n\n"}, {"name": "pandas.Series.plot.bar", "path": "reference/api/pandas.series.plot.bar", "type": "Series", "text": "\nVertical bar plot.\n\nA bar plot is a plot that presents categorical data with rectangular bars with\nlengths proportional to the values that they represent. A bar plot shows\ncomparisons among discrete categories. One axis of the plot shows the specific\ncategories being compared, and the other axis represents a measured value.\n\nAllows plotting of one column versus another. If not specified, the index of\nthe DataFrame is used.\n\nAllows plotting of one column versus another. If not specified, all numerical\ncolumns are used.\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.\n\ncode, which will be used for each column recursively. For instance\n[\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow,\nalternatively. If there is only a single column to be plotted, then only the\nfirst color from the color list will be used.\n\ncolored accordingly. For example, if your columns are called a and b, then\npassing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and\nbars for column b in red.\n\nNew in version 1.1.0.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nAn ndarray is returned with one `matplotlib.axes.Axes` per column when\n`subplots=True`.\n\nSee also\n\nHorizontal bar plot.\n\nMake plots of a DataFrame.\n\nMake a bar plot with matplotlib.\n\nExamples\n\nBasic plot.\n\nPlot a whole dataframe to a bar plot. Each column is assigned a distinct\ncolor, and each row is nested in a group along the horizontal axis.\n\nPlot stacked bar charts for the DataFrame\n\nInstead of nesting, the figure can be split by column with `subplots=True`. In\nthis case, a `numpy.ndarray` of `matplotlib.axes.Axes` are returned.\n\nIf you don\u2019t like the default colours, you can specify how you\u2019d like each\ncolumn to be colored.\n\nPlot a single column.\n\nPlot only selected categories for the DataFrame.\n\n"}, {"name": "pandas.Series.plot.barh", "path": "reference/api/pandas.series.plot.barh", "type": "Series", "text": "\nMake a horizontal bar plot.\n\nA horizontal bar plot is a plot that presents quantitative data with\nrectangular bars with lengths proportional to the values that they represent.\nA bar plot shows comparisons among discrete categories. One axis of the plot\nshows the specific categories being compared, and the other axis represents a\nmeasured value.\n\nAllows plotting of one column versus another. If not specified, the index of\nthe DataFrame is used.\n\nAllows plotting of one column versus another. If not specified, all numerical\ncolumns are used.\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.\n\ncode, which will be used for each column recursively. For instance\n[\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow,\nalternatively. If there is only a single column to be plotted, then only the\nfirst color from the color list will be used.\n\ncolored accordingly. For example, if your columns are called a and b, then\npassing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and\nbars for column b in red.\n\nNew in version 1.1.0.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nAn ndarray is returned with one `matplotlib.axes.Axes` per column when\n`subplots=True`.\n\nSee also\n\nVertical bar plot.\n\nMake plots of DataFrame using matplotlib.\n\nPlot a vertical bar plot using matplotlib.\n\nExamples\n\nBasic example\n\nPlot a whole DataFrame to a horizontal bar plot\n\nPlot stacked barh charts for the DataFrame\n\nWe can specify colors for each column\n\nPlot a column of the DataFrame to a horizontal bar plot\n\nPlot DataFrame versus the desired column\n\n"}, {"name": "pandas.Series.plot.box", "path": "reference/api/pandas.series.plot.box", "type": "Series", "text": "\nMake a box plot of the DataFrame columns.\n\nA box plot is a method for graphically depicting groups of numerical data\nthrough their quartiles. The box extends from the Q1 to Q3 quartile values of\nthe data, with a line at the median (Q2). The whiskers extend from the edges\nof box to show the range of the data. The position of the whiskers is set by\ndefault to 1.5*IQR (IQR = Q3 - Q1) from the edges of the box. Outlier points\nare those past the end of the whiskers.\n\nFor further details see Wikipedia\u2019s entry for boxplot.\n\nA consideration when using this chart is that the box and the whiskers can\noverlap, which is very common when plotting small sets of data.\n\nColumn in the DataFrame to group by.\n\nChanged in version 1.4.0: Previously, by is silently ignore and makes no\ngroupings\n\nAdditional keywords are documented in `DataFrame.plot()`.\n\nSee also\n\nAnother method to draw a box plot.\n\nDraw a box plot from a Series object.\n\nDraw a box plot in matplotlib.\n\nExamples\n\nDraw a box plot from a DataFrame with four columns of randomly generated data.\n\nYou can also generate groupings if you specify the by parameter (which can\ntake a column name, or a list or tuple of column names):\n\nChanged in version 1.4.0.\n\n"}, {"name": "pandas.Series.plot.density", "path": "reference/api/pandas.series.plot.density", "type": "Series", "text": "\nGenerate Kernel Density Estimate plot using Gaussian kernels.\n\nIn statistics, kernel density estimation (KDE) is a non-parametric way to\nestimate the probability density function (PDF) of a random variable. This\nfunction uses Gaussian kernels and includes automatic bandwidth determination.\n\nThe method used to calculate the estimator bandwidth. This can be \u2018scott\u2019,\n\u2018silverman\u2019, a scalar constant or a callable. If None (default), \u2018scott\u2019 is\nused. See `scipy.stats.gaussian_kde` for more information.\n\nEvaluation points for the estimated PDF. If None (default), 1000 equally\nspaced points are used. If ind is a NumPy array, the KDE is evaluated at the\npoints passed. If ind is an integer, ind number of equally spaced points are\nused.\n\nAdditional keyword arguments are documented in `pandas.%(this-\ndatatype)s.plot()`.\n\nSee also\n\nRepresentation of a kernel-density estimate using Gaussian kernels. This is\nthe function used internally to estimate the PDF.\n\nExamples\n\nGiven a Series of points randomly sampled from an unknown distribution,\nestimate its PDF using KDE with automatic bandwidth determination and plot the\nresults, evaluating them at 1000 equally spaced points (default):\n\nA scalar bandwidth can be specified. Using a small bandwidth value can lead to\nover-fitting, while using a large bandwidth value may result in under-fitting:\n\nFinally, the ind parameter determines the evaluation points for the plot of\nthe estimated PDF:\n\nFor DataFrame, it works in the same way:\n\nA scalar bandwidth can be specified. Using a small bandwidth value can lead to\nover-fitting, while using a large bandwidth value may result in under-fitting:\n\nFinally, the ind parameter determines the evaluation points for the plot of\nthe estimated PDF:\n\n"}, {"name": "pandas.Series.plot.hist", "path": "reference/api/pandas.series.plot.hist", "type": "Series", "text": "\nDraw one histogram of the DataFrame\u2019s columns.\n\nA histogram is a representation of the distribution of data. This function\ngroups the values of all given Series in the DataFrame into bins and draws all\nbins in one `matplotlib.axes.Axes`. This is useful when the DataFrame\u2019s Series\nare in a similar scale.\n\nColumn in the DataFrame to group by.\n\nChanged in version 1.4.0: Previously, by is silently ignore and makes no\ngroupings\n\nNumber of histogram bins to be used.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nReturn a histogram plot.\n\nSee also\n\nDraw histograms per DataFrame\u2019s Series.\n\nDraw a histogram with Series\u2019 data.\n\nExamples\n\nWhen we roll a die 6000 times, we expect to get each value around 1000 times.\nBut when we roll two dice and sum the result, the distribution is going to be\nquite different. A histogram illustrates those distributions.\n\nA grouped histogram can be generated by providing the parameter by (which can\nbe a column name, or a list of column names):\n\n"}, {"name": "pandas.Series.plot.kde", "path": "reference/api/pandas.series.plot.kde", "type": "Series", "text": "\nGenerate Kernel Density Estimate plot using Gaussian kernels.\n\nIn statistics, kernel density estimation (KDE) is a non-parametric way to\nestimate the probability density function (PDF) of a random variable. This\nfunction uses Gaussian kernels and includes automatic bandwidth determination.\n\nThe method used to calculate the estimator bandwidth. This can be \u2018scott\u2019,\n\u2018silverman\u2019, a scalar constant or a callable. If None (default), \u2018scott\u2019 is\nused. See `scipy.stats.gaussian_kde` for more information.\n\nEvaluation points for the estimated PDF. If None (default), 1000 equally\nspaced points are used. If ind is a NumPy array, the KDE is evaluated at the\npoints passed. If ind is an integer, ind number of equally spaced points are\nused.\n\nAdditional keyword arguments are documented in `pandas.%(this-\ndatatype)s.plot()`.\n\nSee also\n\nRepresentation of a kernel-density estimate using Gaussian kernels. This is\nthe function used internally to estimate the PDF.\n\nExamples\n\nGiven a Series of points randomly sampled from an unknown distribution,\nestimate its PDF using KDE with automatic bandwidth determination and plot the\nresults, evaluating them at 1000 equally spaced points (default):\n\nA scalar bandwidth can be specified. Using a small bandwidth value can lead to\nover-fitting, while using a large bandwidth value may result in under-fitting:\n\nFinally, the ind parameter determines the evaluation points for the plot of\nthe estimated PDF:\n\nFor DataFrame, it works in the same way:\n\nA scalar bandwidth can be specified. Using a small bandwidth value can lead to\nover-fitting, while using a large bandwidth value may result in under-fitting:\n\nFinally, the ind parameter determines the evaluation points for the plot of\nthe estimated PDF:\n\n"}, {"name": "pandas.Series.plot.line", "path": "reference/api/pandas.series.plot.line", "type": "Series", "text": "\nPlot Series or DataFrame as lines.\n\nThis function is useful to plot lines using DataFrame\u2019s values as coordinates.\n\nAllows plotting of one column versus another. If not specified, the index of\nthe DataFrame is used.\n\nAllows plotting of one column versus another. If not specified, all numerical\ncolumns are used.\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.\n\ncode, which will be used for each column recursively. For instance\n[\u2018green\u2019,\u2019yellow\u2019] each column\u2019s line will be filled in green or yellow,\nalternatively. If there is only a single column to be plotted, then only the\nfirst color from the color list will be used.\n\ncolored accordingly. For example, if your columns are called a and b, then\npassing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color lines for column a in green and\nlines for column b in red.\n\nNew in version 1.1.0.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nAn ndarray is returned with one `matplotlib.axes.Axes` per column when\n`subplots=True`.\n\nSee also\n\nPlot y versus x as lines and/or markers.\n\nExamples\n\nThe following example shows the populations for some animals over the years.\n\nAn example with subplots, so an array of axes is returned.\n\nLet\u2019s repeat the same example, but specifying colors for each column (in this\ncase, for each animal).\n\nThe following example shows the relationship between both populations.\n\n"}, {"name": "pandas.Series.plot.pie", "path": "reference/api/pandas.series.plot.pie", "type": "Series", "text": "\nGenerate a pie plot.\n\nA pie plot is a proportional representation of the numerical data in a column.\nThis function wraps `matplotlib.pyplot.pie()` for the specified column. If no\ncolumn reference is passed and `subplots=True` a pie plot is drawn for each\nnumerical column independently.\n\nLabel or position of the column to plot. If not provided, `subplots=True`\nargument must be passed.\n\nKeyword arguments to pass on to `DataFrame.plot()`.\n\nA NumPy array is returned when subplots is True.\n\nSee also\n\nGenerate a pie plot for a Series.\n\nMake plots of a DataFrame.\n\nExamples\n\nIn the example below we have a DataFrame with the information about planet\u2019s\nmass and radius. We pass the \u2018mass\u2019 column to the pie function to get a pie\nplot.\n\n"}, {"name": "pandas.Series.pop", "path": "reference/api/pandas.series.pop", "type": "Series", "text": "\nReturn item and drops from series. Raise KeyError if not found.\n\nIndex of the element that needs to be removed.\n\nExamples\n\n"}, {"name": "pandas.Series.pow", "path": "reference/api/pandas.series.pow", "type": "Series", "text": "\nReturn Exponential power of series and other, element-wise (binary operator\npow).\n\nEquivalent to `series ** other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nReverse of the Exponential power operator, see Python documentation for more\ndetails.\n\nExamples\n\n"}, {"name": "pandas.Series.prod", "path": "reference/api/pandas.series.prod", "type": "Series", "text": "\nReturn the product of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a scalar.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nAdditional keyword arguments to be passed to the function.\n\nSee also\n\nReturn the sum.\n\nReturn the minimum.\n\nReturn the maximum.\n\nReturn the index of the minimum.\n\nReturn the index of the maximum.\n\nReturn the sum over the requested axis.\n\nReturn the minimum over the requested axis.\n\nReturn the maximum over the requested axis.\n\nReturn the index of the minimum over the requested axis.\n\nReturn the index of the maximum over the requested axis.\n\nExamples\n\nBy default, the product of an empty or all-NA Series is `1`\n\nThis can be controlled with the `min_count` parameter\n\nThanks to the `skipna` parameter, `min_count` handles all-NA and empty series\nidentically.\n\n"}, {"name": "pandas.Series.product", "path": "reference/api/pandas.series.product", "type": "Series", "text": "\nReturn the product of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a scalar.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nAdditional keyword arguments to be passed to the function.\n\nSee also\n\nReturn the sum.\n\nReturn the minimum.\n\nReturn the maximum.\n\nReturn the index of the minimum.\n\nReturn the index of the maximum.\n\nReturn the sum over the requested axis.\n\nReturn the minimum over the requested axis.\n\nReturn the maximum over the requested axis.\n\nReturn the index of the minimum over the requested axis.\n\nReturn the index of the maximum over the requested axis.\n\nExamples\n\nBy default, the product of an empty or all-NA Series is `1`\n\nThis can be controlled with the `min_count` parameter\n\nThanks to the `skipna` parameter, `min_count` handles all-NA and empty series\nidentically.\n\n"}, {"name": "pandas.Series.quantile", "path": "reference/api/pandas.series.quantile", "type": "Series", "text": "\nReturn value at the given quantile.\n\nThe quantile(s) to compute, which can lie in range: 0 <= q <= 1.\n\nThis optional parameter specifies the interpolation method to use, when the\ndesired quantile lies between two data points i and j:\n\nlinear: i + (j - i) * fraction, where fraction is the fractional part of the\nindex surrounded by i and j.\n\nlower: i.\n\nhigher: j.\n\nnearest: i or j whichever is nearest.\n\nmidpoint: (i \\+ j) / 2.\n\nIf `q` is an array, a Series will be returned where the index is `q` and the\nvalues are the quantiles, otherwise a float will be returned.\n\nSee also\n\nCalculate the rolling quantile.\n\nReturns the q-th percentile(s) of the array elements.\n\nExamples\n\n"}, {"name": "pandas.Series.radd", "path": "reference/api/pandas.series.radd", "type": "Series", "text": "\nReturn Addition of series and other, element-wise (binary operator radd).\n\nEquivalent to `other + series`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nElement-wise Addition, see Python documentation for more details.\n\nExamples\n\n"}, {"name": "pandas.Series.rank", "path": "reference/api/pandas.series.rank", "type": "Series", "text": "\nCompute numerical data ranks (1 through n) along axis.\n\nBy default, equal values are assigned a rank that is the average of the ranks\nof those values.\n\nIndex to direct ranking.\n\nHow to rank the group of records that have the same value (i.e. ties):\n\naverage: average rank of the group\n\nmin: lowest rank in the group\n\nmax: highest rank in the group\n\nfirst: ranks assigned in order they appear in the array\n\ndense: like \u2018min\u2019, but rank always increases by 1 between groups.\n\nFor DataFrame objects, rank only numeric columns if set to True.\n\nHow to rank NaN values:\n\nkeep: assign NaN rank to NaN values\n\ntop: assign lowest rank to NaN values\n\nbottom: assign highest rank to NaN values\n\nWhether or not the elements should be ranked in ascending order.\n\nWhether or not to display the returned rankings in percentile form.\n\nReturn a Series or DataFrame with data ranks as values.\n\nSee also\n\nRank of values within each group.\n\nExamples\n\nThe following example shows how the method behaves with the above parameters:\n\ndefault_rank: this is the default behaviour obtained without using any\nparameter.\n\nmax_rank: setting `method = 'max'` the records that have the same values are\nranked using the highest rank (e.g.: since \u2018cat\u2019 and \u2018dog\u2019 are both in the 2nd\nand 3rd position, rank 3 is assigned.)\n\nNA_bottom: choosing `na_option = 'bottom'`, if there are records with NaN\nvalues they are placed at the bottom of the ranking.\n\npct_rank: when setting `pct = True`, the ranking is expressed as percentile\nrank.\n\n"}, {"name": "pandas.Series.ravel", "path": "reference/api/pandas.series.ravel", "type": "Series", "text": "\nReturn the flattened underlying data as an ndarray.\n\nFlattened data of the Series.\n\nSee also\n\nReturn a flattened array.\n\n"}, {"name": "pandas.Series.rdiv", "path": "reference/api/pandas.series.rdiv", "type": "Series", "text": "\nReturn Floating division of series and other, element-wise (binary operator\nrtruediv).\n\nEquivalent to `other / series`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nElement-wise Floating division, see Python documentation for more details.\n\nExamples\n\n"}, {"name": "pandas.Series.rdivmod", "path": "reference/api/pandas.series.rdivmod", "type": "Series", "text": "\nReturn Integer division and modulo of series and other, element-wise (binary\noperator rdivmod).\n\nEquivalent to `other divmod series`, but with support to substitute a\nfill_value for missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nElement-wise Integer division and modulo, see Python documentation for more\ndetails.\n\nExamples\n\n"}, {"name": "pandas.Series.reindex", "path": "reference/api/pandas.series.reindex", "type": "Series", "text": "\nConform Series to new index with optional filling logic.\n\nPlaces NA/NaN in locations having no value in the previous index. A new object\nis produced unless the new index is equivalent to the current one and\n`copy=False`.\n\nNew labels / index to conform to, should be specified using keywords.\nPreferably an Index object to avoid duplicating data.\n\nMethod to use for filling holes in reindexed DataFrame. Please note: this is\nonly applicable to DataFrames/Series with a monotonically\nincreasing/decreasing index.\n\nNone (default): don\u2019t fill gaps\n\npad / ffill: Propagate last valid observation forward to next valid.\n\nbackfill / bfill: Use next valid observation to fill gap.\n\nnearest: Use nearest valid observations to fill gap.\n\nReturn a new object, even if the passed indexes are the same.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nValue to use for missing values. Defaults to NaN, but can be any \u201ccompatible\u201d\nvalue.\n\nMaximum number of consecutive elements to forward or backward fill.\n\nMaximum distance between original and new labels for inexact matches. The\nvalues of the index at the matching locations most satisfy the equation\n`abs(index[indexer] - target) <= tolerance`.\n\nTolerance may be a scalar value, which applies the same tolerance to all\nvalues, or list-like, which applies variable tolerance per element. List-like\nincludes list, tuple, array, Series, and must be the same size as the index\nand its dtype must exactly match the index\u2019s type.\n\nSee also\n\nSet row labels.\n\nRemove row labels or move them to new columns.\n\nChange to same indices as other DataFrame.\n\nExamples\n\n`DataFrame.reindex` supports two calling conventions\n\n`(index=index_labels, columns=column_labels, ...)`\n\n`(labels, axis={'index', 'columns'}, ...)`\n\nWe highly recommend using keyword arguments to clarify your intent.\n\nCreate a dataframe with some fictional data.\n\nCreate a new index and reindex the dataframe. By default values in the new\nindex that do not have corresponding records in the dataframe are assigned\n`NaN`.\n\nWe can fill in the missing values by passing a value to the keyword\n`fill_value`. Because the index is not monotonically increasing or decreasing,\nwe cannot use arguments to the keyword `method` to fill the `NaN` values.\n\nWe can also reindex the columns.\n\nOr we can use \u201caxis-style\u201d keyword arguments\n\nTo further illustrate the filling functionality in `reindex`, we will create a\ndataframe with a monotonically increasing index (for example, a sequence of\ndates).\n\nSuppose we decide to expand the dataframe to cover a wider date range.\n\nThe index entries that did not have a value in the original data frame (for\nexample, \u20182009-12-29\u2019) are by default filled with `NaN`. If desired, we can\nfill in the missing values using one of several options.\n\nFor example, to back-propagate the last valid value to fill the `NaN` values,\npass `bfill` as an argument to the `method` keyword.\n\nPlease note that the `NaN` value present in the original dataframe (at index\nvalue 2010-01-03) will not be filled by any of the value propagation schemes.\nThis is because filling while reindexing does not look at dataframe values,\nbut only compares the original and desired indexes. If you do want to fill in\nthe `NaN` values present in the original dataframe, use the `fillna()` method.\n\nSee the user guide for more.\n\n"}, {"name": "pandas.Series.reindex_like", "path": "reference/api/pandas.series.reindex_like", "type": "Series", "text": "\nReturn an object with matching indices as other object.\n\nConform the object to the same index on all axes. Optional filling logic,\nplacing NaN in locations having no value in the previous index. A new object\nis produced unless the new index is equivalent to the current one and\ncopy=False.\n\nIts row and column indices are used to define the new indices of this object.\n\nMethod to use for filling holes in reindexed DataFrame. Please note: this is\nonly applicable to DataFrames/Series with a monotonically\nincreasing/decreasing index.\n\nNone (default): don\u2019t fill gaps\n\npad / ffill: propagate last valid observation forward to next valid\n\nbackfill / bfill: use next valid observation to fill gap\n\nnearest: use nearest valid observations to fill gap.\n\nReturn a new object, even if the passed indexes are the same.\n\nMaximum number of consecutive labels to fill for inexact matches.\n\nMaximum distance between original and new labels for inexact matches. The\nvalues of the index at the matching locations must satisfy the equation\n`abs(index[indexer] - target) <= tolerance`.\n\nTolerance may be a scalar value, which applies the same tolerance to all\nvalues, or list-like, which applies variable tolerance per element. List-like\nincludes list, tuple, array, Series, and must be the same size as the index\nand its dtype must exactly match the index\u2019s type.\n\nSame type as caller, but with changed indices on each axis.\n\nSee also\n\nSet row labels.\n\nRemove row labels or move them to new columns.\n\nChange to new indices or expand indices.\n\nNotes\n\nSame as calling `.reindex(index=other.index, columns=other.columns,...)`.\n\nExamples\n\n"}, {"name": "pandas.Series.rename", "path": "reference/api/pandas.series.rename", "type": "Series", "text": "\nAlter Series index labels or name.\n\nFunction / dict values must be unique (1-to-1). Labels not contained in a dict\n/ Series will be left as-is. Extra labels listed don\u2019t throw an error.\n\nAlternatively, change `Series.name` with a scalar value.\n\nSee the user guide for more.\n\nUnused. Accepted for compatibility with DataFrame method only.\n\nFunctions or dict-like are transformations to apply to the index. Scalar or\nhashable sequence-like will alter the `Series.name` attribute.\n\nAdditional keyword arguments passed to the function. Only the \u201cinplace\u201d\nkeyword is used.\n\nSeries with index labels or name altered or None if `inplace=True`.\n\nSee also\n\nCorresponding DataFrame method.\n\nSet the name of the axis.\n\nExamples\n\n"}, {"name": "pandas.Series.rename_axis", "path": "reference/api/pandas.series.rename_axis", "type": "Series", "text": "\nSet the name of the axis for the index or columns.\n\nValue to set the axis name attribute.\n\nA scalar, list-like, dict-like or functions transformations to apply to that\naxis\u2019 values. Note that the `columns` parameter is not allowed if the object\nis a Series. This parameter only apply for DataFrame type objects.\n\nUse either `mapper` and `axis` to specify the axis to target with `mapper`, or\n`index` and/or `columns`.\n\nThe axis to rename.\n\nAlso copy underlying data.\n\nModifies the object directly, instead of creating a new Series or DataFrame.\n\nThe same type as the caller or None if `inplace=True`.\n\nSee also\n\nAlter Series index labels or name.\n\nAlter DataFrame index labels or name.\n\nSet new names on index.\n\nNotes\n\n`DataFrame.rename_axis` supports two calling conventions\n\n`(index=index_mapper, columns=columns_mapper, ...)`\n\n`(mapper, axis={'index', 'columns'}, ...)`\n\nThe first calling convention will only modify the names of the index and/or\nthe names of the Index object that is the columns. In this case, the parameter\n`copy` is ignored.\n\nThe second calling convention will modify the names of the corresponding index\nif mapper is a list or a scalar. However, if mapper is dict-like or a\nfunction, it will use the deprecated behavior of modifying the axis labels.\n\nWe highly recommend using keyword arguments to clarify your intent.\n\nExamples\n\nSeries\n\nDataFrame\n\nMultiIndex\n\n"}, {"name": "pandas.Series.reorder_levels", "path": "reference/api/pandas.series.reorder_levels", "type": "Series", "text": "\nRearrange index levels using input order.\n\nMay not drop or duplicate levels.\n\nReference level by number or key.\n\n"}, {"name": "pandas.Series.repeat", "path": "reference/api/pandas.series.repeat", "type": "Series", "text": "\nRepeat elements of a Series.\n\nReturns a new Series where each element of the current Series is repeated\nconsecutively a given number of times.\n\nThe number of repetitions for each element. This should be a non-negative\ninteger. Repeating 0 times will return an empty Series.\n\nMust be `None`. Has no effect but is accepted for compatibility with numpy.\n\nNewly created Series with repeated elements.\n\nSee also\n\nEquivalent function for Index.\n\nSimilar method for `numpy.ndarray`.\n\nExamples\n\n"}, {"name": "pandas.Series.replace", "path": "reference/api/pandas.series.replace", "type": "Series", "text": "\nReplace values given in to_replace with value.\n\nValues of the Series are replaced with other values dynamically.\n\nThis differs from updating with `.loc` or `.iloc`, which require you to\nspecify a location to update with some value.\n\nHow to find the values that will be replaced.\n\nnumeric, str or regex:\n\nnumeric: numeric values equal to to_replace will be replaced with value\n\nstr: string exactly matching to_replace will be replaced with value\n\nregex: regexs matching to_replace will be replaced with value\n\nlist of str, regex, or numeric:\n\nFirst, if to_replace and value are both lists, they must be the same length.\n\nSecond, if `regex=True` then all of the strings in both lists will be\ninterpreted as regexs otherwise they will match directly. This doesn\u2019t matter\nmuch for value since there are only a few possible substitution regexes you\ncan use.\n\nstr, regex and numeric rules apply as above.\n\ndict:\n\nDicts can be used to specify different replacement values for different\nexisting values. For example, `{'a': 'b', 'y': 'z'}` replaces the value \u2018a\u2019\nwith \u2018b\u2019 and \u2018y\u2019 with \u2018z\u2019. To use a dict in this way the value parameter\nshould be None.\n\nFor a DataFrame a dict can specify that different values should be replaced in\ndifferent columns. For example, `{'a': 1, 'b': 'z'}` looks for the value 1 in\ncolumn \u2018a\u2019 and the value \u2018z\u2019 in column \u2018b\u2019 and replaces these values with\nwhatever is specified in value. The value parameter should not be `None` in\nthis case. You can treat this as a special case of passing two lists except\nthat you are specifying the column to search in.\n\nFor a DataFrame nested dictionaries, e.g., `{'a': {'b': np.nan}}`, are read as\nfollows: look in column \u2018a\u2019 for the value \u2018b\u2019 and replace it with NaN. The\nvalue parameter should be `None` to use a nested dict in this way. You can\nnest regular expressions as well. Note that column names (the top-level\ndictionary keys in a nested dictionary) cannot be regular expressions.\n\nNone:\n\nThis means that the regex argument must be a string, compiled regular\nexpression, or list, dict, ndarray or Series of such elements. If value is\nalso `None` then this must be a nested dictionary or Series.\n\nSee the examples section for examples of each of these.\n\nValue to replace any values matching to_replace with. For a DataFrame a dict\nof values can be used to specify which value to use for each column (columns\nnot in the dict will not be filled). Regular expressions, strings and lists or\ndicts of such objects are also allowed.\n\nIf True, performs operation inplace and returns None.\n\nMaximum size gap to forward or backward fill.\n\nWhether to interpret to_replace and/or value as regular expressions. If this\nis `True` then to_replace must be a string. Alternatively, this could be a\nregular expression or a list, dict, or array of regular expressions in which\ncase to_replace must be `None`.\n\nThe method to use when for replacement, when to_replace is a scalar, list or\ntuple and value is `None`.\n\nChanged in version 0.23.0: Added to DataFrame.\n\nObject after replacement.\n\nIf regex is not a `bool` and to_replace is not `None`.\n\nIf to_replace is not a scalar, array-like, `dict`, or `None`\n\nIf to_replace is a `dict` and value is not a `list`, `dict`, `ndarray`, or\n`Series`\n\nIf to_replace is `None` and regex is not compilable into a regular expression\nor is a list, dict, ndarray, or Series.\n\nWhen replacing multiple `bool` or `datetime64` objects and the arguments to\nto_replace does not match the type of the value being replaced\n\nIf a `list` or an `ndarray` is passed to to_replace and value but they are not\nthe same length.\n\nSee also\n\nFill NA values.\n\nReplace values based on boolean condition.\n\nSimple string replacement.\n\nNotes\n\nRegex substitution is performed under the hood with `re.sub`. The rules for\nsubstitution for `re.sub` are the same.\n\nRegular expressions will only substitute on strings, meaning you cannot\nprovide, for example, a regular expression matching floating point numbers and\nexpect the columns in your frame that have a numeric dtype to be matched.\nHowever, if those floating point numbers are strings, then you can do this.\n\nThis method has a lot of options. You are encouraged to experiment and play\nwith this method to gain intuition about how it works.\n\nWhen dict is used as the to_replace value, it is like key(s) in the dict are\nthe to_replace part and value(s) in the dict are the value parameter.\n\nExamples\n\nScalar `to_replace` and `value`\n\nList-like `to_replace`\n\ndict-like `to_replace`\n\nRegular expression `to_replace`\n\nCompare the behavior of `s.replace({'a': None})` and `s.replace('a', None)` to\nunderstand the peculiarities of the to_replace parameter:\n\nWhen one uses a dict as the to_replace value, it is like the value(s) in the\ndict are equal to the value parameter. `s.replace({'a': None})` is equivalent\nto `s.replace(to_replace={'a': None}, value=None, method=None)`:\n\nWhen `value` is not explicitly passed and to_replace is a scalar, list or\ntuple, replace uses the method parameter (default \u2018pad\u2019) to do the\nreplacement. So this is why the \u2018a\u2019 values are being replaced by 10 in rows 1\nand 2 and \u2018b\u2019 in row 4 in this case.\n\nOn the other hand, if `None` is explicitly passed for `value`, it will be\nrespected:\n\nChanged in version 1.4.0: Previously the explicit `None` was silently ignored.\n\n"}, {"name": "pandas.Series.resample", "path": "reference/api/pandas.series.resample", "type": "Series", "text": "\nResample time-series data.\n\nConvenience method for frequency conversion and resampling of time series. The\nobject must have a datetime-like index (DatetimeIndex, PeriodIndex, or\nTimedeltaIndex), or the caller must pass the label of a datetime-like\nseries/index to the `on`/`level` keyword parameter.\n\nThe offset string or object representing target conversion.\n\nWhich axis to use for up- or down-sampling. For Series this will default to 0,\ni.e. along the rows. Must be DatetimeIndex, TimedeltaIndex or PeriodIndex.\n\nWhich side of bin interval is closed. The default is \u2018left\u2019 for all frequency\noffsets except for \u2018M\u2019, \u2018A\u2019, \u2018Q\u2019, \u2018BM\u2019, \u2018BA\u2019, \u2018BQ\u2019, and \u2018W\u2019 which all have a\ndefault of \u2018right\u2019.\n\nWhich bin edge label to label bucket with. The default is \u2018left\u2019 for all\nfrequency offsets except for \u2018M\u2019, \u2018A\u2019, \u2018Q\u2019, \u2018BM\u2019, \u2018BA\u2019, \u2018BQ\u2019, and \u2018W\u2019 which\nall have a default of \u2018right\u2019.\n\nFor PeriodIndex only, controls whether to use the start or end of rule.\n\nPass \u2018timestamp\u2019 to convert the resulting index to a DateTimeIndex or \u2018period\u2019\nto convert it to a PeriodIndex. By default the input representation is\nretained.\n\nAdjust the resampled time labels.\n\nDeprecated since version 1.1.0: You should add the loffset to the df.index\nafter the resample. See below.\n\nFor frequencies that evenly subdivide 1 day, the \u201corigin\u201d of the aggregated\nintervals. For example, for \u20185min\u2019 frequency, base could range from 0 through\n4. Defaults to 0.\n\nDeprecated since version 1.1.0: The new arguments that you should use are\n\u2018offset\u2019 or \u2018origin\u2019.\n\nFor a DataFrame, column to use instead of index for resampling. Column must be\ndatetime-like.\n\nFor a MultiIndex, level (name or number) to use for resampling. level must be\ndatetime-like.\n\nThe timestamp on which to adjust the grouping. The timezone of origin must\nmatch the timezone of the index. If string, must be one of the following:\n\n\u2018epoch\u2019: origin is 1970-01-01\n\n\u2018start\u2019: origin is the first value of the timeseries\n\n\u2018start_day\u2019: origin is the first day at midnight of the timeseries\n\nNew in version 1.1.0.\n\n\u2018end\u2019: origin is the last value of the timeseries\n\n\u2018end_day\u2019: origin is the ceiling midnight of the last day\n\nNew in version 1.3.0.\n\nAn offset timedelta added to the origin.\n\nNew in version 1.1.0.\n\n`Resampler` object.\n\nSee also\n\nResample a Series.\n\nResample a DataFrame.\n\nGroup Series by mapping, function, label, or list of labels.\n\nReindex a Series with the given frequency without grouping.\n\nNotes\n\nSee the user guide for more.\n\nTo learn more about the offset strings, please see this link.\n\nExamples\n\nStart by creating a series with 9 one minute timestamps.\n\nDownsample the series into 3 minute bins and sum the values of the timestamps\nfalling into a bin.\n\nDownsample the series into 3 minute bins as above, but label each bin using\nthe right edge instead of the left. Please note that the value in the bucket\nused as the label is not included in the bucket, which it labels. For example,\nin the original series the bucket `2000-01-01 00:03:00` contains the value 3,\nbut the summed value in the resampled bucket with the label `2000-01-01\n00:03:00` does not include 3 (if it did, the summed value would be 6, not 3).\nTo include this value close the right side of the bin interval as illustrated\nin the example below this one.\n\nDownsample the series into 3 minute bins as above, but close the right side of\nthe bin interval.\n\nUpsample the series into 30 second bins.\n\nUpsample the series into 30 second bins and fill the `NaN` values using the\n`pad` method.\n\nUpsample the series into 30 second bins and fill the `NaN` values using the\n`bfill` method.\n\nPass a custom function via `apply`\n\nFor a Series with a PeriodIndex, the keyword convention can be used to control\nwhether to use the start or end of rule.\n\nResample a year by quarter using \u2018start\u2019 convention. Values are assigned to\nthe first quarter of the period.\n\nResample quarters by month using \u2018end\u2019 convention. Values are assigned to the\nlast month of the period.\n\nFor DataFrame objects, the keyword on can be used to specify the column\ninstead of the index for resampling.\n\nFor a DataFrame with MultiIndex, the keyword level can be used to specify on\nwhich level the resampling needs to take place.\n\nIf you want to adjust the start of the bins based on a fixed timestamp:\n\nIf you want to adjust the start of the bins with an offset Timedelta, the two\nfollowing lines are equivalent:\n\nIf you want to take the largest Timestamp as the end of the bins:\n\nIn contrast with the start_day, you can use end_day to take the ceiling\nmidnight of the largest Timestamp as the end of the bins and drop the bins not\ncontaining data:\n\nTo replace the use of the deprecated base argument, you can now use offset, in\nthis example it is equivalent to have base=2:\n\nTo replace the use of the deprecated loffset argument:\n\n"}, {"name": "pandas.Series.reset_index", "path": "reference/api/pandas.series.reset_index", "type": "Series", "text": "\nGenerate a new DataFrame or Series with the index reset.\n\nThis is useful when the index needs to be treated as a column, or when the\nindex is meaningless and needs to be reset to the default before another\noperation.\n\nFor a Series with a MultiIndex, only remove the specified levels from the\nindex. Removes all levels by default.\n\nJust reset the index, without inserting it as a column in the new DataFrame.\n\nThe name to use for the column containing the original Series values. Uses\n`self.name` by default. This argument is ignored when drop is True.\n\nModify the Series in place (do not create a new object).\n\nWhen drop is False (the default), a DataFrame is returned. The newly created\ncolumns will come first in the DataFrame, followed by the original Series\nvalues. When drop is True, a Series is returned. In either case, if\n`inplace=True`, no value is returned.\n\nSee also\n\nAnalogous function for DataFrame.\n\nExamples\n\nGenerate a DataFrame with default index.\n\nTo specify the name of the new column use name.\n\nTo generate a new Series with the default set drop to True.\n\nTo update the Series in place, without generating a new one set inplace to\nTrue. Note that it also requires `drop=True`.\n\nThe level parameter is interesting for Series with a multi-level index.\n\nTo remove a specific level from the Index, use level.\n\nIf level is not set, all levels are removed from the Index.\n\n"}, {"name": "pandas.Series.rfloordiv", "path": "reference/api/pandas.series.rfloordiv", "type": "Series", "text": "\nReturn Integer division of series and other, element-wise (binary operator\nrfloordiv).\n\nEquivalent to `other // series`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nElement-wise Integer division, see Python documentation for more details.\n\nExamples\n\n"}, {"name": "pandas.Series.rmod", "path": "reference/api/pandas.series.rmod", "type": "Series", "text": "\nReturn Modulo of series and other, element-wise (binary operator rmod).\n\nEquivalent to `other % series`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nElement-wise Modulo, see Python documentation for more details.\n\nExamples\n\n"}, {"name": "pandas.Series.rmul", "path": "reference/api/pandas.series.rmul", "type": "Series", "text": "\nReturn Multiplication of series and other, element-wise (binary operator\nrmul).\n\nEquivalent to `other * series`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nElement-wise Multiplication, see Python documentation for more details.\n\nExamples\n\n"}, {"name": "pandas.Series.rolling", "path": "reference/api/pandas.series.rolling", "type": "Series", "text": "\nProvide rolling window calculations.\n\nSize of the moving window.\n\nIf an integer, the fixed number of observations used for each window.\n\nIf an offset, the time period of each window. Each window will be a variable\nsized based on the observations included in the time-period. This is only\nvalid for datetimelike indexes. To learn more about the offsets & frequency\nstrings, please see this link.\n\nIf a BaseIndexer subclass, the window boundaries based on the defined\n`get_window_bounds` method. Additional rolling keyword arguments, namely\n`min_periods`, `center`, and `closed` will be passed to `get_window_bounds`.\n\nMinimum number of observations in window required to have a value; otherwise,\nresult is `np.nan`.\n\nFor a window that is specified by an offset, `min_periods` will default to 1.\n\nFor a window that is specified by an integer, `min_periods` will default to\nthe size of the window.\n\nIf False, set the window labels as the right edge of the window index.\n\nIf True, set the window labels as the center of the window index.\n\nIf `None`, all points are evenly weighted.\n\nIf a string, it must be a valid scipy.signal window function.\n\nCertain Scipy window types require additional parameters to be passed in the\naggregation function. The additional parameters must match the keywords\nspecified in the Scipy window type method signature.\n\nFor a DataFrame, a column label or Index level on which to calculate the\nrolling window, rather than the DataFrame\u2019s index.\n\nProvided integer column is ignored and excluded from result since an integer\nindex is not used to calculate the rolling window.\n\nIf `0` or `'index'`, roll across the rows.\n\nIf `1` or `'columns'`, roll across the columns.\n\nIf `'right'`, the first point in the window is excluded from calculations.\n\nIf `'left'`, the last point in the window is excluded from calculations.\n\nIf `'both'`, the no points in the window are excluded from calculations.\n\nIf `'neither'`, the first and last points in the window are excluded from\ncalculations.\n\nDefault `None` (`'right'`).\n\nChanged in version 1.2.0: The closed parameter with fixed windows is now\nsupported.\n\nNew in version 1.3.0.\n\nExecute the rolling operation per single column or row (`'single'`) or over\nthe entire object (`'table'`).\n\nThis argument is only implemented when specifying `engine='numba'` in the\nmethod call.\n\nSee also\n\nProvides expanding transformations.\n\nProvides exponential weighted functions.\n\nNotes\n\nSee Windowing Operations for further usage details and examples.\n\nExamples\n\nwindow\n\nRolling sum with a window length of 2 observations.\n\nRolling sum with a window span of 2 seconds.\n\nRolling sum with forward looking windows with 2 observations.\n\nmin_periods\n\nRolling sum with a window length of 2 observations, but only needs a minimum\nof 1 observation to calculate a value.\n\ncenter\n\nRolling sum with the result assigned to the center of the window index.\n\nwin_type\n\nRolling sum with a window length of 2, using the Scipy `'gaussian'` window\ntype. `std` is required in the aggregation function.\n\n"}, {"name": "pandas.Series.round", "path": "reference/api/pandas.series.round", "type": "Series", "text": "\nRound each value in a Series to the given number of decimals.\n\nNumber of decimal places to round to. If decimals is negative, it specifies\nthe number of positions to the left of the decimal point.\n\nAdditional arguments and keywords have no effect but might be accepted for\ncompatibility with NumPy.\n\nRounded values of the Series.\n\nSee also\n\nRound values of an np.array.\n\nRound values of a DataFrame.\n\nExamples\n\n"}, {"name": "pandas.Series.rpow", "path": "reference/api/pandas.series.rpow", "type": "Series", "text": "\nReturn Exponential power of series and other, element-wise (binary operator\nrpow).\n\nEquivalent to `other ** series`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nElement-wise Exponential power, see Python documentation for more details.\n\nExamples\n\n"}, {"name": "pandas.Series.rsub", "path": "reference/api/pandas.series.rsub", "type": "Series", "text": "\nReturn Subtraction of series and other, element-wise (binary operator rsub).\n\nEquivalent to `other - series`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nElement-wise Subtraction, see Python documentation for more details.\n\nExamples\n\n"}, {"name": "pandas.Series.rtruediv", "path": "reference/api/pandas.series.rtruediv", "type": "Series", "text": "\nReturn Floating division of series and other, element-wise (binary operator\nrtruediv).\n\nEquivalent to `other / series`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nElement-wise Floating division, see Python documentation for more details.\n\nExamples\n\n"}, {"name": "pandas.Series.sample", "path": "reference/api/pandas.series.sample", "type": "Series", "text": "\nReturn a random sample of items from an axis of object.\n\nYou can use random_state for reproducibility.\n\nNumber of items from axis to return. Cannot be used with frac. Default = 1 if\nfrac = None.\n\nFraction of axis items to return. Cannot be used with n.\n\nAllow or disallow sampling of the same row more than once.\n\nDefault \u2018None\u2019 results in equal probability weighting. If passed a Series,\nwill align with target object on index. Index values in weights not found in\nsampled object will be ignored and index values in sampled object not in\nweights will be assigned weights of zero. If called on a DataFrame, will\naccept the name of a column when axis = 0. Unless weights are a Series,\nweights must be same length as axis being sampled. If weights do not sum to 1,\nthey will be normalized to sum to 1. Missing values in the weights column will\nbe treated as zero. Infinite values not allowed.\n\nIf int, array-like, or BitGenerator, seed for random number generator. If\nnp.random.RandomState or np.random.Generator, use as given.\n\nChanged in version 1.1.0: array-like and BitGenerator object now passed to\nnp.random.RandomState() as seed\n\nChanged in version 1.4.0: np.random.Generator objects now accepted\n\nAxis to sample. Accepts axis number or name. Default is stat axis for given\ndata type (0 for Series and DataFrames).\n\nIf True, the resulting index will be labeled 0, 1, \u2026, n - 1.\n\nNew in version 1.3.0.\n\nA new object of same type as caller containing n items randomly sampled from\nthe caller object.\n\nSee also\n\nGenerates random samples from each group of a DataFrame object.\n\nGenerates random samples from each group of a Series object.\n\nGenerates a random sample from a given 1-D numpy array.\n\nNotes\n\nIf frac > 1, replacement should be set to True.\n\nExamples\n\nExtract 3 random elements from the `Series` `df['num_legs']`: Note that we use\nrandom_state to ensure the reproducibility of the examples.\n\nA random 50% sample of the `DataFrame` with replacement:\n\nAn upsample sample of the `DataFrame` with replacement: Note that replace\nparameter has to be True for frac parameter > 1.\n\nUsing a DataFrame column as weights. Rows with larger value in the\nnum_specimen_seen column are more likely to be sampled.\n\n"}, {"name": "pandas.Series.searchsorted", "path": "reference/api/pandas.series.searchsorted", "type": "Series", "text": "\nFind indices where elements should be inserted to maintain order.\n\nFind the indices into a sorted Series self such that, if the corresponding\nelements in value were inserted before the indices, the order of self would be\npreserved.\n\nNote\n\nThe Series must be monotonically sorted, otherwise wrong locations will likely\nbe returned. Pandas does not check this for you.\n\nValues to insert into self.\n\nIf \u2018left\u2019, the index of the first suitable location found is given. If\n\u2018right\u2019, return the last such index. If there is no suitable index, return\neither 0 or N (where N is the length of self).\n\nOptional array of integer indices that sort self into ascending order. They\nare typically the result of `np.argsort`.\n\nA scalar or array of insertion points with the same shape as value.\n\nSee also\n\nSort by the values along either axis.\n\nSimilar method from NumPy.\n\nNotes\n\nBinary search is used to find the required insertion points.\n\nExamples\n\nIf the values are not monotonically sorted, wrong locations may be returned:\n\n"}, {"name": "pandas.Series.sem", "path": "reference/api/pandas.series.sem", "type": "Series", "text": "\nReturn unbiased standard error of the mean over requested axis.\n\nNormalized by N-1 by default. This can be changed using the ddof argument\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a scalar.\n\nDelta Degrees of Freedom. The divisor used in calculations is N - ddof, where\nN represents the number of elements.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\n"}, {"name": "pandas.Series.set_axis", "path": "reference/api/pandas.series.set_axis", "type": "Series", "text": "\nAssign desired index to given axis.\n\nIndexes for row labels can be changed by assigning a list-like or Index.\n\nThe values for the new index.\n\nThe axis to update. The value 0 identifies the rows.\n\nWhether to return a new Series instance.\n\nAn object of type Series or None if `inplace=True`.\n\nSee also\n\nAlter the name of the index.\n\nExamples\n\n"}, {"name": "pandas.Series.set_flags", "path": "reference/api/pandas.series.set_flags", "type": "Series", "text": "\nReturn a new object with updated flags.\n\nWhether the returned object allows duplicate labels.\n\nThe same type as the caller.\n\nSee also\n\nGlobal metadata applying to this dataset.\n\nGlobal flags applying to this object.\n\nNotes\n\nThis method returns a new object that\u2019s a view on the same data as the input.\nMutating the input or the output values will be reflected in the other.\n\nThis method is intended to be used in method chains.\n\n\u201cFlags\u201d differ from \u201cmetadata\u201d. Flags reflect properties of the pandas object\n(the Series or DataFrame). Metadata refer to properties of the dataset, and\nshould be stored in `DataFrame.attrs`.\n\nExamples\n\n"}, {"name": "pandas.Series.shape", "path": "reference/api/pandas.series.shape", "type": "Series", "text": "\nReturn a tuple of the shape of the underlying data.\n\n"}, {"name": "pandas.Series.shift", "path": "reference/api/pandas.series.shift", "type": "Series", "text": "\nShift index by desired number of periods with an optional time freq.\n\nWhen freq is not passed, shift the index without realigning the data. If freq\nis passed (in this case, the index must be date or datetime, or it will raise\na NotImplementedError), the index will be increased using the periods and the\nfreq. freq can be inferred when specified as \u201cinfer\u201d as long as either freq or\ninferred_freq attribute is set in the index.\n\nNumber of periods to shift. Can be positive or negative.\n\nOffset to use from the tseries module or time rule (e.g. \u2018EOM\u2019). If freq is\nspecified then the index values are shifted but the data is not realigned.\nThat is, use freq if you would like to extend the index when shifting and\npreserve the original data. If freq is specified as \u201cinfer\u201d then it will be\ninferred from the freq or inferred_freq attributes of the index. If neither of\nthose attributes exist, a ValueError is thrown.\n\nShift direction.\n\nThe scalar value to use for newly introduced missing values. the default\ndepends on the dtype of self. For numeric data, `np.nan` is used. For\ndatetime, timedelta, or period data, etc. `NaT` is used. For extension dtypes,\n`self.dtype.na_value` is used.\n\nChanged in version 1.1.0.\n\nCopy of input object, shifted.\n\nSee also\n\nShift values of Index.\n\nShift values of DatetimeIndex.\n\nShift values of PeriodIndex.\n\nShift the time index, using the index\u2019s frequency if available.\n\nExamples\n\n"}, {"name": "pandas.Series.size", "path": "reference/api/pandas.series.size", "type": "Series", "text": "\nReturn the number of elements in the underlying data.\n\n"}, {"name": "pandas.Series.skew", "path": "reference/api/pandas.series.skew", "type": "Series", "text": "\nReturn unbiased skew over requested axis.\n\nNormalized by N-1.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a scalar.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.Series.slice_shift", "path": "reference/api/pandas.series.slice_shift", "type": "Series", "text": "\nEquivalent to shift without copying data. The shifted data will not include\nthe dropped periods and the shifted axis will be smaller than the original.\n\nDeprecated since version 1.2.0: slice_shift is deprecated, use\nDataFrame/Series.shift instead.\n\nNumber of periods to move, can be positive or negative.\n\nNotes\n\nWhile the slice_shift is faster than shift, you may pay for it later during\nalignment.\n\n"}, {"name": "pandas.Series.sort_index", "path": "reference/api/pandas.series.sort_index", "type": "Series", "text": "\nSort Series by index labels.\n\nReturns a new Series sorted by label if inplace argument is `False`, otherwise\nupdates the original series and returns None.\n\nAxis to direct sorting. This can only be 0 for Series.\n\nIf not None, sort on values in specified index level(s).\n\nSort ascending vs. descending. When the index is a MultiIndex the sort\ndirection can be controlled for each level individually.\n\nIf True, perform operation in-place.\n\nChoice of sorting algorithm. See also `numpy.sort()` for more information.\n\u2018mergesort\u2019 and \u2018stable\u2019 are the only stable algorithms. For DataFrames, this\noption is only applied when sorting on a single column or label.\n\nIf \u2018first\u2019 puts NaNs at the beginning, \u2018last\u2019 puts NaNs at the end. Not\nimplemented for MultiIndex.\n\nIf True and sorting by level and index is multilevel, sort by other levels too\n(in order) after sorting by specified level.\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nNew in version 1.0.0.\n\nIf not None, apply the key function to the index values before sorting. This\nis similar to the key argument in the builtin `sorted()` function, with the\nnotable difference that this key function should be vectorized. It should\nexpect an `Index` and return an `Index` of the same shape.\n\nNew in version 1.1.0.\n\nThe original Series sorted by the labels or None if `inplace=True`.\n\nSee also\n\nSort DataFrame by the index.\n\nSort DataFrame by the value.\n\nSort Series by the value.\n\nExamples\n\nSort Descending\n\nSort Inplace\n\nBy default NaNs are put at the end, but use na_position to place them at the\nbeginning\n\nSpecify index level to sort\n\nDoes not sort by remaining levels when sorting by levels\n\nApply a key function before sorting\n\n"}, {"name": "pandas.Series.sort_values", "path": "reference/api/pandas.series.sort_values", "type": "Series", "text": "\nSort by the values.\n\nSort a Series in ascending or descending order by some criterion.\n\nAxis to direct sorting. The value \u2018index\u2019 is accepted for compatibility with\nDataFrame.sort_values.\n\nIf True, sort values in ascending order, otherwise descending.\n\nIf True, perform operation in-place.\n\nChoice of sorting algorithm. See also `numpy.sort()` for more information.\n\u2018mergesort\u2019 and \u2018stable\u2019 are the only stable algorithms.\n\nArgument \u2018first\u2019 puts NaNs at the beginning, \u2018last\u2019 puts NaNs at the end.\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nNew in version 1.0.0.\n\nIf not None, apply the key function to the series values before sorting. This\nis similar to the key argument in the builtin `sorted()` function, with the\nnotable difference that this key function should be vectorized. It should\nexpect a `Series` and return an array-like.\n\nNew in version 1.1.0.\n\nSeries ordered by values or None if `inplace=True`.\n\nSee also\n\nSort by the Series indices.\n\nSort DataFrame by the values along either axis.\n\nSort DataFrame by indices.\n\nExamples\n\nSort values ascending order (default behaviour)\n\nSort values descending order\n\nSort values inplace\n\nSort values putting NAs first\n\nSort a series of strings\n\nSort using a key function. Your key function will be given the `Series` of\nvalues and should return an array-like.\n\nNumPy ufuncs work well here. For example, we can sort by the `sin` of the\nvalue\n\nMore complicated user-defined functions can be used, as long as they expect a\nSeries and return an array-like\n\n"}, {"name": "pandas.Series.sparse", "path": "reference/api/pandas.series.sparse", "type": "Series", "text": "\nAccessor for SparseSparse from other sparse matrix data types.\n\n"}, {"name": "pandas.Series.sparse.density", "path": "reference/api/pandas.series.sparse.density", "type": "Series", "text": "\nThe percent of non- `fill_value` points, as decimal.\n\nExamples\n\n"}, {"name": "pandas.Series.sparse.fill_value", "path": "reference/api/pandas.series.sparse.fill_value", "type": "Series", "text": "\nElements in data that are fill_value are not stored.\n\nFor memory savings, this should be the most common value in the array.\n\n"}, {"name": "pandas.Series.sparse.from_coo", "path": "reference/api/pandas.series.sparse.from_coo", "type": "Series", "text": "\nCreate a Series with sparse values from a scipy.sparse.coo_matrix.\n\nIf False (default), the SparseSeries index consists of only the coords of the\nnon-null entries of the original coo_matrix. If True, the SparseSeries index\nconsists of the full sorted (row, col) coordinates of the coo_matrix.\n\nA Series with sparse values.\n\nExamples\n\n"}, {"name": "pandas.Series.sparse.npoints", "path": "reference/api/pandas.series.sparse.npoints", "type": "Series", "text": "\nThe number of non- `fill_value` points.\n\nExamples\n\n"}, {"name": "pandas.Series.sparse.sp_values", "path": "reference/api/pandas.series.sparse.sp_values", "type": "Series", "text": "\nAn ndarray containing the non- `fill_value` values.\n\nExamples\n\n"}, {"name": "pandas.Series.sparse.to_coo", "path": "reference/api/pandas.series.sparse.to_coo", "type": "Series", "text": "\nCreate a scipy.sparse.coo_matrix from a Series with MultiIndex.\n\nUse row_levels and column_levels to determine the row and column coordinates\nrespectively. row_levels and column_levels are the names (labels) or numbers\nof the levels. {row_levels, column_levels} must be a partition of the\nMultiIndex level names (or numbers).\n\nSort the row and column labels before forming the sparse matrix. When\nrow_levels and/or column_levels refer to a single level, set to True for a\nfaster execution.\n\nExamples\n\n"}, {"name": "pandas.Series.squeeze", "path": "reference/api/pandas.series.squeeze", "type": "Series", "text": "\nSqueeze 1 dimensional axis objects into scalars.\n\nSeries or DataFrames with a single element are squeezed to a scalar.\nDataFrames with a single column or a single row are squeezed to a Series.\nOtherwise the object is unchanged.\n\nThis method is most useful when you don\u2019t know if your object is a Series or\nDataFrame, but you do know it has just a single column. In that case you can\nsafely call squeeze to ensure you have a Series.\n\nA specific axis to squeeze. By default, all length-1 axes are squeezed.\n\nThe projection after squeezing axis or all the axes.\n\nSee also\n\nInteger-location based indexing for selecting scalars.\n\nInteger-location based indexing for selecting Series.\n\nInverse of DataFrame.squeeze for a single-column DataFrame.\n\nExamples\n\nSlicing might produce a Series with a single value:\n\nSqueezing objects with more than one value in every axis does nothing:\n\nSqueezing is even more effective when used with DataFrames.\n\nSlicing a single column will produce a DataFrame with the columns having only\none value:\n\nSo the columns can be squeezed down, resulting in a Series:\n\nSlicing a single row from a single column will produce a single scalar\nDataFrame:\n\nSqueezing the rows produces a single scalar Series:\n\nSqueezing all axes will project directly into a scalar:\n\n"}, {"name": "pandas.Series.std", "path": "reference/api/pandas.series.std", "type": "Series", "text": "\nReturn sample standard deviation over requested axis.\n\nNormalized by N-1 by default. This can be changed using the ddof argument.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a scalar.\n\nDelta Degrees of Freedom. The divisor used in calculations is N - ddof, where\nN represents the number of elements.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nNotes\n\nTo have the same behaviour as numpy.std, use ddof=0 (instead of the default\nddof=1)\n\nExamples\n\nThe standard deviation of the columns can be found as follows:\n\nAlternatively, ddof=0 can be set to normalize by N instead of N-1:\n\n"}, {"name": "pandas.Series.str", "path": "reference/api/pandas.series.str", "type": "Series", "text": "\nVectorized string functions for Series and Index.\n\nNAs stay NA unless handled otherwise by a particular method. Patterned after\nPython\u2019s string methods, with some inspiration from R\u2019s stringr package.\n\nExamples\n\n"}, {"name": "pandas.Series.str.capitalize", "path": "reference/api/pandas.series.str.capitalize", "type": "Series", "text": "\nConvert strings in the Series/Index to be capitalized.\n\nEquivalent to `str.capitalize()`.\n\nSee also\n\nConverts all characters to lowercase.\n\nConverts all characters to uppercase.\n\nConverts first character of each word to uppercase and remaining to lowercase.\n\nConverts first character to uppercase and remaining to lowercase.\n\nConverts uppercase to lowercase and lowercase to uppercase.\n\nRemoves all case distinctions in the string.\n\nExamples\n\n"}, {"name": "pandas.Series.str.casefold", "path": "reference/api/pandas.series.str.casefold", "type": "Series", "text": "\nConvert strings in the Series/Index to be casefolded.\n\nNew in version 0.25.0.\n\nEquivalent to `str.casefold()`.\n\nSee also\n\nConverts all characters to lowercase.\n\nConverts all characters to uppercase.\n\nConverts first character of each word to uppercase and remaining to lowercase.\n\nConverts first character to uppercase and remaining to lowercase.\n\nConverts uppercase to lowercase and lowercase to uppercase.\n\nRemoves all case distinctions in the string.\n\nExamples\n\n"}, {"name": "pandas.Series.str.cat", "path": "reference/api/pandas.series.str.cat", "type": "Series", "text": "\nConcatenate strings in the Series/Index with given separator.\n\nIf others is specified, this function concatenates the Series/Index and\nelements of others element-wise. If others is not passed, then all values in\nthe Series/Index are concatenated into a single string with a given sep.\n\nSeries, Index, DataFrame, np.ndarray (one- or two-dimensional) and other list-\nlikes of strings must have the same length as the calling Series/Index, with\nthe exception of indexed objects (i.e. Series/Index/DataFrame) if join is not\nNone.\n\nIf others is a list-like that contains a combination of Series, Index or\nnp.ndarray (1-dim), then all elements will be unpacked and must satisfy the\nabove criteria individually.\n\nIf others is None, the method returns the concatenation of all strings in the\ncalling Series/Index.\n\nThe separator between the different elements/columns. By default the empty\nstring \u2018\u2019 is used.\n\nRepresentation that is inserted for all missing values:\n\nIf na_rep is None, and others is None, missing values in the Series/Index are\nomitted from the result.\n\nIf na_rep is None, and others is not None, a row containing a missing value in\nany of the columns (before concatenation) will have a missing value in the\nresult.\n\nDetermines the join-style between the calling Series/Index and any\nSeries/Index/DataFrame in others (objects without an index need to match the\nlength of the calling Series/Index). To disable alignment, use .values on any\nSeries/Index/DataFrame in others.\n\nNew in version 0.23.0.\n\nChanged in version 1.0.0: Changed default of join from None to \u2018left\u2019.\n\nIf others is None, str is returned, otherwise a Series/Index (same type as\ncaller) of objects is returned.\n\nSee also\n\nSplit each string in the Series/Index.\n\nJoin lists contained as elements in the Series/Index.\n\nExamples\n\nWhen not passing others, all values are concatenated into a single string:\n\nBy default, NA values in the Series are ignored. Using na_rep, they can be\ngiven a representation:\n\nIf others is specified, corresponding values are concatenated with the\nseparator. Result will be a Series of strings.\n\nMissing values will remain missing in the result, but can again be represented\nusing na_rep\n\nIf sep is not specified, the values are concatenated without separation.\n\nSeries with different indexes can be aligned before concatenation. The join-\nkeyword works as in other methods.\n\nFor more examples, see here.\n\n"}, {"name": "pandas.Series.str.center", "path": "reference/api/pandas.series.str.center", "type": "Series", "text": "\nPad left and right side of strings in the Series/Index.\n\nEquivalent to `str.center()`.\n\nMinimum width of resulting string; additional characters will be filled with\n`fillchar`.\n\nAdditional character for filling, default is whitespace.\n\n"}, {"name": "pandas.Series.str.contains", "path": "reference/api/pandas.series.str.contains", "type": "Series", "text": "\nTest if pattern or regex is contained within a string of a Series or Index.\n\nReturn boolean Series or Index based on whether a given pattern or regex is\ncontained within a string of a Series or Index.\n\nCharacter sequence or regular expression.\n\nIf True, case sensitive.\n\nFlags to pass through to the re module, e.g. re.IGNORECASE.\n\nFill value for missing values. The default depends on dtype of the array. For\nobject-dtype, `numpy.nan` is used. For `StringDtype`, `pandas.NA` is used.\n\nIf True, assumes the pat is a regular expression.\n\nIf False, treats the pat as a literal string.\n\nA Series or Index of boolean values indicating whether the given pattern is\ncontained within the string of each element of the Series or Index.\n\nSee also\n\nAnalogous, but stricter, relying on re.match instead of re.search.\n\nTest if the start of each string element matches a pattern.\n\nSame as startswith, but tests the end of string.\n\nExamples\n\nReturning a Series of booleans using only a literal pattern.\n\nReturning an Index of booleans using only a literal pattern.\n\nSpecifying case sensitivity using case.\n\nSpecifying na to be False instead of NaN replaces NaN values with False. If\nSeries or Index does not contain NaN values the resultant dtype will be bool,\notherwise, an object dtype.\n\nReturning \u2018house\u2019 or \u2018dog\u2019 when either expression occurs in a string.\n\nIgnoring case sensitivity using flags with regex.\n\nReturning any digit using regular expression.\n\nEnsure pat is a not a literal pattern when regex is set to True. Note in the\nfollowing example one might expect only s2[1] and s2[3] to return True.\nHowever, \u2018.0\u2019 as a regex matches any character followed by a 0.\n\n"}, {"name": "pandas.Series.str.count", "path": "reference/api/pandas.series.str.count", "type": "Series", "text": "\nCount occurrences of pattern in each string of the Series/Index.\n\nThis function is used to count the number of times a particular regex pattern\nis repeated in each of the string elements of the `Series`.\n\nValid regular expression.\n\nFlags for the re module. For a complete list, see here.\n\nFor compatibility with other string methods. Not used.\n\nSame type as the calling object containing the integer counts.\n\nSee also\n\nStandard library module for regular expressions.\n\nStandard library version, without regular expression support.\n\nNotes\n\nSome characters need to be escaped when passing in pat. eg. `'$'` has a\nspecial meaning in regex and must be escaped when finding this literal\ncharacter.\n\nExamples\n\nEscape `'$'` to find the literal dollar sign.\n\nThis is also available on Index\n\n"}, {"name": "pandas.Series.str.decode", "path": "reference/api/pandas.series.str.decode", "type": "Series", "text": "\nDecode character string in the Series/Index using indicated encoding.\n\nEquivalent to `str.decode()` in python2 and `bytes.decode()` in python3.\n\n"}, {"name": "pandas.Series.str.encode", "path": "reference/api/pandas.series.str.encode", "type": "Series", "text": "\nEncode character string in the Series/Index using indicated encoding.\n\nEquivalent to `str.encode()`.\n\n"}, {"name": "pandas.Series.str.endswith", "path": "reference/api/pandas.series.str.endswith", "type": "Series", "text": "\nTest if the end of each string element matches a pattern.\n\nEquivalent to `str.endswith()`.\n\nCharacter sequence. Regular expressions are not accepted.\n\nObject shown if element tested is not a string. The default depends on dtype\nof the array. For object-dtype, `numpy.nan` is used. For `StringDtype`,\n`pandas.NA` is used.\n\nA Series of booleans indicating whether the given pattern matches the end of\neach string element.\n\nSee also\n\nPython standard library string method.\n\nSame as endswith, but tests the start of string.\n\nTests if string element contains a pattern.\n\nExamples\n\nSpecifying na to be False instead of NaN.\n\n"}, {"name": "pandas.Series.str.extract", "path": "reference/api/pandas.series.str.extract", "type": "Series", "text": "\nExtract capture groups in the regex pat as columns in a DataFrame.\n\nFor each subject string in the Series, extract groups from the first match of\nregular expression pat.\n\nRegular expression pattern with capturing groups.\n\nFlags from the `re` module, e.g. `re.IGNORECASE`, that modify regular\nexpression matching for things like case, spaces, etc. For more details, see\n`re`.\n\nIf True, return DataFrame with one column per capture group. If False, return\na Series/Index if there is one capture group or DataFrame if there are\nmultiple capture groups.\n\nA DataFrame with one row for each subject string, and one column for each\ngroup. Any capture group names in regular expression pat will be used for\ncolumn names; otherwise capture group numbers will be used. The dtype of each\nresult column is always object, even when no match is found. If `expand=False`\nand pat has only one capture group, then return a Series (if subject is a\nSeries) or Index (if subject is an Index).\n\nSee also\n\nReturns all matches (not just the first match).\n\nExamples\n\nA pattern with two groups will return a DataFrame with two columns. Non-\nmatches will be NaN.\n\nA pattern may contain optional groups.\n\nNamed groups will become column names in the result.\n\nA pattern with one group will return a DataFrame with one column if\nexpand=True.\n\nA pattern with one group will return a Series if expand=False.\n\n"}, {"name": "pandas.Series.str.extractall", "path": "reference/api/pandas.series.str.extractall", "type": "Series", "text": "\nExtract capture groups in the regex pat as columns in DataFrame.\n\nFor each subject string in the Series, extract groups from all matches of\nregular expression pat. When each subject string in the Series has exactly one\nmatch, extractall(pat).xs(0, level=\u2019match\u2019) is the same as extract(pat).\n\nRegular expression pattern with capturing groups.\n\nA `re` module flag, for example `re.IGNORECASE`. These allow to modify regular\nexpression matching for things like case, spaces, etc. Multiple flags can be\ncombined with the bitwise OR operator, for example `re.IGNORECASE |\nre.MULTILINE`.\n\nA `DataFrame` with one row for each match, and one column for each group. Its\nrows have a `MultiIndex` with first levels that come from the subject\n`Series`. The last level is named \u2018match\u2019 and indexes the matches in each item\nof the `Series`. Any capture group names in regular expression pat will be\nused for column names; otherwise capture group numbers will be used.\n\nSee also\n\nReturns first match only (not all matches).\n\nExamples\n\nA pattern with one group will return a DataFrame with one column. Indices with\nno matches will not appear in the result.\n\nCapture group names are used for column names of the result.\n\nA pattern with two groups will return a DataFrame with two columns.\n\nOptional groups that do not match are NaN in the result.\n\n"}, {"name": "pandas.Series.str.find", "path": "reference/api/pandas.series.str.find", "type": "Series", "text": "\nReturn lowest indexes in each strings in the Series/Index.\n\nEach of returned indexes corresponds to the position where the substring is\nfully contained between [start:end]. Return -1 on failure. Equivalent to\nstandard `str.find()`.\n\nSubstring being searched.\n\nLeft edge index.\n\nRight edge index.\n\nSee also\n\nReturn highest indexes in each strings.\n\n"}, {"name": "pandas.Series.str.findall", "path": "reference/api/pandas.series.str.findall", "type": "Series", "text": "\nFind all occurrences of pattern or regular expression in the Series/Index.\n\nEquivalent to applying `re.findall()` to all the elements in the Series/Index.\n\nPattern or regular expression.\n\nFlags from `re` module, e.g. re.IGNORECASE (default is 0, which means no\nflags).\n\nAll non-overlapping matches of pattern or regular expression in each string of\nthis Series/Index.\n\nSee also\n\nCount occurrences of pattern or regular expression in each string of the\nSeries/Index.\n\nFor each string in the Series, extract groups from all matches of regular\nexpression and return a DataFrame with one row for each match and one column\nfor each group.\n\nThe equivalent `re` function to all non-overlapping matches of pattern or\nregular expression in string, as a list of strings.\n\nExamples\n\nThe search for the pattern \u2018Monkey\u2019 returns one match:\n\nOn the other hand, the search for the pattern \u2018MONKEY\u2019 doesn\u2019t return any\nmatch:\n\nFlags can be added to the pattern or regular expression. For instance, to find\nthe pattern \u2018MONKEY\u2019 ignoring the case:\n\nWhen the pattern matches more than one string in the Series, all matches are\nreturned:\n\nRegular expressions are supported too. For instance, the search for all the\nstrings ending with the word \u2018on\u2019 is shown next:\n\nIf the pattern is found more than once in the same string, then a list of\nmultiple strings is returned:\n\n"}, {"name": "pandas.Series.str.fullmatch", "path": "reference/api/pandas.series.str.fullmatch", "type": "Series", "text": "\nDetermine if each string entirely matches a regular expression.\n\nNew in version 1.1.0.\n\nCharacter sequence or regular expression.\n\nIf True, case sensitive.\n\nRegex module flags, e.g. re.IGNORECASE.\n\nFill value for missing values. The default depends on dtype of the array. For\nobject-dtype, `numpy.nan` is used. For `StringDtype`, `pandas.NA` is used.\n\nSee also\n\nSimilar, but also returns True when only a prefix of the string matches the\nregular expression.\n\nExtract matched groups.\n\n"}, {"name": "pandas.Series.str.get", "path": "reference/api/pandas.series.str.get", "type": "Series", "text": "\nExtract element from each component at specified position.\n\nExtract element from lists, tuples, or strings in each element in the\nSeries/Index.\n\nPosition of element to extract.\n\nExamples\n\n"}, {"name": "pandas.Series.str.get_dummies", "path": "reference/api/pandas.series.str.get_dummies", "type": "Series", "text": "\nReturn DataFrame of dummy/indicator variables for Series.\n\nEach string in Series is split by sep and returned as a DataFrame of\ndummy/indicator variables.\n\nString to split on.\n\nDummy variables corresponding to values of the Series.\n\nSee also\n\nConvert categorical variable into dummy/indicator variables.\n\nExamples\n\n"}, {"name": "pandas.Series.str.index", "path": "reference/api/pandas.series.str.index", "type": "Series", "text": "\nReturn lowest indexes in each string in Series/Index.\n\nEach of the returned indexes corresponds to the position where the substring\nis fully contained between [start:end]. This is the same as `str.find` except\ninstead of returning -1, it raises a ValueError when the substring is not\nfound. Equivalent to standard `str.index`.\n\nSubstring being searched.\n\nLeft edge index.\n\nRight edge index.\n\nSee also\n\nReturn highest indexes in each strings.\n\n"}, {"name": "pandas.Series.str.isalnum", "path": "reference/api/pandas.series.str.isalnum", "type": "Series", "text": "\nCheck whether all characters in each string are alphanumeric.\n\nThis is equivalent to running the Python string method `str.isalnum()` for\neach element of the Series/Index. If a string has zero characters, `False` is\nreturned for that check.\n\nSeries or Index of boolean values with the same length as the original\nSeries/Index.\n\nSee also\n\nCheck whether all characters are alphabetic.\n\nCheck whether all characters are numeric.\n\nCheck whether all characters are alphanumeric.\n\nCheck whether all characters are digits.\n\nCheck whether all characters are decimal.\n\nCheck whether all characters are whitespace.\n\nCheck whether all characters are lowercase.\n\nCheck whether all characters are uppercase.\n\nCheck whether all characters are titlecase.\n\nExamples\n\nChecks for Alphabetic and Numeric Characters\n\nNote that checks against characters mixed with any additional punctuation or\nwhitespace will evaluate to false for an alphanumeric check.\n\nMore Detailed Checks for Numeric Characters\n\nThere are several different but overlapping sets of numeric characters that\ncan be checked for.\n\nThe `s3.str.isdecimal` method checks for characters used to form numbers in\nbase 10.\n\nThe `s.str.isdigit` method is the same as `s3.str.isdecimal` but also includes\nspecial digits, like superscripted and subscripted digits in unicode.\n\nThe `s.str.isnumeric` method is the same as `s3.str.isdigit` but also includes\nother characters that can represent quantities such as unicode fractions.\n\nChecks for Whitespace\n\nChecks for Character Case\n\nThe `s5.str.istitle` method checks for whether all words are in title case\n(whether only the first letter of each word is capitalized). Words are assumed\nto be as any sequence of non-numeric characters separated by whitespace\ncharacters.\n\n"}, {"name": "pandas.Series.str.isalpha", "path": "reference/api/pandas.series.str.isalpha", "type": "Series", "text": "\nCheck whether all characters in each string are alphabetic.\n\nThis is equivalent to running the Python string method `str.isalpha()` for\neach element of the Series/Index. If a string has zero characters, `False` is\nreturned for that check.\n\nSeries or Index of boolean values with the same length as the original\nSeries/Index.\n\nSee also\n\nCheck whether all characters are alphabetic.\n\nCheck whether all characters are numeric.\n\nCheck whether all characters are alphanumeric.\n\nCheck whether all characters are digits.\n\nCheck whether all characters are decimal.\n\nCheck whether all characters are whitespace.\n\nCheck whether all characters are lowercase.\n\nCheck whether all characters are uppercase.\n\nCheck whether all characters are titlecase.\n\nExamples\n\nChecks for Alphabetic and Numeric Characters\n\nNote that checks against characters mixed with any additional punctuation or\nwhitespace will evaluate to false for an alphanumeric check.\n\nMore Detailed Checks for Numeric Characters\n\nThere are several different but overlapping sets of numeric characters that\ncan be checked for.\n\nThe `s3.str.isdecimal` method checks for characters used to form numbers in\nbase 10.\n\nThe `s.str.isdigit` method is the same as `s3.str.isdecimal` but also includes\nspecial digits, like superscripted and subscripted digits in unicode.\n\nThe `s.str.isnumeric` method is the same as `s3.str.isdigit` but also includes\nother characters that can represent quantities such as unicode fractions.\n\nChecks for Whitespace\n\nChecks for Character Case\n\nThe `s5.str.istitle` method checks for whether all words are in title case\n(whether only the first letter of each word is capitalized). Words are assumed\nto be as any sequence of non-numeric characters separated by whitespace\ncharacters.\n\n"}, {"name": "pandas.Series.str.isdecimal", "path": "reference/api/pandas.series.str.isdecimal", "type": "Series", "text": "\nCheck whether all characters in each string are decimal.\n\nThis is equivalent to running the Python string method `str.isdecimal()` for\neach element of the Series/Index. If a string has zero characters, `False` is\nreturned for that check.\n\nSeries or Index of boolean values with the same length as the original\nSeries/Index.\n\nSee also\n\nCheck whether all characters are alphabetic.\n\nCheck whether all characters are numeric.\n\nCheck whether all characters are alphanumeric.\n\nCheck whether all characters are digits.\n\nCheck whether all characters are decimal.\n\nCheck whether all characters are whitespace.\n\nCheck whether all characters are lowercase.\n\nCheck whether all characters are uppercase.\n\nCheck whether all characters are titlecase.\n\nExamples\n\nChecks for Alphabetic and Numeric Characters\n\nNote that checks against characters mixed with any additional punctuation or\nwhitespace will evaluate to false for an alphanumeric check.\n\nMore Detailed Checks for Numeric Characters\n\nThere are several different but overlapping sets of numeric characters that\ncan be checked for.\n\nThe `s3.str.isdecimal` method checks for characters used to form numbers in\nbase 10.\n\nThe `s.str.isdigit` method is the same as `s3.str.isdecimal` but also includes\nspecial digits, like superscripted and subscripted digits in unicode.\n\nThe `s.str.isnumeric` method is the same as `s3.str.isdigit` but also includes\nother characters that can represent quantities such as unicode fractions.\n\nChecks for Whitespace\n\nChecks for Character Case\n\nThe `s5.str.istitle` method checks for whether all words are in title case\n(whether only the first letter of each word is capitalized). Words are assumed\nto be as any sequence of non-numeric characters separated by whitespace\ncharacters.\n\n"}, {"name": "pandas.Series.str.isdigit", "path": "reference/api/pandas.series.str.isdigit", "type": "Series", "text": "\nCheck whether all characters in each string are digits.\n\nThis is equivalent to running the Python string method `str.isdigit()` for\neach element of the Series/Index. If a string has zero characters, `False` is\nreturned for that check.\n\nSeries or Index of boolean values with the same length as the original\nSeries/Index.\n\nSee also\n\nCheck whether all characters are alphabetic.\n\nCheck whether all characters are numeric.\n\nCheck whether all characters are alphanumeric.\n\nCheck whether all characters are digits.\n\nCheck whether all characters are decimal.\n\nCheck whether all characters are whitespace.\n\nCheck whether all characters are lowercase.\n\nCheck whether all characters are uppercase.\n\nCheck whether all characters are titlecase.\n\nExamples\n\nChecks for Alphabetic and Numeric Characters\n\nNote that checks against characters mixed with any additional punctuation or\nwhitespace will evaluate to false for an alphanumeric check.\n\nMore Detailed Checks for Numeric Characters\n\nThere are several different but overlapping sets of numeric characters that\ncan be checked for.\n\nThe `s3.str.isdecimal` method checks for characters used to form numbers in\nbase 10.\n\nThe `s.str.isdigit` method is the same as `s3.str.isdecimal` but also includes\nspecial digits, like superscripted and subscripted digits in unicode.\n\nThe `s.str.isnumeric` method is the same as `s3.str.isdigit` but also includes\nother characters that can represent quantities such as unicode fractions.\n\nChecks for Whitespace\n\nChecks for Character Case\n\nThe `s5.str.istitle` method checks for whether all words are in title case\n(whether only the first letter of each word is capitalized). Words are assumed\nto be as any sequence of non-numeric characters separated by whitespace\ncharacters.\n\n"}, {"name": "pandas.Series.str.islower", "path": "reference/api/pandas.series.str.islower", "type": "Series", "text": "\nCheck whether all characters in each string are lowercase.\n\nThis is equivalent to running the Python string method `str.islower()` for\neach element of the Series/Index. If a string has zero characters, `False` is\nreturned for that check.\n\nSeries or Index of boolean values with the same length as the original\nSeries/Index.\n\nSee also\n\nCheck whether all characters are alphabetic.\n\nCheck whether all characters are numeric.\n\nCheck whether all characters are alphanumeric.\n\nCheck whether all characters are digits.\n\nCheck whether all characters are decimal.\n\nCheck whether all characters are whitespace.\n\nCheck whether all characters are lowercase.\n\nCheck whether all characters are uppercase.\n\nCheck whether all characters are titlecase.\n\nExamples\n\nChecks for Alphabetic and Numeric Characters\n\nNote that checks against characters mixed with any additional punctuation or\nwhitespace will evaluate to false for an alphanumeric check.\n\nMore Detailed Checks for Numeric Characters\n\nThere are several different but overlapping sets of numeric characters that\ncan be checked for.\n\nThe `s3.str.isdecimal` method checks for characters used to form numbers in\nbase 10.\n\nThe `s.str.isdigit` method is the same as `s3.str.isdecimal` but also includes\nspecial digits, like superscripted and subscripted digits in unicode.\n\nThe `s.str.isnumeric` method is the same as `s3.str.isdigit` but also includes\nother characters that can represent quantities such as unicode fractions.\n\nChecks for Whitespace\n\nChecks for Character Case\n\nThe `s5.str.istitle` method checks for whether all words are in title case\n(whether only the first letter of each word is capitalized). Words are assumed\nto be as any sequence of non-numeric characters separated by whitespace\ncharacters.\n\n"}, {"name": "pandas.Series.str.isnumeric", "path": "reference/api/pandas.series.str.isnumeric", "type": "Series", "text": "\nCheck whether all characters in each string are numeric.\n\nThis is equivalent to running the Python string method `str.isnumeric()` for\neach element of the Series/Index. If a string has zero characters, `False` is\nreturned for that check.\n\nSeries or Index of boolean values with the same length as the original\nSeries/Index.\n\nSee also\n\nCheck whether all characters are alphabetic.\n\nCheck whether all characters are numeric.\n\nCheck whether all characters are alphanumeric.\n\nCheck whether all characters are digits.\n\nCheck whether all characters are decimal.\n\nCheck whether all characters are whitespace.\n\nCheck whether all characters are lowercase.\n\nCheck whether all characters are uppercase.\n\nCheck whether all characters are titlecase.\n\nExamples\n\nChecks for Alphabetic and Numeric Characters\n\nNote that checks against characters mixed with any additional punctuation or\nwhitespace will evaluate to false for an alphanumeric check.\n\nMore Detailed Checks for Numeric Characters\n\nThere are several different but overlapping sets of numeric characters that\ncan be checked for.\n\nThe `s3.str.isdecimal` method checks for characters used to form numbers in\nbase 10.\n\nThe `s.str.isdigit` method is the same as `s3.str.isdecimal` but also includes\nspecial digits, like superscripted and subscripted digits in unicode.\n\nThe `s.str.isnumeric` method is the same as `s3.str.isdigit` but also includes\nother characters that can represent quantities such as unicode fractions.\n\nChecks for Whitespace\n\nChecks for Character Case\n\nThe `s5.str.istitle` method checks for whether all words are in title case\n(whether only the first letter of each word is capitalized). Words are assumed\nto be as any sequence of non-numeric characters separated by whitespace\ncharacters.\n\n"}, {"name": "pandas.Series.str.isspace", "path": "reference/api/pandas.series.str.isspace", "type": "Series", "text": "\nCheck whether all characters in each string are whitespace.\n\nThis is equivalent to running the Python string method `str.isspace()` for\neach element of the Series/Index. If a string has zero characters, `False` is\nreturned for that check.\n\nSeries or Index of boolean values with the same length as the original\nSeries/Index.\n\nSee also\n\nCheck whether all characters are alphabetic.\n\nCheck whether all characters are numeric.\n\nCheck whether all characters are alphanumeric.\n\nCheck whether all characters are digits.\n\nCheck whether all characters are decimal.\n\nCheck whether all characters are whitespace.\n\nCheck whether all characters are lowercase.\n\nCheck whether all characters are uppercase.\n\nCheck whether all characters are titlecase.\n\nExamples\n\nChecks for Alphabetic and Numeric Characters\n\nNote that checks against characters mixed with any additional punctuation or\nwhitespace will evaluate to false for an alphanumeric check.\n\nMore Detailed Checks for Numeric Characters\n\nThere are several different but overlapping sets of numeric characters that\ncan be checked for.\n\nThe `s3.str.isdecimal` method checks for characters used to form numbers in\nbase 10.\n\nThe `s.str.isdigit` method is the same as `s3.str.isdecimal` but also includes\nspecial digits, like superscripted and subscripted digits in unicode.\n\nThe `s.str.isnumeric` method is the same as `s3.str.isdigit` but also includes\nother characters that can represent quantities such as unicode fractions.\n\nChecks for Whitespace\n\nChecks for Character Case\n\nThe `s5.str.istitle` method checks for whether all words are in title case\n(whether only the first letter of each word is capitalized). Words are assumed\nto be as any sequence of non-numeric characters separated by whitespace\ncharacters.\n\n"}, {"name": "pandas.Series.str.istitle", "path": "reference/api/pandas.series.str.istitle", "type": "Series", "text": "\nCheck whether all characters in each string are titlecase.\n\nThis is equivalent to running the Python string method `str.istitle()` for\neach element of the Series/Index. If a string has zero characters, `False` is\nreturned for that check.\n\nSeries or Index of boolean values with the same length as the original\nSeries/Index.\n\nSee also\n\nCheck whether all characters are alphabetic.\n\nCheck whether all characters are numeric.\n\nCheck whether all characters are alphanumeric.\n\nCheck whether all characters are digits.\n\nCheck whether all characters are decimal.\n\nCheck whether all characters are whitespace.\n\nCheck whether all characters are lowercase.\n\nCheck whether all characters are uppercase.\n\nCheck whether all characters are titlecase.\n\nExamples\n\nChecks for Alphabetic and Numeric Characters\n\nNote that checks against characters mixed with any additional punctuation or\nwhitespace will evaluate to false for an alphanumeric check.\n\nMore Detailed Checks for Numeric Characters\n\nThere are several different but overlapping sets of numeric characters that\ncan be checked for.\n\nThe `s3.str.isdecimal` method checks for characters used to form numbers in\nbase 10.\n\nThe `s.str.isdigit` method is the same as `s3.str.isdecimal` but also includes\nspecial digits, like superscripted and subscripted digits in unicode.\n\nThe `s.str.isnumeric` method is the same as `s3.str.isdigit` but also includes\nother characters that can represent quantities such as unicode fractions.\n\nChecks for Whitespace\n\nChecks for Character Case\n\nThe `s5.str.istitle` method checks for whether all words are in title case\n(whether only the first letter of each word is capitalized). Words are assumed\nto be as any sequence of non-numeric characters separated by whitespace\ncharacters.\n\n"}, {"name": "pandas.Series.str.isupper", "path": "reference/api/pandas.series.str.isupper", "type": "Series", "text": "\nCheck whether all characters in each string are uppercase.\n\nThis is equivalent to running the Python string method `str.isupper()` for\neach element of the Series/Index. If a string has zero characters, `False` is\nreturned for that check.\n\nSeries or Index of boolean values with the same length as the original\nSeries/Index.\n\nSee also\n\nCheck whether all characters are alphabetic.\n\nCheck whether all characters are numeric.\n\nCheck whether all characters are alphanumeric.\n\nCheck whether all characters are digits.\n\nCheck whether all characters are decimal.\n\nCheck whether all characters are whitespace.\n\nCheck whether all characters are lowercase.\n\nCheck whether all characters are uppercase.\n\nCheck whether all characters are titlecase.\n\nExamples\n\nChecks for Alphabetic and Numeric Characters\n\nNote that checks against characters mixed with any additional punctuation or\nwhitespace will evaluate to false for an alphanumeric check.\n\nMore Detailed Checks for Numeric Characters\n\nThere are several different but overlapping sets of numeric characters that\ncan be checked for.\n\nThe `s3.str.isdecimal` method checks for characters used to form numbers in\nbase 10.\n\nThe `s.str.isdigit` method is the same as `s3.str.isdecimal` but also includes\nspecial digits, like superscripted and subscripted digits in unicode.\n\nThe `s.str.isnumeric` method is the same as `s3.str.isdigit` but also includes\nother characters that can represent quantities such as unicode fractions.\n\nChecks for Whitespace\n\nChecks for Character Case\n\nThe `s5.str.istitle` method checks for whether all words are in title case\n(whether only the first letter of each word is capitalized). Words are assumed\nto be as any sequence of non-numeric characters separated by whitespace\ncharacters.\n\n"}, {"name": "pandas.Series.str.join", "path": "reference/api/pandas.series.str.join", "type": "Series", "text": "\nJoin lists contained as elements in the Series/Index with passed delimiter.\n\nIf the elements of a Series are lists themselves, join the content of these\nlists using the delimiter passed to the function. This function is an\nequivalent to `str.join()`.\n\nDelimiter to use between list entries.\n\nThe list entries concatenated by intervening occurrences of the delimiter.\n\nIf the supplied Series contains neither strings nor lists.\n\nSee also\n\nStandard library version of this method.\n\nSplit strings around given separator/delimiter.\n\nNotes\n\nIf any of the list items is not a string object, the result of the join will\nbe NaN.\n\nExamples\n\nExample with a list that contains non-string elements.\n\nJoin all lists using a \u2018-\u2019. The lists containing object(s) of types other than\nstr will produce a NaN.\n\n"}, {"name": "pandas.Series.str.len", "path": "reference/api/pandas.series.str.len", "type": "Series", "text": "\nCompute the length of each element in the Series/Index.\n\nThe element may be a sequence (such as a string, tuple or list) or a\ncollection (such as a dictionary).\n\nA Series or Index of integer values indicating the length of each element in\nthe Series or Index.\n\nSee also\n\nPython built-in function returning the length of an object.\n\nReturns the length of the Series.\n\nExamples\n\nReturns the length (number of characters) in a string. Returns the number of\nentries for dictionaries, lists or tuples.\n\n"}, {"name": "pandas.Series.str.ljust", "path": "reference/api/pandas.series.str.ljust", "type": "Series", "text": "\nPad right side of strings in the Series/Index.\n\nEquivalent to `str.ljust()`.\n\nMinimum width of resulting string; additional characters will be filled with\n`fillchar`.\n\nAdditional character for filling, default is whitespace.\n\n"}, {"name": "pandas.Series.str.lower", "path": "reference/api/pandas.series.str.lower", "type": "Series", "text": "\nConvert strings in the Series/Index to lowercase.\n\nEquivalent to `str.lower()`.\n\nSee also\n\nConverts all characters to lowercase.\n\nConverts all characters to uppercase.\n\nConverts first character of each word to uppercase and remaining to lowercase.\n\nConverts first character to uppercase and remaining to lowercase.\n\nConverts uppercase to lowercase and lowercase to uppercase.\n\nRemoves all case distinctions in the string.\n\nExamples\n\n"}, {"name": "pandas.Series.str.lstrip", "path": "reference/api/pandas.series.str.lstrip", "type": "Series", "text": "\nRemove leading characters.\n\nStrip whitespaces (including newlines) or a set of specified characters from\neach string in the Series/Index from left side. Equivalent to `str.lstrip()`.\n\nSpecifying the set of characters to be removed. All combinations of this set\nof characters will be stripped. If None then whitespaces are removed.\n\nSee also\n\nRemove leading and trailing characters in Series/Index.\n\nRemove leading characters in Series/Index.\n\nRemove trailing characters in Series/Index.\n\nExamples\n\n"}, {"name": "pandas.Series.str.match", "path": "reference/api/pandas.series.str.match", "type": "Series", "text": "\nDetermine if each string starts with a match of a regular expression.\n\nCharacter sequence or regular expression.\n\nIf True, case sensitive.\n\nRegex module flags, e.g. re.IGNORECASE.\n\nFill value for missing values. The default depends on dtype of the array. For\nobject-dtype, `numpy.nan` is used. For `StringDtype`, `pandas.NA` is used.\n\nSee also\n\nStricter matching that requires the entire string to match.\n\nAnalogous, but less strict, relying on re.search instead of re.match.\n\nExtract matched groups.\n\n"}, {"name": "pandas.Series.str.normalize", "path": "reference/api/pandas.series.str.normalize", "type": "Series", "text": "\nReturn the Unicode normal form for the strings in the Series/Index.\n\nFor more information on the forms, see the `unicodedata.normalize()`.\n\nUnicode form.\n\n"}, {"name": "pandas.Series.str.pad", "path": "reference/api/pandas.series.str.pad", "type": "Series", "text": "\nPad strings in the Series/Index up to width.\n\nMinimum width of resulting string; additional characters will be filled with\ncharacter defined in fillchar.\n\nSide from which to fill resulting string.\n\nAdditional character for filling, default is whitespace.\n\nReturns Series or Index with minimum number of char in object.\n\nSee also\n\nFills the left side of strings with an arbitrary character. Equivalent to\n`Series.str.pad(side='left')`.\n\nFills the right side of strings with an arbitrary character. Equivalent to\n`Series.str.pad(side='right')`.\n\nFills both sides of strings with an arbitrary character. Equivalent to\n`Series.str.pad(side='both')`.\n\nPad strings in the Series/Index by prepending \u20180\u2019 character. Equivalent to\n`Series.str.pad(side='left', fillchar='0')`.\n\nExamples\n\n"}, {"name": "pandas.Series.str.partition", "path": "reference/api/pandas.series.str.partition", "type": "Input/output", "text": "\nSplit the string at the first occurrence of sep.\n\nThis method splits the string at the first occurrence of sep, and returns 3\nelements containing the part before the separator, the separator itself, and\nthe part after the separator. If the separator is not found, return 3 elements\ncontaining the string itself, followed by two empty strings.\n\nString to split on.\n\nIf True, return DataFrame/MultiIndex expanding dimensionality. If False,\nreturn Series/Index.\n\nSee also\n\nSplit the string at the last occurrence of sep.\n\nSplit strings around given separators.\n\nStandard library version.\n\nExamples\n\nTo partition by the last space instead of the first one:\n\nTo partition by something different than a space:\n\nTo return a Series containing tuples instead of a DataFrame:\n\nAlso available on indices:\n\nWhich will create a MultiIndex:\n\nOr an index with tuples with `expand=False`:\n\n"}, {"name": "pandas.Series.str.removeprefix", "path": "reference/api/pandas.series.str.removeprefix", "type": "Series", "text": "\nRemove a prefix from an object series. If the prefix is not present, the\noriginal string will be returned.\n\nRemove the prefix of the string.\n\nThe Series or Index with given prefix removed.\n\nSee also\n\nRemove a suffix from an object series.\n\nExamples\n\n"}, {"name": "pandas.Series.str.removesuffix", "path": "reference/api/pandas.series.str.removesuffix", "type": "Series", "text": "\nRemove a suffix from an object series. If the suffix is not present, the\noriginal string will be returned.\n\nRemove the suffix of the string.\n\nThe Series or Index with given suffix removed.\n\nSee also\n\nRemove a prefix from an object series.\n\nExamples\n\n"}, {"name": "pandas.Series.str.repeat", "path": "reference/api/pandas.series.str.repeat", "type": "Series", "text": "\nDuplicate each string in the Series or Index.\n\nSame value for all (int) or different value per (sequence).\n\nSeries or Index of repeated string objects specified by input parameter\nrepeats.\n\nExamples\n\nSingle int repeats string in Series\n\nSequence of int repeats corresponding string in Series\n\n"}, {"name": "pandas.Series.str.replace", "path": "reference/api/pandas.series.str.replace", "type": "Series", "text": "\nReplace each occurrence of pattern/regex in the Series/Index.\n\nEquivalent to `str.replace()` or `re.sub()`, depending on the regex value.\n\nString can be a character sequence or regular expression.\n\nReplacement string or a callable. The callable is passed the regex match\nobject and must return a replacement string to be used. See `re.sub()`.\n\nNumber of replacements to make from start.\n\nDetermines if replace is case sensitive:\n\nIf True, case sensitive (the default if pat is a string)\n\nSet to False for case insensitive\n\nCannot be set if pat is a compiled regex.\n\nRegex module flags, e.g. re.IGNORECASE. Cannot be set if pat is a compiled\nregex.\n\nDetermines if the passed-in pattern is a regular expression:\n\nIf True, assumes the passed-in pattern is a regular expression.\n\nIf False, treats the pattern as a literal string\n\nCannot be set to False if pat is a compiled regex or repl is a callable.\n\nNew in version 0.23.0.\n\nA copy of the object with all matching occurrences of pat replaced by repl.\n\nif regex is False and repl is a callable or pat is a compiled regex\n\nif pat is a compiled regex and case or flags is set\n\nNotes\n\nWhen pat is a compiled regex, all flags should be included in the compiled\nregex. Use of case, flags, or regex=False with a compiled regex will raise an\nerror.\n\nExamples\n\nWhen pat is a string and regex is True (the default), the given pat is\ncompiled as a regex. When repl is a string, it replaces matching regex\npatterns as with `re.sub()`. NaN value(s) in the Series are left as is:\n\nWhen pat is a string and regex is False, every pat is replaced with repl as\nwith `str.replace()`:\n\nWhen repl is a callable, it is called on every pat using `re.sub()`. The\ncallable should expect one positional argument (a regex object) and return a\nstring.\n\nTo get the idea:\n\nReverse every lowercase alphabetic word:\n\nUsing regex groups (extract second group and swap case):\n\nUsing a compiled regex with flags\n\n"}, {"name": "pandas.Series.str.rfind", "path": "reference/api/pandas.series.str.rfind", "type": "Series", "text": "\nReturn highest indexes in each strings in the Series/Index.\n\nEach of returned indexes corresponds to the position where the substring is\nfully contained between [start:end]. Return -1 on failure. Equivalent to\nstandard `str.rfind()`.\n\nSubstring being searched.\n\nLeft edge index.\n\nRight edge index.\n\nSee also\n\nReturn lowest indexes in each strings.\n\n"}, {"name": "pandas.Series.str.rindex", "path": "reference/api/pandas.series.str.rindex", "type": "Series", "text": "\nReturn highest indexes in each string in Series/Index.\n\nEach of the returned indexes corresponds to the position where the substring\nis fully contained between [start:end]. This is the same as `str.rfind` except\ninstead of returning -1, it raises a ValueError when the substring is not\nfound. Equivalent to standard `str.rindex`.\n\nSubstring being searched.\n\nLeft edge index.\n\nRight edge index.\n\nSee also\n\nReturn lowest indexes in each strings.\n\n"}, {"name": "pandas.Series.str.rjust", "path": "reference/api/pandas.series.str.rjust", "type": "Series", "text": "\nPad left side of strings in the Series/Index.\n\nEquivalent to `str.rjust()`.\n\nMinimum width of resulting string; additional characters will be filled with\n`fillchar`.\n\nAdditional character for filling, default is whitespace.\n\n"}, {"name": "pandas.Series.str.rpartition", "path": "reference/api/pandas.series.str.rpartition", "type": "Input/output", "text": "\nSplit the string at the last occurrence of sep.\n\nThis method splits the string at the last occurrence of sep, and returns 3\nelements containing the part before the separator, the separator itself, and\nthe part after the separator. If the separator is not found, return 3 elements\ncontaining two empty strings, followed by the string itself.\n\nString to split on.\n\nIf True, return DataFrame/MultiIndex expanding dimensionality. If False,\nreturn Series/Index.\n\nSee also\n\nSplit the string at the first occurrence of sep.\n\nSplit strings around given separators.\n\nStandard library version.\n\nExamples\n\nTo partition by the last space instead of the first one:\n\nTo partition by something different than a space:\n\nTo return a Series containing tuples instead of a DataFrame:\n\nAlso available on indices:\n\nWhich will create a MultiIndex:\n\nOr an index with tuples with `expand=False`:\n\n"}, {"name": "pandas.Series.str.rsplit", "path": "reference/api/pandas.series.str.rsplit", "type": "Series", "text": "\nSplit strings around given separator/delimiter.\n\nSplits the string in the Series/Index from the end, at the specified delimiter\nstring.\n\nString or regular expression to split on. If not specified, split on\nwhitespace.\n\nLimit number of splits in output. `None`, 0 and -1 will be interpreted as\nreturn all splits.\n\nExpand the split strings into separate columns.\n\nIf `True`, return DataFrame/MultiIndex expanding dimensionality.\n\nIf `False`, return Series/Index, containing lists of strings.\n\nDetermines if the passed-in pattern is a regular expression:\n\nIf `True`, assumes the passed-in pattern is a regular expression\n\nIf `False`, treats the pattern as a literal string.\n\nIf `None` and pat length is 1, treats pat as a literal string.\n\nIf `None` and pat length is not 1, treats pat as a regular expression.\n\nCannot be set to False if pat is a compiled regex\n\nNew in version 1.4.0.\n\nType matches caller unless `expand=True` (see Notes).\n\nif regex is False and pat is a compiled regex\n\nSee also\n\nSplit strings around given separator/delimiter.\n\nSplits string around given separator/delimiter, starting from the right.\n\nJoin lists contained as elements in the Series/Index with passed delimiter.\n\nStandard library version for split.\n\nStandard library version for rsplit.\n\nNotes\n\nThe handling of the n keyword depends on the number of found splits:\n\nIf found splits > n, make first n splits only\n\nIf found splits <= n, make all splits\n\nIf for a certain row the number of found splits < n, append None for padding\nup to n if `expand=True`\n\nIf using `expand=True`, Series and Index callers return DataFrame and\nMultiIndex objects, respectively.\n\nUse of regex=False with a pat as a compiled regex will raise an error.\n\nExamples\n\nIn the default setting, the string is split by whitespace.\n\nWithout the n parameter, the outputs of rsplit and split are identical.\n\nThe n parameter can be used to limit the number of splits on the delimiter.\nThe outputs of split and rsplit are different.\n\nThe pat parameter can be used to split by other characters.\n\nWhen using `expand=True`, the split elements will expand out into separate\ncolumns. If NaN is present, it is propagated throughout the columns during the\nsplit.\n\nFor slightly more complex use cases like splitting the html document name from\na url, a combination of parameter settings can be used.\n\nRemember to escape special characters when explicitly using regular\nexpressions.\n\nRegular expressions can be used to handle urls or file names. When pat is a\nstring and `regex=None` (the default), the given pat is compiled as a regex\nonly if `len(pat) != 1`.\n\nWhen `regex=True`, pat is interpreted as a regex\n\nA compiled regex can be passed as pat\n\nWhen `regex=False`, pat is interpreted as the string itself\n\n"}, {"name": "pandas.Series.str.rstrip", "path": "reference/api/pandas.series.str.rstrip", "type": "Series", "text": "\nRemove trailing characters.\n\nStrip whitespaces (including newlines) or a set of specified characters from\neach string in the Series/Index from right side. Equivalent to `str.rstrip()`.\n\nSpecifying the set of characters to be removed. All combinations of this set\nof characters will be stripped. If None then whitespaces are removed.\n\nSee also\n\nRemove leading and trailing characters in Series/Index.\n\nRemove leading characters in Series/Index.\n\nRemove trailing characters in Series/Index.\n\nExamples\n\n"}, {"name": "pandas.Series.str.slice", "path": "reference/api/pandas.series.str.slice", "type": "Series", "text": "\nSlice substrings from each element in the Series or Index.\n\nStart position for slice operation.\n\nStop position for slice operation.\n\nStep size for slice operation.\n\nSeries or Index from sliced substring from original string object.\n\nSee also\n\nReplace a slice with a string.\n\nReturn element at position. Equivalent to Series.str.slice(start=i, stop=i+1)\nwith i being the position.\n\nExamples\n\nEquivalent behaviour to:\n\n"}, {"name": "pandas.Series.str.slice_replace", "path": "reference/api/pandas.series.str.slice_replace", "type": "Series", "text": "\nReplace a positional slice of a string with another value.\n\nLeft index position to use for the slice. If not specified (None), the slice\nis unbounded on the left, i.e. slice from the start of the string.\n\nRight index position to use for the slice. If not specified (None), the slice\nis unbounded on the right, i.e. slice until the end of the string.\n\nString for replacement. If not specified (None), the sliced region is replaced\nwith an empty string.\n\nSame type as the original object.\n\nSee also\n\nJust slicing without replacement.\n\nExamples\n\nSpecify just start, meaning replace start until the end of the string with\nrepl.\n\nSpecify just stop, meaning the start of the string to stop is replaced with\nrepl, and the rest of the string is included.\n\nSpecify start and stop, meaning the slice from start to stop is replaced with\nrepl. Everything before or after start and stop is included as is.\n\n"}, {"name": "pandas.Series.str.split", "path": "reference/api/pandas.series.str.split", "type": "Series", "text": "\nSplit strings around given separator/delimiter.\n\nSplits the string in the Series/Index from the beginning, at the specified\ndelimiter string.\n\nString or regular expression to split on. If not specified, split on\nwhitespace.\n\nLimit number of splits in output. `None`, 0 and -1 will be interpreted as\nreturn all splits.\n\nExpand the split strings into separate columns.\n\nIf `True`, return DataFrame/MultiIndex expanding dimensionality.\n\nIf `False`, return Series/Index, containing lists of strings.\n\nDetermines if the passed-in pattern is a regular expression:\n\nIf `True`, assumes the passed-in pattern is a regular expression\n\nIf `False`, treats the pattern as a literal string.\n\nIf `None` and pat length is 1, treats pat as a literal string.\n\nIf `None` and pat length is not 1, treats pat as a regular expression.\n\nCannot be set to False if pat is a compiled regex\n\nNew in version 1.4.0.\n\nType matches caller unless `expand=True` (see Notes).\n\nif regex is False and pat is a compiled regex\n\nSee also\n\nSplit strings around given separator/delimiter.\n\nSplits string around given separator/delimiter, starting from the right.\n\nJoin lists contained as elements in the Series/Index with passed delimiter.\n\nStandard library version for split.\n\nStandard library version for rsplit.\n\nNotes\n\nThe handling of the n keyword depends on the number of found splits:\n\nIf found splits > n, make first n splits only\n\nIf found splits <= n, make all splits\n\nIf for a certain row the number of found splits < n, append None for padding\nup to n if `expand=True`\n\nIf using `expand=True`, Series and Index callers return DataFrame and\nMultiIndex objects, respectively.\n\nUse of regex=False with a pat as a compiled regex will raise an error.\n\nExamples\n\nIn the default setting, the string is split by whitespace.\n\nWithout the n parameter, the outputs of rsplit and split are identical.\n\nThe n parameter can be used to limit the number of splits on the delimiter.\nThe outputs of split and rsplit are different.\n\nThe pat parameter can be used to split by other characters.\n\nWhen using `expand=True`, the split elements will expand out into separate\ncolumns. If NaN is present, it is propagated throughout the columns during the\nsplit.\n\nFor slightly more complex use cases like splitting the html document name from\na url, a combination of parameter settings can be used.\n\nRemember to escape special characters when explicitly using regular\nexpressions.\n\nRegular expressions can be used to handle urls or file names. When pat is a\nstring and `regex=None` (the default), the given pat is compiled as a regex\nonly if `len(pat) != 1`.\n\nWhen `regex=True`, pat is interpreted as a regex\n\nA compiled regex can be passed as pat\n\nWhen `regex=False`, pat is interpreted as the string itself\n\n"}, {"name": "pandas.Series.str.startswith", "path": "reference/api/pandas.series.str.startswith", "type": "Series", "text": "\nTest if the start of each string element matches a pattern.\n\nEquivalent to `str.startswith()`.\n\nCharacter sequence. Regular expressions are not accepted.\n\nObject shown if element tested is not a string. The default depends on dtype\nof the array. For object-dtype, `numpy.nan` is used. For `StringDtype`,\n`pandas.NA` is used.\n\nA Series of booleans indicating whether the given pattern matches the start of\neach string element.\n\nSee also\n\nPython standard library string method.\n\nSame as startswith, but tests the end of string.\n\nTests if string element contains a pattern.\n\nExamples\n\nSpecifying na to be False instead of NaN.\n\n"}, {"name": "pandas.Series.str.strip", "path": "reference/api/pandas.series.str.strip", "type": "Series", "text": "\nRemove leading and trailing characters.\n\nStrip whitespaces (including newlines) or a set of specified characters from\neach string in the Series/Index from left and right sides. Equivalent to\n`str.strip()`.\n\nSpecifying the set of characters to be removed. All combinations of this set\nof characters will be stripped. If None then whitespaces are removed.\n\nSee also\n\nRemove leading and trailing characters in Series/Index.\n\nRemove leading characters in Series/Index.\n\nRemove trailing characters in Series/Index.\n\nExamples\n\n"}, {"name": "pandas.Series.str.swapcase", "path": "reference/api/pandas.series.str.swapcase", "type": "Series", "text": "\nConvert strings in the Series/Index to be swapcased.\n\nEquivalent to `str.swapcase()`.\n\nSee also\n\nConverts all characters to lowercase.\n\nConverts all characters to uppercase.\n\nConverts first character of each word to uppercase and remaining to lowercase.\n\nConverts first character to uppercase and remaining to lowercase.\n\nConverts uppercase to lowercase and lowercase to uppercase.\n\nRemoves all case distinctions in the string.\n\nExamples\n\n"}, {"name": "pandas.Series.str.title", "path": "reference/api/pandas.series.str.title", "type": "Series", "text": "\nConvert strings in the Series/Index to titlecase.\n\nEquivalent to `str.title()`.\n\nSee also\n\nConverts all characters to lowercase.\n\nConverts all characters to uppercase.\n\nConverts first character of each word to uppercase and remaining to lowercase.\n\nConverts first character to uppercase and remaining to lowercase.\n\nConverts uppercase to lowercase and lowercase to uppercase.\n\nRemoves all case distinctions in the string.\n\nExamples\n\n"}, {"name": "pandas.Series.str.translate", "path": "reference/api/pandas.series.str.translate", "type": "Series", "text": "\nMap all characters in the string through the given mapping table.\n\nEquivalent to standard `str.translate()`.\n\nTable is a mapping of Unicode ordinals to Unicode ordinals, strings, or None.\nUnmapped characters are left untouched. Characters mapped to None are deleted.\n`str.maketrans()` is a helper function for making translation tables.\n\n"}, {"name": "pandas.Series.str.upper", "path": "reference/api/pandas.series.str.upper", "type": "Series", "text": "\nConvert strings in the Series/Index to uppercase.\n\nEquivalent to `str.upper()`.\n\nSee also\n\nConverts all characters to lowercase.\n\nConverts all characters to uppercase.\n\nConverts first character of each word to uppercase and remaining to lowercase.\n\nConverts first character to uppercase and remaining to lowercase.\n\nConverts uppercase to lowercase and lowercase to uppercase.\n\nRemoves all case distinctions in the string.\n\nExamples\n\n"}, {"name": "pandas.Series.str.wrap", "path": "reference/api/pandas.series.str.wrap", "type": "Series", "text": "\nWrap strings in Series/Index at specified line width.\n\nThis method has the same keyword parameters and defaults as\n`textwrap.TextWrapper`.\n\nMaximum line width.\n\nIf True, tab characters will be expanded to spaces (default: True).\n\nIf True, each whitespace character (as defined by string.whitespace) remaining\nafter tab expansion will be replaced by a single space (default: True).\n\nIf True, whitespace that, after wrapping, happens to end up at the beginning\nor end of a line is dropped (default: True).\n\nIf True, then words longer than width will be broken in order to ensure that\nno lines are longer than width. If it is false, long words will not be broken,\nand some lines may be longer than width (default: True).\n\nIf True, wrapping will occur preferably on whitespace and right after hyphens\nin compound words, as it is customary in English. If false, only whitespaces\nwill be considered as potentially good places for line breaks, but you need to\nset break_long_words to false if you want truly insecable words (default:\nTrue).\n\nNotes\n\nInternally, this method uses a `textwrap.TextWrapper` instance with default\nsettings. To achieve behavior matching R\u2019s stringr library str_wrap function,\nuse the arguments:\n\nexpand_tabs = False\n\nreplace_whitespace = True\n\ndrop_whitespace = True\n\nbreak_long_words = False\n\nbreak_on_hyphens = False\n\nExamples\n\n"}, {"name": "pandas.Series.str.zfill", "path": "reference/api/pandas.series.str.zfill", "type": "Series", "text": "\nPad strings in the Series/Index by prepending \u20180\u2019 characters.\n\nStrings in the Series/Index are padded with \u20180\u2019 characters on the left of the\nstring to reach a total string length width. Strings in the Series/Index with\nlength greater or equal to width are unchanged.\n\nMinimum length of resulting string; strings with length less than width be\nprepended with \u20180\u2019 characters.\n\nSee also\n\nFills the left side of strings with an arbitrary character.\n\nFills the right side of strings with an arbitrary character.\n\nFills the specified sides of strings with an arbitrary character.\n\nFills both sides of strings with an arbitrary character.\n\nNotes\n\nDiffers from `str.zfill()` which has special handling for \u2018+\u2019/\u2019-\u2019 in the\nstring.\n\nExamples\n\nNote that `10` and `NaN` are not strings, therefore they are converted to\n`NaN`. The minus sign in `'-1'` is treated as a regular character and the zero\nis added to the left of it (`str.zfill()` would have moved it to the left).\n`1000` remains unchanged as it is longer than width.\n\n"}, {"name": "pandas.Series.sub", "path": "reference/api/pandas.series.sub", "type": "Series", "text": "\nReturn Subtraction of series and other, element-wise (binary operator sub).\n\nEquivalent to `series - other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nReverse of the Subtraction operator, see Python documentation for more\ndetails.\n\nExamples\n\n"}, {"name": "pandas.Series.subtract", "path": "reference/api/pandas.series.subtract", "type": "Series", "text": "\nReturn Subtraction of series and other, element-wise (binary operator sub).\n\nEquivalent to `series - other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nReverse of the Subtraction operator, see Python documentation for more\ndetails.\n\nExamples\n\n"}, {"name": "pandas.Series.sum", "path": "reference/api/pandas.series.sum", "type": "Series", "text": "\nReturn the sum of the values over the requested axis.\n\nThis is equivalent to the method `numpy.sum`.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a scalar.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nAdditional keyword arguments to be passed to the function.\n\nSee also\n\nReturn the sum.\n\nReturn the minimum.\n\nReturn the maximum.\n\nReturn the index of the minimum.\n\nReturn the index of the maximum.\n\nReturn the sum over the requested axis.\n\nReturn the minimum over the requested axis.\n\nReturn the maximum over the requested axis.\n\nReturn the index of the minimum over the requested axis.\n\nReturn the index of the maximum over the requested axis.\n\nExamples\n\nBy default, the sum of an empty or all-NA Series is `0`.\n\nThis can be controlled with the `min_count` parameter. For example, if you\u2019d\nlike the sum of an empty series to be NaN, pass `min_count=1`.\n\nThanks to the `skipna` parameter, `min_count` handles all-NA and empty series\nidentically.\n\n"}, {"name": "pandas.Series.swapaxes", "path": "reference/api/pandas.series.swapaxes", "type": "Series", "text": "\nInterchange axes and swap values axes appropriately.\n\n"}, {"name": "pandas.Series.swaplevel", "path": "reference/api/pandas.series.swaplevel", "type": "Series", "text": "\nSwap levels i and j in a `MultiIndex`.\n\nDefault is to swap the two innermost levels of the index.\n\nLevels of the indices to be swapped. Can pass level name as string.\n\nWhether to copy underlying data.\n\nSeries with levels swapped in MultiIndex.\n\nExamples\n\nIn the following example, we will swap the levels of the indices. Here, we\nwill swap the levels column-wise, but levels can be swapped row-wise in a\nsimilar manner. Note that column-wise is the default behaviour. By not\nsupplying any arguments for i and j, we swap the last and second to last\nindices.\n\nBy supplying one argument, we can choose which index to swap the last index\nwith. We can for example swap the first index with the last one as follows.\n\nWe can also define explicitly which indices we want to swap by supplying\nvalues for both i and j. Here, we for example swap the first and second\nindices.\n\n"}, {"name": "pandas.Series.T", "path": "reference/api/pandas.series.t", "type": "Series", "text": "\nReturn the transpose, which is by definition self.\n\n"}, {"name": "pandas.Series.tail", "path": "reference/api/pandas.series.tail", "type": "Series", "text": "\nReturn the last n rows.\n\nThis function returns last n rows from the object based on position. It is\nuseful for quickly verifying data, for example, after sorting or appending\nrows.\n\nFor negative values of n, this function returns all rows except the first n\nrows, equivalent to `df[n:]`.\n\nNumber of rows to select.\n\nThe last n rows of the caller object.\n\nSee also\n\nThe first n rows of the caller object.\n\nExamples\n\nViewing the last 5 lines\n\nViewing the last n lines (three in this case)\n\nFor negative values of n\n\n"}, {"name": "pandas.Series.take", "path": "reference/api/pandas.series.take", "type": "Series", "text": "\nReturn the elements in the given positional indices along an axis.\n\nThis means that we are not indexing according to actual values in the index\nattribute of the object. We are indexing according to the actual position of\nthe element in the object.\n\nAn array of ints indicating which positions to take.\n\nThe axis on which to select elements. `0` means that we are selecting rows,\n`1` means that we are selecting columns.\n\nBefore pandas 1.0, `is_copy=False` can be specified to ensure that the return\nvalue is an actual copy. Starting with pandas 1.0, `take` always returns a\ncopy, and the keyword is therefore deprecated.\n\nDeprecated since version 1.0.0.\n\nFor compatibility with `numpy.take()`. Has no effect on the output.\n\nAn array-like containing the elements taken from the object.\n\nSee also\n\nSelect a subset of a DataFrame by labels.\n\nSelect a subset of a DataFrame by positions.\n\nTake elements from an array along an axis.\n\nExamples\n\nTake elements at positions 0 and 3 along the axis 0 (default).\n\nNote how the actual indices selected (0 and 1) do not correspond to our\nselected indices 0 and 3. That\u2019s because we are selecting the 0th and 3rd\nrows, not rows whose indices equal 0 and 3.\n\nTake elements at indices 1 and 2 along the axis 1 (column selection).\n\nWe may take elements using negative integers for positive indices, starting\nfrom the end of the object, just like with Python lists.\n\n"}, {"name": "pandas.Series.to_clipboard", "path": "reference/api/pandas.series.to_clipboard", "type": "Series", "text": "\nCopy object to the system clipboard.\n\nWrite a text representation of object to the system clipboard. This can be\npasted into Excel, for example.\n\nProduce output in a csv format for easy pasting into excel.\n\nTrue, use the provided separator for csv pasting.\n\nFalse, write a string representation of the object to the clipboard.\n\nField delimiter.\n\nThese parameters will be passed to DataFrame.to_csv.\n\nSee also\n\nWrite a DataFrame to a comma-separated values (csv) file.\n\nRead text from clipboard and pass to read_csv.\n\nNotes\n\nRequirements for your platform.\n\nLinux : xclip, or xsel (with PyQt4 modules)\n\nWindows : none\n\nmacOS : none\n\nExamples\n\nCopy the contents of a DataFrame to the clipboard.\n\nWe can omit the index by passing the keyword index and setting it to false.\n\n"}, {"name": "pandas.Series.to_csv", "path": "reference/api/pandas.series.to_csv", "type": "Series", "text": "\nWrite object to a comma-separated values (csv) file.\n\nString, path object (implementing os.PathLike[str]), or file-like object\nimplementing a write() function. If None, the result is returned as a string.\nIf a non-binary file object is passed, it should be opened with newline=\u2019\u2019,\ndisabling universal newlines. If a binary file object is passed, mode might\nneed to contain a \u2018b\u2019.\n\nChanged in version 1.2.0: Support for binary file objects was introduced.\n\nString of length 1. Field delimiter for the output file.\n\nMissing data representation.\n\nFormat string for floating point numbers.\n\nColumns to write.\n\nWrite out the column names. If a list of strings is given it is assumed to be\naliases for the column names.\n\nWrite row names (index).\n\nColumn label for index column(s) if desired. If None is given, and header and\nindex are True, then the index names are used. A sequence should be given if\nthe object uses MultiIndex. If False do not print fields for index names. Use\nindex_label=False for easier importing in R.\n\nPython write mode, default \u2018w\u2019.\n\nA string representing the encoding to use in the output file, defaults to\n\u2018utf-8\u2019. encoding is not supported if path_or_buf is a non-binary file object.\n\nFor on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018%s\u2019 path-like,\nthen detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019,\n\u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to `None` for no compression.\nCan also be a dict with key `'method'` set to one of {`'zip'`, `'gzip'`,\n`'bz2'`, `'zstd'`} and other key-value pairs are forwarded to\n`zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or\n`zstandard.ZstdDecompressor`, respectively. As an example, the following could\nbe passed for faster compression and to create a reproducible gzip archive:\n`compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}`.\n\nChanged in version 1.0.0: May now be a dict with key \u2018method\u2019 as compression\nmode and other entries as additional compression options if compression mode\nis \u2018zip\u2019.\n\nChanged in version 1.1.0: Passing compression options as keys in dict is\nsupported for compression modes \u2018gzip\u2019, \u2018bz2\u2019, \u2018zstd\u2019, and \u2018zip\u2019.\n\nChanged in version 1.2.0: Compression is supported for binary file objects.\n\nChanged in version 1.2.0: Previous versions forwarded dict entries for \u2018gzip\u2019\nto gzip.open instead of gzip.GzipFile which prevented setting mtime.\n\nDefaults to csv.QUOTE_MINIMAL. If you have set a float_format then floats are\nconverted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-\nnumeric.\n\nString of length 1. Character used to quote fields.\n\nThe newline character or character sequence to use in the output file.\nDefaults to os.linesep, which depends on the OS in which this method is called\n(\u2019\\n\u2019 for linux, \u2018\\r\\n\u2019 for Windows, i.e.).\n\nRows to write at a time.\n\nFormat string for datetime objects.\n\nControl quoting of quotechar inside a field.\n\nString of length 1. Character used to escape sep and quotechar when\nappropriate.\n\nCharacter recognized as decimal separator. E.g. use \u2018,\u2019 for European data.\n\nSpecifies how encoding and decoding errors are to be handled. See the errors\nargument for `open()` for a full list of options.\n\nNew in version 1.1.0.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nIf path_or_buf is None, returns the resulting csv format as a string.\nOtherwise returns None.\n\nSee also\n\nLoad a CSV file into a DataFrame.\n\nWrite DataFrame to an Excel file.\n\nExamples\n\nCreate \u2018out.zip\u2019 containing \u2018out.csv\u2019\n\nTo write a csv file to a new folder or nested folder you will first need to\ncreate it using either Pathlib or os:\n\n"}, {"name": "pandas.Series.to_dict", "path": "reference/api/pandas.series.to_dict", "type": "Series", "text": "\nConvert Series to {label -> value} dict or dict-like object.\n\nThe collections.abc.Mapping subclass to use as the return object. Can be the\nactual class or an empty instance of the mapping type you want. If you want a\ncollections.defaultdict, you must pass it initialized.\n\nKey-value representation of Series.\n\nExamples\n\n"}, {"name": "pandas.Series.to_excel", "path": "reference/api/pandas.series.to_excel", "type": "Series", "text": "\nWrite object to an Excel sheet.\n\nTo write a single object to an Excel .xlsx file it is only necessary to\nspecify a target file name. To write to multiple sheets it is necessary to\ncreate an ExcelWriter object with a target file name, and specify a sheet in\nthe file to write to.\n\nMultiple sheets may be written to by specifying unique sheet_name. With all\ndata written to the file it is necessary to save the changes. Note that\ncreating an ExcelWriter object with a file name that already exists will\nresult in the contents of the existing file being erased.\n\nFile path or existing ExcelWriter.\n\nName of sheet which will contain DataFrame.\n\nMissing data representation.\n\nFormat string for floating point numbers. For example `float_format=\"%.2f\"`\nwill format 0.1234 to 0.12.\n\nColumns to write.\n\nWrite out the column names. If a list of string is given it is assumed to be\naliases for the column names.\n\nWrite row names (index).\n\nColumn label for index column(s) if desired. If not specified, and header and\nindex are True, then the index names are used. A sequence should be given if\nthe DataFrame uses MultiIndex.\n\nUpper left cell row to dump data frame.\n\nUpper left cell column to dump data frame.\n\nWrite engine to use, \u2018openpyxl\u2019 or \u2018xlsxwriter\u2019. You can also set this via the\noptions `io.excel.xlsx.writer`, `io.excel.xls.writer`, and\n`io.excel.xlsm.writer`.\n\nDeprecated since version 1.2.0: As the xlwt package is no longer maintained,\nthe `xlwt` engine will be removed in a future version of pandas.\n\nWrite MultiIndex and Hierarchical Rows as merged cells.\n\nEncoding of the resulting excel file. Only necessary for xlwt, other writers\nsupport unicode natively.\n\nRepresentation for infinity (there is no native representation for infinity in\nExcel).\n\nDisplay more information in the error logs.\n\nSpecifies the one-based bottommost row and rightmost column that is to be\nfrozen.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nSee also\n\nWrite DataFrame to a comma-separated values (csv) file.\n\nClass for writing DataFrame objects into excel sheets.\n\nRead an Excel file into a pandas DataFrame.\n\nRead a comma-separated values (csv) file into DataFrame.\n\nNotes\n\nFor compatibility with `to_csv()`, to_excel serializes lists and dicts to\nstrings before writing.\n\nOnce a workbook has been saved it is not possible to write further data\nwithout rewriting the whole workbook.\n\nExamples\n\nCreate, write to and save a workbook:\n\nTo specify the sheet name:\n\nIf you wish to write to more than one sheet in the workbook, it is necessary\nto specify an ExcelWriter object:\n\nExcelWriter can also be used to append to an existing Excel file:\n\nTo set the library that is used to write the Excel file, you can pass the\nengine keyword (the default engine is automatically chosen depending on the\nfile extension):\n\n"}, {"name": "pandas.Series.to_frame", "path": "reference/api/pandas.series.to_frame", "type": "Series", "text": "\nConvert Series to DataFrame.\n\nThe passed name should substitute for the series name (if it has one).\n\nDataFrame representation of Series.\n\nExamples\n\n"}, {"name": "pandas.Series.to_hdf", "path": "reference/api/pandas.series.to_hdf", "type": "Series", "text": "\nWrite the contained data to an HDF5 file using HDFStore.\n\nHierarchical Data Format (HDF) is self-describing, allowing an application to\ninterpret the structure and contents of a file with no outside information.\nOne HDF file can hold a mix of related objects which can be accessed as a\ngroup or as individual objects.\n\nIn order to add another DataFrame or Series to an existing HDF file please use\nappend mode and a different a key.\n\nWarning\n\nOne can store a subclass of `DataFrame` or `Series` to HDF5, but the type of\nthe subclass is lost upon storing.\n\nFor more information see the user guide.\n\nFile path or HDFStore object.\n\nIdentifier for the group in the store.\n\nMode to open file:\n\n\u2018w\u2019: write, a new file is created (an existing file with the same name would\nbe deleted).\n\n\u2018a\u2019: append, an existing file is opened for reading and writing, and if the\nfile does not exist it is created.\n\n\u2018r+\u2019: similar to \u2018a\u2019, but the file must already exist.\n\nSpecifies a compression level for data. A value of 0 or None disables\ncompression.\n\nSpecifies the compression library to be used. As of v0.20.2 these additional\ncompressors for Blosc are supported (default if no compressor specified:\n\u2018blosc:blosclz\u2019): {\u2018blosc:blosclz\u2019, \u2018blosc:lz4\u2019, \u2018blosc:lz4hc\u2019,\n\u2018blosc:snappy\u2019, \u2018blosc:zlib\u2019, \u2018blosc:zstd\u2019}. Specifying a compression library\nwhich is not available issues a ValueError.\n\nFor Table formats, append the input data to the existing.\n\nPossible values:\n\n\u2018fixed\u2019: Fixed format. Fast writing/reading. Not-appendable, nor searchable.\n\n\u2018table\u2019: Table format. Write as a PyTables Table structure which may perform\nworse but allow more flexible operations like searching / selecting subsets of\nthe data.\n\nIf None, pd.get_option(\u2018io.hdf.default_format\u2019) is checked, followed by\nfallback to \u201cfixed\u201d.\n\nSpecifies how encoding and decoding errors are to be handled. See the errors\nargument for `open()` for a full list of options.\n\nMap column names to minimum string sizes for columns.\n\nHow to represent null values as str. Not allowed with append=True.\n\nList of columns to create as indexed data columns for on-disk queries, or True\nto use all columns. By default only the axes of the object are indexed. See\nQuery via data columns. Applicable only to format=\u2019table\u2019.\n\nSee also\n\nRead from HDF file.\n\nWrite a DataFrame to the binary parquet format.\n\nWrite to a SQL table.\n\nWrite out feather-format for DataFrames.\n\nWrite out to a csv file.\n\nExamples\n\nWe can add another object to the same file:\n\nReading from HDF file:\n\n"}, {"name": "pandas.Series.to_json", "path": "reference/api/pandas.series.to_json", "type": "Series", "text": "\nConvert the object to a JSON string.\n\nNote NaN\u2019s and None will be converted to null and datetime objects will be\nconverted to UNIX timestamps.\n\nString, path object (implementing os.PathLike[str]), or file-like object\nimplementing a write() function. If None, the result is returned as a string.\n\nIndication of expected JSON string format.\n\nSeries:\n\ndefault is \u2018index\u2019\n\nallowed values are: {\u2018split\u2019, \u2018records\u2019, \u2018index\u2019, \u2018table\u2019}.\n\nDataFrame:\n\ndefault is \u2018columns\u2019\n\nallowed values are: {\u2018split\u2019, \u2018records\u2019, \u2018index\u2019, \u2018columns\u2019, \u2018values\u2019,\n\u2018table\u2019}.\n\nThe format of the JSON string:\n\n\u2018split\u2019 : dict like {\u2018index\u2019 -> [index], \u2018columns\u2019 -> [columns], \u2018data\u2019 ->\n[values]}\n\n\u2018records\u2019 : list like [{column -> value}, \u2026 , {column -> value}]\n\n\u2018index\u2019 : dict like {index -> {column -> value}}\n\n\u2018columns\u2019 : dict like {column -> {index -> value}}\n\n\u2018values\u2019 : just the values array\n\n\u2018table\u2019 : dict like {\u2018schema\u2019: {schema}, \u2018data\u2019: {data}}\n\nDescribing the data, where data component is like `orient='records'`.\n\nType of date conversion. \u2018epoch\u2019 = epoch milliseconds, \u2018iso\u2019 = ISO8601. The\ndefault depends on the orient. For `orient='table'`, the default is \u2018iso\u2019. For\nall other orients, the default is \u2018epoch\u2019.\n\nThe number of decimal places to use when encoding floating point values.\n\nForce encoded string to be ASCII.\n\nThe time unit to encode to, governs timestamp and ISO8601 precision. One of\n\u2018s\u2019, \u2018ms\u2019, \u2018us\u2019, \u2018ns\u2019 for second, millisecond, microsecond, and nanosecond\nrespectively.\n\nHandler to call if object cannot otherwise be converted to a suitable format\nfor JSON. Should receive a single argument which is the object to convert and\nreturn a serialisable object.\n\nIf \u2018orient\u2019 is \u2018records\u2019 write out line-delimited json format. Will throw\nValueError if incorrect \u2018orient\u2019 since others are not list-like.\n\nFor on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018path_or_buf\u2019\npath-like, then detect compression from the following extensions: \u2018.gz\u2019,\n\u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to `None` for\nno compression. Can also be a dict with key `'method'` set to one of {`'zip'`,\n`'gzip'`, `'bz2'`, `'zstd'`} and other key-value pairs are forwarded to\n`zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or\n`zstandard.ZstdDecompressor`, respectively. As an example, the following could\nbe passed for faster compression and to create a reproducible gzip archive:\n`compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}`.\n\nChanged in version 1.4.0: Zstandard support.\n\nWhether to include the index values in the JSON string. Not including the\nindex (`index=False`) is only supported when orient is \u2018split\u2019 or \u2018table\u2019.\n\nLength of whitespace used to indent each record.\n\nNew in version 1.0.0.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nIf path_or_buf is None, returns the resulting json format as a string.\nOtherwise returns None.\n\nSee also\n\nConvert a JSON string to pandas object.\n\nNotes\n\nThe behavior of `indent=0` varies from the stdlib, which does not indent the\noutput but does insert newlines. Currently, `indent=0` and the default\n`indent=None` are equivalent in pandas, though this may change in a future\nrelease.\n\n`orient='table'` contains a \u2018pandas_version\u2019 field under \u2018schema\u2019. This stores\nthe version of pandas used in the latest revision of the schema.\n\nExamples\n\nEncoding/decoding a Dataframe using `'records'` formatted JSON. Note that\nindex labels are not preserved with this encoding.\n\nEncoding/decoding a Dataframe using `'index'` formatted JSON:\n\nEncoding/decoding a Dataframe using `'columns'` formatted JSON:\n\nEncoding/decoding a Dataframe using `'values'` formatted JSON:\n\nEncoding with Table Schema:\n\n"}, {"name": "pandas.Series.to_latex", "path": "reference/api/pandas.series.to_latex", "type": "Series", "text": "\nRender object to a LaTeX tabular, longtable, or nested table.\n\nRequires `\\usepackage{booktabs}`. The output can be copy/pasted into a main\nLaTeX document or read from an external file with `\\input{table.tex}`.\n\nChanged in version 1.0.0: Added caption and label arguments.\n\nChanged in version 1.2.0: Added position argument, changed meaning of caption\nargument.\n\nBuffer to write to. If None, the output is returned as a string.\n\nThe subset of columns to write. Writes all columns by default.\n\nThe minimum width of each column.\n\nWrite out the column names. If a list of strings is given, it is assumed to be\naliases for the column names.\n\nWrite row names (index).\n\nMissing data representation.\n\nFormatter functions to apply to columns\u2019 elements by position or name. The\nresult of each function must be a unicode string. List must be of length equal\nto the number of columns.\n\nFormatter for floating point numbers. For example `float_format=\"%.2f\"` and\n`float_format=\"{:0.2f}\".format` will both result in 0.1234 being formatted as\n0.12.\n\nSet to False for a DataFrame with a hierarchical index to print every\nmultiindex key at each row. By default, the value will be read from the config\nmodule.\n\nPrints the names of the indexes.\n\nMake the row labels bold in the output.\n\nThe columns format as specified in LaTeX table format e.g. \u2018rcl\u2019 for 3\ncolumns. By default, \u2018l\u2019 will be used for all columns except columns of\nnumbers, which default to \u2018r\u2019.\n\nBy default, the value will be read from the pandas config module. Use a\nlongtable environment instead of tabular. Requires adding a\nusepackage{longtable} to your LaTeX preamble.\n\nBy default, the value will be read from the pandas config module. When set to\nFalse prevents from escaping latex special characters in column names.\n\nA string representing the encoding to use in the output file, defaults to\n\u2018utf-8\u2019.\n\nCharacter recognized as decimal separator, e.g. \u2018,\u2019 in Europe.\n\nUse multicolumn to enhance MultiIndex columns. The default will be read from\nthe config module.\n\nThe alignment for multicolumns, similar to column_format The default will be\nread from the config module.\n\nUse multirow to enhance MultiIndex rows. Requires adding a\nusepackage{multirow} to your LaTeX preamble. Will print centered labels\n(instead of top-aligned) across the contained rows, separating groups via\nclines. The default will be read from the pandas config module.\n\nTuple (full_caption, short_caption), which results in\n`\\caption[short_caption]{full_caption}`; if a single string is passed, no\nshort caption will be set.\n\nNew in version 1.0.0.\n\nChanged in version 1.2.0: Optionally allow caption to be a tuple\n`(full_caption, short_caption)`.\n\nThe LaTeX label to be placed inside `\\label{}` in the output. This is used\nwith `\\ref{}` in the main `.tex` file.\n\nNew in version 1.0.0.\n\nThe LaTeX positional argument for tables, to be placed after `\\begin{}` in the\noutput.\n\nNew in version 1.2.0.\n\nIf buf is None, returns the result as a string. Otherwise returns None.\n\nSee also\n\nRender a DataFrame to LaTeX with conditional formatting.\n\nRender a DataFrame to a console-friendly tabular output.\n\nRender a DataFrame as an HTML table.\n\nExamples\n\n"}, {"name": "pandas.Series.to_list", "path": "reference/api/pandas.series.to_list", "type": "Series", "text": "\nReturn a list of the values.\n\nThese are each a scalar type, which is a Python scalar (for str, int, float)\nor a pandas scalar (for Timestamp/Timedelta/Interval/Period)\n\nSee also\n\nReturn the array as an a.ndim-levels deep nested list of Python scalars.\n\n"}, {"name": "pandas.Series.to_markdown", "path": "reference/api/pandas.series.to_markdown", "type": "Series", "text": "\nPrint Series in Markdown-friendly format.\n\nNew in version 1.0.0.\n\nBuffer to write to. If None, the output is returned as a string.\n\nMode in which file is opened, \u201cwt\u201d by default.\n\nAdd index (row) labels.\n\nNew in version 1.1.0.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nThese parameters will be passed to tabulate.\n\nSeries in Markdown-friendly format.\n\nNotes\n\nRequires the tabulate package.\n\nExamples\n\nOutput markdown with a tabulate option.\n\n"}, {"name": "pandas.Series.to_numpy", "path": "reference/api/pandas.series.to_numpy", "type": "Series", "text": "\nA NumPy ndarray representing the values in this Series or Index.\n\nThe dtype to pass to `numpy.asarray()`.\n\nWhether to ensure that the returned value is not a view on another array. Note\nthat `copy=False` does not ensure that `to_numpy()` is no-copy. Rather,\n`copy=True` ensure that a copy is made, even if not strictly necessary.\n\nThe value to use for missing values. The default value depends on dtype and\nthe type of the array.\n\nNew in version 1.0.0.\n\nAdditional keywords passed through to the `to_numpy` method of the underlying\narray (for extension arrays).\n\nNew in version 1.0.0.\n\nSee also\n\nGet the actual data stored within.\n\nGet the actual data stored within.\n\nSimilar method for DataFrame.\n\nNotes\n\nThe returned array will be the same up to equality (values equal in self will\nbe equal in the returned array; likewise for values that are not equal). When\nself contains an ExtensionArray, the dtype may be different. For example, for\na category-dtype Series, `to_numpy()` will return a NumPy array and the\ncategorical dtype will be lost.\n\nFor NumPy dtypes, this will be a reference to the actual data stored in this\nSeries or Index (assuming `copy=False`). Modifying the result in place will\nmodify the data stored in the Series or Index (not that we recommend doing\nthat).\n\nFor extension types, `to_numpy()` may require copying data and coercing the\nresult to a NumPy type (possibly object), which may be expensive. When you\nneed a no-copy reference to the underlying data, `Series.array` should be used\ninstead.\n\nThis table lays out the different dtypes and default return types of\n`to_numpy()` for various dtypes within pandas.\n\ndtype\n\narray type\n\ncategory[T]\n\nndarray[T] (same dtype as input)\n\nperiod\n\nndarray[object] (Periods)\n\ninterval\n\nndarray[object] (Intervals)\n\nIntegerNA\n\nndarray[object]\n\ndatetime64[ns]\n\ndatetime64[ns]\n\ndatetime64[ns, tz]\n\nndarray[object] (Timestamps)\n\nExamples\n\nSpecify the dtype to control how datetime-aware data is represented. Use\n`dtype=object` to return an ndarray of pandas `Timestamp` objects, each with\nthe correct `tz`.\n\nOr `dtype='datetime64[ns]'` to return an ndarray of native datetime64 values.\nThe values are converted to UTC and the timezone info is dropped.\n\n"}, {"name": "pandas.Series.to_period", "path": "reference/api/pandas.series.to_period", "type": "Input/output", "text": "\nConvert Series from DatetimeIndex to PeriodIndex.\n\nFrequency associated with the PeriodIndex.\n\nWhether or not to return a copy.\n\nSeries with index converted to PeriodIndex.\n\n"}, {"name": "pandas.Series.to_pickle", "path": "reference/api/pandas.series.to_pickle", "type": "Series", "text": "\nPickle (serialize) object to file.\n\nFile path where the pickled object will be stored.\n\nFor on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018path\u2019 path-\nlike, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019,\n\u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to `None` for no\ncompression. Can also be a dict with key `'method'` set to one of {`'zip'`,\n`'gzip'`, `'bz2'`, `'zstd'`} and other key-value pairs are forwarded to\n`zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or\n`zstandard.ZstdDecompressor`, respectively. As an example, the following could\nbe passed for faster compression and to create a reproducible gzip archive:\n`compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}`.\n\nInt which indicates which protocol should be used by the pickler, default\nHIGHEST_PROTOCOL (see [1] paragraph 12.1.2). The possible values are 0, 1, 2,\n3, 4, 5. A negative value for the protocol parameter is equivalent to setting\nits value to HIGHEST_PROTOCOL.\n\nhttps://docs.python.org/3/library/pickle.html.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nSee also\n\nLoad pickled pandas object (or any object) from file.\n\nWrite DataFrame to an HDF5 file.\n\nWrite DataFrame to a SQL database.\n\nWrite a DataFrame to the binary parquet format.\n\nExamples\n\n"}, {"name": "pandas.Series.to_sql", "path": "reference/api/pandas.series.to_sql", "type": "Series", "text": "\nWrite records stored in a DataFrame to a SQL database.\n\nDatabases supported by SQLAlchemy [1] are supported. Tables can be newly\ncreated, appended to, or overwritten.\n\nName of SQL table.\n\nUsing SQLAlchemy makes it possible to use any DB supported by that library.\nLegacy support is provided for sqlite3.Connection objects. The user is\nresponsible for engine disposal and connection closure for the SQLAlchemy\nconnectable See here.\n\nSpecify the schema (if database flavor supports this). If None, use default\nschema.\n\nHow to behave if the table already exists.\n\nfail: Raise a ValueError.\n\nreplace: Drop the table before inserting new values.\n\nappend: Insert new values to the existing table.\n\nWrite DataFrame index as a column. Uses index_label as the column name in the\ntable.\n\nColumn label for index column(s). If None is given (default) and index is\nTrue, then the index names are used. A sequence should be given if the\nDataFrame uses MultiIndex.\n\nSpecify the number of rows in each batch to be written at a time. By default,\nall rows will be written at once.\n\nSpecifying the datatype for columns. If a dictionary is used, the keys should\nbe the column names and the values should be the SQLAlchemy types or strings\nfor the sqlite3 legacy mode. If a scalar is provided, it will be applied to\nall columns.\n\nControls the SQL insertion clause used:\n\nNone : Uses standard SQL `INSERT` clause (one per row).\n\n\u2018multi\u2019: Pass multiple values in a single `INSERT` clause.\n\ncallable with signature `(pd_table, conn, keys, data_iter)`.\n\nDetails and a sample callable implementation can be found in the section\ninsert method.\n\nNumber of rows affected by to_sql. None is returned if the callable passed\ninto `method` does not return the number of rows.\n\nThe number of returned rows affected is the sum of the `rowcount` attribute of\n`sqlite3.Cursor` or SQLAlchemy connectable which may not reflect the exact\nnumber of written rows as stipulated in the sqlite3 or SQLAlchemy.\n\nNew in version 1.4.0.\n\nWhen the table already exists and if_exists is \u2018fail\u2019 (the default).\n\nSee also\n\nRead a DataFrame from a table.\n\nNotes\n\nTimezone aware datetime columns will be written as `Timestamp with timezone`\ntype with SQLAlchemy if supported by the database. Otherwise, the datetimes\nwill be stored as timezone unaware timestamps local to the original timezone.\n\nReferences\n\nhttps://docs.sqlalchemy.org\n\nhttps://www.python.org/dev/peps/pep-0249/\n\nExamples\n\nCreate an in-memory SQLite database.\n\nCreate a table from scratch with 3 rows.\n\nAn sqlalchemy.engine.Connection can also be passed to con:\n\nThis is allowed to support operations that require that the same DBAPI\nconnection is used for the entire operation.\n\nOverwrite the table with just `df2`.\n\nSpecify the dtype (especially useful for integers with missing values). Notice\nthat while pandas is forced to store the data as floating point, the database\nsupports nullable integers. When fetching the data with Python, we get back\ninteger scalars.\n\n"}, {"name": "pandas.Series.to_string", "path": "reference/api/pandas.series.to_string", "type": "Series", "text": "\nRender a string representation of the Series.\n\nBuffer to write to.\n\nString representation of NaN to use, default \u2018NaN\u2019.\n\nFormatter function to apply to columns\u2019 elements if they are floats, default\nNone.\n\nAdd the Series header (index name).\n\nAdd index (row) labels, default True.\n\nAdd the Series length.\n\nAdd the Series dtype.\n\nAdd the Series name if not None.\n\nMaximum number of rows to show before truncating. If None, show all.\n\nThe number of rows to display in a truncated repr (when number of rows is\nabove max_rows).\n\nString representation of Series if `buf=None`, otherwise None.\n\n"}, {"name": "pandas.Series.to_timestamp", "path": "reference/api/pandas.series.to_timestamp", "type": "Series", "text": "\nCast to DatetimeIndex of Timestamps, at beginning of period.\n\nDesired frequency.\n\nConvention for converting period to timestamp; start of period vs. end.\n\nWhether or not to return a copy.\n\n"}, {"name": "pandas.Series.to_xarray", "path": "reference/api/pandas.series.to_xarray", "type": "Series", "text": "\nReturn an xarray object from the pandas object.\n\nData in the pandas structure converted to Dataset if the object is a\nDataFrame, or a DataArray if the object is a Series.\n\nSee also\n\nWrite DataFrame to an HDF5 file.\n\nWrite a DataFrame to the binary parquet format.\n\nNotes\n\nSee the xarray docs\n\nExamples\n\n"}, {"name": "pandas.Series.tolist", "path": "reference/api/pandas.series.tolist", "type": "Series", "text": "\nReturn a list of the values.\n\nThese are each a scalar type, which is a Python scalar (for str, int, float)\nor a pandas scalar (for Timestamp/Timedelta/Interval/Period)\n\nSee also\n\nReturn the array as an a.ndim-levels deep nested list of Python scalars.\n\n"}, {"name": "pandas.Series.transform", "path": "reference/api/pandas.series.transform", "type": "Series", "text": "\nCall `func` on self producing a Series with the same axis shape as self.\n\nFunction to use for transforming the data. If a function, must either work\nwhen passed a Series or when passed to Series.apply. If func is both list-like\nand dict-like, dict-like behavior takes precedence.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist-like of functions and/or function names, e.g. `[np.exp, 'sqrt']`\n\ndict-like of axis labels -> functions, function names or list-like of such.\n\nParameter needed for compatibility with DataFrame.\n\nPositional arguments to pass to func.\n\nKeyword arguments to pass to func.\n\nA Series that must have the same length as self.\n\nSee also\n\nOnly perform aggregating type operations.\n\nInvoke function on a Series.\n\nNotes\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nExamples\n\nEven though the resulting Series must have the same length as the input\nSeries, it is possible to provide several input functions:\n\nYou can call transform on a GroupBy object:\n\n"}, {"name": "pandas.Series.transpose", "path": "reference/api/pandas.series.transpose", "type": "Series", "text": "\nReturn the transpose, which is by definition self.\n\n"}, {"name": "pandas.Series.truediv", "path": "reference/api/pandas.series.truediv", "type": "Series", "text": "\nReturn Floating division of series and other, element-wise (binary operator\ntruediv).\n\nEquivalent to `series / other`, but with support to substitute a fill_value\nfor missing data in either one of the inputs.\n\nFill existing missing (NaN) values, and any new element needed for successful\nSeries alignment, with this value before computation. If data in both\ncorresponding Series locations is missing the result of filling (at that\nlocation) will be missing.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nThe result of the operation.\n\nSee also\n\nReverse of the Floating division operator, see Python documentation for more\ndetails.\n\nExamples\n\n"}, {"name": "pandas.Series.truncate", "path": "reference/api/pandas.series.truncate", "type": "Series", "text": "\nTruncate a Series or DataFrame before and after some index value.\n\nThis is a useful shorthand for boolean indexing based on index values above or\nbelow certain thresholds.\n\nTruncate all rows before this index value.\n\nTruncate all rows after this index value.\n\nAxis to truncate. Truncates the index (rows) by default.\n\nReturn a copy of the truncated section.\n\nThe truncated Series or DataFrame.\n\nSee also\n\nSelect a subset of a DataFrame by label.\n\nSelect a subset of a DataFrame by position.\n\nNotes\n\nIf the index being truncated contains only datetime values, before and after\nmay be specified as strings instead of Timestamps.\n\nExamples\n\nThe columns of a DataFrame can be truncated.\n\nFor Series, only rows can be truncated.\n\nThe index values in `truncate` can be datetimes or string dates.\n\nBecause the index is a DatetimeIndex containing only dates, we can specify\nbefore and after as strings. They will be coerced to Timestamps before\ntruncation.\n\nNote that `truncate` assumes a 0 value for any unspecified time component\n(midnight). This differs from partial string slicing, which returns any\npartially matching dates.\n\n"}, {"name": "pandas.Series.tshift", "path": "reference/api/pandas.series.tshift", "type": "Series", "text": "\nShift the time index, using the index\u2019s frequency if available.\n\nDeprecated since version 1.1.0: Use shift instead.\n\nNumber of periods to move, can be positive or negative.\n\nIncrement to use from the tseries module or time rule expressed as a string\n(e.g. \u2018EOM\u2019).\n\nCorresponds to the axis that contains the Index.\n\nNotes\n\nIf freq is not specified then tries to use the freq or inferred_freq\nattributes of the index. If neither of those attributes exist, a ValueError is\nthrown\n\n"}, {"name": "pandas.Series.tz_convert", "path": "reference/api/pandas.series.tz_convert", "type": "Series", "text": "\nConvert tz-aware axis to target time zone.\n\nIf axis is a MultiIndex, convert a specific level. Otherwise must be None.\n\nAlso make a copy of the underlying data.\n\nObject with time zone converted axis.\n\nIf the axis is tz-naive.\n\n"}, {"name": "pandas.Series.tz_localize", "path": "reference/api/pandas.series.tz_localize", "type": "Series", "text": "\nLocalize tz-naive index of a Series or DataFrame to target time zone.\n\nThis operation localizes the Index. To localize the values in a timezone-naive\nSeries, use `Series.dt.tz_localize()`.\n\nIf axis ia a MultiIndex, localize a specific level. Otherwise must be None.\n\nAlso make a copy of the underlying data.\n\nWhen clocks moved backward due to DST, ambiguous times may arise. For example\nin Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST,\n02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a\nsituation, the ambiguous parameter dictates how ambiguous times should be\nhandled.\n\n\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order\n\nbool-ndarray where True signifies a DST time, False designates a non-DST time\n(note that this flag is only applicable for ambiguous times)\n\n\u2018NaT\u2019 will return NaT where there are ambiguous times\n\n\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST. Valid values are:\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times\n\ntimedelta objects will shift nonexistent times by the timedelta\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nSame type as the input.\n\nIf the TimeSeries is tz-aware and tz is not None.\n\nExamples\n\nLocalize local times:\n\nBe careful with DST changes. When there is sequential data, pandas can infer\nthe DST time:\n\nIn some cases, inferring the DST is impossible. In such cases, you can pass an\nndarray to the ambiguous parameter to set the DST explicitly\n\nIf the DST transition causes nonexistent times, you can shift these dates\nforward or backward with a timedelta object or \u2018shift_forward\u2019 or\n\u2018shift_backward\u2019.\n\n"}, {"name": "pandas.Series.unique", "path": "reference/api/pandas.series.unique", "type": "Series", "text": "\nReturn unique values of Series object.\n\nUniques are returned in order of appearance. Hash table-based unique,\ntherefore does NOT sort.\n\nThe unique values returned as a NumPy array. See Notes.\n\nSee also\n\nTop-level unique method for any 1-d array-like object.\n\nReturn Index with unique values from an Index object.\n\nNotes\n\nReturns the unique values as a NumPy array. In case of an extension-array\nbacked Series, a new `ExtensionArray` of that type with just the unique values\nis returned. This includes\n\nCategorical\n\nPeriod\n\nDatetime with Timezone\n\nInterval\n\nSparse\n\nIntegerNA\n\nSee Examples section.\n\nExamples\n\nAn Categorical will return categories in the order of appearance and with the\nsame dtype.\n\n"}, {"name": "pandas.Series.unstack", "path": "reference/api/pandas.series.unstack", "type": "Series", "text": "\nUnstack, also known as pivot, Series with MultiIndex to produce DataFrame.\n\nLevel(s) to unstack, can pass level name.\n\nValue to use when replacing NaN values.\n\nUnstacked Series.\n\nExamples\n\n"}, {"name": "pandas.Series.update", "path": "reference/api/pandas.series.update", "type": "Series", "text": "\nModify Series in place using values from passed Series.\n\nUses non-NA values from passed Series to make updates. Aligns on index.\n\nExamples\n\nIf `other` contains NaNs the corresponding values are not updated in the\noriginal Series.\n\n`other` can also be a non-Series object type that is coercible into a Series\n\n"}, {"name": "pandas.Series.value_counts", "path": "reference/api/pandas.series.value_counts", "type": "Series", "text": "\nReturn a Series containing counts of unique values.\n\nThe resulting object will be in descending order so that the first element is\nthe most frequently-occurring element. Excludes NA values by default.\n\nIf True then the object returned will contain the relative frequencies of the\nunique values.\n\nSort by frequencies.\n\nSort in ascending order.\n\nRather than count values, group them into half-open bins, a convenience for\n`pd.cut`, only works with numeric data.\n\nDon\u2019t include counts of NaN.\n\nSee also\n\nNumber of non-NA elements in a Series.\n\nNumber of non-NA elements in a DataFrame.\n\nEquivalent method on DataFrames.\n\nExamples\n\nWith normalize set to True, returns the relative frequency by dividing all\nvalues by the sum of values.\n\nbins\n\nBins can be useful for going from a continuous variable to a categorical\nvariable; instead of counting unique apparitions of values, divide the index\nin the specified number of half-open bins.\n\ndropna\n\nWith dropna set to False we can also see NaN index values.\n\n"}, {"name": "pandas.Series.values", "path": "reference/api/pandas.series.values", "type": "Series", "text": "\nReturn Series as ndarray or ndarray-like depending on the dtype.\n\nWarning\n\nWe recommend using `Series.array` or `Series.to_numpy()`, depending on whether\nyou need a reference to the underlying data or a NumPy array.\n\nSee also\n\nReference to the underlying data.\n\nA NumPy array representing the underlying data.\n\nExamples\n\nTimezone aware datetime data is converted to UTC:\n\n"}, {"name": "pandas.Series.var", "path": "reference/api/pandas.series.var", "type": "Series", "text": "\nReturn unbiased variance over requested axis.\n\nNormalized by N-1 by default. This can be changed using the ddof argument.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a scalar.\n\nDelta Degrees of Freedom. The divisor used in calculations is N - ddof, where\nN represents the number of elements.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nExamples\n\nAlternatively, `ddof=0` can be set to normalize by N instead of N-1:\n\n"}, {"name": "pandas.Series.view", "path": "reference/api/pandas.series.view", "type": "Series", "text": "\nCreate a new view of the Series.\n\nThis function will return a new Series with a view of the same underlying\nvalues in memory, optionally reinterpreted with a new data type. The new data\ntype must preserve the same size in bytes as to not cause index misalignment.\n\nData type object or one of their string representations.\n\nA new Series object as a view of the same data in memory.\n\nSee also\n\nEquivalent numpy function to create a new view of the same data in memory.\n\nNotes\n\nSeries are instantiated with `dtype=float64` by default. While\n`numpy.ndarray.view()` will return a view with the same data type as the\noriginal array, `Series.view()` (without specified dtype) will try using\n`float64` and may fail if the original data type size in bytes is not the\nsame.\n\nExamples\n\nThe 8 bit signed integer representation of -1 is 0b11111111, but the same\nbytes represent 255 if read as an 8 bit unsigned integer:\n\nThe views share the same underlying values:\n\n"}, {"name": "pandas.Series.where", "path": "reference/api/pandas.series.where", "type": "Series", "text": "\nReplace values where the condition is False.\n\nWhere cond is True, keep the original value. Where False, replace with\ncorresponding value from other. If cond is callable, it is computed on the\nSeries/DataFrame and should return boolean Series/DataFrame or array. The\ncallable must not change input Series/DataFrame (though pandas doesn\u2019t check\nit).\n\nEntries where cond is False are replaced with corresponding value from other.\nIf other is callable, it is computed on the Series/DataFrame and should return\nscalar or Series/DataFrame. The callable must not change input\nSeries/DataFrame (though pandas doesn\u2019t check it).\n\nWhether to perform the operation in place on the data.\n\nAlignment axis if needed.\n\nAlignment level if needed.\n\nNote that currently this parameter won\u2019t affect the results and will always\ncoerce to a suitable dtype.\n\n\u2018raise\u2019 : allow exceptions to be raised.\n\n\u2018ignore\u2019 : suppress exceptions. On error return original object.\n\nTry to cast the result back to the input type (if possible).\n\nDeprecated since version 1.3.0: Manually cast back if necessary.\n\nSee also\n\nReturn an object of same shape as self.\n\nNotes\n\nThe where method is an application of the if-then idiom. For each element in\nthe calling DataFrame, if `cond` is `True` the element is used; otherwise the\ncorresponding element from the DataFrame `other` is used.\n\nThe signature for `DataFrame.where()` differs from `numpy.where()`. Roughly\n`df1.where(m, df2)` is equivalent to `np.where(m, df1, df2)`.\n\nFor further details and examples see the `where` documentation in indexing.\n\nExamples\n\n"}, {"name": "pandas.Series.xs", "path": "reference/api/pandas.series.xs", "type": "Series", "text": "\nReturn cross-section from the Series/DataFrame.\n\nThis method takes a key argument to select data at a particular level of a\nMultiIndex.\n\nLabel contained in the index, or partially in a MultiIndex.\n\nAxis to retrieve cross-section on.\n\nIn case of a key partially contained in a MultiIndex, indicate which levels\nare used. Levels can be referred by label or position.\n\nIf False, returns object with same levels as self.\n\nCross-section from the original Series or DataFrame corresponding to the\nselected index levels.\n\nSee also\n\nAccess a group of rows and columns by label(s) or a boolean array.\n\nPurely integer-location based indexing for selection by position.\n\nNotes\n\nxs can not be used to set values.\n\nMultiIndex Slicers is a generic way to get/set values on any level or levels.\nIt is a superset of xs functionality, see MultiIndex Slicers.\n\nExamples\n\nGet values at specified index\n\nGet values at several indexes\n\nGet values at specified index and level\n\nGet values at several indexes and levels\n\nGet values at specified column and axis\n\n"}, {"name": "pandas.set_option", "path": "reference/api/pandas.set_option", "type": "General utility functions", "text": "\nSets the value of the specified option.\n\nAvailable options:\n\ncompute.[use_bottleneck, use_numba, use_numexpr]\n\ndisplay.[chop_threshold, colheader_justify, column_space, date_dayfirst,\ndate_yearfirst, encoding, expand_frame_repr, float_format]\n\ndisplay.html.[border, table_schema, use_mathjax]\n\ndisplay.[large_repr]\n\ndisplay.latex.[escape, longtable, multicolumn, multicolumn_format, multirow,\nrepr]\n\ndisplay.[max_categories, max_columns, max_colwidth, max_dir_items,\nmax_info_columns, max_info_rows, max_rows, max_seq_items, memory_usage,\nmin_rows, multi_sparse, notebook_repr_html, pprint_nest_depth, precision,\nshow_dimensions]\n\ndisplay.unicode.[ambiguous_as_wide, east_asian_width]\n\ndisplay.[width]\n\nio.excel.ods.[reader, writer]\n\nio.excel.xls.[reader, writer]\n\nio.excel.xlsb.[reader]\n\nio.excel.xlsm.[reader, writer]\n\nio.excel.xlsx.[reader, writer]\n\nio.hdf.[default_format, dropna_table]\n\nio.parquet.[engine]\n\nio.sql.[engine]\n\nmode.[chained_assignment, data_manager, sim_interactive, string_storage,\nuse_inf_as_na, use_inf_as_null]\n\nplotting.[backend]\n\nplotting.matplotlib.[register_converters]\n\nstyler.format.[decimal, escape, formatter, na_rep, precision, thousands]\n\nstyler.html.[mathjax]\n\nstyler.latex.[environment, hrules, multicol_align, multirow_align]\n\nstyler.render.[encoding, max_columns, max_elements, max_rows, repr]\n\nstyler.sparse.[columns, index]\n\nRegexp which should match a single option. Note: partial matches are supported\nfor convenience, but unless you use the full option name (e.g.\nx.y.z.option_name), your code may break in future versions if new options with\nsimilar names are introduced.\n\nNew value of option.\n\nNotes\n\nThe available options with its descriptions:\n\nUse the bottleneck library to accelerate if it is installed, the default is\nTrue Valid values: False,True [default: True] [currently: True]\n\nUse the numba engine option for select operations if it is installed, the\ndefault is False Valid values: False,True [default: False] [currently: False]\n\nUse the numexpr library to accelerate computation if it is installed, the\ndefault is True Valid values: False,True [default: True] [currently: True]\n\nif set to a float value, all float values smaller then the given threshold\nwill be displayed as exactly 0 by repr and friends. [default: None]\n[currently: None]\n\nControls the justification of column headers. used by DataFrameFormatter.\n[default: right] [currently: right]\n\n[default: 12] [currently: 12]\n\nWhen True, prints and parses dates with the day first, eg 20/01/2005 [default:\nFalse] [currently: False]\n\nWhen True, prints and parses dates with the year first, eg 2005/01/20\n[default: False] [currently: False]\n\nDefaults to the detected encoding of the console. Specifies the encoding to be\nused for strings returned by to_string, these are generally strings meant to\nbe displayed on the console. [default: utf-8] [currently: utf-8]\n\nWhether to print out the full DataFrame repr for wide DataFrames across\nmultiple lines, max_columns is still respected, but the output will wrap-\naround across multiple \u201cpages\u201d if its width exceeds display.width. [default:\nTrue] [currently: True]\n\nThe callable should accept a floating point number and return a string with\nthe desired format of the number. This is used in some places like\nSeriesFormatter. See formats.format.EngFormatter for an example. [default:\nNone] [currently: None]\n\nA `border=value` attribute is inserted in the `<table>` tag for the DataFrame\nHTML repr. [default: 1] [currently: 1]\n\nWhether to publish a Table Schema representation for frontends that support\nit. (default: False) [default: False] [currently: False]\n\nWhen True, Jupyter notebook will process table contents using MathJax,\nrendering mathematical expressions enclosed by the dollar symbol. (default:\nTrue) [default: True] [currently: True]\n\nFor DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show\na truncated table (the default from 0.13), or switch to the view from\ndf.info() (the behaviour in earlier versions of pandas). [default: truncate]\n[currently: truncate]\n\nThis specifies if the to_latex method of a Dataframe uses escapes special\ncharacters. Valid values: False,True [default: True] [currently: True]\n\nThis specifies if the to_latex method of a Dataframe uses the longtable\nformat. Valid values: False,True [default: False] [currently: False]\n\nThis specifies if the to_latex method of a Dataframe uses multicolumns to\npretty-print MultiIndex columns. Valid values: False,True [default: True]\n[currently: True]\n\nThis specifies if the to_latex method of a Dataframe uses multicolumns to\npretty-print MultiIndex columns. Valid values: False,True [default: l]\n[currently: l]\n\nThis specifies if the to_latex method of a Dataframe uses multirows to pretty-\nprint MultiIndex rows. Valid values: False,True [default: False] [currently:\nFalse]\n\nWhether to produce a latex DataFrame representation for jupyter environments\nthat support it. (default: False) [default: False] [currently: False]\n\nThis sets the maximum number of categories pandas should output when printing\nout a Categorical or a Series of dtype \u201ccategory\u201d. [default: 8] [currently: 8]\n\nIf max_cols is exceeded, switch to truncate view. Depending on large_repr,\nobjects are either centrally truncated or printed as a summary view. \u2018None\u2019\nvalue means unlimited.\n\nIn case python/IPython is running in a terminal and large_repr equals\n\u2018truncate\u2019 this can be set to 0 and pandas will auto-detect the width of the\nterminal and print a truncated object which fits the screen width. The IPython\nnotebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is\nnot possible to do correct auto-detection. [default: 0] [currently: 0]\n\nThe maximum width in characters of a column in the repr of a pandas data\nstructure. When the column overflows, a \u201c\u2026\u201d placeholder is embedded in the\noutput. A \u2018None\u2019 value means unlimited. [default: 50] [currently: 50]\n\nThe number of items that will be added to dir(\u2026). \u2018None\u2019 value means\nunlimited. Because dir is cached, changing this option will not immediately\naffect already existing dataframes until a column is deleted or added.\n\nThis is for instance used to suggest columns from a dataframe to tab\ncompletion. [default: 100] [currently: 100]\n\nmax_info_columns is used in DataFrame.info method to decide if per column\ninformation will be printed. [default: 100] [currently: 100]\n\ndf.info() will usually show null-counts for each column. For large frames this\ncan be quite slow. max_info_rows and max_info_cols limit this null check only\nto frames with smaller dimensions than specified. [default: 1690785]\n[currently: 1690785]\n\nIf max_rows is exceeded, switch to truncate view. Depending on large_repr,\nobjects are either centrally truncated or printed as a summary view. \u2018None\u2019\nvalue means unlimited.\n\nIn case python/IPython is running in a terminal and large_repr equals\n\u2018truncate\u2019 this can be set to 0 and pandas will auto-detect the height of the\nterminal and print a truncated object which fits the screen height. The\nIPython notebook, IPython qtconsole, or IDLE do not run in a terminal and\nhence it is not possible to do correct auto-detection. [default: 60]\n[currently: 60]\n\nWhen pretty-printing a long sequence, no more then max_seq_items will be\nprinted. If items are omitted, they will be denoted by the addition of \u201c\u2026\u201d to\nthe resulting string.\n\nIf set to None, the number of items to be printed is unlimited. [default: 100]\n[currently: 100]\n\nThis specifies if the memory usage of a DataFrame should be displayed when\ndf.info() is called. Valid values True,False,\u2019deep\u2019 [default: True]\n[currently: True]\n\nThe numbers of rows to show in a truncated view (when max_rows is exceeded).\nIgnored when max_rows is set to None or 0. When set to None, follows the value\nof max_rows. [default: 10] [currently: 10]\n\n\u201csparsify\u201d MultiIndex display (don\u2019t display repeated elements in outer levels\nwithin groups) [default: True] [currently: True]\n\nWhen True, IPython notebook will use html representation for pandas objects\n(if it is available). [default: True] [currently: True]\n\nControls the number of nested levels to process when pretty-printing [default:\n3] [currently: 3]\n\nFloating point output precision in terms of number of places after the\ndecimal, for regular formatting as well as scientific notation. Similar to\n`precision` in `numpy.set_printoptions()`. [default: 6] [currently: 6]\n\nWhether to print out dimensions at the end of DataFrame repr. If \u2018truncate\u2019 is\nspecified, only print out the dimensions if the frame is truncated (e.g. not\ndisplay all rows and/or columns) [default: truncate] [currently: truncate]\n\nWhether to use the Unicode East Asian Width to calculate the display text\nwidth. Enabling this may affect to the performance (default: False) [default:\nFalse] [currently: False]\n\nWhether to use the Unicode East Asian Width to calculate the display text\nwidth. Enabling this may affect to the performance (default: False) [default:\nFalse] [currently: False]\n\nWidth of the display in characters. In case python/IPython is running in a\nterminal this can be set to None and pandas will correctly auto-detect the\nwidth. Note that the IPython notebook, IPython qtconsole, or IDLE do not run\nin a terminal and hence it is not possible to correctly detect the width.\n[default: 80] [currently: 80]\n\nThe default Excel reader engine for \u2018ods\u2019 files. Available options: auto, odf.\n[default: auto] [currently: auto]\n\nThe default Excel writer engine for \u2018ods\u2019 files. Available options: auto, odf.\n[default: auto] [currently: auto]\n\nThe default Excel reader engine for \u2018xls\u2019 files. Available options: auto,\nxlrd. [default: auto] [currently: auto]\n\nThe default Excel writer engine for \u2018xls\u2019 files. Available options: auto,\nxlwt. [default: auto] [currently: auto] (Deprecated, use `` instead.)\n\nThe default Excel reader engine for \u2018xlsb\u2019 files. Available options: auto,\npyxlsb. [default: auto] [currently: auto]\n\nThe default Excel reader engine for \u2018xlsm\u2019 files. Available options: auto,\nxlrd, openpyxl. [default: auto] [currently: auto]\n\nThe default Excel writer engine for \u2018xlsm\u2019 files. Available options: auto,\nopenpyxl. [default: auto] [currently: auto]\n\nThe default Excel reader engine for \u2018xlsx\u2019 files. Available options: auto,\nxlrd, openpyxl. [default: auto] [currently: auto]\n\nThe default Excel writer engine for \u2018xlsx\u2019 files. Available options: auto,\nopenpyxl, xlsxwriter. [default: auto] [currently: auto]\n\ndefault format writing format, if None, then put will default to \u2018fixed\u2019 and\nappend will default to \u2018table\u2019 [default: None] [currently: None]\n\ndrop ALL nan rows when appending to a table [default: False] [currently:\nFalse]\n\nThe default parquet reader/writer engine. Available options: \u2018auto\u2019,\n\u2018pyarrow\u2019, \u2018fastparquet\u2019, the default is \u2018auto\u2019 [default: auto] [currently:\nauto]\n\nThe default sql reader/writer engine. Available options: \u2018auto\u2019, \u2018sqlalchemy\u2019,\nthe default is \u2018auto\u2019 [default: auto] [currently: auto]\n\nRaise an exception, warn, or no action if trying to use chained assignment,\nThe default is warn [default: warn] [currently: warn]\n\nInternal data manager type; can be \u201cblock\u201d or \u201carray\u201d. Defaults to \u201cblock\u201d,\nunless overridden by the \u2018PANDAS_DATA_MANAGER\u2019 environment variable (needs to\nbe set before pandas is imported). [default: block] [currently: block]\n\nWhether to simulate interactive mode for purposes of testing [default: False]\n[currently: False]\n\nThe default storage for StringDtype. [default: python] [currently: python]\n\nTrue means treat None, NaN, INF, -INF as NA (old way), False means None and\nNaN are null, but INF, -INF are not NA (new way). [default: False] [currently:\nFalse]\n\nuse_inf_as_null had been deprecated and will be removed in a future version.\nUse use_inf_as_na instead. [default: False] [currently: False] (Deprecated,\nuse mode.use_inf_as_na instead.)\n\nThe plotting backend to use. The default value is \u201cmatplotlib\u201d, the backend\nprovided with pandas. Other backends can be specified by providing the name of\nthe module that implements the backend. [default: matplotlib] [currently:\nmatplotlib]\n\nWhether to register converters with matplotlib\u2019s units registry for dates,\ntimes, datetimes, and Periods. Toggling to False will remove the converters,\nrestoring any converters that pandas overwrote. [default: auto] [currently:\nauto]\n\nThe character representation for the decimal separator for floats and complex.\n[default: .] [currently: .]\n\nWhether to escape certain characters according to the given context; html or\nlatex. [default: None] [currently: None]\n\nA formatter object to be used as default within `Styler.format`. [default:\nNone] [currently: None]\n\nThe string representation for values identified as missing. [default: None]\n[currently: None]\n\nThe precision for floats and complex numbers. [default: 6] [currently: 6]\n\nThe character representation for thousands separator for floats, int and\ncomplex. [default: None] [currently: None]\n\nIf False will render special CSS classes to table attributes that indicate\nMathjax will not be used in Jupyter Notebook. [default: True] [currently:\nTrue]\n\nThe environment to replace `\\begin{table}`. If \u201clongtable\u201d is used results in\na specific longtable environment format. [default: None] [currently: None]\n\nWhether to add horizontal rules on top and bottom and below the headers.\n[default: False] [currently: False]\n\nThe specifier for horizontal alignment of sparsified LaTeX multicolumns. Pipe\ndecorators can also be added to non-naive values to draw vertical rules, e.g.\n\u201c|r\u201d will draw a rule on the left side of right aligned merged cells.\n[default: r] [currently: r]\n\nThe specifier for vertical alignment of sparsified LaTeX multirows. [default:\nc] [currently: c]\n\nThe encoding used for output HTML and LaTeX files. [default: utf-8]\n[currently: utf-8]\n\nThe maximum number of columns that will be rendered. May still be reduced to\nsatsify `max_elements`, which takes precedence. [default: None] [currently:\nNone]\n\nThe maximum number of data-cell (<td>) elements that will be rendered before\ntrimming will occur over columns, rows or both if needed. [default: 262144]\n[currently: 262144]\n\nThe maximum number of rows that will be rendered. May still be reduced to\nsatsify `max_elements`, which takes precedence. [default: None] [currently:\nNone]\n\nDetermine which output to use in Jupyter Notebook in {\u201chtml\u201d, \u201clatex\u201d}.\n[default: html] [currently: html]\n\nWhether to sparsify the display of hierarchical columns. Setting to False will\ndisplay each explicit level element in a hierarchical key for each column.\n[default: True] [currently: True]\n\nWhether to sparsify the display of a hierarchical index. Setting to False will\ndisplay each explicit level element in a hierarchical key for each row.\n[default: True] [currently: True]\n\n"}, {"name": "pandas.show_versions", "path": "reference/api/pandas.show_versions", "type": "General utility functions", "text": "\nProvide useful information, important for bug reports.\n\nIt comprises info about hosting operation system, pandas version, and versions\nof other installed relative packages.\n\nIf False, outputs info in a human readable form to the console.\n\nIf str, it will be considered as a path to a file. Info will be written to\nthat file in JSON format.\n\nIf True, outputs info in JSON format to the console.\n\n"}, {"name": "pandas.SparseDtype", "path": "reference/api/pandas.sparsedtype", "type": "Pandas arrays", "text": "\nDtype for data stored in `SparseArray`.\n\nThis dtype implements the pandas ExtensionDtype interface.\n\nThe dtype of the underlying array storing the non-fill value values.\n\nThe scalar value not stored in the SparseArray. By default, this depends on\ndtype.\n\ndtype\n\nna_value\n\nfloat\n\n`np.nan`\n\nint\n\n`0`\n\nbool\n\n`False`\n\ndatetime64\n\n`pd.NaT`\n\ntimedelta64\n\n`pd.NaT`\n\nThe default value may be overridden by specifying a fill_value.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.StringDtype", "path": "reference/api/pandas.stringdtype", "type": "Pandas arrays", "text": "\nExtension dtype for string data.\n\nNew in version 1.0.0.\n\nWarning\n\nStringDtype is considered experimental. The implementation and parts of the\nAPI may change without warning.\n\nIn particular, StringDtype.na_value may change to no longer be `numpy.nan`.\n\nIf not given, the value of `pd.options.mode.string_storage`.\n\nExamples\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.test", "path": "reference/api/pandas.test", "type": "General functions", "text": "\nRun the pandas test suite using pytest.\n\n"}, {"name": "pandas.testing.assert_extension_array_equal", "path": "reference/api/pandas.testing.assert_extension_array_equal", "type": "General utility functions", "text": "\nCheck that left and right ExtensionArrays are equal.\n\nThe two arrays to compare.\n\nWhether to check if the ExtensionArray dtypes are identical.\n\nOptional index (shared by both left and right), used in output.\n\nSpecify comparison precision. Only used when check_exact is False. 5 digits\n(False) or 3 digits (True) after decimal points are compared. If int, then\nspecify the digits to compare.\n\nDeprecated since version 1.1.0: Use rtol and atol instead to define\nrelative/absolute tolerance, respectively. Similar to `math.isclose()`.\n\nWhether to compare number exactly.\n\nRelative tolerance. Only used when check_exact is False.\n\nNew in version 1.1.0.\n\nAbsolute tolerance. Only used when check_exact is False.\n\nNew in version 1.1.0.\n\nNotes\n\nMissing values are checked separately from valid values. A mask of missing\nvalues is computed for each and checked to match. The remaining all-valid\nvalues are cast to object dtype and checked.\n\nExamples\n\n"}, {"name": "pandas.testing.assert_frame_equal", "path": "reference/api/pandas.testing.assert_frame_equal", "type": "General utility functions", "text": "\nCheck that left and right DataFrame are equal.\n\nThis function is intended to compare two DataFrames and output any\ndifferences. Is is mostly intended for use in unit tests. Additional\nparameters allow varying the strictness of the equality checks performed.\n\nFirst DataFrame to compare.\n\nSecond DataFrame to compare.\n\nWhether to check the DataFrame dtype is identical.\n\nWhether to check the Index class, dtype and inferred_type are identical.\n\nWhether to check the columns class, dtype and inferred_type are identical. Is\npassed as the `exact` argument of `assert_index_equal()`.\n\nWhether to check the DataFrame class is identical.\n\nSpecify comparison precision. Only used when check_exact is False. 5 digits\n(False) or 3 digits (True) after decimal points are compared. If int, then\nspecify the digits to compare.\n\nWhen comparing two numbers, if the first number has magnitude less than 1e-5,\nwe compare the two numbers directly and check whether they are equivalent\nwithin the specified precision. Otherwise, we compare the ratio of the second\nnumber to the first number and check whether it is equivalent to 1 within the\nspecified precision.\n\nDeprecated since version 1.1.0: Use rtol and atol instead to define\nrelative/absolute tolerance, respectively. Similar to `math.isclose()`.\n\nWhether to check that the names attribute for both the index and column\nattributes of the DataFrame is identical.\n\nSpecify how to compare internal data. If False, compare by columns. If True,\ncompare by blocks.\n\nWhether to compare number exactly.\n\nCompare datetime-like which is comparable ignoring dtype.\n\nWhether to compare internal Categorical exactly.\n\nIf True, ignore the order of index & columns. Note: index labels must match\ntheir respective rows (same as in columns) - same labels must be with the same\ndata.\n\nWhether to check the freq attribute on a DatetimeIndex or TimedeltaIndex.\n\nNew in version 1.1.0.\n\nWhether to check the flags attribute.\n\nRelative tolerance. Only used when check_exact is False.\n\nNew in version 1.1.0.\n\nAbsolute tolerance. Only used when check_exact is False.\n\nNew in version 1.1.0.\n\nSpecify object name being compared, internally used to show appropriate\nassertion message.\n\nSee also\n\nEquivalent method for asserting Series equality.\n\nCheck DataFrame equality.\n\nExamples\n\nThis example shows comparing two DataFrames that are equal but with columns of\ndiffering dtypes.\n\ndf1 equals itself.\n\ndf1 differs from df2 as column \u2018b\u2019 is of a different type.\n\nAttribute \u201cdtype\u201d are different [left]: int64 [right]: float64\n\nIgnore differing dtypes in columns with check_dtype.\n\n"}, {"name": "pandas.testing.assert_index_equal", "path": "reference/api/pandas.testing.assert_index_equal", "type": "General utility functions", "text": "\nCheck that left and right Index are equal.\n\nWhether to check the Index class, dtype and inferred_type are identical. If\n\u2018equiv\u2019, then RangeIndex can be substituted for Int64Index as well.\n\nWhether to check the names attribute.\n\nSpecify comparison precision. Only used when check_exact is False. 5 digits\n(False) or 3 digits (True) after decimal points are compared. If int, then\nspecify the digits to compare.\n\nDeprecated since version 1.1.0: Use rtol and atol instead to define\nrelative/absolute tolerance, respectively. Similar to `math.isclose()`.\n\nWhether to compare number exactly.\n\nWhether to compare internal Categorical exactly.\n\nWhether to compare the order of index entries as well as their values. If\nTrue, both indexes must contain the same elements, in the same order. If\nFalse, both indexes must contain the same elements, but in any order.\n\nNew in version 1.2.0.\n\nRelative tolerance. Only used when check_exact is False.\n\nNew in version 1.1.0.\n\nAbsolute tolerance. Only used when check_exact is False.\n\nNew in version 1.1.0.\n\nSpecify object name being compared, internally used to show appropriate\nassertion message.\n\nExamples\n\n"}, {"name": "pandas.testing.assert_series_equal", "path": "reference/api/pandas.testing.assert_series_equal", "type": "General utility functions", "text": "\nCheck that left and right Series are equal.\n\nWhether to check the Series dtype is identical.\n\nWhether to check the Index class, dtype and inferred_type are identical.\n\nWhether to check the Series class is identical.\n\nSpecify comparison precision. Only used when check_exact is False. 5 digits\n(False) or 3 digits (True) after decimal points are compared. If int, then\nspecify the digits to compare.\n\nWhen comparing two numbers, if the first number has magnitude less than 1e-5,\nwe compare the two numbers directly and check whether they are equivalent\nwithin the specified precision. Otherwise, we compare the ratio of the second\nnumber to the first number and check whether it is equivalent to 1 within the\nspecified precision.\n\nDeprecated since version 1.1.0: Use rtol and atol instead to define\nrelative/absolute tolerance, respectively. Similar to `math.isclose()`.\n\nWhether to check the Series and Index names attribute.\n\nWhether to compare number exactly.\n\nCompare datetime-like which is comparable ignoring dtype.\n\nWhether to compare internal Categorical exactly.\n\nWhether to compare category order of internal Categoricals.\n\nNew in version 1.0.2.\n\nWhether to check the freq attribute on a DatetimeIndex or TimedeltaIndex.\n\nNew in version 1.1.0.\n\nWhether to check the flags attribute.\n\nNew in version 1.2.0.\n\nRelative tolerance. Only used when check_exact is False.\n\nNew in version 1.1.0.\n\nAbsolute tolerance. Only used when check_exact is False.\n\nNew in version 1.1.0.\n\nSpecify object name being compared, internally used to show appropriate\nassertion message.\n\nWhether to check index equivalence. If False, then compare only values.\n\nNew in version 1.3.0.\n\nExamples\n\n"}, {"name": "pandas.Timedelta", "path": "reference/api/pandas.timedelta", "type": "Pandas arrays", "text": "\nRepresents a duration, the difference between two dates or times.\n\nTimedelta is the pandas equivalent of python\u2019s `datetime.timedelta` and is\ninterchangeable with it in most cases.\n\nDenote the unit of the input, if input is an integer.\n\nPossible values:\n\n\u2018W\u2019, \u2018D\u2019, \u2018T\u2019, \u2018S\u2019, \u2018L\u2019, \u2018U\u2019, or \u2018N\u2019\n\n\u2018days\u2019 or \u2018day\u2019\n\n\u2018hours\u2019, \u2018hour\u2019, \u2018hr\u2019, or \u2018h\u2019\n\n\u2018minutes\u2019, \u2018minute\u2019, \u2018min\u2019, or \u2018m\u2019\n\n\u2018seconds\u2019, \u2018second\u2019, or \u2018sec\u2019\n\n\u2018milliseconds\u2019, \u2018millisecond\u2019, \u2018millis\u2019, or \u2018milli\u2019\n\n\u2018microseconds\u2019, \u2018microsecond\u2019, \u2018micros\u2019, or \u2018micro\u2019\n\n\u2018nanoseconds\u2019, \u2018nanosecond\u2019, \u2018nanos\u2019, \u2018nano\u2019, or \u2018ns\u2019.\n\nAvailable kwargs: {days, seconds, microseconds, milliseconds, minutes, hours,\nweeks}. Values for construction in compat with datetime.timedelta. Numpy ints\nand floats will be coerced to python ints and floats.\n\nNotes\n\nThe constructor may take in either both values of value and unit or kwargs as\nabove. Either one of them must be used during initialization\n\nThe `.value` attribute is always in ns.\n\nIf the precision is higher than nanoseconds, the precision of the duration is\ntruncated to nanoseconds.\n\nExamples\n\nHere we initialize Timedelta object with both value and unit\n\nHere we initialize the Timedelta object with kwargs\n\nWe see that either way we get the same result\n\nAttributes\n\n`asm8`\n\nReturn a numpy timedelta64 array scalar view.\n\n`components`\n\nReturn a components namedtuple-like.\n\n`days`\n\nNumber of days.\n\n`delta`\n\nReturn the timedelta in nanoseconds (ns), for internal compatibility.\n\n`microseconds`\n\nNumber of microseconds (>= 0 and less than 1 second).\n\n`nanoseconds`\n\nReturn the number of nanoseconds (n), where 0 <= n < 1 microsecond.\n\n`resolution_string`\n\nReturn a string representing the lowest timedelta resolution.\n\n`seconds`\n\nNumber of seconds (>= 0 and less than 1 day).\n\nfreq\n\nis_populated\n\nvalue\n\nMethods\n\n`ceil`(freq)\n\nReturn a new Timedelta ceiled to this resolution.\n\n`floor`(freq)\n\nReturn a new Timedelta floored to this resolution.\n\n`isoformat`\n\nFormat Timedelta as ISO 8601 Duration like `P[n]Y[n]M[n]DT[n]H[n]M[n]S`, where\nthe `[n]` s are replaced by the values.\n\n`round`(freq)\n\nRound the Timedelta to the specified resolution.\n\n`to_numpy`\n\nConvert the Timedelta to a NumPy timedelta64.\n\n`to_pytimedelta`\n\nConvert a pandas Timedelta object into a python `datetime.timedelta` object.\n\n`to_timedelta64`\n\nReturn a numpy.timedelta64 object with 'ns' precision.\n\n`total_seconds`\n\nTotal seconds in the duration.\n\n`view`\n\nArray view compatibility.\n\n"}, {"name": "pandas.Timedelta.asm8", "path": "reference/api/pandas.timedelta.asm8", "type": "Pandas arrays", "text": "\nReturn a numpy timedelta64 array scalar view.\n\nProvides access to the array scalar view (i.e. a combination of the value and\nthe units) associated with the numpy.timedelta64().view(), including a 64-bit\ninteger representation of the timedelta in nanoseconds (Python int\ncompatible).\n\nArray scalar view of the timedelta in nanoseconds.\n\nExamples\n\n"}, {"name": "pandas.Timedelta.ceil", "path": "reference/api/pandas.timedelta.ceil", "type": "Pandas arrays", "text": "\nReturn a new Timedelta ceiled to this resolution.\n\nFrequency string indicating the ceiling resolution.\n\n"}, {"name": "pandas.Timedelta.components", "path": "reference/api/pandas.timedelta.components", "type": "Pandas arrays", "text": "\nReturn a components namedtuple-like.\n\n"}, {"name": "pandas.Timedelta.days", "path": "reference/api/pandas.timedelta.days", "type": "Pandas arrays", "text": "\nNumber of days.\n\n"}, {"name": "pandas.Timedelta.delta", "path": "reference/api/pandas.timedelta.delta", "type": "Pandas arrays", "text": "\nReturn the timedelta in nanoseconds (ns), for internal compatibility.\n\nTimedelta in nanoseconds.\n\nExamples\n\n"}, {"name": "pandas.Timedelta.floor", "path": "reference/api/pandas.timedelta.floor", "type": "Pandas arrays", "text": "\nReturn a new Timedelta floored to this resolution.\n\nFrequency string indicating the flooring resolution.\n\n"}, {"name": "pandas.Timedelta.freq", "path": "reference/api/pandas.timedelta.freq", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timedelta.is_populated", "path": "reference/api/pandas.timedelta.is_populated", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timedelta.isoformat", "path": "reference/api/pandas.timedelta.isoformat", "type": "Pandas arrays", "text": "\nFormat Timedelta as ISO 8601 Duration like `P[n]Y[n]M[n]DT[n]H[n]M[n]S`, where\nthe `[n]` s are replaced by the values. See\nhttps://en.wikipedia.org/wiki/ISO_8601#Durations.\n\nSee also\n\nFunction is used to convert the given Timestamp object into the ISO format.\n\nNotes\n\nThe longest component is days, whose value may be larger than 365. Every\ncomponent is always included, even if its value is 0. Pandas uses nanosecond\nprecision, so up to 9 decimal places may be included in the seconds component.\nTrailing 0\u2019s are removed from the seconds component after the decimal. We do\nnot 0 pad components, so it\u2019s \u2026T5H\u2026, not \u2026T05H\u2026\n\nExamples\n\n"}, {"name": "pandas.Timedelta.max", "path": "reference/api/pandas.timedelta.max", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timedelta.microseconds", "path": "reference/api/pandas.timedelta.microseconds", "type": "Pandas arrays", "text": "\nNumber of microseconds (>= 0 and less than 1 second).\n\n"}, {"name": "pandas.Timedelta.min", "path": "reference/api/pandas.timedelta.min", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timedelta.nanoseconds", "path": "reference/api/pandas.timedelta.nanoseconds", "type": "Pandas arrays", "text": "\nReturn the number of nanoseconds (n), where 0 <= n < 1 microsecond.\n\nNumber of nanoseconds.\n\nSee also\n\nReturn all attributes with assigned values (i.e. days, hours, minutes,\nseconds, milliseconds, microseconds, nanoseconds).\n\nExamples\n\nUsing string input\n\nUsing integer input\n\n"}, {"name": "pandas.Timedelta.resolution", "path": "reference/api/pandas.timedelta.resolution", "type": "Input/output", "text": "\n\n"}, {"name": "pandas.Timedelta.resolution_string", "path": "reference/api/pandas.timedelta.resolution_string", "type": "Input/output", "text": "\nReturn a string representing the lowest timedelta resolution.\n\nEach timedelta has a defined resolution that represents the lowest OR most\ngranular level of precision. Each level of resolution is represented by a\nshort string as defined below:\n\nResolution: Return value\n\nDays: \u2018D\u2019\n\nHours: \u2018H\u2019\n\nMinutes: \u2018T\u2019\n\nSeconds: \u2018S\u2019\n\nMilliseconds: \u2018L\u2019\n\nMicroseconds: \u2018U\u2019\n\nNanoseconds: \u2018N\u2019\n\nTimedelta resolution.\n\nExamples\n\n"}, {"name": "pandas.Timedelta.round", "path": "reference/api/pandas.timedelta.round", "type": "Pandas arrays", "text": "\nRound the Timedelta to the specified resolution.\n\nFrequency string indicating the rounding resolution.\n\n"}, {"name": "pandas.Timedelta.seconds", "path": "reference/api/pandas.timedelta.seconds", "type": "Pandas arrays", "text": "\nNumber of seconds (>= 0 and less than 1 day).\n\n"}, {"name": "pandas.Timedelta.to_numpy", "path": "reference/api/pandas.timedelta.to_numpy", "type": "Pandas arrays", "text": "\nConvert the Timedelta to a NumPy timedelta64.\n\nNew in version 0.25.0.\n\nThis is an alias method for Timedelta.to_timedelta64(). The dtype and copy\nparameters are available here only for compatibility. Their values will not\naffect the return value.\n\nSee also\n\nSimilar method for Series.\n\n"}, {"name": "pandas.Timedelta.to_pytimedelta", "path": "reference/api/pandas.timedelta.to_pytimedelta", "type": "Pandas arrays", "text": "\nConvert a pandas Timedelta object into a python `datetime.timedelta` object.\n\nTimedelta objects are internally saved as numpy datetime64[ns] dtype. Use\nto_pytimedelta() to convert to object dtype.\n\nSee also\n\nConvert argument to Timedelta type.\n\nNotes\n\nAny nanosecond resolution will be lost.\n\n"}, {"name": "pandas.Timedelta.to_timedelta64", "path": "reference/api/pandas.timedelta.to_timedelta64", "type": "Pandas arrays", "text": "\nReturn a numpy.timedelta64 object with \u2018ns\u2019 precision.\n\n"}, {"name": "pandas.Timedelta.total_seconds", "path": "reference/api/pandas.timedelta.total_seconds", "type": "Pandas arrays", "text": "\nTotal seconds in the duration.\n\n"}, {"name": "pandas.Timedelta.value", "path": "reference/api/pandas.timedelta.value", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timedelta.view", "path": "reference/api/pandas.timedelta.view", "type": "Pandas arrays", "text": "\nArray view compatibility.\n\n"}, {"name": "pandas.timedelta_range", "path": "reference/api/pandas.timedelta_range", "type": "General functions", "text": "\nReturn a fixed frequency TimedeltaIndex, with day as the default frequency.\n\nLeft bound for generating timedeltas.\n\nRight bound for generating timedeltas.\n\nNumber of periods to generate.\n\nFrequency strings can have multiples, e.g. \u20185H\u2019.\n\nName of the resulting TimedeltaIndex.\n\nMake the interval closed with respect to the given frequency to the \u2018left\u2019,\n\u2018right\u2019, or both sides (None).\n\nNotes\n\nOf the four parameters `start`, `end`, `periods`, and `freq`, exactly three\nmust be specified. If `freq` is omitted, the resulting `TimedeltaIndex` will\nhave `periods` linearly spaced elements between `start` and `end` (closed on\nboth sides).\n\nTo learn more about the frequency strings, please see this link.\n\nExamples\n\nThe `closed` parameter specifies which endpoint is included. The default\nbehavior is to include both endpoints.\n\nThe `freq` parameter specifies the frequency of the TimedeltaIndex. Only fixed\nfrequencies can be passed, non-fixed frequencies such as \u2018M\u2019 (month end) will\nraise.\n\nSpecify `start`, `end`, and `periods`; the frequency is generated\nautomatically (linearly spaced).\n\n"}, {"name": "pandas.TimedeltaIndex", "path": "reference/api/pandas.timedeltaindex", "type": "Index Objects", "text": "\nImmutable ndarray of timedelta64 data, represented internally as int64, and\nwhich can be boxed to timedelta objects.\n\nOptional timedelta-like data to construct index with.\n\nWhich is an integer/float number.\n\nOne of pandas date offset strings or corresponding objects. The string \u2018infer\u2019\ncan be passed in order to set the frequency of the index as the inferred\nfrequency upon creation.\n\nMake a copy of input ndarray.\n\nName to be stored in the index.\n\nSee also\n\nThe base pandas Index type.\n\nRepresents a duration between two dates or times.\n\nIndex of datetime64 data.\n\nIndex of Period data.\n\nCreate a fixed-frequency TimedeltaIndex.\n\nNotes\n\nTo learn more about the frequency strings, please see this link.\n\nAttributes\n\n`days`\n\nNumber of days for each element.\n\n`seconds`\n\nNumber of seconds (>= 0 and less than 1 day) for each element.\n\n`microseconds`\n\nNumber of microseconds (>= 0 and less than 1 second) for each element.\n\n`nanoseconds`\n\nNumber of nanoseconds (>= 0 and less than 1 microsecond) for each element.\n\n`components`\n\nReturn a dataframe of the components (days, hours, minutes, seconds,\nmilliseconds, microseconds, nanoseconds) of the Timedeltas.\n\n`inferred_freq`\n\nTries to return a string representing a frequency guess, generated by\ninfer_freq.\n\nMethods\n\n`to_pytimedelta`(*args, **kwargs)\n\nReturn Timedelta Array/Index as object ndarray of datetime.timedelta objects.\n\n`to_series`([index, name])\n\nCreate a Series with both index and values equal to the index keys.\n\n`round`(*args, **kwargs)\n\nPerform round operation on the data to the specified freq.\n\n`floor`(*args, **kwargs)\n\nPerform floor operation on the data to the specified freq.\n\n`ceil`(*args, **kwargs)\n\nPerform ceil operation on the data to the specified freq.\n\n`to_frame`([index, name])\n\nCreate a DataFrame with a column containing the Index.\n\n`mean`(*args, **kwargs)\n\nReturn the mean value of the Array.\n\n"}, {"name": "pandas.TimedeltaIndex.ceil", "path": "reference/api/pandas.timedeltaindex.ceil", "type": "Index Objects", "text": "\nPerform ceil operation on the data to the specified freq.\n\nThe frequency level to ceil the index to. Must be a fixed frequency like \u2018S\u2019\n(second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible\nfreq values.\n\nOnly relevant for DatetimeIndex:\n\n\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order\n\nbool-ndarray where True signifies a DST time, False designates a non-DST time\n(note that this flag is only applicable for ambiguous times)\n\n\u2018NaT\u2019 will return NaT where there are ambiguous times\n\n\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST.\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times\n\ntimedelta objects will shift nonexistent times by the timedelta\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with\nthe same index for a Series.\n\nNotes\n\nIf the timestamps have a timezone, ceiling will take place relative to the\nlocal (\u201cwall\u201d) time and re-localized to the same timezone. When ceiling near\ndaylight savings time, use `nonexistent` and `ambiguous` to control the re-\nlocalization behavior.\n\nExamples\n\nDatetimeIndex\n\nSeries\n\nWhen rounding near a daylight savings time transition, use `ambiguous` or\n`nonexistent` to control how the timestamp should be re-localized.\n\n"}, {"name": "pandas.TimedeltaIndex.components", "path": "reference/api/pandas.timedeltaindex.components", "type": "Index Objects", "text": "\nReturn a dataframe of the components (days, hours, minutes, seconds,\nmilliseconds, microseconds, nanoseconds) of the Timedeltas.\n\n"}, {"name": "pandas.TimedeltaIndex.days", "path": "reference/api/pandas.timedeltaindex.days", "type": "Index Objects", "text": "\nNumber of days for each element.\n\n"}, {"name": "pandas.TimedeltaIndex.floor", "path": "reference/api/pandas.timedeltaindex.floor", "type": "Index Objects", "text": "\nPerform floor operation on the data to the specified freq.\n\nThe frequency level to floor the index to. Must be a fixed frequency like \u2018S\u2019\n(second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible\nfreq values.\n\nOnly relevant for DatetimeIndex:\n\n\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order\n\nbool-ndarray where True signifies a DST time, False designates a non-DST time\n(note that this flag is only applicable for ambiguous times)\n\n\u2018NaT\u2019 will return NaT where there are ambiguous times\n\n\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST.\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times\n\ntimedelta objects will shift nonexistent times by the timedelta\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with\nthe same index for a Series.\n\nNotes\n\nIf the timestamps have a timezone, flooring will take place relative to the\nlocal (\u201cwall\u201d) time and re-localized to the same timezone. When flooring near\ndaylight savings time, use `nonexistent` and `ambiguous` to control the re-\nlocalization behavior.\n\nExamples\n\nDatetimeIndex\n\nSeries\n\nWhen rounding near a daylight savings time transition, use `ambiguous` or\n`nonexistent` to control how the timestamp should be re-localized.\n\n"}, {"name": "pandas.TimedeltaIndex.inferred_freq", "path": "reference/api/pandas.timedeltaindex.inferred_freq", "type": "Index Objects", "text": "\nTries to return a string representing a frequency guess, generated by\ninfer_freq. Returns None if it can\u2019t autodetect the frequency.\n\n"}, {"name": "pandas.TimedeltaIndex.mean", "path": "reference/api/pandas.timedeltaindex.mean", "type": "Index Objects", "text": "\nReturn the mean value of the Array.\n\nNew in version 0.25.0.\n\nWhether to ignore any NaT elements.\n\nTimestamp or Timedelta.\n\nSee also\n\nReturns the average of array elements along a given axis.\n\nReturn the mean value in a Series.\n\nNotes\n\nmean is only defined for Datetime and Timedelta dtypes, not for Period.\n\n"}, {"name": "pandas.TimedeltaIndex.microseconds", "path": "reference/api/pandas.timedeltaindex.microseconds", "type": "Index Objects", "text": "\nNumber of microseconds (>= 0 and less than 1 second) for each element.\n\n"}, {"name": "pandas.TimedeltaIndex.nanoseconds", "path": "reference/api/pandas.timedeltaindex.nanoseconds", "type": "Index Objects", "text": "\nNumber of nanoseconds (>= 0 and less than 1 microsecond) for each element.\n\n"}, {"name": "pandas.TimedeltaIndex.round", "path": "reference/api/pandas.timedeltaindex.round", "type": "Index Objects", "text": "\nPerform round operation on the data to the specified freq.\n\nThe frequency level to round the index to. Must be a fixed frequency like \u2018S\u2019\n(second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible\nfreq values.\n\nOnly relevant for DatetimeIndex:\n\n\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order\n\nbool-ndarray where True signifies a DST time, False designates a non-DST time\n(note that this flag is only applicable for ambiguous times)\n\n\u2018NaT\u2019 will return NaT where there are ambiguous times\n\n\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST.\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times\n\ntimedelta objects will shift nonexistent times by the timedelta\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with\nthe same index for a Series.\n\nNotes\n\nIf the timestamps have a timezone, rounding will take place relative to the\nlocal (\u201cwall\u201d) time and re-localized to the same timezone. When rounding near\ndaylight savings time, use `nonexistent` and `ambiguous` to control the re-\nlocalization behavior.\n\nExamples\n\nDatetimeIndex\n\nSeries\n\nWhen rounding near a daylight savings time transition, use `ambiguous` or\n`nonexistent` to control how the timestamp should be re-localized.\n\n"}, {"name": "pandas.TimedeltaIndex.seconds", "path": "reference/api/pandas.timedeltaindex.seconds", "type": "Index Objects", "text": "\nNumber of seconds (>= 0 and less than 1 day) for each element.\n\n"}, {"name": "pandas.TimedeltaIndex.to_frame", "path": "reference/api/pandas.timedeltaindex.to_frame", "type": "DataFrame", "text": "\nCreate a DataFrame with a column containing the Index.\n\nSet the index of the returned DataFrame as the original Index.\n\nThe passed name should substitute for the index name (if it has one).\n\nDataFrame containing the original Index data.\n\nSee also\n\nConvert an Index to a Series.\n\nConvert Series to DataFrame.\n\nExamples\n\nBy default, the original Index is reused. To enforce a new Index:\n\nTo override the name of the resulting column, specify name:\n\n"}, {"name": "pandas.TimedeltaIndex.to_pytimedelta", "path": "reference/api/pandas.timedeltaindex.to_pytimedelta", "type": "Index Objects", "text": "\nReturn Timedelta Array/Index as object ndarray of datetime.timedelta objects.\n\n"}, {"name": "pandas.TimedeltaIndex.to_series", "path": "reference/api/pandas.timedeltaindex.to_series", "type": "Index Objects", "text": "\nCreate a Series with both index and values equal to the index keys.\n\nUseful with map for returning an indexer based on an index.\n\nIndex of resulting Series. If None, defaults to original index.\n\nName of resulting Series. If None, defaults to name of original index.\n\nThe dtype will be based on the type of the Index values.\n\nSee also\n\nConvert an Index to a DataFrame.\n\nConvert Series to DataFrame.\n\nExamples\n\nBy default, the original Index and original name is reused.\n\nTo enforce a new Index, specify new labels to `index`:\n\nTo override the name of the resulting column, specify name:\n\n"}, {"name": "pandas.Timestamp", "path": "reference/api/pandas.timestamp", "type": "Pandas arrays", "text": "\nPandas replacement for python datetime.datetime object.\n\nTimestamp is the pandas equivalent of python\u2019s Datetime and is interchangeable\nwith it in most cases. It\u2019s the type used for the entries that make up a\nDatetimeIndex, and other timeseries oriented data structures in pandas.\n\nValue to be converted to Timestamp.\n\nOffset which Timestamp will have.\n\nTime zone for time which Timestamp will have.\n\nUnit used for conversion if ts_input is of type int or float. The valid values\nare \u2018D\u2019, \u2018h\u2019, \u2018m\u2019, \u2018s\u2019, \u2018ms\u2019, \u2018us\u2019, and \u2018ns\u2019. For example, \u2018s\u2019 means seconds\nand \u2018ms\u2019 means milliseconds.\n\nDue to daylight saving time, one wall clock time can occur twice when shifting\nfrom summer to winter time; fold describes whether the datetime-like\ncorresponds to the first (0) or the second time (1) the wall clock hits the\nambiguous time.\n\nNew in version 1.1.0.\n\nNotes\n\nThere are essentially three calling conventions for the constructor. The\nprimary form accepts four parameters. They can be passed by position or\nkeyword.\n\nThe other two forms mimic the parameters from `datetime.datetime`. They can be\npassed by either position or keyword, but not both mixed together.\n\nExamples\n\nUsing the primary calling convention:\n\nThis converts a datetime-like string\n\nThis converts a float representing a Unix epoch in units of seconds\n\nThis converts an int representing a Unix-epoch in units of seconds and for a\nparticular timezone\n\nUsing the other two forms that mimic the API for `datetime.datetime`:\n\nAttributes\n\n`asm8`\n\nReturn numpy datetime64 format in nanoseconds.\n\n`day_of_week`\n\nReturn day of the week.\n\n`day_of_year`\n\nReturn the day of the year.\n\n`dayofweek`\n\nReturn day of the week.\n\n`dayofyear`\n\nReturn the day of the year.\n\n`days_in_month`\n\nReturn the number of days in the month.\n\n`daysinmonth`\n\nReturn the number of days in the month.\n\n`freqstr`\n\nReturn the total number of days in the month.\n\n`is_leap_year`\n\nReturn True if year is a leap year.\n\n`is_month_end`\n\nReturn True if date is last day of month.\n\n`is_month_start`\n\nReturn True if date is first day of month.\n\n`is_quarter_end`\n\nReturn True if date is last day of the quarter.\n\n`is_quarter_start`\n\nReturn True if date is first day of the quarter.\n\n`is_year_end`\n\nReturn True if date is last day of the year.\n\n`is_year_start`\n\nReturn True if date is first day of the year.\n\n`quarter`\n\nReturn the quarter of the year.\n\n`tz`\n\nAlias for tzinfo.\n\n`week`\n\nReturn the week number of the year.\n\n`weekofyear`\n\nReturn the week number of the year.\n\nday\n\nfold\n\nfreq\n\nhour\n\nmicrosecond\n\nminute\n\nmonth\n\nnanosecond\n\nsecond\n\ntzinfo\n\nvalue\n\nyear\n\nMethods\n\n`astimezone`(tz)\n\nConvert timezone-aware Timestamp to another time zone.\n\n`ceil`(freq[, ambiguous, nonexistent])\n\nReturn a new Timestamp ceiled to this resolution.\n\n`combine`(date, time)\n\nCombine date, time into datetime with same date and time fields.\n\n`ctime`\n\nReturn ctime() style string.\n\n`date`\n\nReturn date object with same year, month and day.\n\n`day_name`\n\nReturn the day name of the Timestamp with specified locale.\n\n`dst`\n\nReturn self.tzinfo.dst(self).\n\n`floor`(freq[, ambiguous, nonexistent])\n\nReturn a new Timestamp floored to this resolution.\n\n`fromisocalendar`\n\nint, int, int -> Construct a date from the ISO year, week number and weekday.\n\n`fromisoformat`\n\nstring -> datetime from datetime.isoformat() output\n\n`fromordinal`(ordinal[, freq, tz])\n\nPassed an ordinal, translate and convert to a ts.\n\n`fromtimestamp`(ts)\n\nTransform timestamp[, tz] to tz's local time from POSIX timestamp.\n\n`isocalendar`\n\nReturn a 3-tuple containing ISO year, week number, and weekday.\n\n`isoformat`\n\nReturn the time formatted according to ISO 8610.\n\n`isoweekday`()\n\nReturn the day of the week represented by the date.\n\n`month_name`\n\nReturn the month name of the Timestamp with specified locale.\n\n`normalize`\n\nNormalize Timestamp to midnight, preserving tz information.\n\n`now`([tz])\n\nReturn new Timestamp object representing current time local to tz.\n\n`replace`([year, month, day, hour, minute, ...])\n\nImplements datetime.replace, handles nanoseconds.\n\n`round`(freq[, ambiguous, nonexistent])\n\nRound the Timestamp to the specified resolution.\n\n`strftime`(format)\n\nReturn a string representing the given POSIX timestamp controlled by an\nexplicit format string.\n\n`strptime`(string, format)\n\nFunction is not implemented.\n\n`time`\n\nReturn time object with same time but with tzinfo=None.\n\n`timestamp`\n\nReturn POSIX timestamp as float.\n\n`timetuple`\n\nReturn time tuple, compatible with time.localtime().\n\n`timetz`\n\nReturn time object with same time and tzinfo.\n\n`to_datetime64`\n\nReturn a numpy.datetime64 object with 'ns' precision.\n\n`to_julian_date`()\n\nConvert TimeStamp to a Julian Date.\n\n`to_numpy`\n\nConvert the Timestamp to a NumPy datetime64.\n\n`to_period`\n\nReturn an period of which this timestamp is an observation.\n\n`to_pydatetime`\n\nConvert a Timestamp object to a native Python datetime object.\n\n`today`(cls[, tz])\n\nReturn the current time in the local timezone.\n\n`toordinal`\n\nReturn proleptic Gregorian ordinal.\n\n`tz_convert`(tz)\n\nConvert timezone-aware Timestamp to another time zone.\n\n`tz_localize`(tz[, ambiguous, nonexistent])\n\nConvert naive Timestamp to local time zone, or remove timezone from timezone-\naware Timestamp.\n\n`tzname`\n\nReturn self.tzinfo.tzname(self).\n\n`utcfromtimestamp`(ts)\n\nConstruct a naive UTC datetime from a POSIX timestamp.\n\n`utcnow`()\n\nReturn a new Timestamp representing UTC day and time.\n\n`utcoffset`\n\nReturn self.tzinfo.utcoffset(self).\n\n`utctimetuple`\n\nReturn UTC time tuple, compatible with time.localtime().\n\n`weekday`()\n\nReturn the day of the week represented by the date.\n\n"}, {"name": "pandas.Timestamp.asm8", "path": "reference/api/pandas.timestamp.asm8", "type": "Pandas arrays", "text": "\nReturn numpy datetime64 format in nanoseconds.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.astimezone", "path": "reference/api/pandas.timestamp.astimezone", "type": "Pandas arrays", "text": "\nConvert timezone-aware Timestamp to another time zone.\n\nTime zone for time which Timestamp will be converted to. None will remove\ntimezone holding UTC time.\n\nIf Timestamp is tz-naive.\n\nExamples\n\nCreate a timestamp object with UTC timezone:\n\nChange to Tokyo timezone:\n\nCan also use `astimezone`:\n\nAnalogous for `pd.NaT`:\n\n"}, {"name": "pandas.Timestamp.ceil", "path": "reference/api/pandas.timestamp.ceil", "type": "Pandas arrays", "text": "\nReturn a new Timestamp ceiled to this resolution.\n\nFrequency string indicating the ceiling resolution.\n\nThe behavior is as follows:\n\nbool contains flags to determine if time is dst or not (note that this flag is\nonly applicable for ambiguous fall dst dates).\n\n\u2018NaT\u2019 will return NaT for an ambiguous time.\n\n\u2018raise\u2019 will raise an AmbiguousTimeError for an ambiguous time.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST.\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time.\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time.\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times.\n\ntimedelta objects will shift nonexistent times by the timedelta.\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nNotes\n\nIf the Timestamp has a timezone, ceiling will take place relative to the local\n(\u201cwall\u201d) time and re-localized to the same timezone. When ceiling near\ndaylight savings time, use `nonexistent` and `ambiguous` to control the re-\nlocalization behavior.\n\nExamples\n\nCreate a timestamp object:\n\nA timestamp can be ceiled using multiple frequency units:\n\n`freq` can also be a multiple of a single unit, like \u20185T\u2019 (i.e. 5 minutes):\n\nor a combination of multiple units, like \u20181H30T\u2019 (i.e. 1 hour and 30 minutes):\n\nAnalogous for `pd.NaT`:\n\nWhen rounding near a daylight savings time transition, use `ambiguous` or\n`nonexistent` to control how the timestamp should be re-localized.\n\n"}, {"name": "pandas.Timestamp.combine", "path": "reference/api/pandas.timestamp.combine", "type": "Pandas arrays", "text": "\nCombine date, time into datetime with same date and time fields.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.ctime", "path": "reference/api/pandas.timestamp.ctime", "type": "Pandas arrays", "text": "\nReturn ctime() style string.\n\n"}, {"name": "pandas.Timestamp.date", "path": "reference/api/pandas.timestamp.date", "type": "Pandas arrays", "text": "\nReturn date object with same year, month and day.\n\n"}, {"name": "pandas.Timestamp.day", "path": "reference/api/pandas.timestamp.day", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timestamp.day_name", "path": "reference/api/pandas.timestamp.day_name", "type": "Pandas arrays", "text": "\nReturn the day name of the Timestamp with specified locale.\n\nLocale determining the language in which to return the day name.\n\nExamples\n\nAnalogous for `pd.NaT`:\n\n"}, {"name": "pandas.Timestamp.day_of_week", "path": "reference/api/pandas.timestamp.day_of_week", "type": "Pandas arrays", "text": "\nReturn day of the week.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.day_of_year", "path": "reference/api/pandas.timestamp.day_of_year", "type": "Pandas arrays", "text": "\nReturn the day of the year.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.dayofweek", "path": "reference/api/pandas.timestamp.dayofweek", "type": "Pandas arrays", "text": "\nReturn day of the week.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.dayofyear", "path": "reference/api/pandas.timestamp.dayofyear", "type": "Pandas arrays", "text": "\nReturn the day of the year.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.days_in_month", "path": "reference/api/pandas.timestamp.days_in_month", "type": "Pandas arrays", "text": "\nReturn the number of days in the month.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.daysinmonth", "path": "reference/api/pandas.timestamp.daysinmonth", "type": "Pandas arrays", "text": "\nReturn the number of days in the month.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.dst", "path": "reference/api/pandas.timestamp.dst", "type": "Pandas arrays", "text": "\nReturn self.tzinfo.dst(self).\n\n"}, {"name": "pandas.Timestamp.floor", "path": "reference/api/pandas.timestamp.floor", "type": "Pandas arrays", "text": "\nReturn a new Timestamp floored to this resolution.\n\nFrequency string indicating the flooring resolution.\n\nThe behavior is as follows:\n\nbool contains flags to determine if time is dst or not (note that this flag is\nonly applicable for ambiguous fall dst dates).\n\n\u2018NaT\u2019 will return NaT for an ambiguous time.\n\n\u2018raise\u2019 will raise an AmbiguousTimeError for an ambiguous time.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST.\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time.\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time.\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times.\n\ntimedelta objects will shift nonexistent times by the timedelta.\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nNotes\n\nIf the Timestamp has a timezone, flooring will take place relative to the\nlocal (\u201cwall\u201d) time and re-localized to the same timezone. When flooring near\ndaylight savings time, use `nonexistent` and `ambiguous` to control the re-\nlocalization behavior.\n\nExamples\n\nCreate a timestamp object:\n\nA timestamp can be floored using multiple frequency units:\n\n`freq` can also be a multiple of a single unit, like \u20185T\u2019 (i.e. 5 minutes):\n\nor a combination of multiple units, like \u20181H30T\u2019 (i.e. 1 hour and 30 minutes):\n\nAnalogous for `pd.NaT`:\n\nWhen rounding near a daylight savings time transition, use `ambiguous` or\n`nonexistent` to control how the timestamp should be re-localized.\n\n"}, {"name": "pandas.Timestamp.fold", "path": "reference/api/pandas.timestamp.fold", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timestamp.freq", "path": "reference/api/pandas.timestamp.freq", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timestamp.freqstr", "path": "reference/api/pandas.timestamp.freqstr", "type": "Pandas arrays", "text": "\nReturn the total number of days in the month.\n\n"}, {"name": "pandas.Timestamp.fromisocalendar", "path": "reference/api/pandas.timestamp.fromisocalendar", "type": "Pandas arrays", "text": "\nint, int, int -> Construct a date from the ISO year, week number and weekday.\n\nThis is the inverse of the date.isocalendar() function\n\n"}, {"name": "pandas.Timestamp.fromisoformat", "path": "reference/api/pandas.timestamp.fromisoformat", "type": "Pandas arrays", "text": "\nstring -> datetime from datetime.isoformat() output\n\n"}, {"name": "pandas.Timestamp.fromordinal", "path": "reference/api/pandas.timestamp.fromordinal", "type": "Pandas arrays", "text": "\nPassed an ordinal, translate and convert to a ts. Note: by definition there\ncannot be any tz info on the ordinal itself.\n\nDate corresponding to a proleptic Gregorian ordinal.\n\nOffset to apply to the Timestamp.\n\nTime zone for the Timestamp.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.fromtimestamp", "path": "reference/api/pandas.timestamp.fromtimestamp", "type": "Pandas arrays", "text": "\nTransform timestamp[, tz] to tz\u2019s local time from POSIX timestamp.\n\nExamples\n\nNote that the output may change depending on your local time.\n\n"}, {"name": "pandas.Timestamp.hour", "path": "reference/api/pandas.timestamp.hour", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timestamp.is_leap_year", "path": "reference/api/pandas.timestamp.is_leap_year", "type": "Pandas arrays", "text": "\nReturn True if year is a leap year.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.is_month_end", "path": "reference/api/pandas.timestamp.is_month_end", "type": "Pandas arrays", "text": "\nReturn True if date is last day of month.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.is_month_start", "path": "reference/api/pandas.timestamp.is_month_start", "type": "Pandas arrays", "text": "\nReturn True if date is first day of month.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.is_quarter_end", "path": "reference/api/pandas.timestamp.is_quarter_end", "type": "Pandas arrays", "text": "\nReturn True if date is last day of the quarter.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.is_quarter_start", "path": "reference/api/pandas.timestamp.is_quarter_start", "type": "Pandas arrays", "text": "\nReturn True if date is first day of the quarter.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.is_year_end", "path": "reference/api/pandas.timestamp.is_year_end", "type": "Pandas arrays", "text": "\nReturn True if date is last day of the year.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.is_year_start", "path": "reference/api/pandas.timestamp.is_year_start", "type": "Pandas arrays", "text": "\nReturn True if date is first day of the year.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.isocalendar", "path": "reference/api/pandas.timestamp.isocalendar", "type": "Pandas arrays", "text": "\nReturn a 3-tuple containing ISO year, week number, and weekday.\n\n"}, {"name": "pandas.Timestamp.isoformat", "path": "reference/api/pandas.timestamp.isoformat", "type": "Pandas arrays", "text": "\nReturn the time formatted according to ISO 8610.\n\nThe full format looks like \u2018YYYY-MM-DD HH:MM:SS.mmmmmmnnn\u2019. By default, the\nfractional part is omitted if self.microsecond == 0 and self.nanosecond == 0.\n\nIf self.tzinfo is not None, the UTC offset is also attached, giving giving a\nfull format of \u2018YYYY-MM-DD HH:MM:SS.mmmmmmnnn+HH:MM\u2019.\n\nString used as the separator between the date and time.\n\nSpecifies the number of additional terms of the time to include. The valid\nvalues are \u2018auto\u2019, \u2018hours\u2019, \u2018minutes\u2019, \u2018seconds\u2019, \u2018milliseconds\u2019,\n\u2018microseconds\u2019, and \u2018nanoseconds\u2019.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.isoweekday", "path": "reference/api/pandas.timestamp.isoweekday", "type": "Pandas arrays", "text": "\nReturn the day of the week represented by the date. Monday == 1 \u2026 Sunday == 7.\n\n"}, {"name": "pandas.Timestamp.max", "path": "reference/api/pandas.timestamp.max", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timestamp.microsecond", "path": "reference/api/pandas.timestamp.microsecond", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timestamp.min", "path": "reference/api/pandas.timestamp.min", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timestamp.minute", "path": "reference/api/pandas.timestamp.minute", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timestamp.month", "path": "reference/api/pandas.timestamp.month", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timestamp.month_name", "path": "reference/api/pandas.timestamp.month_name", "type": "Pandas arrays", "text": "\nReturn the month name of the Timestamp with specified locale.\n\nLocale determining the language in which to return the month name.\n\nExamples\n\nAnalogous for `pd.NaT`:\n\n"}, {"name": "pandas.Timestamp.nanosecond", "path": "reference/api/pandas.timestamp.nanosecond", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timestamp.normalize", "path": "reference/api/pandas.timestamp.normalize", "type": "Pandas arrays", "text": "\nNormalize Timestamp to midnight, preserving tz information.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.now", "path": "reference/api/pandas.timestamp.now", "type": "Pandas arrays", "text": "\nReturn new Timestamp object representing current time local to tz.\n\nTimezone to localize to.\n\nExamples\n\nAnalogous for `pd.NaT`:\n\n"}, {"name": "pandas.Timestamp.quarter", "path": "reference/api/pandas.timestamp.quarter", "type": "Pandas arrays", "text": "\nReturn the quarter of the year.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.replace", "path": "reference/api/pandas.timestamp.replace", "type": "Pandas arrays", "text": "\nImplements datetime.replace, handles nanoseconds.\n\nExamples\n\nCreate a timestamp object:\n\nReplace year and the hour:\n\nReplace timezone (not a conversion):\n\nAnalogous for `pd.NaT`:\n\n"}, {"name": "pandas.Timestamp.resolution", "path": "reference/api/pandas.timestamp.resolution", "type": "Input/output", "text": "\n\n"}, {"name": "pandas.Timestamp.round", "path": "reference/api/pandas.timestamp.round", "type": "Pandas arrays", "text": "\nRound the Timestamp to the specified resolution.\n\nFrequency string indicating the rounding resolution.\n\nThe behavior is as follows:\n\nbool contains flags to determine if time is dst or not (note that this flag is\nonly applicable for ambiguous fall dst dates).\n\n\u2018NaT\u2019 will return NaT for an ambiguous time.\n\n\u2018raise\u2019 will raise an AmbiguousTimeError for an ambiguous time.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST.\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time.\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time.\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times.\n\ntimedelta objects will shift nonexistent times by the timedelta.\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nNotes\n\nIf the Timestamp has a timezone, rounding will take place relative to the\nlocal (\u201cwall\u201d) time and re-localized to the same timezone. When rounding near\ndaylight savings time, use `nonexistent` and `ambiguous` to control the re-\nlocalization behavior.\n\nExamples\n\nCreate a timestamp object:\n\nA timestamp can be rounded using multiple frequency units:\n\n`freq` can also be a multiple of a single unit, like \u20185T\u2019 (i.e. 5 minutes):\n\nor a combination of multiple units, like \u20181H30T\u2019 (i.e. 1 hour and 30 minutes):\n\nAnalogous for `pd.NaT`:\n\nWhen rounding near a daylight savings time transition, use `ambiguous` or\n`nonexistent` to control how the timestamp should be re-localized.\n\n"}, {"name": "pandas.Timestamp.second", "path": "reference/api/pandas.timestamp.second", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timestamp.strftime", "path": "reference/api/pandas.timestamp.strftime", "type": "Pandas arrays", "text": "\nReturn a string representing the given POSIX timestamp controlled by an\nexplicit format string.\n\nFormat string to convert Timestamp to string. See strftime documentation for\nmore information on the format string:\nhttps://docs.python.org/3/library/datetime.html#strftime-and-strptime-\nbehavior.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.strptime", "path": "reference/api/pandas.timestamp.strptime", "type": "Pandas arrays", "text": "\nFunction is not implemented. Use pd.to_datetime().\n\n"}, {"name": "pandas.Timestamp.time", "path": "reference/api/pandas.timestamp.time", "type": "Pandas arrays", "text": "\nReturn time object with same time but with tzinfo=None.\n\n"}, {"name": "pandas.Timestamp.timestamp", "path": "reference/api/pandas.timestamp.timestamp", "type": "Pandas arrays", "text": "\nReturn POSIX timestamp as float.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.timetuple", "path": "reference/api/pandas.timestamp.timetuple", "type": "Pandas arrays", "text": "\nReturn time tuple, compatible with time.localtime().\n\n"}, {"name": "pandas.Timestamp.timetz", "path": "reference/api/pandas.timestamp.timetz", "type": "Pandas arrays", "text": "\nReturn time object with same time and tzinfo.\n\n"}, {"name": "pandas.Timestamp.to_datetime64", "path": "reference/api/pandas.timestamp.to_datetime64", "type": "Pandas arrays", "text": "\nReturn a numpy.datetime64 object with \u2018ns\u2019 precision.\n\n"}, {"name": "pandas.Timestamp.to_julian_date", "path": "reference/api/pandas.timestamp.to_julian_date", "type": "Pandas arrays", "text": "\nConvert TimeStamp to a Julian Date. 0 Julian date is noon January 1, 4713 BC.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.to_numpy", "path": "reference/api/pandas.timestamp.to_numpy", "type": "Pandas arrays", "text": "\nConvert the Timestamp to a NumPy datetime64.\n\nNew in version 0.25.0.\n\nThis is an alias method for Timestamp.to_datetime64(). The dtype and copy\nparameters are available here only for compatibility. Their values will not\naffect the return value.\n\nSee also\n\nSimilar method for DatetimeIndex.\n\nExamples\n\nAnalogous for `pd.NaT`:\n\n"}, {"name": "pandas.Timestamp.to_period", "path": "reference/api/pandas.timestamp.to_period", "type": "Input/output", "text": "\nReturn an period of which this timestamp is an observation.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.to_pydatetime", "path": "reference/api/pandas.timestamp.to_pydatetime", "type": "Pandas arrays", "text": "\nConvert a Timestamp object to a native Python datetime object.\n\nIf warn=True, issue a warning if nanoseconds is nonzero.\n\nExamples\n\nAnalogous for `pd.NaT`:\n\n"}, {"name": "pandas.Timestamp.today", "path": "reference/api/pandas.timestamp.today", "type": "Pandas arrays", "text": "\nReturn the current time in the local timezone. This differs from\ndatetime.today() in that it can be localized to a passed timezone.\n\nTimezone to localize to.\n\nExamples\n\nAnalogous for `pd.NaT`:\n\n"}, {"name": "pandas.Timestamp.toordinal", "path": "reference/api/pandas.timestamp.toordinal", "type": "Pandas arrays", "text": "\nReturn proleptic Gregorian ordinal. January 1 of year 1 is day 1.\n\n"}, {"name": "pandas.Timestamp.tz", "path": "reference/api/pandas.timestamp.tz", "type": "Pandas arrays", "text": "\nAlias for tzinfo.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.tz_convert", "path": "reference/api/pandas.timestamp.tz_convert", "type": "Pandas arrays", "text": "\nConvert timezone-aware Timestamp to another time zone.\n\nTime zone for time which Timestamp will be converted to. None will remove\ntimezone holding UTC time.\n\nIf Timestamp is tz-naive.\n\nExamples\n\nCreate a timestamp object with UTC timezone:\n\nChange to Tokyo timezone:\n\nCan also use `astimezone`:\n\nAnalogous for `pd.NaT`:\n\n"}, {"name": "pandas.Timestamp.tz_localize", "path": "reference/api/pandas.timestamp.tz_localize", "type": "Pandas arrays", "text": "\nConvert naive Timestamp to local time zone, or remove timezone from timezone-\naware Timestamp.\n\nTime zone for time which Timestamp will be converted to. None will remove\ntimezone holding local time.\n\nWhen clocks moved backward due to DST, ambiguous times may arise. For example\nin Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST,\n02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a\nsituation, the ambiguous parameter dictates how ambiguous times should be\nhandled.\n\nThe behavior is as follows:\n\nbool contains flags to determine if time is dst or not (note that this flag is\nonly applicable for ambiguous fall dst dates).\n\n\u2018NaT\u2019 will return NaT for an ambiguous time.\n\n\u2018raise\u2019 will raise an AmbiguousTimeError for an ambiguous time.\n\nA nonexistent time does not exist in a particular timezone where clocks moved\nforward due to DST.\n\nThe behavior is as follows:\n\n\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest\nexisting time.\n\n\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest\nexisting time.\n\n\u2018NaT\u2019 will return NaT where there are nonexistent times.\n\ntimedelta objects will shift nonexistent times by the timedelta.\n\n\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.\n\nIf the Timestamp is tz-aware and tz is not None.\n\nExamples\n\nCreate a naive timestamp object:\n\nAdd \u2018Europe/Stockholm\u2019 as timezone:\n\nAnalogous for `pd.NaT`:\n\n"}, {"name": "pandas.Timestamp.tzinfo", "path": "reference/api/pandas.timestamp.tzinfo", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timestamp.tzname", "path": "reference/api/pandas.timestamp.tzname", "type": "Pandas arrays", "text": "\nReturn self.tzinfo.tzname(self).\n\n"}, {"name": "pandas.Timestamp.utcfromtimestamp", "path": "reference/api/pandas.timestamp.utcfromtimestamp", "type": "Pandas arrays", "text": "\nConstruct a naive UTC datetime from a POSIX timestamp.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.utcnow", "path": "reference/api/pandas.timestamp.utcnow", "type": "Pandas arrays", "text": "\nReturn a new Timestamp representing UTC day and time.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.utcoffset", "path": "reference/api/pandas.timestamp.utcoffset", "type": "Data offsets", "text": "\nReturn self.tzinfo.utcoffset(self).\n\n"}, {"name": "pandas.Timestamp.utctimetuple", "path": "reference/api/pandas.timestamp.utctimetuple", "type": "Pandas arrays", "text": "\nReturn UTC time tuple, compatible with time.localtime().\n\n"}, {"name": "pandas.Timestamp.value", "path": "reference/api/pandas.timestamp.value", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.Timestamp.week", "path": "reference/api/pandas.timestamp.week", "type": "Pandas arrays", "text": "\nReturn the week number of the year.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.weekday", "path": "reference/api/pandas.timestamp.weekday", "type": "Pandas arrays", "text": "\nReturn the day of the week represented by the date. Monday == 0 \u2026 Sunday == 6.\n\n"}, {"name": "pandas.Timestamp.weekofyear", "path": "reference/api/pandas.timestamp.weekofyear", "type": "Pandas arrays", "text": "\nReturn the week number of the year.\n\nExamples\n\n"}, {"name": "pandas.Timestamp.year", "path": "reference/api/pandas.timestamp.year", "type": "Pandas arrays", "text": "\n\n"}, {"name": "pandas.to_datetime", "path": "reference/api/pandas.to_datetime", "type": "General functions", "text": "\nConvert argument to datetime.\n\nThis function converts a scalar, array-like, `Series` or `DataFrame`/dict-like\nto a pandas datetime object.\n\nThe object to convert to a datetime. If a `DataFrame` is provided, the method\nexpects minimally the following columns: `\"year\"`, `\"month\"`, `\"day\"`.\n\nIf `'raise'`, then invalid parsing will raise an exception.\n\nIf `'coerce'`, then invalid parsing will be set as `NaT`.\n\nIf `'ignore'`, then invalid parsing will return the input.\n\nSpecify a date parse order if arg is str or is list-like. If `True`, parses\ndates with the day first, e.g. `\"10/11/12\"` is parsed as `2012-11-10`.\n\nWarning\n\n`dayfirst=True` is not strict, but will prefer to parse with day first. If a\ndelimited date string cannot be parsed in accordance with the given dayfirst\noption, e.g. `to_datetime(['31-12-2021'])`, then a warning will be shown.\n\nSpecify a date parse order if arg is str or is list-like.\n\nIf `True` parses dates with the year first, e.g. `\"10/11/12\"` is parsed as\n`2010-11-12`.\n\nIf both dayfirst and yearfirst are `True`, yearfirst is preceded (same as\n`dateutil`).\n\nWarning\n\n`yearfirst=True` is not strict, but will prefer to parse with year first.\n\nControl timezone-related parsing, localization and conversion.\n\nIf `True`, the function always returns a timezone-aware UTC-localized\n`Timestamp`, `Series` or `DatetimeIndex`. To do this, timezone-naive inputs\nare localized as UTC, while timezone-aware inputs are converted to UTC.\n\nIf `False` (default), inputs will not be coerced to UTC. Timezone-naive inputs\nwill remain naive, while timezone-aware ones will keep their time offsets.\nLimitations exist for mixed offsets (typically, daylight savings), see\nExamples section for details.\n\nSee also: pandas general documentation about timezone conversion and\nlocalization.\n\nThe strftime to parse time, e.g. `\"%d/%m/%Y\"`. Note that `\"%f\"` will parse all\nthe way up to nanoseconds. See strftime documentation for more information on\nchoices.\n\nControl how format is used:\n\nIf `True`, require an exact format match.\n\nIf `False`, allow the format to match anywhere in the target string.\n\nThe unit of the arg (D,s,ms,us,ns) denote the unit, which is an integer or\nfloat number. This will be based off the origin. Example, with `unit='ms'` and\n`origin='unix'` (the default), this would calculate the number of milliseconds\nto the unix epoch start.\n\nIf `True` and no format is given, attempt to infer the format of the datetime\nstrings based on the first non-NaN element, and if it can be inferred, switch\nto a faster method of parsing them. In some cases this can increase the\nparsing speed by ~5-10x.\n\nDefine the reference date. The numeric values would be parsed as number of\nunits (defined by unit) since this reference date.\n\nIf `'unix'` (or POSIX) time; origin is set to 1970-01-01.\n\nIf `'julian'`, unit must be `'D'`, and origin is set to beginning of Julian\nCalendar. Julian day number `0` is assigned to the day starting at noon on\nJanuary 1, 4713 BC.\n\nIf Timestamp convertible, origin is set to Timestamp identified by origin.\n\nIf `True`, use a cache of unique, converted dates to apply the datetime\nconversion. May produce significant speed-up when parsing duplicate date\nstrings, especially ones with timezone offsets. The cache is only used when\nthere are at least 50 values. The presence of out-of-bounds values will render\nthe cache unusable and may slow down parsing.\n\nChanged in version 0.25.0: changed default value from `False` to `True`.\n\nIf parsing succeeded. Return type depends on input (types in parenthesis\ncorrespond to fallback in case of unsuccessful timezone or out-of-range\ntimestamp parsing):\n\nscalar: `Timestamp` (or `datetime.datetime`)\n\narray-like: `DatetimeIndex` (or `Series` with `object` dtype containing\n`datetime.datetime`)\n\nSeries: `Series` of `datetime64` dtype (or `Series` of `object` dtype\ncontaining `datetime.datetime`)\n\nDataFrame: `Series` of `datetime64` dtype (or `Series` of `object` dtype\ncontaining `datetime.datetime`)\n\nWhen parsing a date from string fails.\n\nWhen another datetime conversion error happens. For example when one of\n\u2018year\u2019, \u2018month\u2019, day\u2019 columns is missing in a `DataFrame`, or when a Timezone-\naware `datetime.datetime` is found in an array-like of mixed time offsets, and\n`utc=False`.\n\nSee also\n\nCast argument to a specified dtype.\n\nConvert argument to timedelta.\n\nConvert dtypes.\n\nNotes\n\nMany input types are supported, and lead to different output types:\n\nscalars can be int, float, str, datetime object (from stdlib `datetime` module\nor `numpy`). They are converted to `Timestamp` when possible, otherwise they\nare converted to `datetime.datetime`. None/NaN/null scalars are converted to\n`NaT`.\n\narray-like can contain int, float, str, datetime objects. They are converted\nto `DatetimeIndex` when possible, otherwise they are converted to `Index` with\n`object` dtype, containing `datetime.datetime`. None/NaN/null entries are\nconverted to `NaT` in both cases.\n\nSeries are converted to `Series` with `datetime64` dtype when possible,\notherwise they are converted to `Series` with `object` dtype, containing\n`datetime.datetime`. None/NaN/null entries are converted to `NaT` in both\ncases.\n\nDataFrame/dict-like are converted to `Series` with `datetime64` dtype. For\neach row a datetime is created from assembling the various dataframe columns.\nColumn keys can be common abbreviations like [\u2018year\u2019, \u2018month\u2019, \u2018day\u2019,\n\u2018minute\u2019, \u2018second\u2019, \u2018ms\u2019, \u2018us\u2019, \u2018ns\u2019]) or plurals of the same.\n\nThe following causes are responsible for `datetime.datetime` objects being\nreturned (possibly inside an `Index` or a `Series` with `object` dtype)\ninstead of a proper pandas designated type (`Timestamp`, `DatetimeIndex` or\n`Series` with `datetime64` dtype):\n\nwhen any input element is before `Timestamp.min` or after `Timestamp.max`, see\ntimestamp limitations.\n\nwhen `utc=False` (default) and the input is an array-like or `Series`\ncontaining mixed naive/aware datetime, or aware with mixed time offsets. Note\nthat this happens in the (quite frequent) situation when the timezone has a\ndaylight savings policy. In that case you may wish to use `utc=True`.\n\nExamples\n\nHandling various input formats\n\nAssembling a datetime from multiple columns of a `DataFrame`. The keys can be\ncommon abbreviations like [\u2018year\u2019, \u2018month\u2019, \u2018day\u2019, \u2018minute\u2019, \u2018second\u2019, \u2018ms\u2019,\n\u2018us\u2019, \u2018ns\u2019]) or plurals of the same\n\nPassing `infer_datetime_format=True` can often-times speedup a parsing if its\nnot an ISO8601 format exactly, but in a regular format.\n\nUsing a unix epoch time\n\nWarning\n\nFor float arg, precision rounding might happen. To prevent unexpected behavior\nuse a fixed-width exact type.\n\nUsing a non-unix epoch origin\n\nNon-convertible date/times\n\nIf a date does not meet the timestamp limitations, passing `errors='ignore'`\nwill return the original input instead of raising any exception.\n\nPassing `errors='coerce'` will force an out-of-bounds date to `NaT`, in\naddition to forcing non-dates (or non-parseable dates) to `NaT`.\n\nTimezones and time offsets\n\nThe default behaviour (`utc=False`) is as follows:\n\nTimezone-naive inputs are converted to timezone-naive `DatetimeIndex`:\n\nTimezone-aware inputs with constant time offset are converted to timezone-\naware `DatetimeIndex`:\n\nHowever, timezone-aware inputs with mixed time offsets (for example issued\nfrom a timezone with daylight savings, such as Europe/Paris) are not\nsuccessfully converted to a `DatetimeIndex`. Instead a simple `Index`\ncontaining `datetime.datetime` objects is returned:\n\nA mix of timezone-aware and timezone-naive inputs is converted to a timezone-\naware `DatetimeIndex` if the offsets of the timezone-aware are constant:\n\nFinally, mixing timezone-aware strings and `datetime.datetime` always raises\nan error, even if the elements all have the same time offset.\n\nSetting `utc=True` solves most of the above issues:\n\nTimezone-naive inputs are localized as UTC\n\nTimezone-aware inputs are converted to UTC (the output represents the exact\nsame datetime, but viewed from the UTC time offset +00:00).\n\nInputs can contain both naive and aware, string or datetime, the above rules\nstill apply\n\n"}, {"name": "pandas.to_numeric", "path": "reference/api/pandas.to_numeric", "type": "General functions", "text": "\nConvert argument to a numeric type.\n\nThe default return dtype is float64 or int64 depending on the data supplied.\nUse the downcast parameter to obtain other dtypes.\n\nPlease note that precision loss may occur if really large numbers are passed\nin. Due to the internal limitations of ndarray, if numbers smaller than\n-9223372036854775808 (np.iinfo(np.int64).min) or larger than\n18446744073709551615 (np.iinfo(np.uint64).max) are passed in, it is very\nlikely they will be converted to float so that they can stored in an ndarray.\nThese warnings apply similarly to Series since it internally leverages\nndarray.\n\nArgument to be converted.\n\nIf \u2018raise\u2019, then invalid parsing will raise an exception.\n\nIf \u2018coerce\u2019, then invalid parsing will be set as NaN.\n\nIf \u2018ignore\u2019, then invalid parsing will return the input.\n\nCan be \u2018integer\u2019, \u2018signed\u2019, \u2018unsigned\u2019, or \u2018float\u2019. If not None, and if the\ndata has been successfully cast to a numerical dtype (or if the data was\nnumeric to begin with), downcast that resulting data to the smallest numerical\ndtype possible according to the following rules:\n\n\u2018integer\u2019 or \u2018signed\u2019: smallest signed int dtype (min.: np.int8)\n\n\u2018unsigned\u2019: smallest unsigned int dtype (min.: np.uint8)\n\n\u2018float\u2019: smallest float dtype (min.: np.float32)\n\nAs this behaviour is separate from the core conversion to numeric values, any\nerrors raised during the downcasting will be surfaced regardless of the value\nof the \u2018errors\u2019 input.\n\nIn addition, downcasting will only occur if the size of the resulting data\u2019s\ndtype is strictly larger than the dtype it is to be cast to, so if none of the\ndtypes checked satisfy that specification, no downcasting will be performed on\nthe data.\n\nNumeric if parsing succeeded. Return type depends on input. Series if Series,\notherwise ndarray.\n\nSee also\n\nCast argument to a specified dtype.\n\nConvert argument to datetime.\n\nConvert argument to timedelta.\n\nCast a numpy array to a specified type.\n\nConvert dtypes.\n\nExamples\n\nTake separate series and convert to numeric, coercing when told to\n\nDowncasting of nullable integer and floating dtypes is supported:\n\n"}, {"name": "pandas.to_timedelta", "path": "reference/api/pandas.to_timedelta", "type": "General functions", "text": "\nConvert argument to timedelta.\n\nTimedeltas are absolute differences in times, expressed in difference units\n(e.g. days, hours, minutes, seconds). This method converts an argument from a\nrecognized timedelta format / value into a Timedelta type.\n\nThe data to be converted to timedelta.\n\nDeprecated since version 1.2: Strings with units \u2018M\u2019, \u2018Y\u2019 and \u2018y\u2019 do not\nrepresent unambiguous timedelta values and will be removed in a future version\n\nDenotes the unit of the arg for numeric arg. Defaults to `\"ns\"`.\n\nPossible values:\n\n\u2018W\u2019\n\n\u2018D\u2019 / \u2018days\u2019 / \u2018day\u2019\n\n\u2018hours\u2019 / \u2018hour\u2019 / \u2018hr\u2019 / \u2018h\u2019\n\n\u2018m\u2019 / \u2018minute\u2019 / \u2018min\u2019 / \u2018minutes\u2019 / \u2018T\u2019\n\n\u2018S\u2019 / \u2018seconds\u2019 / \u2018sec\u2019 / \u2018second\u2019\n\n\u2018ms\u2019 / \u2018milliseconds\u2019 / \u2018millisecond\u2019 / \u2018milli\u2019 / \u2018millis\u2019 / \u2018L\u2019\n\n\u2018us\u2019 / \u2018microseconds\u2019 / \u2018microsecond\u2019 / \u2018micro\u2019 / \u2018micros\u2019 / \u2018U\u2019\n\n\u2018ns\u2019 / \u2018nanoseconds\u2019 / \u2018nano\u2019 / \u2018nanos\u2019 / \u2018nanosecond\u2019 / \u2018N\u2019\n\nChanged in version 1.1.0: Must not be specified when arg context strings and\n`errors=\"raise\"`.\n\nIf \u2018raise\u2019, then invalid parsing will raise an exception.\n\nIf \u2018coerce\u2019, then invalid parsing will be set as NaT.\n\nIf \u2018ignore\u2019, then invalid parsing will return the input.\n\nIf parsing succeeded. Return type depends on input:\n\nlist-like: TimedeltaIndex of timedelta64 dtype\n\nSeries: Series of timedelta64 dtype\n\nscalar: Timedelta\n\nSee also\n\nCast argument to a specified dtype.\n\nConvert argument to datetime.\n\nConvert dtypes.\n\nNotes\n\nIf the precision is higher than nanoseconds, the precision of the duration is\ntruncated to nanoseconds for string inputs.\n\nExamples\n\nParsing a single string to a Timedelta:\n\nParsing a list or array of strings:\n\nConverting numbers by specifying the unit keyword argument:\n\n"}, {"name": "pandas.tseries.frequencies.to_offset", "path": "reference/api/pandas.tseries.frequencies.to_offset", "type": "Data offsets", "text": "\nReturn DateOffset object from string or tuple representation or\ndatetime.timedelta object.\n\nIf freq is an invalid frequency\n\nSee also\n\nStandard kind of date increment used for a date range.\n\nExamples\n\n"}, {"name": "pandas.tseries.offsets.BDay", "path": "reference/api/pandas.tseries.offsets.bday", "type": "Data offsets", "text": "\nalias of `pandas._libs.tslibs.offsets.BusinessDay`\n\n"}, {"name": "pandas.tseries.offsets.BMonthBegin", "path": "reference/api/pandas.tseries.offsets.bmonthbegin", "type": "Data offsets", "text": "\nalias of `pandas._libs.tslibs.offsets.BusinessMonthBegin`\n\n"}, {"name": "pandas.tseries.offsets.BMonthEnd", "path": "reference/api/pandas.tseries.offsets.bmonthend", "type": "Data offsets", "text": "\nalias of `pandas._libs.tslibs.offsets.BusinessMonthEnd`\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin", "path": "reference/api/pandas.tseries.offsets.bquarterbegin", "type": "Data offsets", "text": "\nDateOffset increments between the first business day of each Quarter.\n\nstartingMonth = 1 corresponds to dates like 1/01/2007, 4/01/2007, \u2026\nstartingMonth = 2 corresponds to dates like 2/01/2007, 5/01/2007, \u2026\nstartingMonth = 3 corresponds to dates like 3/01/2007, 6/01/2007, \u2026\n\nExamples\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nstartingMonth\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.__call__", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.apply", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.apply_index", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.base", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.copy", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.freqstr", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.is_anchored", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.is_month_end", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.is_month_start", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.is_on_offset", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.is_year_end", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.is_year_start", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.isAnchored", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.kwds", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.n", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.name", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.nanos", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.normalize", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.onOffset", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.rollback", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.rollforward", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.rule_code", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterBegin.startingMonth", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.startingmonth", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd", "path": "reference/api/pandas.tseries.offsets.bquarterend", "type": "Data offsets", "text": "\nDateOffset increments between the last business day of each Quarter.\n\nstartingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, \u2026\nstartingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, \u2026\nstartingMonth = 3 corresponds to dates like 3/30/2007, 6/29/2007, \u2026\n\nExamples\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nstartingMonth\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.__call__", "path": "reference/api/pandas.tseries.offsets.bquarterend.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.apply", "path": "reference/api/pandas.tseries.offsets.bquarterend.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.apply_index", "path": "reference/api/pandas.tseries.offsets.bquarterend.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.base", "path": "reference/api/pandas.tseries.offsets.bquarterend.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.copy", "path": "reference/api/pandas.tseries.offsets.bquarterend.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.freqstr", "path": "reference/api/pandas.tseries.offsets.bquarterend.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.is_anchored", "path": "reference/api/pandas.tseries.offsets.bquarterend.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.is_month_end", "path": "reference/api/pandas.tseries.offsets.bquarterend.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.is_month_start", "path": "reference/api/pandas.tseries.offsets.bquarterend.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.is_on_offset", "path": "reference/api/pandas.tseries.offsets.bquarterend.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.bquarterend.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.bquarterend.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.is_year_end", "path": "reference/api/pandas.tseries.offsets.bquarterend.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.is_year_start", "path": "reference/api/pandas.tseries.offsets.bquarterend.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.isAnchored", "path": "reference/api/pandas.tseries.offsets.bquarterend.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.kwds", "path": "reference/api/pandas.tseries.offsets.bquarterend.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.n", "path": "reference/api/pandas.tseries.offsets.bquarterend.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.name", "path": "reference/api/pandas.tseries.offsets.bquarterend.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.nanos", "path": "reference/api/pandas.tseries.offsets.bquarterend.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.normalize", "path": "reference/api/pandas.tseries.offsets.bquarterend.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.onOffset", "path": "reference/api/pandas.tseries.offsets.bquarterend.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.rollback", "path": "reference/api/pandas.tseries.offsets.bquarterend.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.rollforward", "path": "reference/api/pandas.tseries.offsets.bquarterend.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.rule_code", "path": "reference/api/pandas.tseries.offsets.bquarterend.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BQuarterEnd.startingMonth", "path": "reference/api/pandas.tseries.offsets.bquarterend.startingmonth", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay", "path": "reference/api/pandas.tseries.offsets.businessday", "type": "Data offsets", "text": "\nDateOffset subclass representing possibly n business days.\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n`offset`\n\nAlias for self._offset.\n\ncalendar\n\nfreqstr\n\nholidays\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nweekmask\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.__call__", "path": "reference/api/pandas.tseries.offsets.businessday.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.apply", "path": "reference/api/pandas.tseries.offsets.businessday.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.apply_index", "path": "reference/api/pandas.tseries.offsets.businessday.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.base", "path": "reference/api/pandas.tseries.offsets.businessday.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.calendar", "path": "reference/api/pandas.tseries.offsets.businessday.calendar", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.copy", "path": "reference/api/pandas.tseries.offsets.businessday.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.freqstr", "path": "reference/api/pandas.tseries.offsets.businessday.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.holidays", "path": "reference/api/pandas.tseries.offsets.businessday.holidays", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.is_anchored", "path": "reference/api/pandas.tseries.offsets.businessday.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.is_month_end", "path": "reference/api/pandas.tseries.offsets.businessday.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.is_month_start", "path": "reference/api/pandas.tseries.offsets.businessday.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.is_on_offset", "path": "reference/api/pandas.tseries.offsets.businessday.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.businessday.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.businessday.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.is_year_end", "path": "reference/api/pandas.tseries.offsets.businessday.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.is_year_start", "path": "reference/api/pandas.tseries.offsets.businessday.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.isAnchored", "path": "reference/api/pandas.tseries.offsets.businessday.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.kwds", "path": "reference/api/pandas.tseries.offsets.businessday.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.n", "path": "reference/api/pandas.tseries.offsets.businessday.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.name", "path": "reference/api/pandas.tseries.offsets.businessday.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.nanos", "path": "reference/api/pandas.tseries.offsets.businessday.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.normalize", "path": "reference/api/pandas.tseries.offsets.businessday.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.offset", "path": "reference/api/pandas.tseries.offsets.businessday.offset", "type": "Data offsets", "text": "\nAlias for self._offset.\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.onOffset", "path": "reference/api/pandas.tseries.offsets.businessday.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.rollback", "path": "reference/api/pandas.tseries.offsets.businessday.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.rollforward", "path": "reference/api/pandas.tseries.offsets.businessday.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.rule_code", "path": "reference/api/pandas.tseries.offsets.businessday.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessDay.weekmask", "path": "reference/api/pandas.tseries.offsets.businessday.weekmask", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour", "path": "reference/api/pandas.tseries.offsets.businesshour", "type": "Data offsets", "text": "\nDateOffset subclass representing possibly n business hours.\n\nThe number of months represented.\n\nNormalize start/end dates to midnight before generating date range.\n\nWeekmask of valid business days, passed to `numpy.busdaycalendar`.\n\nStart time of your custom business hour in 24h format.\n\nEnd time of your custom business hour in 24h format.\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n`next_bday`\n\nUsed for moving to next business day.\n\n`offset`\n\nAlias for self._offset.\n\ncalendar\n\nend\n\nfreqstr\n\nholidays\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nstart\n\nweekmask\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`(other)\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`(other)\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.__call__", "path": "reference/api/pandas.tseries.offsets.businesshour.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.apply", "path": "reference/api/pandas.tseries.offsets.businesshour.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.apply_index", "path": "reference/api/pandas.tseries.offsets.businesshour.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.base", "path": "reference/api/pandas.tseries.offsets.businesshour.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.calendar", "path": "reference/api/pandas.tseries.offsets.businesshour.calendar", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.copy", "path": "reference/api/pandas.tseries.offsets.businesshour.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.end", "path": "reference/api/pandas.tseries.offsets.businesshour.end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.freqstr", "path": "reference/api/pandas.tseries.offsets.businesshour.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.holidays", "path": "reference/api/pandas.tseries.offsets.businesshour.holidays", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.is_anchored", "path": "reference/api/pandas.tseries.offsets.businesshour.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.is_month_end", "path": "reference/api/pandas.tseries.offsets.businesshour.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.is_month_start", "path": "reference/api/pandas.tseries.offsets.businesshour.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.is_on_offset", "path": "reference/api/pandas.tseries.offsets.businesshour.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.businesshour.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.businesshour.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.is_year_end", "path": "reference/api/pandas.tseries.offsets.businesshour.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.is_year_start", "path": "reference/api/pandas.tseries.offsets.businesshour.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.isAnchored", "path": "reference/api/pandas.tseries.offsets.businesshour.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.kwds", "path": "reference/api/pandas.tseries.offsets.businesshour.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.n", "path": "reference/api/pandas.tseries.offsets.businesshour.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.name", "path": "reference/api/pandas.tseries.offsets.businesshour.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.nanos", "path": "reference/api/pandas.tseries.offsets.businesshour.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.next_bday", "path": "reference/api/pandas.tseries.offsets.businesshour.next_bday", "type": "Data offsets", "text": "\nUsed for moving to next business day.\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.normalize", "path": "reference/api/pandas.tseries.offsets.businesshour.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.offset", "path": "reference/api/pandas.tseries.offsets.businesshour.offset", "type": "Data offsets", "text": "\nAlias for self._offset.\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.onOffset", "path": "reference/api/pandas.tseries.offsets.businesshour.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.rollback", "path": "reference/api/pandas.tseries.offsets.businesshour.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.rollforward", "path": "reference/api/pandas.tseries.offsets.businesshour.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.rule_code", "path": "reference/api/pandas.tseries.offsets.businesshour.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.start", "path": "reference/api/pandas.tseries.offsets.businesshour.start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessHour.weekmask", "path": "reference/api/pandas.tseries.offsets.businesshour.weekmask", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin", "type": "Data offsets", "text": "\nDateOffset of one month at the first business day.\n\nExamples\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.__call__", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.apply", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.apply_index", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.base", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.copy", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.freqstr", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.is_anchored", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.is_month_end", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.is_month_start", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.is_on_offset", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.is_year_end", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.is_year_start", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.isAnchored", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.kwds", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.n", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.name", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.nanos", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.normalize", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.onOffset", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.rollback", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.rollforward", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.rule_code", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd", "path": "reference/api/pandas.tseries.offsets.businessmonthend", "type": "Data offsets", "text": "\nDateOffset increments between the last business day of the month.\n\nExamples\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.__call__", "path": "reference/api/pandas.tseries.offsets.businessmonthend.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.apply", "path": "reference/api/pandas.tseries.offsets.businessmonthend.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.apply_index", "path": "reference/api/pandas.tseries.offsets.businessmonthend.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.base", "path": "reference/api/pandas.tseries.offsets.businessmonthend.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.copy", "path": "reference/api/pandas.tseries.offsets.businessmonthend.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.freqstr", "path": "reference/api/pandas.tseries.offsets.businessmonthend.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.is_anchored", "path": "reference/api/pandas.tseries.offsets.businessmonthend.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.is_month_end", "path": "reference/api/pandas.tseries.offsets.businessmonthend.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.is_month_start", "path": "reference/api/pandas.tseries.offsets.businessmonthend.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.is_on_offset", "path": "reference/api/pandas.tseries.offsets.businessmonthend.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.businessmonthend.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.businessmonthend.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.is_year_end", "path": "reference/api/pandas.tseries.offsets.businessmonthend.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.is_year_start", "path": "reference/api/pandas.tseries.offsets.businessmonthend.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.isAnchored", "path": "reference/api/pandas.tseries.offsets.businessmonthend.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.kwds", "path": "reference/api/pandas.tseries.offsets.businessmonthend.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.n", "path": "reference/api/pandas.tseries.offsets.businessmonthend.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.name", "path": "reference/api/pandas.tseries.offsets.businessmonthend.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.nanos", "path": "reference/api/pandas.tseries.offsets.businessmonthend.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.normalize", "path": "reference/api/pandas.tseries.offsets.businessmonthend.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.onOffset", "path": "reference/api/pandas.tseries.offsets.businessmonthend.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.rollback", "path": "reference/api/pandas.tseries.offsets.businessmonthend.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.rollforward", "path": "reference/api/pandas.tseries.offsets.businessmonthend.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.rule_code", "path": "reference/api/pandas.tseries.offsets.businessmonthend.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin", "path": "reference/api/pandas.tseries.offsets.byearbegin", "type": "Data offsets", "text": "\nDateOffset increments between the first business day of the year.\n\nExamples\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nmonth\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.__call__", "path": "reference/api/pandas.tseries.offsets.byearbegin.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.apply", "path": "reference/api/pandas.tseries.offsets.byearbegin.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.apply_index", "path": "reference/api/pandas.tseries.offsets.byearbegin.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.base", "path": "reference/api/pandas.tseries.offsets.byearbegin.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.copy", "path": "reference/api/pandas.tseries.offsets.byearbegin.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.freqstr", "path": "reference/api/pandas.tseries.offsets.byearbegin.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.is_anchored", "path": "reference/api/pandas.tseries.offsets.byearbegin.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.is_month_end", "path": "reference/api/pandas.tseries.offsets.byearbegin.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.is_month_start", "path": "reference/api/pandas.tseries.offsets.byearbegin.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.is_on_offset", "path": "reference/api/pandas.tseries.offsets.byearbegin.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.byearbegin.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.byearbegin.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.is_year_end", "path": "reference/api/pandas.tseries.offsets.byearbegin.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.is_year_start", "path": "reference/api/pandas.tseries.offsets.byearbegin.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.isAnchored", "path": "reference/api/pandas.tseries.offsets.byearbegin.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.kwds", "path": "reference/api/pandas.tseries.offsets.byearbegin.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.month", "path": "reference/api/pandas.tseries.offsets.byearbegin.month", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.n", "path": "reference/api/pandas.tseries.offsets.byearbegin.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.name", "path": "reference/api/pandas.tseries.offsets.byearbegin.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.nanos", "path": "reference/api/pandas.tseries.offsets.byearbegin.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.normalize", "path": "reference/api/pandas.tseries.offsets.byearbegin.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.onOffset", "path": "reference/api/pandas.tseries.offsets.byearbegin.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.rollback", "path": "reference/api/pandas.tseries.offsets.byearbegin.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.rollforward", "path": "reference/api/pandas.tseries.offsets.byearbegin.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.BYearBegin.rule_code", "path": "reference/api/pandas.tseries.offsets.byearbegin.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd", "path": "reference/api/pandas.tseries.offsets.byearend", "type": "Data offsets", "text": "\nDateOffset increments between the last business day of the year.\n\nExamples\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nmonth\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.__call__", "path": "reference/api/pandas.tseries.offsets.byearend.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.apply", "path": "reference/api/pandas.tseries.offsets.byearend.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.apply_index", "path": "reference/api/pandas.tseries.offsets.byearend.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.base", "path": "reference/api/pandas.tseries.offsets.byearend.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.copy", "path": "reference/api/pandas.tseries.offsets.byearend.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.freqstr", "path": "reference/api/pandas.tseries.offsets.byearend.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.is_anchored", "path": "reference/api/pandas.tseries.offsets.byearend.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.is_month_end", "path": "reference/api/pandas.tseries.offsets.byearend.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.is_month_start", "path": "reference/api/pandas.tseries.offsets.byearend.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.is_on_offset", "path": "reference/api/pandas.tseries.offsets.byearend.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.byearend.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.byearend.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.is_year_end", "path": "reference/api/pandas.tseries.offsets.byearend.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.is_year_start", "path": "reference/api/pandas.tseries.offsets.byearend.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.isAnchored", "path": "reference/api/pandas.tseries.offsets.byearend.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.kwds", "path": "reference/api/pandas.tseries.offsets.byearend.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.month", "path": "reference/api/pandas.tseries.offsets.byearend.month", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.n", "path": "reference/api/pandas.tseries.offsets.byearend.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.name", "path": "reference/api/pandas.tseries.offsets.byearend.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.nanos", "path": "reference/api/pandas.tseries.offsets.byearend.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.normalize", "path": "reference/api/pandas.tseries.offsets.byearend.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.onOffset", "path": "reference/api/pandas.tseries.offsets.byearend.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.rollback", "path": "reference/api/pandas.tseries.offsets.byearend.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.rollforward", "path": "reference/api/pandas.tseries.offsets.byearend.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.BYearEnd.rule_code", "path": "reference/api/pandas.tseries.offsets.byearend.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CBMonthBegin", "path": "reference/api/pandas.tseries.offsets.cbmonthbegin", "type": "Data offsets", "text": "\nalias of `pandas._libs.tslibs.offsets.CustomBusinessMonthBegin`\n\n"}, {"name": "pandas.tseries.offsets.CBMonthEnd", "path": "reference/api/pandas.tseries.offsets.cbmonthend", "type": "Data offsets", "text": "\nalias of `pandas._libs.tslibs.offsets.CustomBusinessMonthEnd`\n\n"}, {"name": "pandas.tseries.offsets.CDay", "path": "reference/api/pandas.tseries.offsets.cday", "type": "Data offsets", "text": "\nalias of `pandas._libs.tslibs.offsets.CustomBusinessDay`\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay", "path": "reference/api/pandas.tseries.offsets.custombusinessday", "type": "Data offsets", "text": "\nDateOffset subclass representing custom business days excluding holidays.\n\nNormalize start/end dates to midnight before generating date range.\n\nWeekmask of valid business days, passed to `numpy.busdaycalendar`.\n\nList/array of dates to exclude from the set of valid business days, passed to\n`numpy.busdaycalendar`.\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n`offset`\n\nAlias for self._offset.\n\ncalendar\n\nfreqstr\n\nholidays\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nweekmask\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.__call__", "path": "reference/api/pandas.tseries.offsets.custombusinessday.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.apply", "path": "reference/api/pandas.tseries.offsets.custombusinessday.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.apply_index", "path": "reference/api/pandas.tseries.offsets.custombusinessday.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.base", "path": "reference/api/pandas.tseries.offsets.custombusinessday.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.calendar", "path": "reference/api/pandas.tseries.offsets.custombusinessday.calendar", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.copy", "path": "reference/api/pandas.tseries.offsets.custombusinessday.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.freqstr", "path": "reference/api/pandas.tseries.offsets.custombusinessday.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.holidays", "path": "reference/api/pandas.tseries.offsets.custombusinessday.holidays", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.is_anchored", "path": "reference/api/pandas.tseries.offsets.custombusinessday.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.is_month_end", "path": "reference/api/pandas.tseries.offsets.custombusinessday.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.is_month_start", "path": "reference/api/pandas.tseries.offsets.custombusinessday.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.is_on_offset", "path": "reference/api/pandas.tseries.offsets.custombusinessday.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.custombusinessday.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.custombusinessday.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.is_year_end", "path": "reference/api/pandas.tseries.offsets.custombusinessday.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.is_year_start", "path": "reference/api/pandas.tseries.offsets.custombusinessday.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.isAnchored", "path": "reference/api/pandas.tseries.offsets.custombusinessday.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.kwds", "path": "reference/api/pandas.tseries.offsets.custombusinessday.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.n", "path": "reference/api/pandas.tseries.offsets.custombusinessday.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.name", "path": "reference/api/pandas.tseries.offsets.custombusinessday.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.nanos", "path": "reference/api/pandas.tseries.offsets.custombusinessday.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.normalize", "path": "reference/api/pandas.tseries.offsets.custombusinessday.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.offset", "path": "reference/api/pandas.tseries.offsets.custombusinessday.offset", "type": "Data offsets", "text": "\nAlias for self._offset.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.onOffset", "path": "reference/api/pandas.tseries.offsets.custombusinessday.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.rollback", "path": "reference/api/pandas.tseries.offsets.custombusinessday.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.rollforward", "path": "reference/api/pandas.tseries.offsets.custombusinessday.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.rule_code", "path": "reference/api/pandas.tseries.offsets.custombusinessday.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessDay.weekmask", "path": "reference/api/pandas.tseries.offsets.custombusinessday.weekmask", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour", "path": "reference/api/pandas.tseries.offsets.custombusinesshour", "type": "Data offsets", "text": "\nDateOffset subclass representing possibly n custom business days.\n\nThe number of months represented.\n\nNormalize start/end dates to midnight before generating date range.\n\nWeekmask of valid business days, passed to `numpy.busdaycalendar`.\n\nStart time of your custom business hour in 24h format.\n\nEnd time of your custom business hour in 24h format.\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n`next_bday`\n\nUsed for moving to next business day.\n\n`offset`\n\nAlias for self._offset.\n\ncalendar\n\nend\n\nfreqstr\n\nholidays\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nstart\n\nweekmask\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`(other)\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`(other)\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.__call__", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.apply", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.apply_index", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.base", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.calendar", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.calendar", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.copy", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.end", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.freqstr", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.holidays", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.holidays", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.is_anchored", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.is_month_end", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.is_month_start", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.is_on_offset", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.is_year_end", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.is_year_start", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.isAnchored", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.kwds", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.n", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.name", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.nanos", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.next_bday", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.next_bday", "type": "Data offsets", "text": "\nUsed for moving to next business day.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.normalize", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.offset", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.offset", "type": "Data offsets", "text": "\nAlias for self._offset.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.onOffset", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.rollback", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.rollforward", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.rule_code", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.start", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessHour.weekmask", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.weekmask", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin", "type": "Data offsets", "text": "\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n`cbday_roll`\n\nDefine default roll function to be called in apply method.\n\n`month_roll`\n\nDefine default roll function to be called in apply method.\n\n`offset`\n\nAlias for self._offset.\n\ncalendar\n\nfreqstr\n\nholidays\n\nkwds\n\nm_offset\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nweekmask\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.__call__", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.apply", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.apply_index", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.base", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.calendar", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.calendar", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.cbday_roll", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.cbday_roll", "type": "Data offsets", "text": "\nDefine default roll function to be called in apply method.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.copy", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.freqstr", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.holidays", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.holidays", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.is_anchored", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.is_month_end", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.is_month_start", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.is_on_offset", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.is_year_end", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.is_year_start", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.isAnchored", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.kwds", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.m_offset", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.m_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.month_roll", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.month_roll", "type": "Data offsets", "text": "\nDefine default roll function to be called in apply method.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.n", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.name", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.nanos", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.normalize", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.offset", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.offset", "type": "Data offsets", "text": "\nAlias for self._offset.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.onOffset", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.rollback", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.rollforward", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.rule_code", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.weekmask", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.weekmask", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend", "type": "Data offsets", "text": "\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n`cbday_roll`\n\nDefine default roll function to be called in apply method.\n\n`month_roll`\n\nDefine default roll function to be called in apply method.\n\n`offset`\n\nAlias for self._offset.\n\ncalendar\n\nfreqstr\n\nholidays\n\nkwds\n\nm_offset\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nweekmask\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.__call__", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.apply", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.apply_index", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.base", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.calendar", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.calendar", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.cbday_roll", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.cbday_roll", "type": "Data offsets", "text": "\nDefine default roll function to be called in apply method.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.copy", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.freqstr", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.holidays", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.holidays", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.is_anchored", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.is_month_end", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.is_month_start", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.is_on_offset", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.is_year_end", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.is_year_start", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.isAnchored", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.kwds", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.m_offset", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.m_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.month_roll", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.month_roll", "type": "Data offsets", "text": "\nDefine default roll function to be called in apply method.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.n", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.name", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.nanos", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.normalize", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.offset", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.offset", "type": "Data offsets", "text": "\nAlias for self._offset.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.onOffset", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.rollback", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.rollforward", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.rule_code", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.weekmask", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.weekmask", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset", "path": "reference/api/pandas.tseries.offsets.dateoffset", "type": "Data offsets", "text": "\nStandard kind of date increment used for a date range.\n\nWorks exactly like the keyword argument form of relativedelta. Note that the\npositional argument form of relativedelata is not supported. Use of the\nkeyword n is discouraged\u2013 you would be better off specifying n in the keywords\nyou use, but regardless it is there for you. n is needed for DateOffset\nsubclasses.\n\nDateOffset works as follows. Each offset specify a set of dates that conform\nto the DateOffset. For example, Bday defines this set to be the set of dates\nthat are weekdays (M-F). To test if a date is in the set of a DateOffset\ndateOffset we can use the is_on_offset method: dateOffset.is_on_offset(date).\n\nIf a date is not on a valid date, the rollback and rollforward methods can be\nused to roll the date to the nearest valid date before/after the date.\n\nDateOffsets can be created to move dates forward a given number of valid\ndates. For example, Bday(2) can be added to a date to move it two business\ndays forward. If the date does not start on a valid date, first it is moved to\na valid date. Thus pseudo code is:\n\ndate = rollback(date) # does nothing if date is valid return date + <n number\nof periods>\n\nWhen a date offset is created for a negative number of periods, the date is\nfirst rolled forward. The pseudo code is:\n\ndate = rollforward(date) # does nothing is date is valid return date + <n\nnumber of periods>\n\nZero presents a problem. Should it roll forward or back? We arbitrarily have\nit rollforward:\n\ndate + BDay(0) == BDay.rollforward(date)\n\nSince 0 is a bit weird, we suggest avoiding its use.\n\nThe number of time periods the offset represents.\n\nWhether to round the result of a DateOffset addition down to the previous\nmidnight.\n\nTemporal parameter that add to or replace the offset value.\n\nParameters that add to the offset (like Timedelta):\n\nyears\n\nmonths\n\nweeks\n\ndays\n\nhours\n\nminutes\n\nseconds\n\nmicroseconds\n\nnanoseconds\n\nParameters that replace the offset value:\n\nyear\n\nmonth\n\nday\n\nweekday\n\nhour\n\nminute\n\nsecond\n\nmicrosecond\n\nnanosecond.\n\nSee also\n\nThe relativedelta type is designed to be applied to an existing datetime an\ncan replace specific components of that datetime, or represents an interval of\ntime.\n\nExamples\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.__call__", "path": "reference/api/pandas.tseries.offsets.dateoffset.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.apply", "path": "reference/api/pandas.tseries.offsets.dateoffset.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.apply_index", "path": "reference/api/pandas.tseries.offsets.dateoffset.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.base", "path": "reference/api/pandas.tseries.offsets.dateoffset.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.copy", "path": "reference/api/pandas.tseries.offsets.dateoffset.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.freqstr", "path": "reference/api/pandas.tseries.offsets.dateoffset.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.is_anchored", "path": "reference/api/pandas.tseries.offsets.dateoffset.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.is_month_end", "path": "reference/api/pandas.tseries.offsets.dateoffset.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.is_month_start", "path": "reference/api/pandas.tseries.offsets.dateoffset.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.is_on_offset", "path": "reference/api/pandas.tseries.offsets.dateoffset.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.dateoffset.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.dateoffset.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.is_year_end", "path": "reference/api/pandas.tseries.offsets.dateoffset.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.is_year_start", "path": "reference/api/pandas.tseries.offsets.dateoffset.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.isAnchored", "path": "reference/api/pandas.tseries.offsets.dateoffset.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.kwds", "path": "reference/api/pandas.tseries.offsets.dateoffset.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.n", "path": "reference/api/pandas.tseries.offsets.dateoffset.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.name", "path": "reference/api/pandas.tseries.offsets.dateoffset.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.nanos", "path": "reference/api/pandas.tseries.offsets.dateoffset.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.normalize", "path": "reference/api/pandas.tseries.offsets.dateoffset.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.onOffset", "path": "reference/api/pandas.tseries.offsets.dateoffset.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.rollback", "path": "reference/api/pandas.tseries.offsets.dateoffset.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.rollforward", "path": "reference/api/pandas.tseries.offsets.dateoffset.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.DateOffset.rule_code", "path": "reference/api/pandas.tseries.offsets.dateoffset.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day", "path": "reference/api/pandas.tseries.offsets.day", "type": "Data offsets", "text": "\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\ndelta\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.Day.__call__", "path": "reference/api/pandas.tseries.offsets.day.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.Day.apply", "path": "reference/api/pandas.tseries.offsets.day.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.apply_index", "path": "reference/api/pandas.tseries.offsets.day.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.base", "path": "reference/api/pandas.tseries.offsets.day.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.Day.copy", "path": "reference/api/pandas.tseries.offsets.day.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.delta", "path": "reference/api/pandas.tseries.offsets.day.delta", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.freqstr", "path": "reference/api/pandas.tseries.offsets.day.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.is_anchored", "path": "reference/api/pandas.tseries.offsets.day.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.is_month_end", "path": "reference/api/pandas.tseries.offsets.day.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.is_month_start", "path": "reference/api/pandas.tseries.offsets.day.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.is_on_offset", "path": "reference/api/pandas.tseries.offsets.day.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.day.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.day.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.is_year_end", "path": "reference/api/pandas.tseries.offsets.day.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.is_year_start", "path": "reference/api/pandas.tseries.offsets.day.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.isAnchored", "path": "reference/api/pandas.tseries.offsets.day.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.kwds", "path": "reference/api/pandas.tseries.offsets.day.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.n", "path": "reference/api/pandas.tseries.offsets.day.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.name", "path": "reference/api/pandas.tseries.offsets.day.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.nanos", "path": "reference/api/pandas.tseries.offsets.day.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.normalize", "path": "reference/api/pandas.tseries.offsets.day.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.onOffset", "path": "reference/api/pandas.tseries.offsets.day.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Day.rollback", "path": "reference/api/pandas.tseries.offsets.day.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Day.rollforward", "path": "reference/api/pandas.tseries.offsets.day.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Day.rule_code", "path": "reference/api/pandas.tseries.offsets.day.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter", "path": "reference/api/pandas.tseries.offsets.easter", "type": "Data offsets", "text": "\nDateOffset for the Easter holiday using logic defined in dateutil.\n\nRight now uses the revised method which is valid in years 1583-4099.\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.Easter.__call__", "path": "reference/api/pandas.tseries.offsets.easter.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.Easter.apply", "path": "reference/api/pandas.tseries.offsets.easter.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.apply_index", "path": "reference/api/pandas.tseries.offsets.easter.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.base", "path": "reference/api/pandas.tseries.offsets.easter.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.Easter.copy", "path": "reference/api/pandas.tseries.offsets.easter.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.freqstr", "path": "reference/api/pandas.tseries.offsets.easter.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.is_anchored", "path": "reference/api/pandas.tseries.offsets.easter.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.is_month_end", "path": "reference/api/pandas.tseries.offsets.easter.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.is_month_start", "path": "reference/api/pandas.tseries.offsets.easter.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.is_on_offset", "path": "reference/api/pandas.tseries.offsets.easter.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.easter.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.easter.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.is_year_end", "path": "reference/api/pandas.tseries.offsets.easter.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.is_year_start", "path": "reference/api/pandas.tseries.offsets.easter.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.isAnchored", "path": "reference/api/pandas.tseries.offsets.easter.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.kwds", "path": "reference/api/pandas.tseries.offsets.easter.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.n", "path": "reference/api/pandas.tseries.offsets.easter.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.name", "path": "reference/api/pandas.tseries.offsets.easter.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.nanos", "path": "reference/api/pandas.tseries.offsets.easter.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.normalize", "path": "reference/api/pandas.tseries.offsets.easter.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.onOffset", "path": "reference/api/pandas.tseries.offsets.easter.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Easter.rollback", "path": "reference/api/pandas.tseries.offsets.easter.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Easter.rollforward", "path": "reference/api/pandas.tseries.offsets.easter.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Easter.rule_code", "path": "reference/api/pandas.tseries.offsets.easter.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253", "path": "reference/api/pandas.tseries.offsets.fy5253", "type": "Data offsets", "text": "\nDescribes 52-53 week fiscal year. This is also known as a 4-4-5 calendar.\n\nIt is used by companies that desire that their fiscal year always end on the\nsame day of the week.\n\nIt is a method of managing accounting periods. It is a common calendar\nstructure for some industries, such as retail, manufacturing and parking\nindustry.\n\nFor more information see: https://en.wikipedia.org/wiki/4-4-5_calendar\n\nThe year may either:\n\nend on the last X day of the Y month.\n\nend on the last X day closest to the last day of the Y month.\n\nX is a specific day of the week. Y is a certain month of the year\n\nA specific integer for the day of the week.\n\n0 is Monday\n\n1 is Tuesday\n\n2 is Wednesday\n\n3 is Thursday\n\n4 is Friday\n\n5 is Saturday\n\n6 is Sunday.\n\nThe month in which the fiscal year ends.\n\nMethod of employing 4-4-5 calendar.\n\nThere are two options:\n\n\u201cnearest\u201d means year end is weekday closest to last day of month in year.\n\n\u201clast\u201d means year end is final weekday of the final month in fiscal year.\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nstartingMonth\n\nvariation\n\nweekday\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nget_rule_code_suffix\n\nget_year_end\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.FY5253.__call__", "path": "reference/api/pandas.tseries.offsets.fy5253.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.FY5253.apply", "path": "reference/api/pandas.tseries.offsets.fy5253.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.apply_index", "path": "reference/api/pandas.tseries.offsets.fy5253.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.base", "path": "reference/api/pandas.tseries.offsets.fy5253.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.FY5253.copy", "path": "reference/api/pandas.tseries.offsets.fy5253.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.freqstr", "path": "reference/api/pandas.tseries.offsets.fy5253.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.get_rule_code_suffix", "path": "reference/api/pandas.tseries.offsets.fy5253.get_rule_code_suffix", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.get_year_end", "path": "reference/api/pandas.tseries.offsets.fy5253.get_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.is_anchored", "path": "reference/api/pandas.tseries.offsets.fy5253.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.is_month_end", "path": "reference/api/pandas.tseries.offsets.fy5253.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.is_month_start", "path": "reference/api/pandas.tseries.offsets.fy5253.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.is_on_offset", "path": "reference/api/pandas.tseries.offsets.fy5253.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.fy5253.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.fy5253.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.is_year_end", "path": "reference/api/pandas.tseries.offsets.fy5253.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.is_year_start", "path": "reference/api/pandas.tseries.offsets.fy5253.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.isAnchored", "path": "reference/api/pandas.tseries.offsets.fy5253.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.kwds", "path": "reference/api/pandas.tseries.offsets.fy5253.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.n", "path": "reference/api/pandas.tseries.offsets.fy5253.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.name", "path": "reference/api/pandas.tseries.offsets.fy5253.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.nanos", "path": "reference/api/pandas.tseries.offsets.fy5253.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.normalize", "path": "reference/api/pandas.tseries.offsets.fy5253.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.onOffset", "path": "reference/api/pandas.tseries.offsets.fy5253.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.rollback", "path": "reference/api/pandas.tseries.offsets.fy5253.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.FY5253.rollforward", "path": "reference/api/pandas.tseries.offsets.fy5253.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.FY5253.rule_code", "path": "reference/api/pandas.tseries.offsets.fy5253.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.startingMonth", "path": "reference/api/pandas.tseries.offsets.fy5253.startingmonth", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.variation", "path": "reference/api/pandas.tseries.offsets.fy5253.variation", "type": "Input/output", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253.weekday", "path": "reference/api/pandas.tseries.offsets.fy5253.weekday", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter", "path": "reference/api/pandas.tseries.offsets.fy5253quarter", "type": "Data offsets", "text": "\nDateOffset increments between business quarter dates for 52-53 week fiscal\nyear (also known as a 4-4-5 calendar).\n\nIt is used by companies that desire that their fiscal year always end on the\nsame day of the week.\n\nIt is a method of managing accounting periods. It is a common calendar\nstructure for some industries, such as retail, manufacturing and parking\nindustry.\n\nFor more information see: https://en.wikipedia.org/wiki/4-4-5_calendar\n\nThe year may either:\n\nend on the last X day of the Y month.\n\nend on the last X day closest to the last day of the Y month.\n\nX is a specific day of the week. Y is a certain month of the year\n\nstartingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, \u2026\nstartingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, \u2026\nstartingMonth = 3 corresponds to dates like 3/30/2007, 6/29/2007, \u2026\n\nA specific integer for the day of the week.\n\n0 is Monday\n\n1 is Tuesday\n\n2 is Wednesday\n\n3 is Thursday\n\n4 is Friday\n\n5 is Saturday\n\n6 is Sunday.\n\nThe month in which fiscal years end.\n\nThe quarter number that has the leap or 14 week when needed.\n\nMethod of employing 4-4-5 calendar.\n\nThere are two options:\n\n\u201cnearest\u201d means year end is weekday closest to last day of month in year.\n\n\u201clast\u201d means year end is final weekday of the final month in fiscal year.\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nqtr_with_extra_week\n\nrule_code\n\nstartingMonth\n\nvariation\n\nweekday\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nget_rule_code_suffix\n\nget_weeks\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\nyear_has_extra_week\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.__call__", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.apply", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.apply_index", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.base", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.copy", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.freqstr", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.get_rule_code_suffix", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.get_rule_code_suffix", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.get_weeks", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.get_weeks", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.is_anchored", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.is_month_end", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.is_month_start", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.is_on_offset", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.is_year_end", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.is_year_start", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.isAnchored", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.kwds", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.n", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.name", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.nanos", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.normalize", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.onOffset", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.qtr_with_extra_week", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.qtr_with_extra_week", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.rollback", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.rollforward", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.rule_code", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.startingMonth", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.startingmonth", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.variation", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.variation", "type": "Input/output", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.weekday", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.weekday", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.FY5253Quarter.year_has_extra_week", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.year_has_extra_week", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour", "path": "reference/api/pandas.tseries.offsets.hour", "type": "Data offsets", "text": "\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\ndelta\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.Hour.__call__", "path": "reference/api/pandas.tseries.offsets.hour.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.Hour.apply", "path": "reference/api/pandas.tseries.offsets.hour.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.apply_index", "path": "reference/api/pandas.tseries.offsets.hour.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.base", "path": "reference/api/pandas.tseries.offsets.hour.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.Hour.copy", "path": "reference/api/pandas.tseries.offsets.hour.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.delta", "path": "reference/api/pandas.tseries.offsets.hour.delta", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.freqstr", "path": "reference/api/pandas.tseries.offsets.hour.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.is_anchored", "path": "reference/api/pandas.tseries.offsets.hour.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.is_month_end", "path": "reference/api/pandas.tseries.offsets.hour.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.is_month_start", "path": "reference/api/pandas.tseries.offsets.hour.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.is_on_offset", "path": "reference/api/pandas.tseries.offsets.hour.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.hour.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.hour.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.is_year_end", "path": "reference/api/pandas.tseries.offsets.hour.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.is_year_start", "path": "reference/api/pandas.tseries.offsets.hour.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.isAnchored", "path": "reference/api/pandas.tseries.offsets.hour.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.kwds", "path": "reference/api/pandas.tseries.offsets.hour.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.n", "path": "reference/api/pandas.tseries.offsets.hour.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.name", "path": "reference/api/pandas.tseries.offsets.hour.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.nanos", "path": "reference/api/pandas.tseries.offsets.hour.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.normalize", "path": "reference/api/pandas.tseries.offsets.hour.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.onOffset", "path": "reference/api/pandas.tseries.offsets.hour.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Hour.rollback", "path": "reference/api/pandas.tseries.offsets.hour.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Hour.rollforward", "path": "reference/api/pandas.tseries.offsets.hour.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Hour.rule_code", "path": "reference/api/pandas.tseries.offsets.hour.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth", "type": "Data offsets", "text": "\nDescribes monthly dates in last week of month like \u201cthe last Tuesday of each\nmonth\u201d.\n\nA specific integer for the day of the week.\n\n0 is Monday\n\n1 is Tuesday\n\n2 is Wednesday\n\n3 is Thursday\n\n4 is Friday\n\n5 is Saturday\n\n6 is Sunday.\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nweek\n\nweekday\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.__call__", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.apply", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.apply_index", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.base", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.copy", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.freqstr", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.is_anchored", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.is_month_end", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.is_month_start", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.is_on_offset", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.is_year_end", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.is_year_start", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.isAnchored", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.kwds", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.n", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.name", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.nanos", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.normalize", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.onOffset", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.rollback", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.rollforward", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.rule_code", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.week", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.week", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.weekday", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.weekday", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro", "path": "reference/api/pandas.tseries.offsets.micro", "type": "Data offsets", "text": "\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\ndelta\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.Micro.__call__", "path": "reference/api/pandas.tseries.offsets.micro.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.Micro.apply", "path": "reference/api/pandas.tseries.offsets.micro.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.apply_index", "path": "reference/api/pandas.tseries.offsets.micro.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.base", "path": "reference/api/pandas.tseries.offsets.micro.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.Micro.copy", "path": "reference/api/pandas.tseries.offsets.micro.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.delta", "path": "reference/api/pandas.tseries.offsets.micro.delta", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.freqstr", "path": "reference/api/pandas.tseries.offsets.micro.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.is_anchored", "path": "reference/api/pandas.tseries.offsets.micro.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.is_month_end", "path": "reference/api/pandas.tseries.offsets.micro.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.is_month_start", "path": "reference/api/pandas.tseries.offsets.micro.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.is_on_offset", "path": "reference/api/pandas.tseries.offsets.micro.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.micro.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.micro.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.is_year_end", "path": "reference/api/pandas.tseries.offsets.micro.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.is_year_start", "path": "reference/api/pandas.tseries.offsets.micro.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.isAnchored", "path": "reference/api/pandas.tseries.offsets.micro.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.kwds", "path": "reference/api/pandas.tseries.offsets.micro.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.n", "path": "reference/api/pandas.tseries.offsets.micro.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.name", "path": "reference/api/pandas.tseries.offsets.micro.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.nanos", "path": "reference/api/pandas.tseries.offsets.micro.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.normalize", "path": "reference/api/pandas.tseries.offsets.micro.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.onOffset", "path": "reference/api/pandas.tseries.offsets.micro.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Micro.rollback", "path": "reference/api/pandas.tseries.offsets.micro.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Micro.rollforward", "path": "reference/api/pandas.tseries.offsets.micro.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Micro.rule_code", "path": "reference/api/pandas.tseries.offsets.micro.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli", "path": "reference/api/pandas.tseries.offsets.milli", "type": "Data offsets", "text": "\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\ndelta\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.Milli.__call__", "path": "reference/api/pandas.tseries.offsets.milli.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.Milli.apply", "path": "reference/api/pandas.tseries.offsets.milli.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.apply_index", "path": "reference/api/pandas.tseries.offsets.milli.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.base", "path": "reference/api/pandas.tseries.offsets.milli.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.Milli.copy", "path": "reference/api/pandas.tseries.offsets.milli.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.delta", "path": "reference/api/pandas.tseries.offsets.milli.delta", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.freqstr", "path": "reference/api/pandas.tseries.offsets.milli.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.is_anchored", "path": "reference/api/pandas.tseries.offsets.milli.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.is_month_end", "path": "reference/api/pandas.tseries.offsets.milli.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.is_month_start", "path": "reference/api/pandas.tseries.offsets.milli.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.is_on_offset", "path": "reference/api/pandas.tseries.offsets.milli.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.milli.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.milli.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.is_year_end", "path": "reference/api/pandas.tseries.offsets.milli.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.is_year_start", "path": "reference/api/pandas.tseries.offsets.milli.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.isAnchored", "path": "reference/api/pandas.tseries.offsets.milli.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.kwds", "path": "reference/api/pandas.tseries.offsets.milli.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.n", "path": "reference/api/pandas.tseries.offsets.milli.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.name", "path": "reference/api/pandas.tseries.offsets.milli.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.nanos", "path": "reference/api/pandas.tseries.offsets.milli.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.normalize", "path": "reference/api/pandas.tseries.offsets.milli.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.onOffset", "path": "reference/api/pandas.tseries.offsets.milli.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Milli.rollback", "path": "reference/api/pandas.tseries.offsets.milli.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Milli.rollforward", "path": "reference/api/pandas.tseries.offsets.milli.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Milli.rule_code", "path": "reference/api/pandas.tseries.offsets.milli.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute", "path": "reference/api/pandas.tseries.offsets.minute", "type": "Data offsets", "text": "\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\ndelta\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.Minute.__call__", "path": "reference/api/pandas.tseries.offsets.minute.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.Minute.apply", "path": "reference/api/pandas.tseries.offsets.minute.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.apply_index", "path": "reference/api/pandas.tseries.offsets.minute.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.base", "path": "reference/api/pandas.tseries.offsets.minute.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.Minute.copy", "path": "reference/api/pandas.tseries.offsets.minute.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.delta", "path": "reference/api/pandas.tseries.offsets.minute.delta", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.freqstr", "path": "reference/api/pandas.tseries.offsets.minute.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.is_anchored", "path": "reference/api/pandas.tseries.offsets.minute.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.is_month_end", "path": "reference/api/pandas.tseries.offsets.minute.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.is_month_start", "path": "reference/api/pandas.tseries.offsets.minute.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.is_on_offset", "path": "reference/api/pandas.tseries.offsets.minute.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.minute.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.minute.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.is_year_end", "path": "reference/api/pandas.tseries.offsets.minute.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.is_year_start", "path": "reference/api/pandas.tseries.offsets.minute.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.isAnchored", "path": "reference/api/pandas.tseries.offsets.minute.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.kwds", "path": "reference/api/pandas.tseries.offsets.minute.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.n", "path": "reference/api/pandas.tseries.offsets.minute.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.name", "path": "reference/api/pandas.tseries.offsets.minute.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.nanos", "path": "reference/api/pandas.tseries.offsets.minute.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.normalize", "path": "reference/api/pandas.tseries.offsets.minute.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.onOffset", "path": "reference/api/pandas.tseries.offsets.minute.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Minute.rollback", "path": "reference/api/pandas.tseries.offsets.minute.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Minute.rollforward", "path": "reference/api/pandas.tseries.offsets.minute.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Minute.rule_code", "path": "reference/api/pandas.tseries.offsets.minute.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin", "path": "reference/api/pandas.tseries.offsets.monthbegin", "type": "Data offsets", "text": "\nDateOffset of one month at beginning.\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.__call__", "path": "reference/api/pandas.tseries.offsets.monthbegin.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.apply", "path": "reference/api/pandas.tseries.offsets.monthbegin.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.apply_index", "path": "reference/api/pandas.tseries.offsets.monthbegin.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.base", "path": "reference/api/pandas.tseries.offsets.monthbegin.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.copy", "path": "reference/api/pandas.tseries.offsets.monthbegin.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.freqstr", "path": "reference/api/pandas.tseries.offsets.monthbegin.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.is_anchored", "path": "reference/api/pandas.tseries.offsets.monthbegin.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.is_month_end", "path": "reference/api/pandas.tseries.offsets.monthbegin.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.is_month_start", "path": "reference/api/pandas.tseries.offsets.monthbegin.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.is_on_offset", "path": "reference/api/pandas.tseries.offsets.monthbegin.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.monthbegin.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.monthbegin.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.is_year_end", "path": "reference/api/pandas.tseries.offsets.monthbegin.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.is_year_start", "path": "reference/api/pandas.tseries.offsets.monthbegin.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.isAnchored", "path": "reference/api/pandas.tseries.offsets.monthbegin.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.kwds", "path": "reference/api/pandas.tseries.offsets.monthbegin.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.n", "path": "reference/api/pandas.tseries.offsets.monthbegin.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.name", "path": "reference/api/pandas.tseries.offsets.monthbegin.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.nanos", "path": "reference/api/pandas.tseries.offsets.monthbegin.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.normalize", "path": "reference/api/pandas.tseries.offsets.monthbegin.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.onOffset", "path": "reference/api/pandas.tseries.offsets.monthbegin.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.rollback", "path": "reference/api/pandas.tseries.offsets.monthbegin.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.rollforward", "path": "reference/api/pandas.tseries.offsets.monthbegin.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.MonthBegin.rule_code", "path": "reference/api/pandas.tseries.offsets.monthbegin.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd", "path": "reference/api/pandas.tseries.offsets.monthend", "type": "Data offsets", "text": "\nDateOffset of one month end.\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.__call__", "path": "reference/api/pandas.tseries.offsets.monthend.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.apply", "path": "reference/api/pandas.tseries.offsets.monthend.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.apply_index", "path": "reference/api/pandas.tseries.offsets.monthend.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.base", "path": "reference/api/pandas.tseries.offsets.monthend.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.copy", "path": "reference/api/pandas.tseries.offsets.monthend.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.freqstr", "path": "reference/api/pandas.tseries.offsets.monthend.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.is_anchored", "path": "reference/api/pandas.tseries.offsets.monthend.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.is_month_end", "path": "reference/api/pandas.tseries.offsets.monthend.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.is_month_start", "path": "reference/api/pandas.tseries.offsets.monthend.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.is_on_offset", "path": "reference/api/pandas.tseries.offsets.monthend.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.monthend.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.monthend.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.is_year_end", "path": "reference/api/pandas.tseries.offsets.monthend.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.is_year_start", "path": "reference/api/pandas.tseries.offsets.monthend.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.isAnchored", "path": "reference/api/pandas.tseries.offsets.monthend.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.kwds", "path": "reference/api/pandas.tseries.offsets.monthend.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.n", "path": "reference/api/pandas.tseries.offsets.monthend.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.name", "path": "reference/api/pandas.tseries.offsets.monthend.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.nanos", "path": "reference/api/pandas.tseries.offsets.monthend.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.normalize", "path": "reference/api/pandas.tseries.offsets.monthend.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.onOffset", "path": "reference/api/pandas.tseries.offsets.monthend.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.rollback", "path": "reference/api/pandas.tseries.offsets.monthend.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.rollforward", "path": "reference/api/pandas.tseries.offsets.monthend.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.MonthEnd.rule_code", "path": "reference/api/pandas.tseries.offsets.monthend.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano", "path": "reference/api/pandas.tseries.offsets.nano", "type": "Data offsets", "text": "\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\ndelta\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.Nano.__call__", "path": "reference/api/pandas.tseries.offsets.nano.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.Nano.apply", "path": "reference/api/pandas.tseries.offsets.nano.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.apply_index", "path": "reference/api/pandas.tseries.offsets.nano.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.base", "path": "reference/api/pandas.tseries.offsets.nano.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.Nano.copy", "path": "reference/api/pandas.tseries.offsets.nano.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.delta", "path": "reference/api/pandas.tseries.offsets.nano.delta", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.freqstr", "path": "reference/api/pandas.tseries.offsets.nano.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.is_anchored", "path": "reference/api/pandas.tseries.offsets.nano.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.is_month_end", "path": "reference/api/pandas.tseries.offsets.nano.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.is_month_start", "path": "reference/api/pandas.tseries.offsets.nano.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.is_on_offset", "path": "reference/api/pandas.tseries.offsets.nano.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.nano.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.nano.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.is_year_end", "path": "reference/api/pandas.tseries.offsets.nano.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.is_year_start", "path": "reference/api/pandas.tseries.offsets.nano.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.isAnchored", "path": "reference/api/pandas.tseries.offsets.nano.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.kwds", "path": "reference/api/pandas.tseries.offsets.nano.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.n", "path": "reference/api/pandas.tseries.offsets.nano.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.name", "path": "reference/api/pandas.tseries.offsets.nano.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.nanos", "path": "reference/api/pandas.tseries.offsets.nano.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.normalize", "path": "reference/api/pandas.tseries.offsets.nano.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.onOffset", "path": "reference/api/pandas.tseries.offsets.nano.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Nano.rollback", "path": "reference/api/pandas.tseries.offsets.nano.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Nano.rollforward", "path": "reference/api/pandas.tseries.offsets.nano.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Nano.rule_code", "path": "reference/api/pandas.tseries.offsets.nano.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin", "path": "reference/api/pandas.tseries.offsets.quarterbegin", "type": "Data offsets", "text": "\nDateOffset increments between Quarter start dates.\n\nstartingMonth = 1 corresponds to dates like 1/01/2007, 4/01/2007, \u2026\nstartingMonth = 2 corresponds to dates like 2/01/2007, 5/01/2007, \u2026\nstartingMonth = 3 corresponds to dates like 3/01/2007, 6/01/2007, \u2026\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nstartingMonth\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.__call__", "path": "reference/api/pandas.tseries.offsets.quarterbegin.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.apply", "path": "reference/api/pandas.tseries.offsets.quarterbegin.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.apply_index", "path": "reference/api/pandas.tseries.offsets.quarterbegin.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.base", "path": "reference/api/pandas.tseries.offsets.quarterbegin.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.copy", "path": "reference/api/pandas.tseries.offsets.quarterbegin.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.freqstr", "path": "reference/api/pandas.tseries.offsets.quarterbegin.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.is_anchored", "path": "reference/api/pandas.tseries.offsets.quarterbegin.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.is_month_end", "path": "reference/api/pandas.tseries.offsets.quarterbegin.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.is_month_start", "path": "reference/api/pandas.tseries.offsets.quarterbegin.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.is_on_offset", "path": "reference/api/pandas.tseries.offsets.quarterbegin.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.quarterbegin.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.quarterbegin.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.is_year_end", "path": "reference/api/pandas.tseries.offsets.quarterbegin.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.is_year_start", "path": "reference/api/pandas.tseries.offsets.quarterbegin.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.isAnchored", "path": "reference/api/pandas.tseries.offsets.quarterbegin.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.kwds", "path": "reference/api/pandas.tseries.offsets.quarterbegin.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.n", "path": "reference/api/pandas.tseries.offsets.quarterbegin.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.name", "path": "reference/api/pandas.tseries.offsets.quarterbegin.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.nanos", "path": "reference/api/pandas.tseries.offsets.quarterbegin.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.normalize", "path": "reference/api/pandas.tseries.offsets.quarterbegin.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.onOffset", "path": "reference/api/pandas.tseries.offsets.quarterbegin.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.rollback", "path": "reference/api/pandas.tseries.offsets.quarterbegin.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.rollforward", "path": "reference/api/pandas.tseries.offsets.quarterbegin.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.rule_code", "path": "reference/api/pandas.tseries.offsets.quarterbegin.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterBegin.startingMonth", "path": "reference/api/pandas.tseries.offsets.quarterbegin.startingmonth", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd", "path": "reference/api/pandas.tseries.offsets.quarterend", "type": "Data offsets", "text": "\nDateOffset increments between Quarter end dates.\n\nstartingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, \u2026\nstartingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, \u2026\nstartingMonth = 3 corresponds to dates like 3/31/2007, 6/30/2007, \u2026\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nstartingMonth\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.__call__", "path": "reference/api/pandas.tseries.offsets.quarterend.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.apply", "path": "reference/api/pandas.tseries.offsets.quarterend.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.apply_index", "path": "reference/api/pandas.tseries.offsets.quarterend.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.base", "path": "reference/api/pandas.tseries.offsets.quarterend.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.copy", "path": "reference/api/pandas.tseries.offsets.quarterend.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.freqstr", "path": "reference/api/pandas.tseries.offsets.quarterend.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.is_anchored", "path": "reference/api/pandas.tseries.offsets.quarterend.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.is_month_end", "path": "reference/api/pandas.tseries.offsets.quarterend.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.is_month_start", "path": "reference/api/pandas.tseries.offsets.quarterend.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.is_on_offset", "path": "reference/api/pandas.tseries.offsets.quarterend.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.quarterend.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.quarterend.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.is_year_end", "path": "reference/api/pandas.tseries.offsets.quarterend.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.is_year_start", "path": "reference/api/pandas.tseries.offsets.quarterend.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.isAnchored", "path": "reference/api/pandas.tseries.offsets.quarterend.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.kwds", "path": "reference/api/pandas.tseries.offsets.quarterend.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.n", "path": "reference/api/pandas.tseries.offsets.quarterend.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.name", "path": "reference/api/pandas.tseries.offsets.quarterend.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.nanos", "path": "reference/api/pandas.tseries.offsets.quarterend.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.normalize", "path": "reference/api/pandas.tseries.offsets.quarterend.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.onOffset", "path": "reference/api/pandas.tseries.offsets.quarterend.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.rollback", "path": "reference/api/pandas.tseries.offsets.quarterend.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.rollforward", "path": "reference/api/pandas.tseries.offsets.quarterend.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.rule_code", "path": "reference/api/pandas.tseries.offsets.quarterend.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.QuarterEnd.startingMonth", "path": "reference/api/pandas.tseries.offsets.quarterend.startingmonth", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second", "path": "reference/api/pandas.tseries.offsets.second", "type": "Data offsets", "text": "\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\ndelta\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.Second.__call__", "path": "reference/api/pandas.tseries.offsets.second.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.Second.apply", "path": "reference/api/pandas.tseries.offsets.second.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.apply_index", "path": "reference/api/pandas.tseries.offsets.second.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.base", "path": "reference/api/pandas.tseries.offsets.second.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.Second.copy", "path": "reference/api/pandas.tseries.offsets.second.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.delta", "path": "reference/api/pandas.tseries.offsets.second.delta", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.freqstr", "path": "reference/api/pandas.tseries.offsets.second.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.is_anchored", "path": "reference/api/pandas.tseries.offsets.second.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.is_month_end", "path": "reference/api/pandas.tseries.offsets.second.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.is_month_start", "path": "reference/api/pandas.tseries.offsets.second.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.is_on_offset", "path": "reference/api/pandas.tseries.offsets.second.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.second.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.second.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.is_year_end", "path": "reference/api/pandas.tseries.offsets.second.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.is_year_start", "path": "reference/api/pandas.tseries.offsets.second.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.isAnchored", "path": "reference/api/pandas.tseries.offsets.second.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.kwds", "path": "reference/api/pandas.tseries.offsets.second.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.n", "path": "reference/api/pandas.tseries.offsets.second.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.name", "path": "reference/api/pandas.tseries.offsets.second.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.nanos", "path": "reference/api/pandas.tseries.offsets.second.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.normalize", "path": "reference/api/pandas.tseries.offsets.second.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.onOffset", "path": "reference/api/pandas.tseries.offsets.second.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Second.rollback", "path": "reference/api/pandas.tseries.offsets.second.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Second.rollforward", "path": "reference/api/pandas.tseries.offsets.second.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Second.rule_code", "path": "reference/api/pandas.tseries.offsets.second.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin", "path": "reference/api/pandas.tseries.offsets.semimonthbegin", "type": "Data offsets", "text": "\nTwo DateOffset\u2019s per month repeating on the first day of the month and\nday_of_month.\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nday_of_month\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.__call__", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.apply", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.apply_index", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.base", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.copy", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.day_of_month", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.day_of_month", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.freqstr", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.is_anchored", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.is_month_end", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.is_month_start", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.is_on_offset", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.is_year_end", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.is_year_start", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.isAnchored", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.kwds", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.n", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.name", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.nanos", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.normalize", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.onOffset", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.rollback", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.rollforward", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthBegin.rule_code", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd", "path": "reference/api/pandas.tseries.offsets.semimonthend", "type": "Data offsets", "text": "\nTwo DateOffset\u2019s per month repeating on the last day of the month and\nday_of_month.\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nday_of_month\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.__call__", "path": "reference/api/pandas.tseries.offsets.semimonthend.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.apply", "path": "reference/api/pandas.tseries.offsets.semimonthend.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.apply_index", "path": "reference/api/pandas.tseries.offsets.semimonthend.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.base", "path": "reference/api/pandas.tseries.offsets.semimonthend.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.copy", "path": "reference/api/pandas.tseries.offsets.semimonthend.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.day_of_month", "path": "reference/api/pandas.tseries.offsets.semimonthend.day_of_month", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.freqstr", "path": "reference/api/pandas.tseries.offsets.semimonthend.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.is_anchored", "path": "reference/api/pandas.tseries.offsets.semimonthend.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.is_month_end", "path": "reference/api/pandas.tseries.offsets.semimonthend.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.is_month_start", "path": "reference/api/pandas.tseries.offsets.semimonthend.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.is_on_offset", "path": "reference/api/pandas.tseries.offsets.semimonthend.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.semimonthend.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.semimonthend.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.is_year_end", "path": "reference/api/pandas.tseries.offsets.semimonthend.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.is_year_start", "path": "reference/api/pandas.tseries.offsets.semimonthend.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.isAnchored", "path": "reference/api/pandas.tseries.offsets.semimonthend.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.kwds", "path": "reference/api/pandas.tseries.offsets.semimonthend.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.n", "path": "reference/api/pandas.tseries.offsets.semimonthend.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.name", "path": "reference/api/pandas.tseries.offsets.semimonthend.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.nanos", "path": "reference/api/pandas.tseries.offsets.semimonthend.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.normalize", "path": "reference/api/pandas.tseries.offsets.semimonthend.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.onOffset", "path": "reference/api/pandas.tseries.offsets.semimonthend.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.rollback", "path": "reference/api/pandas.tseries.offsets.semimonthend.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.rollforward", "path": "reference/api/pandas.tseries.offsets.semimonthend.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.SemiMonthEnd.rule_code", "path": "reference/api/pandas.tseries.offsets.semimonthend.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick", "path": "reference/api/pandas.tseries.offsets.tick", "type": "Data offsets", "text": "\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\ndelta\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.Tick.__call__", "path": "reference/api/pandas.tseries.offsets.tick.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.Tick.apply", "path": "reference/api/pandas.tseries.offsets.tick.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.apply_index", "path": "reference/api/pandas.tseries.offsets.tick.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.base", "path": "reference/api/pandas.tseries.offsets.tick.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.Tick.copy", "path": "reference/api/pandas.tseries.offsets.tick.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.delta", "path": "reference/api/pandas.tseries.offsets.tick.delta", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.freqstr", "path": "reference/api/pandas.tseries.offsets.tick.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.is_anchored", "path": "reference/api/pandas.tseries.offsets.tick.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.is_month_end", "path": "reference/api/pandas.tseries.offsets.tick.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.is_month_start", "path": "reference/api/pandas.tseries.offsets.tick.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.is_on_offset", "path": "reference/api/pandas.tseries.offsets.tick.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.tick.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.tick.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.is_year_end", "path": "reference/api/pandas.tseries.offsets.tick.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.is_year_start", "path": "reference/api/pandas.tseries.offsets.tick.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.isAnchored", "path": "reference/api/pandas.tseries.offsets.tick.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.kwds", "path": "reference/api/pandas.tseries.offsets.tick.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.n", "path": "reference/api/pandas.tseries.offsets.tick.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.name", "path": "reference/api/pandas.tseries.offsets.tick.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.nanos", "path": "reference/api/pandas.tseries.offsets.tick.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.normalize", "path": "reference/api/pandas.tseries.offsets.tick.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.onOffset", "path": "reference/api/pandas.tseries.offsets.tick.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Tick.rollback", "path": "reference/api/pandas.tseries.offsets.tick.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Tick.rollforward", "path": "reference/api/pandas.tseries.offsets.tick.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Tick.rule_code", "path": "reference/api/pandas.tseries.offsets.tick.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week", "path": "reference/api/pandas.tseries.offsets.week", "type": "Data offsets", "text": "\nWeekly offset.\n\nAlways generate specific day of week. 0 for Monday.\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nweekday\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.Week.__call__", "path": "reference/api/pandas.tseries.offsets.week.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.Week.apply", "path": "reference/api/pandas.tseries.offsets.week.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.apply_index", "path": "reference/api/pandas.tseries.offsets.week.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.base", "path": "reference/api/pandas.tseries.offsets.week.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.Week.copy", "path": "reference/api/pandas.tseries.offsets.week.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.freqstr", "path": "reference/api/pandas.tseries.offsets.week.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.is_anchored", "path": "reference/api/pandas.tseries.offsets.week.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.is_month_end", "path": "reference/api/pandas.tseries.offsets.week.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.is_month_start", "path": "reference/api/pandas.tseries.offsets.week.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.is_on_offset", "path": "reference/api/pandas.tseries.offsets.week.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.week.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.week.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.is_year_end", "path": "reference/api/pandas.tseries.offsets.week.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.is_year_start", "path": "reference/api/pandas.tseries.offsets.week.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.isAnchored", "path": "reference/api/pandas.tseries.offsets.week.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.kwds", "path": "reference/api/pandas.tseries.offsets.week.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.n", "path": "reference/api/pandas.tseries.offsets.week.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.name", "path": "reference/api/pandas.tseries.offsets.week.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.nanos", "path": "reference/api/pandas.tseries.offsets.week.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.normalize", "path": "reference/api/pandas.tseries.offsets.week.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.onOffset", "path": "reference/api/pandas.tseries.offsets.week.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.rollback", "path": "reference/api/pandas.tseries.offsets.week.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Week.rollforward", "path": "reference/api/pandas.tseries.offsets.week.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.Week.rule_code", "path": "reference/api/pandas.tseries.offsets.week.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.Week.weekday", "path": "reference/api/pandas.tseries.offsets.week.weekday", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth", "path": "reference/api/pandas.tseries.offsets.weekofmonth", "type": "Data offsets", "text": "\nDescribes monthly dates like \u201cthe Tuesday of the 2nd week of each month\u201d.\n\nA specific integer for the week of the month. e.g. 0 is 1st week of month, 1\nis the 2nd week, etc.\n\nA specific integer for the day of the week.\n\n0 is Monday\n\n1 is Tuesday\n\n2 is Wednesday\n\n3 is Thursday\n\n4 is Friday\n\n5 is Saturday\n\n6 is Sunday.\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nweek\n\nweekday\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.__call__", "path": "reference/api/pandas.tseries.offsets.weekofmonth.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.apply", "path": "reference/api/pandas.tseries.offsets.weekofmonth.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.apply_index", "path": "reference/api/pandas.tseries.offsets.weekofmonth.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.base", "path": "reference/api/pandas.tseries.offsets.weekofmonth.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.copy", "path": "reference/api/pandas.tseries.offsets.weekofmonth.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.freqstr", "path": "reference/api/pandas.tseries.offsets.weekofmonth.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.is_anchored", "path": "reference/api/pandas.tseries.offsets.weekofmonth.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.is_month_end", "path": "reference/api/pandas.tseries.offsets.weekofmonth.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.is_month_start", "path": "reference/api/pandas.tseries.offsets.weekofmonth.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.is_on_offset", "path": "reference/api/pandas.tseries.offsets.weekofmonth.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.weekofmonth.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.weekofmonth.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.is_year_end", "path": "reference/api/pandas.tseries.offsets.weekofmonth.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.is_year_start", "path": "reference/api/pandas.tseries.offsets.weekofmonth.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.isAnchored", "path": "reference/api/pandas.tseries.offsets.weekofmonth.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.kwds", "path": "reference/api/pandas.tseries.offsets.weekofmonth.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.n", "path": "reference/api/pandas.tseries.offsets.weekofmonth.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.name", "path": "reference/api/pandas.tseries.offsets.weekofmonth.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.nanos", "path": "reference/api/pandas.tseries.offsets.weekofmonth.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.normalize", "path": "reference/api/pandas.tseries.offsets.weekofmonth.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.onOffset", "path": "reference/api/pandas.tseries.offsets.weekofmonth.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.rollback", "path": "reference/api/pandas.tseries.offsets.weekofmonth.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.rollforward", "path": "reference/api/pandas.tseries.offsets.weekofmonth.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.rule_code", "path": "reference/api/pandas.tseries.offsets.weekofmonth.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.week", "path": "reference/api/pandas.tseries.offsets.weekofmonth.week", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.WeekOfMonth.weekday", "path": "reference/api/pandas.tseries.offsets.weekofmonth.weekday", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin", "path": "reference/api/pandas.tseries.offsets.yearbegin", "type": "Data offsets", "text": "\nDateOffset increments between calendar year begin dates.\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nmonth\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.__call__", "path": "reference/api/pandas.tseries.offsets.yearbegin.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.apply", "path": "reference/api/pandas.tseries.offsets.yearbegin.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.apply_index", "path": "reference/api/pandas.tseries.offsets.yearbegin.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.base", "path": "reference/api/pandas.tseries.offsets.yearbegin.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.copy", "path": "reference/api/pandas.tseries.offsets.yearbegin.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.freqstr", "path": "reference/api/pandas.tseries.offsets.yearbegin.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.is_anchored", "path": "reference/api/pandas.tseries.offsets.yearbegin.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.is_month_end", "path": "reference/api/pandas.tseries.offsets.yearbegin.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.is_month_start", "path": "reference/api/pandas.tseries.offsets.yearbegin.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.is_on_offset", "path": "reference/api/pandas.tseries.offsets.yearbegin.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.yearbegin.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.yearbegin.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.is_year_end", "path": "reference/api/pandas.tseries.offsets.yearbegin.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.is_year_start", "path": "reference/api/pandas.tseries.offsets.yearbegin.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.isAnchored", "path": "reference/api/pandas.tseries.offsets.yearbegin.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.kwds", "path": "reference/api/pandas.tseries.offsets.yearbegin.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.month", "path": "reference/api/pandas.tseries.offsets.yearbegin.month", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.n", "path": "reference/api/pandas.tseries.offsets.yearbegin.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.name", "path": "reference/api/pandas.tseries.offsets.yearbegin.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.nanos", "path": "reference/api/pandas.tseries.offsets.yearbegin.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.normalize", "path": "reference/api/pandas.tseries.offsets.yearbegin.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.onOffset", "path": "reference/api/pandas.tseries.offsets.yearbegin.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.rollback", "path": "reference/api/pandas.tseries.offsets.yearbegin.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.rollforward", "path": "reference/api/pandas.tseries.offsets.yearbegin.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.YearBegin.rule_code", "path": "reference/api/pandas.tseries.offsets.yearbegin.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd", "path": "reference/api/pandas.tseries.offsets.yearend", "type": "Data offsets", "text": "\nDateOffset increments between calendar year ends.\n\nAttributes\n\n`base`\n\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\nfreqstr\n\nkwds\n\nmonth\n\nn\n\nname\n\nnanos\n\nnormalize\n\nrule_code\n\nMethods\n\n`__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`rollback`\n\nRoll provided date backward to next offset only if not on offset.\n\n`rollforward`\n\nRoll provided date forward to next offset only if not on offset.\n\napply\n\napply_index\n\ncopy\n\nisAnchored\n\nis_anchored\n\nis_month_end\n\nis_month_start\n\nis_on_offset\n\nis_quarter_end\n\nis_quarter_start\n\nis_year_end\n\nis_year_start\n\nonOffset\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.__call__", "path": "reference/api/pandas.tseries.offsets.yearend.__call__", "type": "Data offsets", "text": "\nCall self as a function.\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.apply", "path": "reference/api/pandas.tseries.offsets.yearend.apply", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.apply_index", "path": "reference/api/pandas.tseries.offsets.yearend.apply_index", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.base", "path": "reference/api/pandas.tseries.offsets.yearend.base", "type": "Data offsets", "text": "\nReturns a copy of the calling offset object with n=1 and all other attributes\nequal.\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.copy", "path": "reference/api/pandas.tseries.offsets.yearend.copy", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.freqstr", "path": "reference/api/pandas.tseries.offsets.yearend.freqstr", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.is_anchored", "path": "reference/api/pandas.tseries.offsets.yearend.is_anchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.is_month_end", "path": "reference/api/pandas.tseries.offsets.yearend.is_month_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.is_month_start", "path": "reference/api/pandas.tseries.offsets.yearend.is_month_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.is_on_offset", "path": "reference/api/pandas.tseries.offsets.yearend.is_on_offset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.yearend.is_quarter_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.yearend.is_quarter_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.is_year_end", "path": "reference/api/pandas.tseries.offsets.yearend.is_year_end", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.is_year_start", "path": "reference/api/pandas.tseries.offsets.yearend.is_year_start", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.isAnchored", "path": "reference/api/pandas.tseries.offsets.yearend.isanchored", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.kwds", "path": "reference/api/pandas.tseries.offsets.yearend.kwds", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.month", "path": "reference/api/pandas.tseries.offsets.yearend.month", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.n", "path": "reference/api/pandas.tseries.offsets.yearend.n", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.name", "path": "reference/api/pandas.tseries.offsets.yearend.name", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.nanos", "path": "reference/api/pandas.tseries.offsets.yearend.nanos", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.normalize", "path": "reference/api/pandas.tseries.offsets.yearend.normalize", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.onOffset", "path": "reference/api/pandas.tseries.offsets.yearend.onoffset", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.rollback", "path": "reference/api/pandas.tseries.offsets.yearend.rollback", "type": "Data offsets", "text": "\nRoll provided date backward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.rollforward", "path": "reference/api/pandas.tseries.offsets.yearend.rollforward", "type": "Data offsets", "text": "\nRoll provided date forward to next offset only if not on offset.\n\nRolled timestamp if not on offset, otherwise unchanged timestamp.\n\n"}, {"name": "pandas.tseries.offsets.YearEnd.rule_code", "path": "reference/api/pandas.tseries.offsets.yearend.rule_code", "type": "Data offsets", "text": "\n\n"}, {"name": "pandas.UInt16Dtype", "path": "reference/api/pandas.uint16dtype", "type": "Pandas arrays", "text": "\nAn ExtensionDtype for uint16 integer data.\n\nChanged in version 1.0.0: Now uses `pandas.NA` as its missing value, rather\nthan `numpy.nan`.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.UInt32Dtype", "path": "reference/api/pandas.uint32dtype", "type": "Pandas arrays", "text": "\nAn ExtensionDtype for uint32 integer data.\n\nChanged in version 1.0.0: Now uses `pandas.NA` as its missing value, rather\nthan `numpy.nan`.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.UInt64Dtype", "path": "reference/api/pandas.uint64dtype", "type": "Pandas arrays", "text": "\nAn ExtensionDtype for uint64 integer data.\n\nChanged in version 1.0.0: Now uses `pandas.NA` as its missing value, rather\nthan `numpy.nan`.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.UInt64Index", "path": "reference/api/pandas.uint64index", "type": "Index Objects", "text": "\nImmutable sequence used for indexing and alignment. The basic object storing\naxis labels for all pandas objects. UInt64Index is a special case of Index\nwith purely unsigned integer labels. .\n\nDeprecated since version 1.4.0: In pandas v2.0 UInt64Index will be removed and\n`NumericIndex` used instead. UInt64Index will remain fully functional for the\nduration of pandas 1.x.\n\nMake a copy of input ndarray.\n\nName to be stored in the index.\n\nSee also\n\nThe base pandas Index type.\n\nIndex of numpy int/uint/float data.\n\nNotes\n\nAn Index instance can only contain hashable objects.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.UInt8Dtype", "path": "reference/api/pandas.uint8dtype", "type": "Pandas arrays", "text": "\nAn ExtensionDtype for uint8 integer data.\n\nChanged in version 1.0.0: Now uses `pandas.NA` as its missing value, rather\nthan `numpy.nan`.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.unique", "path": "reference/api/pandas.unique", "type": "General functions", "text": "\nReturn unique values based on a hash table.\n\nUniques are returned in order of appearance. This does NOT sort.\n\nSignificantly faster than numpy.unique for long enough sequences. Includes NA\nvalues.\n\nThe return can be:\n\nIndex : when the input is an Index\n\nCategorical : when the input is a Categorical dtype\n\nndarray : when the input is a Series/ndarray\n\nReturn numpy.ndarray or ExtensionArray.\n\nSee also\n\nReturn unique values from an Index.\n\nReturn unique values of Series object.\n\nExamples\n\nAn unordered Categorical will return categories in the order of appearance.\n\nAn ordered Categorical preserves the category ordering.\n\nAn array of tuples\n\n"}, {"name": "pandas.util.hash_array", "path": "reference/api/pandas.util.hash_array", "type": "Pandas arrays", "text": "\nGiven a 1d array, return an array of deterministic integers.\n\nEncoding for data & key when strings.\n\nHash_key for string key to encode.\n\nWhether to first categorize object arrays before hashing. This is more\nefficient when the array contains duplicate values.\n\nHashed values, same length as the vals.\n\n"}, {"name": "pandas.util.hash_pandas_object", "path": "reference/api/pandas.util.hash_pandas_object", "type": "General functions", "text": "\nReturn a data hash of the Index/Series/DataFrame.\n\nInclude the index in the hash (if Series/DataFrame).\n\nEncoding for data & key when strings.\n\nHash_key for string key to encode.\n\nWhether to first categorize object arrays before hashing. This is more\nefficient when the array contains duplicate values.\n\n"}, {"name": "pandas.wide_to_long", "path": "reference/api/pandas.wide_to_long", "type": "General functions", "text": "\nUnpivot a DataFrame from wide to long format.\n\nLess flexible but more user-friendly than melt.\n\nWith stubnames [\u2018A\u2019, \u2018B\u2019], this function expects to find one or more group of\ncolumns with format A-suffix1, A-suffix2,\u2026, B-suffix1, B-suffix2,\u2026 You specify\nwhat you want to call this suffix in the resulting long format with j (for\nexample j=\u2019year\u2019)\n\nEach row of these wide variables are assumed to be uniquely identified by i\n(can be a single column name or a list of column names)\n\nAll remaining variables in the data frame are left intact.\n\nThe wide-format DataFrame.\n\nThe stub name(s). The wide format variables are assumed to start with the stub\nnames.\n\nColumn(s) to use as id variable(s).\n\nThe name of the sub-observation variable. What you wish to name your suffix in\nthe long format.\n\nA character indicating the separation of the variable names in the wide\nformat, to be stripped from the names in the long format. For example, if your\ncolumn names are A-suffix1, A-suffix2, you can strip the hyphen by specifying\nsep=\u2019-\u2019.\n\nA regular expression capturing the wanted suffixes. \u2018\\d+\u2019 captures numeric\nsuffixes. Suffixes with no numbers could be specified with the negated\ncharacter class \u2018\\D+\u2019. You can also further disambiguate suffixes, for\nexample, if your wide variables are of the form A-one, B-two,.., and you have\nan unrelated column A-rating, you can ignore the last one by specifying\nsuffix=\u2019(!?one|two)\u2019. When all suffixes are numeric, they are cast to\nint64/float64.\n\nA DataFrame that contains each stub name as a variable, with new index (i, j).\n\nSee also\n\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers\nset.\n\nCreate a spreadsheet-style pivot table as a DataFrame.\n\nPivot without aggregation that can handle non-numeric data.\n\nGeneralization of pivot that can handle duplicate values for one index/column\npair.\n\nPivot based on the index values instead of a column.\n\nNotes\n\nAll extra variables are left untouched. This simply uses pandas.melt under the\nhood, but is hard-coded to \u201cdo the right thing\u201d in a typical case.\n\nExamples\n\nWith multiple id columns\n\nGoing from long back to wide just takes some creative use of unstack\n\nLess wieldy column names are also handled\n\nIf we have many columns, we could also use a regex to find our stubnames and\npass that list on to wide_to_long\n\nAll of the above examples have integers as suffixes. It is possible to have\nnon-integers as suffixes.\n\n"}, {"name": "Plotting", "path": "reference/plotting", "type": "Plotting", "text": "\nThe following functions are contained in the `pandas.plotting` module.\n\n`andrews_curves`(frame, class_column[, ax, ...])\n\nGenerate a matplotlib plot of Andrews curves, for visualising clusters of\nmultivariate data.\n\n`autocorrelation_plot`(series[, ax])\n\nAutocorrelation plot for time series.\n\n`bootstrap_plot`(series[, fig, size, samples])\n\nBootstrap plot on mean, median and mid-range statistics.\n\n`boxplot`(data[, column, by, ax, fontsize, ...])\n\nMake a box plot from DataFrame columns.\n\n`deregister_matplotlib_converters`()\n\nRemove pandas formatters and converters.\n\n`lag_plot`(series[, lag, ax])\n\nLag plot for time series.\n\n`parallel_coordinates`(frame, class_column[, ...])\n\nParallel coordinates plotting.\n\n`plot_params`\n\nStores pandas plotting options.\n\n`radviz`(frame, class_column[, ax, color, ...])\n\nPlot a multidimensional dataset in 2D.\n\n`register_matplotlib_converters`()\n\nRegister pandas formatters and converters with matplotlib.\n\n`scatter_matrix`(frame[, alpha, figsize, ax, ...])\n\nDraw a matrix of scatter plots.\n\n`table`(ax, data[, rowLabels, colLabels])\n\nHelper function to convert DataFrame and Series to matplotlib.table.\n\n"}, {"name": "Resampling", "path": "reference/resampling", "type": "General functions", "text": "\nResampler objects are returned by resample calls:\n`pandas.DataFrame.resample()`, `pandas.Series.resample()`.\n\n`Resampler.__iter__`()\n\nGroupby iterator.\n\n`Resampler.groups`\n\nDict {group name -> group labels}.\n\n`Resampler.indices`\n\nDict {group name -> group indices}.\n\n`Resampler.get_group`(name[, obj])\n\nConstruct DataFrame from group with provided name.\n\n`Resampler.apply`([func])\n\nAggregate using one or more operations over the specified axis.\n\n`Resampler.aggregate`([func])\n\nAggregate using one or more operations over the specified axis.\n\n`Resampler.transform`(arg, *args, **kwargs)\n\nCall function producing a like-indexed Series on each group and return a\nSeries with the transformed values.\n\n`Resampler.pipe`(func, *args, **kwargs)\n\nApply a function func with arguments to this Resampler object and return the\nfunction's result.\n\n`Resampler.ffill`([limit])\n\nForward fill the values.\n\n`Resampler.backfill`([limit])\n\nBackward fill the new missing values in the resampled data.\n\n`Resampler.bfill`([limit])\n\nBackward fill the new missing values in the resampled data.\n\n`Resampler.pad`([limit])\n\nForward fill the values.\n\n`Resampler.nearest`([limit])\n\nResample by using the nearest value.\n\n`Resampler.fillna`(method[, limit])\n\nFill missing values introduced by upsampling.\n\n`Resampler.asfreq`([fill_value])\n\nReturn the values at the new freq, essentially a reindex.\n\n`Resampler.interpolate`([method, axis, limit, ...])\n\nInterpolate values according to different methods.\n\n`Resampler.count`()\n\nCompute count of group, excluding missing values.\n\n`Resampler.nunique`([_method])\n\nReturn number of unique elements in the group.\n\n`Resampler.first`([_method, min_count])\n\nCompute first of group values.\n\n`Resampler.last`([_method, min_count])\n\nCompute last of group values.\n\n`Resampler.max`([_method, min_count])\n\nCompute max of group values.\n\n`Resampler.mean`([_method])\n\nCompute mean of groups, excluding missing values.\n\n`Resampler.median`([_method])\n\nCompute median of groups, excluding missing values.\n\n`Resampler.min`([_method, min_count])\n\nCompute min of group values.\n\n`Resampler.ohlc`([_method])\n\nCompute open, high, low and close values of a group, excluding missing values.\n\n`Resampler.prod`([_method, min_count])\n\nCompute prod of group values.\n\n`Resampler.size`()\n\nCompute group sizes.\n\n`Resampler.sem`([_method])\n\nCompute standard error of the mean of groups, excluding missing values.\n\n`Resampler.std`([ddof])\n\nCompute standard deviation of groups, excluding missing values.\n\n`Resampler.sum`([_method, min_count])\n\nCompute sum of group values.\n\n`Resampler.var`([ddof])\n\nCompute variance of groups, excluding missing values.\n\n`Resampler.quantile`([q])\n\nReturn value at the given quantile.\n\n"}, {"name": "Reshaping and pivot tables", "path": "user_guide/reshaping", "type": "Manual", "text": "\nData is often stored in so-called \u201cstacked\u201d or \u201crecord\u201d format:\n\nFor the curious here is how the above `DataFrame` was created:\n\nTo select out everything for variable `A` we could do:\n\nBut suppose we wish to do time series operations with the variables. A better\nrepresentation would be where the `columns` are the unique variables and an\n`index` of dates identifies individual observations. To reshape the data into\nthis form, we use the `DataFrame.pivot()` method (also implemented as a top\nlevel function `pivot()`):\n\nIf the `values` argument is omitted, and the input `DataFrame` has more than\none column of values which are not used as column or index inputs to `pivot`,\nthen the resulting \u201cpivoted\u201d `DataFrame` will have hierarchical columns whose\ntopmost level indicates the respective value column:\n\nYou can then select subsets from the pivoted `DataFrame`:\n\nNote that this returns a view on the underlying data in the case where the\ndata are homogeneously-typed.\n\nNote\n\n`pivot()` will error with a `ValueError: Index contains duplicate entries,\ncannot reshape` if the index/column pair is not unique. In this case, consider\nusing `pivot_table()` which is a generalization of pivot that can handle\nduplicate values for one index/column pair.\n\nClosely related to the `pivot()` method are the related `stack()` and\n`unstack()` methods available on `Series` and `DataFrame`. These methods are\ndesigned to work together with `MultiIndex` objects (see the section on\nhierarchical indexing). Here are essentially what these methods do:\n\n`stack`: \u201cpivot\u201d a level of the (possibly hierarchical) column labels,\nreturning a `DataFrame` with an index with a new inner-most level of row\nlabels.\n\n`unstack`: (inverse operation of `stack`) \u201cpivot\u201d a level of the (possibly\nhierarchical) row index to the column axis, producing a reshaped `DataFrame`\nwith a new inner-most level of column labels.\n\nThe clearest way to explain is by example. Let\u2019s take a prior example data set\nfrom the hierarchical indexing section:\n\nThe `stack` function \u201ccompresses\u201d a level in the `DataFrame`\u2019s columns to\nproduce either:\n\nA `Series`, in the case of a simple column Index.\n\nA `DataFrame`, in the case of a `MultiIndex` in the columns.\n\nIf the columns have a `MultiIndex`, you can choose which level to stack. The\nstacked level becomes the new lowest level in a `MultiIndex` on the columns:\n\nWith a \u201cstacked\u201d `DataFrame` or `Series` (having a `MultiIndex` as the\n`index`), the inverse operation of `stack` is `unstack`, which by default\nunstacks the last level:\n\nIf the indexes have names, you can use the level names instead of specifying\nthe level numbers:\n\nNotice that the `stack` and `unstack` methods implicitly sort the index levels\ninvolved. Hence a call to `stack` and then `unstack`, or vice versa, will\nresult in a sorted copy of the original `DataFrame` or `Series`:\n\nThe above code will raise a `TypeError` if the call to `sort_index` is\nremoved.\n\nYou may also stack or unstack more than one level at a time by passing a list\nof levels, in which case the end result is as if each level in the list were\nprocessed individually.\n\nThe list of levels can contain either level names or level numbers (but not a\nmixture of the two).\n\nThese functions are intelligent about handling missing data and do not expect\neach subgroup within the hierarchical index to have the same set of labels.\nThey also can handle the index being unsorted (but you can make it sorted by\ncalling `sort_index`, of course). Here is a more complex example:\n\nAs mentioned above, `stack` can be called with a `level` argument to select\nwhich level in the columns to stack:\n\nUnstacking can result in missing values if subgroups do not have the same set\nof labels. By default, missing values will be replaced with the default fill\nvalue for that data type, `NaN` for float, `NaT` for datetimelike, etc. For\ninteger types, by default data will converted to float and missing values will\nbe set to `NaN`.\n\nAlternatively, unstack takes an optional `fill_value` argument, for specifying\nthe value of missing data.\n\nUnstacking when the columns are a `MultiIndex` is also careful about doing the\nright thing:\n\nThe top-level `melt()` function and the corresponding `DataFrame.melt()` are\nuseful to massage a `DataFrame` into a format where one or more columns are\nidentifier variables, while all other columns, considered measured variables,\nare \u201cunpivoted\u201d to the row axis, leaving just two non-identifier columns,\n\u201cvariable\u201d and \u201cvalue\u201d. The names of those columns can be customized by\nsupplying the `var_name` and `value_name` parameters.\n\nFor instance,\n\nWhen transforming a DataFrame using `melt()`, the index will be ignored. The\noriginal index values can be kept around by setting the `ignore_index`\nparameter to `False` (default is `True`). This will however duplicate them.\n\nNew in version 1.1.0.\n\nAnother way to transform is to use the `wide_to_long()` panel data convenience\nfunction. It is less flexible than `melt()`, but more user-friendly.\n\nIt should be no shock that combining `pivot` / `stack` / `unstack` with\nGroupBy and the basic Series and DataFrame statistical functions can produce\nsome very expressive and fast data manipulations.\n\nWhile `pivot()` provides general purpose pivoting with various data types\n(strings, numerics, etc.), pandas also provides `pivot_table()` for pivoting\nwith aggregation of numeric data.\n\nThe function `pivot_table()` can be used to create spreadsheet-style pivot\ntables. See the cookbook for some advanced strategies.\n\nIt takes a number of arguments:\n\n`data`: a DataFrame object.\n\n`values`: a column or a list of columns to aggregate.\n\n`index`: a column, Grouper, array which has the same length as data, or list\nof them. Keys to group by on the pivot table index. If an array is passed, it\nis being used as the same manner as column values.\n\n`columns`: a column, Grouper, array which has the same length as data, or list\nof them. Keys to group by on the pivot table column. If an array is passed, it\nis being used as the same manner as column values.\n\n`aggfunc`: function to use for aggregation, defaulting to `numpy.mean`.\n\nConsider a data set like this:\n\nWe can produce pivot tables from this data very easily:\n\nThe result object is a `DataFrame` having potentially hierarchical indexes on\nthe rows and columns. If the `values` column name is not given, the pivot\ntable will include all of the data that can be aggregated in an additional\nlevel of hierarchy in the columns:\n\nAlso, you can use `Grouper` for `index` and `columns` keywords. For detail of\n`Grouper`, see Grouping with a Grouper specification.\n\nYou can render a nice output of the table omitting the missing values by\ncalling `to_string` if you wish:\n\ni.e. `DataFrame.pivot_table()`.\n\nIf you pass `margins=True` to `pivot_table`, special `All` columns and rows\nwill be added with partial group aggregates across the categories on the rows\nand columns:\n\nAdditionally, you can call `DataFrame.stack()` to display a pivoted DataFrame\nas having a multi-level index:\n\nUse `crosstab()` to compute a cross-tabulation of two (or more) factors. By\ndefault `crosstab` computes a frequency table of the factors unless an array\nof values and an aggregation function are passed.\n\nIt takes a number of arguments\n\n`index`: array-like, values to group by in the rows.\n\n`columns`: array-like, values to group by in the columns.\n\n`values`: array-like, optional, array of values to aggregate according to the\nfactors.\n\n`aggfunc`: function, optional, If no values array is passed, computes a\nfrequency table.\n\n`rownames`: sequence, default `None`, must match number of row arrays passed.\n\n`colnames`: sequence, default `None`, if passed, must match number of column\narrays passed.\n\n`margins`: boolean, default `False`, Add row/column margins (subtotals)\n\n`normalize`: boolean, {\u2018all\u2019, \u2018index\u2019, \u2018columns\u2019}, or {0,1}, default `False`.\nNormalize by dividing all values by the sum of values.\n\nAny `Series` passed will have their name attributes used unless row or column\nnames for the cross-tabulation are specified\n\nFor example:\n\nIf `crosstab` receives only two Series, it will provide a frequency table.\n\n`crosstab` can also be implemented to `Categorical` data.\n\nIf you want to include all of data categories even if the actual data does not\ncontain any instances of a particular category, you should set `dropna=False`.\n\nFor example:\n\nFrequency tables can also be normalized to show percentages rather than counts\nusing the `normalize` argument:\n\n`normalize` can also normalize values within each row or within each column:\n\n`crosstab` can also be passed a third `Series` and an aggregation function\n(`aggfunc`) that will be applied to the values of the third `Series` within\neach group defined by the first two `Series`:\n\nFinally, one can also add margins or normalize this output.\n\nThe `cut()` function computes groupings for the values of the input array and\nis often used to transform continuous variables to discrete or categorical\nvariables:\n\nIf the `bins` keyword is an integer, then equal-width bins are formed.\nAlternatively we can specify custom bin-edges:\n\nIf the `bins` keyword is an `IntervalIndex`, then these will be used to bin\nthe passed data.:\n\nTo convert a categorical variable into a \u201cdummy\u201d or \u201cindicator\u201d `DataFrame`,\nfor example a column in a `DataFrame` (a `Series`) which has `k` distinct\nvalues, can derive a `DataFrame` containing `k` columns of 1s and 0s using\n`get_dummies()`:\n\nSometimes it\u2019s useful to prefix the column names, for example when merging the\nresult with the original `DataFrame`:\n\nThis function is often used along with discretization functions like `cut`:\n\nSee also `Series.str.get_dummies`.\n\n`get_dummies()` also accepts a `DataFrame`. By default all categorical\nvariables (categorical in the statistical sense, those with `object` or\n`categorical` dtype) are encoded as dummy variables.\n\nAll non-object columns are included untouched in the output. You can control\nthe columns that are encoded with the `columns` keyword.\n\nNotice that the `B` column is still included in the output, it just hasn\u2019t\nbeen encoded. You can drop `B` before calling `get_dummies` if you don\u2019t want\nto include it in the output.\n\nAs with the `Series` version, you can pass values for the `prefix` and\n`prefix_sep`. By default the column name is used as the prefix, and \u2018_\u2019 as the\nprefix separator. You can specify `prefix` and `prefix_sep` in 3 ways:\n\nstring: Use the same value for `prefix` or `prefix_sep` for each column to be\nencoded.\n\nlist: Must be the same length as the number of columns being encoded.\n\ndict: Mapping column name to prefix.\n\nSometimes it will be useful to only keep k-1 levels of a categorical variable\nto avoid collinearity when feeding the result to statistical models. You can\nswitch to this mode by turn on `drop_first`.\n\nWhen a column contains only one level, it will be omitted in the result.\n\nBy default new columns will have `np.uint8` dtype. To choose another dtype,\nuse the `dtype` argument:\n\nTo encode 1-d values as an enumerated type use `factorize()`:\n\nNote that `factorize` is similar to `numpy.unique`, but differs in its\nhandling of NaN:\n\nNote\n\nThe following `numpy.unique` will fail under Python 3 with a `TypeError`\nbecause of an ordering bug. See also here.\n\nNote\n\nIf you just want to handle one column as a categorical variable (like R\u2019s\nfactor), you can use `df[\"cat_col\"] = pd.Categorical(df[\"col\"])` or\n`df[\"cat_col\"] = df[\"col\"].astype(\"category\")`. For full docs on\n`Categorical`, see the Categorical introduction and the API documentation.\n\nIn this section, we will review frequently asked questions and examples. The\ncolumn names and relevant column values are named to correspond with how this\nDataFrame will be pivoted in the answers below.\n\nSuppose we wanted to pivot `df` such that the `col` values are columns, `row`\nvalues are the index, and the mean of `val0` are the values? In particular,\nthe resulting DataFrame should look like:\n\nThis solution uses `pivot_table()`. Also note that `aggfunc='mean'` is the\ndefault. It is included here to be explicit.\n\nNote that we can also replace the missing values by using the `fill_value`\nparameter.\n\nAlso note that we can pass in other aggregation functions as well. For\nexample, we can also pass in `sum`.\n\nAnother aggregation we can do is calculate the frequency in which the columns\nand rows occur together a.k.a. \u201ccross tabulation\u201d. To do this, we can pass\n`size` to the `aggfunc` parameter.\n\nWe can also perform multiple aggregations. For example, to perform both a\n`sum` and `mean`, we can pass in a list to the `aggfunc` argument.\n\nNote to aggregate over multiple value columns, we can pass in a list to the\n`values` parameter.\n\nNote to subdivide over multiple columns we can pass in a list to the `columns`\nparameter.\n\nNew in version 0.25.0.\n\nSometimes the values in a column are list-like.\n\nWe can \u2018explode\u2019 the `values` column, transforming each list-like to a\nseparate row, by using `explode()`. This will replicate the index values from\nthe original row:\n\nYou can also explode the column in the `DataFrame`.\n\n`Series.explode()` will replace empty lists with `np.nan` and preserve scalar\nentries. The dtype of the resulting `Series` is always `object`.\n\nHere is a typical usecase. You have comma separated strings in a column and\nwant to expand this.\n\nCreating a long form DataFrame is now straightforward using explode and\nchained operations\n\n"}, {"name": "Scaling to large datasets", "path": "user_guide/scale", "type": "Manual", "text": "\npandas provides data structures for in-memory analytics, which makes using\npandas to analyze datasets that are larger than memory datasets somewhat\ntricky. Even datasets that are a sizable fraction of memory become unwieldy,\nas some pandas operations need to make intermediate copies.\n\nThis document provides a few recommendations for scaling your analysis to\nlarger datasets. It\u2019s a complement to Enhancing performance, which focuses on\nspeeding up analysis for datasets that fit in memory.\n\nBut first, it\u2019s worth considering not using pandas. pandas isn\u2019t the right\ntool for all situations. If you\u2019re working with very large datasets and a tool\nlike PostgreSQL fits your needs, then you should probably be using that.\nAssuming you want or need the expressiveness and power of pandas, let\u2019s carry\non.\n\nSuppose our raw dataset on disk has many columns:\n\nTo load the columns we want, we have two options. Option 1 loads in all the\ndata and then filters to what we need.\n\nOption 2 only loads the columns we request.\n\nIf we were to measure the memory usage of the two calls, we\u2019d see that\nspecifying `columns` uses about 1/10th the memory in this case.\n\nWith `pandas.read_csv()`, you can specify `usecols` to limit the columns read\ninto memory. Not all file formats that can be read by pandas provide an option\nto read a subset of columns.\n\nThe default pandas data types are not the most memory efficient. This is\nespecially true for text data columns with relatively few unique values\n(commonly referred to as \u201clow-cardinality\u201d data). By using more efficient data\ntypes, you can store larger datasets in memory.\n\nNow, let\u2019s inspect the data types and memory usage to see where we should\nfocus our attention.\n\nThe `name` column is taking up much more memory than any other. It has just a\nfew unique values, so it\u2019s a good candidate for converting to a `Categorical`.\nWith a Categorical, we store each unique name once and use space-efficient\nintegers to know which specific name is used in each row.\n\nWe can go a bit further and downcast the numeric columns to their smallest\ntypes using `pandas.to_numeric()`.\n\nIn all, we\u2019ve reduced the in-memory footprint of this dataset to 1/5 of its\noriginal size.\n\nSee Categorical data for more on `Categorical` and dtypes for an overview of\nall of pandas\u2019 dtypes.\n\nSome workloads can be achieved with chunking: splitting a large problem like\n\u201cconvert this directory of CSVs to parquet\u201d into a bunch of small problems\n(\u201cconvert this individual CSV file into a Parquet file. Now repeat that for\neach file in this directory.\u201d). As long as each chunk fits in memory, you can\nwork with datasets that are much larger than memory.\n\nNote\n\nChunking works well when the operation you\u2019re performing requires zero or\nminimal coordination between chunks. For more complicated workflows, you\u2019re\nbetter off using another library.\n\nSuppose we have an even larger \u201clogical dataset\u201d on disk that\u2019s a directory of\nparquet files. Each file in the directory represents a different year of the\nentire dataset.\n\nNow we\u2019ll implement an out-of-core `value_counts`. The peak memory usage of\nthis workflow is the single largest chunk, plus a small series storing the\nunique value counts up to this point. As long as each individual file fits in\nmemory, this will work for arbitrary-sized datasets.\n\nSome readers, like `pandas.read_csv()`, offer parameters to control the\n`chunksize` when reading a single file.\n\nManually chunking is an OK option for workflows that don\u2019t require too\nsophisticated of operations. Some operations, like `groupby`, are much harder\nto do chunkwise. In these cases, you may be better switching to a different\nlibrary that implements these out-of-core algorithms for you.\n\npandas is just one library offering a DataFrame API. Because of its\npopularity, pandas\u2019 API has become something of a standard that other\nlibraries implement. The pandas documentation maintains a list of libraries\nimplementing a DataFrame API in our ecosystem page.\n\nFor example, Dask, a parallel computing library, has dask.dataframe, a pandas-\nlike API for working with larger than memory datasets in parallel. Dask can\nuse multiple threads or processes on a single machine, or a cluster of\nmachines to process data in parallel.\n\nWe\u2019ll import `dask.dataframe` and notice that the API feels similar to pandas.\nWe can use Dask\u2019s `read_parquet` function, but provide a globstring of files\nto read in.\n\nInspecting the `ddf` object, we see a few things\n\nThere are familiar attributes like `.columns` and `.dtypes`\n\nThere are familiar methods like `.groupby`, `.sum`, etc.\n\nThere are new attributes like `.npartitions` and `.divisions`\n\nThe partitions and divisions are how Dask parallelizes computation. A Dask\nDataFrame is made up of many pandas DataFrames. A single method call on a Dask\nDataFrame ends up making many pandas method calls, and Dask knows how to\ncoordinate everything to get the result.\n\nOne major difference: the `dask.dataframe` API is lazy. If you look at the\nrepr above, you\u2019ll notice that the values aren\u2019t actually printed out; just\nthe column names and dtypes. That\u2019s because Dask hasn\u2019t actually read the data\nyet. Rather than executing immediately, doing operations build up a task\ngraph.\n\nEach of these calls is instant because the result isn\u2019t being computed yet.\nWe\u2019re just building up a list of computation to do when someone needs the\nresult. Dask knows that the return type of a `pandas.Series.value_counts` is a\npandas Series with a certain dtype and a certain name. So the Dask version\nreturns a Dask Series with the same dtype and the same name.\n\nTo get the actual result you can call `.compute()`.\n\nAt that point, you get back the same thing you\u2019d get with pandas, in this case\na concrete pandas Series with the count of each `name`.\n\nCalling `.compute` causes the full task graph to be executed. This includes\nreading the data, selecting the columns, and doing the `value_counts`. The\nexecution is done in parallel where possible, and Dask tries to keep the\noverall memory footprint small. You can work with datasets that are much\nlarger than memory, as long as each partition (a regular pandas DataFrame)\nfits in memory.\n\nBy default, `dask.dataframe` operations use a threadpool to do operations in\nparallel. We can also connect to a cluster to distribute the work on many\nmachines. In this case we\u2019ll connect to a local \u201ccluster\u201d made up of several\nprocesses on this single machine.\n\nOnce this `client` is created, all of Dask\u2019s computation will take place on\nthe cluster (which is just processes in this case).\n\nDask implements the most used parts of the pandas API. For example, we can do\na familiar groupby aggregation.\n\nThe grouping and aggregation is done out-of-core and in parallel.\n\nWhen Dask knows the `divisions` of a dataset, certain optimizations are\npossible. When reading parquet datasets written by dask, the divisions will be\nknown automatically. In this case, since we created the parquet files\nmanually, we need to supply the divisions manually.\n\nNow we can do things like fast random access with `.loc`.\n\nDask knows to just look in the 3rd partition for selecting values in 2002. It\ndoesn\u2019t need to look at any other data.\n\nMany workflows involve a large amount of data and processing it in a way that\nreduces the size to something that fits in memory. In this case, we\u2019ll\nresample to daily frequency and take the mean. Once we\u2019ve taken the mean, we\nknow the results will fit in memory, so we can safely call `compute` without\nrunning out of memory. At that point it\u2019s just a regular pandas object.\n\nThese Dask examples have all be done using multiple processes on a single\nmachine. Dask can be deployed on a cluster to scale up to even larger\ndatasets.\n\nYou see more dask examples at https://examples.dask.org.\n\n"}, {"name": "Series", "path": "reference/series", "type": "General functions", "text": "\n`Series`([data, index, dtype, name, copy, ...])\n\nOne-dimensional ndarray with axis labels (including time series).\n\nAxes\n\n`Series.index`\n\nThe index (axis labels) of the Series.\n\n`Series.array`\n\nThe ExtensionArray of the data backing this Series or Index.\n\n`Series.values`\n\nReturn Series as ndarray or ndarray-like depending on the dtype.\n\n`Series.dtype`\n\nReturn the dtype object of the underlying data.\n\n`Series.shape`\n\nReturn a tuple of the shape of the underlying data.\n\n`Series.nbytes`\n\nReturn the number of bytes in the underlying data.\n\n`Series.ndim`\n\nNumber of dimensions of the underlying data, by definition 1.\n\n`Series.size`\n\nReturn the number of elements in the underlying data.\n\n`Series.T`\n\nReturn the transpose, which is by definition self.\n\n`Series.memory_usage`([index, deep])\n\nReturn the memory usage of the Series.\n\n`Series.hasnans`\n\nReturn True if there are any NaNs.\n\n`Series.empty`\n\nIndicator whether Series/DataFrame is empty.\n\n`Series.dtypes`\n\nReturn the dtype object of the underlying data.\n\n`Series.name`\n\nReturn the name of the Series.\n\n`Series.flags`\n\nGet the properties associated with this pandas object.\n\n`Series.set_flags`(*[, copy, ...])\n\nReturn a new object with updated flags.\n\n`Series.astype`(dtype[, copy, errors])\n\nCast a pandas object to a specified dtype `dtype`.\n\n`Series.convert_dtypes`([infer_objects, ...])\n\nConvert columns to best possible dtypes using dtypes supporting `pd.NA`.\n\n`Series.infer_objects`()\n\nAttempt to infer better dtypes for object columns.\n\n`Series.copy`([deep])\n\nMake a copy of this object's indices and data.\n\n`Series.bool`()\n\nReturn the bool of a single element Series or DataFrame.\n\n`Series.to_numpy`([dtype, copy, na_value])\n\nA NumPy ndarray representing the values in this Series or Index.\n\n`Series.to_period`([freq, copy])\n\nConvert Series from DatetimeIndex to PeriodIndex.\n\n`Series.to_timestamp`([freq, how, copy])\n\nCast to DatetimeIndex of Timestamps, at beginning of period.\n\n`Series.to_list`()\n\nReturn a list of the values.\n\n`Series.__array__`([dtype])\n\nReturn the values as a NumPy array.\n\n`Series.get`(key[, default])\n\nGet item from object for given key (ex: DataFrame column).\n\n`Series.at`\n\nAccess a single value for a row/column label pair.\n\n`Series.iat`\n\nAccess a single value for a row/column pair by integer position.\n\n`Series.loc`\n\nAccess a group of rows and columns by label(s) or a boolean array.\n\n`Series.iloc`\n\nPurely integer-location based indexing for selection by position.\n\n`Series.__iter__`()\n\nReturn an iterator of the values.\n\n`Series.items`()\n\nLazily iterate over (index, value) tuples.\n\n`Series.iteritems`()\n\nLazily iterate over (index, value) tuples.\n\n`Series.keys`()\n\nReturn alias for index.\n\n`Series.pop`(item)\n\nReturn item and drops from series.\n\n`Series.item`()\n\nReturn the first element of the underlying data as a Python scalar.\n\n`Series.xs`(key[, axis, level, drop_level])\n\nReturn cross-section from the Series/DataFrame.\n\nFor more information on `.at`, `.iat`, `.loc`, and `.iloc`, see the indexing\ndocumentation.\n\n`Series.add`(other[, level, fill_value, axis])\n\nReturn Addition of series and other, element-wise (binary operator add).\n\n`Series.sub`(other[, level, fill_value, axis])\n\nReturn Subtraction of series and other, element-wise (binary operator sub).\n\n`Series.mul`(other[, level, fill_value, axis])\n\nReturn Multiplication of series and other, element-wise (binary operator mul).\n\n`Series.div`(other[, level, fill_value, axis])\n\nReturn Floating division of series and other, element-wise (binary operator\ntruediv).\n\n`Series.truediv`(other[, level, fill_value, axis])\n\nReturn Floating division of series and other, element-wise (binary operator\ntruediv).\n\n`Series.floordiv`(other[, level, fill_value, axis])\n\nReturn Integer division of series and other, element-wise (binary operator\nfloordiv).\n\n`Series.mod`(other[, level, fill_value, axis])\n\nReturn Modulo of series and other, element-wise (binary operator mod).\n\n`Series.pow`(other[, level, fill_value, axis])\n\nReturn Exponential power of series and other, element-wise (binary operator\npow).\n\n`Series.radd`(other[, level, fill_value, axis])\n\nReturn Addition of series and other, element-wise (binary operator radd).\n\n`Series.rsub`(other[, level, fill_value, axis])\n\nReturn Subtraction of series and other, element-wise (binary operator rsub).\n\n`Series.rmul`(other[, level, fill_value, axis])\n\nReturn Multiplication of series and other, element-wise (binary operator\nrmul).\n\n`Series.rdiv`(other[, level, fill_value, axis])\n\nReturn Floating division of series and other, element-wise (binary operator\nrtruediv).\n\n`Series.rtruediv`(other[, level, fill_value, axis])\n\nReturn Floating division of series and other, element-wise (binary operator\nrtruediv).\n\n`Series.rfloordiv`(other[, level, fill_value, ...])\n\nReturn Integer division of series and other, element-wise (binary operator\nrfloordiv).\n\n`Series.rmod`(other[, level, fill_value, axis])\n\nReturn Modulo of series and other, element-wise (binary operator rmod).\n\n`Series.rpow`(other[, level, fill_value, axis])\n\nReturn Exponential power of series and other, element-wise (binary operator\nrpow).\n\n`Series.combine`(other, func[, fill_value])\n\nCombine the Series with a Series or scalar according to func.\n\n`Series.combine_first`(other)\n\nUpdate null elements with value in the same location in 'other'.\n\n`Series.round`([decimals])\n\nRound each value in a Series to the given number of decimals.\n\n`Series.lt`(other[, level, fill_value, axis])\n\nReturn Less than of series and other, element-wise (binary operator lt).\n\n`Series.gt`(other[, level, fill_value, axis])\n\nReturn Greater than of series and other, element-wise (binary operator gt).\n\n`Series.le`(other[, level, fill_value, axis])\n\nReturn Less than or equal to of series and other, element-wise (binary\noperator le).\n\n`Series.ge`(other[, level, fill_value, axis])\n\nReturn Greater than or equal to of series and other, element-wise (binary\noperator ge).\n\n`Series.ne`(other[, level, fill_value, axis])\n\nReturn Not equal to of series and other, element-wise (binary operator ne).\n\n`Series.eq`(other[, level, fill_value, axis])\n\nReturn Equal to of series and other, element-wise (binary operator eq).\n\n`Series.product`([axis, skipna, level, ...])\n\nReturn the product of the values over the requested axis.\n\n`Series.dot`(other)\n\nCompute the dot product between the Series and the columns of other.\n\n`Series.apply`(func[, convert_dtype, args])\n\nInvoke function on values of Series.\n\n`Series.agg`([func, axis])\n\nAggregate using one or more operations over the specified axis.\n\n`Series.aggregate`([func, axis])\n\nAggregate using one or more operations over the specified axis.\n\n`Series.transform`(func[, axis])\n\nCall `func` on self producing a Series with the same axis shape as self.\n\n`Series.map`(arg[, na_action])\n\nMap values of Series according to an input mapping or function.\n\n`Series.groupby`([by, axis, level, as_index, ...])\n\nGroup Series using a mapper or by a Series of columns.\n\n`Series.rolling`(window[, min_periods, ...])\n\nProvide rolling window calculations.\n\n`Series.expanding`([min_periods, center, ...])\n\nProvide expanding window calculations.\n\n`Series.ewm`([com, span, halflife, alpha, ...])\n\nProvide exponentially weighted (EW) calculations.\n\n`Series.pipe`(func, *args, **kwargs)\n\nApply chainable functions that expect Series or DataFrames.\n\n`Series.abs`()\n\nReturn a Series/DataFrame with absolute numeric value of each element.\n\n`Series.all`([axis, bool_only, skipna, level])\n\nReturn whether all elements are True, potentially over an axis.\n\n`Series.any`([axis, bool_only, skipna, level])\n\nReturn whether any element is True, potentially over an axis.\n\n`Series.autocorr`([lag])\n\nCompute the lag-N autocorrelation.\n\n`Series.between`(left, right[, inclusive])\n\nReturn boolean Series equivalent to left <= series <= right.\n\n`Series.clip`([lower, upper, axis, inplace])\n\nTrim values at input threshold(s).\n\n`Series.corr`(other[, method, min_periods])\n\nCompute correlation with other Series, excluding missing values.\n\n`Series.count`([level])\n\nReturn number of non-NA/null observations in the Series.\n\n`Series.cov`(other[, min_periods, ddof])\n\nCompute covariance with Series, excluding missing values.\n\n`Series.cummax`([axis, skipna])\n\nReturn cumulative maximum over a DataFrame or Series axis.\n\n`Series.cummin`([axis, skipna])\n\nReturn cumulative minimum over a DataFrame or Series axis.\n\n`Series.cumprod`([axis, skipna])\n\nReturn cumulative product over a DataFrame or Series axis.\n\n`Series.cumsum`([axis, skipna])\n\nReturn cumulative sum over a DataFrame or Series axis.\n\n`Series.describe`([percentiles, include, ...])\n\nGenerate descriptive statistics.\n\n`Series.diff`([periods])\n\nFirst discrete difference of element.\n\n`Series.factorize`([sort, na_sentinel])\n\nEncode the object as an enumerated type or categorical variable.\n\n`Series.kurt`([axis, skipna, level, numeric_only])\n\nReturn unbiased kurtosis over requested axis.\n\n`Series.mad`([axis, skipna, level])\n\nReturn the mean absolute deviation of the values over the requested axis.\n\n`Series.max`([axis, skipna, level, numeric_only])\n\nReturn the maximum of the values over the requested axis.\n\n`Series.mean`([axis, skipna, level, numeric_only])\n\nReturn the mean of the values over the requested axis.\n\n`Series.median`([axis, skipna, level, ...])\n\nReturn the median of the values over the requested axis.\n\n`Series.min`([axis, skipna, level, numeric_only])\n\nReturn the minimum of the values over the requested axis.\n\n`Series.mode`([dropna])\n\nReturn the mode(s) of the Series.\n\n`Series.nlargest`([n, keep])\n\nReturn the largest n elements.\n\n`Series.nsmallest`([n, keep])\n\nReturn the smallest n elements.\n\n`Series.pct_change`([periods, fill_method, ...])\n\nPercentage change between the current and a prior element.\n\n`Series.prod`([axis, skipna, level, ...])\n\nReturn the product of the values over the requested axis.\n\n`Series.quantile`([q, interpolation])\n\nReturn value at the given quantile.\n\n`Series.rank`([axis, method, numeric_only, ...])\n\nCompute numerical data ranks (1 through n) along axis.\n\n`Series.sem`([axis, skipna, level, ddof, ...])\n\nReturn unbiased standard error of the mean over requested axis.\n\n`Series.skew`([axis, skipna, level, numeric_only])\n\nReturn unbiased skew over requested axis.\n\n`Series.std`([axis, skipna, level, ddof, ...])\n\nReturn sample standard deviation over requested axis.\n\n`Series.sum`([axis, skipna, level, ...])\n\nReturn the sum of the values over the requested axis.\n\n`Series.var`([axis, skipna, level, ddof, ...])\n\nReturn unbiased variance over requested axis.\n\n`Series.kurtosis`([axis, skipna, level, ...])\n\nReturn unbiased kurtosis over requested axis.\n\n`Series.unique`()\n\nReturn unique values of Series object.\n\n`Series.nunique`([dropna])\n\nReturn number of unique elements in the object.\n\n`Series.is_unique`\n\nReturn boolean if values in the object are unique.\n\n`Series.is_monotonic`\n\nReturn boolean if values in the object are monotonic_increasing.\n\n`Series.is_monotonic_increasing`\n\nAlias for is_monotonic.\n\n`Series.is_monotonic_decreasing`\n\nReturn boolean if values in the object are monotonic_decreasing.\n\n`Series.value_counts`([normalize, sort, ...])\n\nReturn a Series containing counts of unique values.\n\n`Series.align`(other[, join, axis, level, ...])\n\nAlign two objects on their axes with the specified join method.\n\n`Series.drop`([labels, axis, index, columns, ...])\n\nReturn Series with specified index labels removed.\n\n`Series.droplevel`(level[, axis])\n\nReturn Series/DataFrame with requested index / column level(s) removed.\n\n`Series.drop_duplicates`([keep, inplace])\n\nReturn Series with duplicate values removed.\n\n`Series.duplicated`([keep])\n\nIndicate duplicate Series values.\n\n`Series.equals`(other)\n\nTest whether two objects contain the same elements.\n\n`Series.first`(offset)\n\nSelect initial periods of time series data based on a date offset.\n\n`Series.head`([n])\n\nReturn the first n rows.\n\n`Series.idxmax`([axis, skipna])\n\nReturn the row label of the maximum value.\n\n`Series.idxmin`([axis, skipna])\n\nReturn the row label of the minimum value.\n\n`Series.isin`(values)\n\nWhether elements in Series are contained in values.\n\n`Series.last`(offset)\n\nSelect final periods of time series data based on a date offset.\n\n`Series.reindex`(*args, **kwargs)\n\nConform Series to new index with optional filling logic.\n\n`Series.reindex_like`(other[, method, copy, ...])\n\nReturn an object with matching indices as other object.\n\n`Series.rename`([index, axis, copy, inplace, ...])\n\nAlter Series index labels or name.\n\n`Series.rename_axis`([mapper, index, columns, ...])\n\nSet the name of the axis for the index or columns.\n\n`Series.reset_index`([level, drop, name, inplace])\n\nGenerate a new DataFrame or Series with the index reset.\n\n`Series.sample`([n, frac, replace, weights, ...])\n\nReturn a random sample of items from an axis of object.\n\n`Series.set_axis`(labels[, axis, inplace])\n\nAssign desired index to given axis.\n\n`Series.take`(indices[, axis, is_copy])\n\nReturn the elements in the given positional indices along an axis.\n\n`Series.tail`([n])\n\nReturn the last n rows.\n\n`Series.truncate`([before, after, axis, copy])\n\nTruncate a Series or DataFrame before and after some index value.\n\n`Series.where`(cond[, other, inplace, axis, ...])\n\nReplace values where the condition is False.\n\n`Series.mask`(cond[, other, inplace, axis, ...])\n\nReplace values where the condition is True.\n\n`Series.add_prefix`(prefix)\n\nPrefix labels with string prefix.\n\n`Series.add_suffix`(suffix)\n\nSuffix labels with string suffix.\n\n`Series.filter`([items, like, regex, axis])\n\nSubset the dataframe rows or columns according to the specified index labels.\n\n`Series.backfill`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\n`Series.bfill`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\n`Series.dropna`([axis, inplace, how])\n\nReturn a new Series with missing values removed.\n\n`Series.ffill`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\n`Series.fillna`([value, method, axis, ...])\n\nFill NA/NaN values using the specified method.\n\n`Series.interpolate`([method, axis, limit, ...])\n\nFill NaN values using an interpolation method.\n\n`Series.isna`()\n\nDetect missing values.\n\n`Series.isnull`()\n\nSeries.isnull is an alias for Series.isna.\n\n`Series.notna`()\n\nDetect existing (non-missing) values.\n\n`Series.notnull`()\n\nSeries.notnull is an alias for Series.notna.\n\n`Series.pad`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\n`Series.replace`([to_replace, value, inplace, ...])\n\nReplace values given in to_replace with value.\n\n`Series.argsort`([axis, kind, order])\n\nReturn the integer indices that would sort the Series values.\n\n`Series.argmin`([axis, skipna])\n\nReturn int position of the smallest value in the Series.\n\n`Series.argmax`([axis, skipna])\n\nReturn int position of the largest value in the Series.\n\n`Series.reorder_levels`(order)\n\nRearrange index levels using input order.\n\n`Series.sort_values`([axis, ascending, ...])\n\nSort by the values.\n\n`Series.sort_index`([axis, level, ascending, ...])\n\nSort Series by index labels.\n\n`Series.swaplevel`([i, j, copy])\n\nSwap levels i and j in a `MultiIndex`.\n\n`Series.unstack`([level, fill_value])\n\nUnstack, also known as pivot, Series with MultiIndex to produce DataFrame.\n\n`Series.explode`([ignore_index])\n\nTransform each element of a list-like to a row.\n\n`Series.searchsorted`(value[, side, sorter])\n\nFind indices where elements should be inserted to maintain order.\n\n`Series.ravel`([order])\n\nReturn the flattened underlying data as an ndarray.\n\n`Series.repeat`(repeats[, axis])\n\nRepeat elements of a Series.\n\n`Series.squeeze`([axis])\n\nSqueeze 1 dimensional axis objects into scalars.\n\n`Series.view`([dtype])\n\nCreate a new view of the Series.\n\n`Series.append`(to_append[, ignore_index, ...])\n\nConcatenate two or more Series.\n\n`Series.compare`(other[, align_axis, ...])\n\nCompare to another Series and show the differences.\n\n`Series.update`(other)\n\nModify Series in place using values from passed Series.\n\n`Series.asfreq`(freq[, method, how, ...])\n\nConvert time series to specified frequency.\n\n`Series.asof`(where[, subset])\n\nReturn the last row(s) without any NaNs before where.\n\n`Series.shift`([periods, freq, axis, fill_value])\n\nShift index by desired number of periods with an optional time freq.\n\n`Series.first_valid_index`()\n\nReturn index for first non-NA value or None, if no NA value is found.\n\n`Series.last_valid_index`()\n\nReturn index for last non-NA value or None, if no NA value is found.\n\n`Series.resample`(rule[, axis, closed, label, ...])\n\nResample time-series data.\n\n`Series.tz_convert`(tz[, axis, level, copy])\n\nConvert tz-aware axis to target time zone.\n\n`Series.tz_localize`(tz[, axis, level, copy, ...])\n\nLocalize tz-naive index of a Series or DataFrame to target time zone.\n\n`Series.at_time`(time[, asof, axis])\n\nSelect values at particular time of day (e.g., 9:30AM).\n\n`Series.between_time`(start_time, end_time[, ...])\n\nSelect values between particular times of the day (e.g., 9:00-9:30 AM).\n\n`Series.tshift`([periods, freq, axis])\n\n(DEPRECATED) Shift the time index, using the index's frequency if available.\n\n`Series.slice_shift`([periods, axis])\n\n(DEPRECATED) Equivalent to shift without copying data.\n\npandas provides dtype-specific methods under various accessors. These are\nseparate namespaces within `Series` that only apply to specific data types.\n\nData Type\n\nAccessor\n\nDatetime, Timedelta, Period\n\ndt\n\nString\n\nstr\n\nCategorical\n\ncat\n\nSparse\n\nsparse\n\n`Series.dt` can be used to access the values of the series as datetimelike and\nreturn several properties. These can be accessed like `Series.dt.<property>`.\n\n`Series.dt.date`\n\nReturns numpy array of python `datetime.date` objects.\n\n`Series.dt.time`\n\nReturns numpy array of `datetime.time` objects.\n\n`Series.dt.timetz`\n\nReturns numpy array of `datetime.time` objects with timezone information.\n\n`Series.dt.year`\n\nThe year of the datetime.\n\n`Series.dt.month`\n\nThe month as January=1, December=12.\n\n`Series.dt.day`\n\nThe day of the datetime.\n\n`Series.dt.hour`\n\nThe hours of the datetime.\n\n`Series.dt.minute`\n\nThe minutes of the datetime.\n\n`Series.dt.second`\n\nThe seconds of the datetime.\n\n`Series.dt.microsecond`\n\nThe microseconds of the datetime.\n\n`Series.dt.nanosecond`\n\nThe nanoseconds of the datetime.\n\n`Series.dt.week`\n\n(DEPRECATED) The week ordinal of the year.\n\n`Series.dt.weekofyear`\n\n(DEPRECATED) The week ordinal of the year.\n\n`Series.dt.dayofweek`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`Series.dt.day_of_week`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`Series.dt.weekday`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`Series.dt.dayofyear`\n\nThe ordinal day of the year.\n\n`Series.dt.day_of_year`\n\nThe ordinal day of the year.\n\n`Series.dt.quarter`\n\nThe quarter of the date.\n\n`Series.dt.is_month_start`\n\nIndicates whether the date is the first day of the month.\n\n`Series.dt.is_month_end`\n\nIndicates whether the date is the last day of the month.\n\n`Series.dt.is_quarter_start`\n\nIndicator for whether the date is the first day of a quarter.\n\n`Series.dt.is_quarter_end`\n\nIndicator for whether the date is the last day of a quarter.\n\n`Series.dt.is_year_start`\n\nIndicate whether the date is the first day of a year.\n\n`Series.dt.is_year_end`\n\nIndicate whether the date is the last day of the year.\n\n`Series.dt.is_leap_year`\n\nBoolean indicator if the date belongs to a leap year.\n\n`Series.dt.daysinmonth`\n\nThe number of days in the month.\n\n`Series.dt.days_in_month`\n\nThe number of days in the month.\n\n`Series.dt.tz`\n\nReturn the timezone.\n\n`Series.dt.freq`\n\nReturn the frequency object for this PeriodArray.\n\n`Series.dt.to_period`(*args, **kwargs)\n\nCast to PeriodArray/Index at a particular frequency.\n\n`Series.dt.to_pydatetime`()\n\nReturn the data as an array of `datetime.datetime` objects.\n\n`Series.dt.tz_localize`(*args, **kwargs)\n\nLocalize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index.\n\n`Series.dt.tz_convert`(*args, **kwargs)\n\nConvert tz-aware Datetime Array/Index from one time zone to another.\n\n`Series.dt.normalize`(*args, **kwargs)\n\nConvert times to midnight.\n\n`Series.dt.strftime`(*args, **kwargs)\n\nConvert to Index using specified date_format.\n\n`Series.dt.round`(*args, **kwargs)\n\nPerform round operation on the data to the specified freq.\n\n`Series.dt.floor`(*args, **kwargs)\n\nPerform floor operation on the data to the specified freq.\n\n`Series.dt.ceil`(*args, **kwargs)\n\nPerform ceil operation on the data to the specified freq.\n\n`Series.dt.month_name`(*args, **kwargs)\n\nReturn the month names of the DateTimeIndex with specified locale.\n\n`Series.dt.day_name`(*args, **kwargs)\n\nReturn the day names of the DateTimeIndex with specified locale.\n\n`Series.dt.qyear`\n\n`Series.dt.start_time`\n\n`Series.dt.end_time`\n\n`Series.dt.days`\n\nNumber of days for each element.\n\n`Series.dt.seconds`\n\nNumber of seconds (>= 0 and less than 1 day) for each element.\n\n`Series.dt.microseconds`\n\nNumber of microseconds (>= 0 and less than 1 second) for each element.\n\n`Series.dt.nanoseconds`\n\nNumber of nanoseconds (>= 0 and less than 1 microsecond) for each element.\n\n`Series.dt.components`\n\nReturn a Dataframe of the components of the Timedeltas.\n\n`Series.dt.to_pytimedelta`()\n\nReturn an array of native `datetime.timedelta` objects.\n\n`Series.dt.total_seconds`(*args, **kwargs)\n\nReturn total duration of each element expressed in seconds.\n\n`Series.str` can be used to access the values of the series as strings and\napply several methods to it. These can be accessed like\n`Series.str.<function/property>`.\n\n`Series.str.capitalize`()\n\nConvert strings in the Series/Index to be capitalized.\n\n`Series.str.casefold`()\n\nConvert strings in the Series/Index to be casefolded.\n\n`Series.str.cat`([others, sep, na_rep, join])\n\nConcatenate strings in the Series/Index with given separator.\n\n`Series.str.center`(width[, fillchar])\n\nPad left and right side of strings in the Series/Index.\n\n`Series.str.contains`(pat[, case, flags, na, ...])\n\nTest if pattern or regex is contained within a string of a Series or Index.\n\n`Series.str.count`(pat[, flags])\n\nCount occurrences of pattern in each string of the Series/Index.\n\n`Series.str.decode`(encoding[, errors])\n\nDecode character string in the Series/Index using indicated encoding.\n\n`Series.str.encode`(encoding[, errors])\n\nEncode character string in the Series/Index using indicated encoding.\n\n`Series.str.endswith`(pat[, na])\n\nTest if the end of each string element matches a pattern.\n\n`Series.str.extract`(pat[, flags, expand])\n\nExtract capture groups in the regex pat as columns in a DataFrame.\n\n`Series.str.extractall`(pat[, flags])\n\nExtract capture groups in the regex pat as columns in DataFrame.\n\n`Series.str.find`(sub[, start, end])\n\nReturn lowest indexes in each strings in the Series/Index.\n\n`Series.str.findall`(pat[, flags])\n\nFind all occurrences of pattern or regular expression in the Series/Index.\n\n`Series.str.fullmatch`(pat[, case, flags, na])\n\nDetermine if each string entirely matches a regular expression.\n\n`Series.str.get`(i)\n\nExtract element from each component at specified position.\n\n`Series.str.index`(sub[, start, end])\n\nReturn lowest indexes in each string in Series/Index.\n\n`Series.str.join`(sep)\n\nJoin lists contained as elements in the Series/Index with passed delimiter.\n\n`Series.str.len`()\n\nCompute the length of each element in the Series/Index.\n\n`Series.str.ljust`(width[, fillchar])\n\nPad right side of strings in the Series/Index.\n\n`Series.str.lower`()\n\nConvert strings in the Series/Index to lowercase.\n\n`Series.str.lstrip`([to_strip])\n\nRemove leading characters.\n\n`Series.str.match`(pat[, case, flags, na])\n\nDetermine if each string starts with a match of a regular expression.\n\n`Series.str.normalize`(form)\n\nReturn the Unicode normal form for the strings in the Series/Index.\n\n`Series.str.pad`(width[, side, fillchar])\n\nPad strings in the Series/Index up to width.\n\n`Series.str.partition`([sep, expand])\n\nSplit the string at the first occurrence of sep.\n\n`Series.str.removeprefix`(prefix)\n\nRemove a prefix from an object series.\n\n`Series.str.removesuffix`(suffix)\n\nRemove a suffix from an object series.\n\n`Series.str.repeat`(repeats)\n\nDuplicate each string in the Series or Index.\n\n`Series.str.replace`(pat, repl[, n, case, ...])\n\nReplace each occurrence of pattern/regex in the Series/Index.\n\n`Series.str.rfind`(sub[, start, end])\n\nReturn highest indexes in each strings in the Series/Index.\n\n`Series.str.rindex`(sub[, start, end])\n\nReturn highest indexes in each string in Series/Index.\n\n`Series.str.rjust`(width[, fillchar])\n\nPad left side of strings in the Series/Index.\n\n`Series.str.rpartition`([sep, expand])\n\nSplit the string at the last occurrence of sep.\n\n`Series.str.rstrip`([to_strip])\n\nRemove trailing characters.\n\n`Series.str.slice`([start, stop, step])\n\nSlice substrings from each element in the Series or Index.\n\n`Series.str.slice_replace`([start, stop, repl])\n\nReplace a positional slice of a string with another value.\n\n`Series.str.split`([pat, n, expand, regex])\n\nSplit strings around given separator/delimiter.\n\n`Series.str.rsplit`([pat, n, expand])\n\nSplit strings around given separator/delimiter.\n\n`Series.str.startswith`(pat[, na])\n\nTest if the start of each string element matches a pattern.\n\n`Series.str.strip`([to_strip])\n\nRemove leading and trailing characters.\n\n`Series.str.swapcase`()\n\nConvert strings in the Series/Index to be swapcased.\n\n`Series.str.title`()\n\nConvert strings in the Series/Index to titlecase.\n\n`Series.str.translate`(table)\n\nMap all characters in the string through the given mapping table.\n\n`Series.str.upper`()\n\nConvert strings in the Series/Index to uppercase.\n\n`Series.str.wrap`(width, **kwargs)\n\nWrap strings in Series/Index at specified line width.\n\n`Series.str.zfill`(width)\n\nPad strings in the Series/Index by prepending '0' characters.\n\n`Series.str.isalnum`()\n\nCheck whether all characters in each string are alphanumeric.\n\n`Series.str.isalpha`()\n\nCheck whether all characters in each string are alphabetic.\n\n`Series.str.isdigit`()\n\nCheck whether all characters in each string are digits.\n\n`Series.str.isspace`()\n\nCheck whether all characters in each string are whitespace.\n\n`Series.str.islower`()\n\nCheck whether all characters in each string are lowercase.\n\n`Series.str.isupper`()\n\nCheck whether all characters in each string are uppercase.\n\n`Series.str.istitle`()\n\nCheck whether all characters in each string are titlecase.\n\n`Series.str.isnumeric`()\n\nCheck whether all characters in each string are numeric.\n\n`Series.str.isdecimal`()\n\nCheck whether all characters in each string are decimal.\n\n`Series.str.get_dummies`([sep])\n\nReturn DataFrame of dummy/indicator variables for Series.\n\nCategorical-dtype specific methods and attributes are available under the\n`Series.cat` accessor.\n\n`Series.cat.categories`\n\nThe categories of this categorical.\n\n`Series.cat.ordered`\n\nWhether the categories have an ordered relationship.\n\n`Series.cat.codes`\n\nReturn Series of codes as well as the index.\n\n`Series.cat.rename_categories`(*args, **kwargs)\n\nRename categories.\n\n`Series.cat.reorder_categories`(*args, **kwargs)\n\nReorder categories as specified in new_categories.\n\n`Series.cat.add_categories`(*args, **kwargs)\n\nAdd new categories.\n\n`Series.cat.remove_categories`(*args, **kwargs)\n\nRemove the specified categories.\n\n`Series.cat.remove_unused_categories`(*args, ...)\n\nRemove categories which are not used.\n\n`Series.cat.set_categories`(*args, **kwargs)\n\nSet the categories to the specified new_categories.\n\n`Series.cat.as_ordered`(*args, **kwargs)\n\nSet the Categorical to be ordered.\n\n`Series.cat.as_unordered`(*args, **kwargs)\n\nSet the Categorical to be unordered.\n\nSparse-dtype specific methods and attributes are provided under the\n`Series.sparse` accessor.\n\n`Series.sparse.npoints`\n\nThe number of non- `fill_value` points.\n\n`Series.sparse.density`\n\nThe percent of non- `fill_value` points, as decimal.\n\n`Series.sparse.fill_value`\n\nElements in data that are fill_value are not stored.\n\n`Series.sparse.sp_values`\n\nAn ndarray containing the non- `fill_value` values.\n\n`Series.sparse.from_coo`(A[, dense_index])\n\nCreate a Series with sparse values from a scipy.sparse.coo_matrix.\n\n`Series.sparse.to_coo`([row_levels, ...])\n\nCreate a scipy.sparse.coo_matrix from a Series with MultiIndex.\n\nFlags refer to attributes of the pandas object. Properties of the dataset\n(like the date is was recorded, the URL it was accessed from, etc.) should be\nstored in `Series.attrs`.\n\n`Flags`(obj, *, allows_duplicate_labels)\n\nFlags that apply to pandas objects.\n\n`Series.attrs` is a dictionary for storing global metadata for this Series.\n\nWarning\n\n`Series.attrs` is considered experimental and may change without warning.\n\n`Series.attrs`\n\nDictionary of global attributes of this dataset.\n\n`Series.plot` is both a callable method and a namespace attribute for specific\nplotting methods of the form `Series.plot.<kind>`.\n\n`Series.plot`([kind, ax, figsize, ....])\n\nSeries plotting accessor and method\n\n`Series.plot.area`([x, y])\n\nDraw a stacked area plot.\n\n`Series.plot.bar`([x, y])\n\nVertical bar plot.\n\n`Series.plot.barh`([x, y])\n\nMake a horizontal bar plot.\n\n`Series.plot.box`([by])\n\nMake a box plot of the DataFrame columns.\n\n`Series.plot.density`([bw_method, ind])\n\nGenerate Kernel Density Estimate plot using Gaussian kernels.\n\n`Series.plot.hist`([by, bins])\n\nDraw one histogram of the DataFrame's columns.\n\n`Series.plot.kde`([bw_method, ind])\n\nGenerate Kernel Density Estimate plot using Gaussian kernels.\n\n`Series.plot.line`([x, y])\n\nPlot Series or DataFrame as lines.\n\n`Series.plot.pie`(**kwargs)\n\nGenerate a pie plot.\n\n`Series.hist`([by, ax, grid, xlabelsize, ...])\n\nDraw histogram of the input series using matplotlib.\n\n`Series.to_pickle`(path[, compression, ...])\n\nPickle (serialize) object to file.\n\n`Series.to_csv`([path_or_buf, sep, na_rep, ...])\n\nWrite object to a comma-separated values (csv) file.\n\n`Series.to_dict`([into])\n\nConvert Series to {label -> value} dict or dict-like object.\n\n`Series.to_excel`(excel_writer[, sheet_name, ...])\n\nWrite object to an Excel sheet.\n\n`Series.to_frame`([name])\n\nConvert Series to DataFrame.\n\n`Series.to_xarray`()\n\nReturn an xarray object from the pandas object.\n\n`Series.to_hdf`(path_or_buf, key[, mode, ...])\n\nWrite the contained data to an HDF5 file using HDFStore.\n\n`Series.to_sql`(name, con[, schema, ...])\n\nWrite records stored in a DataFrame to a SQL database.\n\n`Series.to_json`([path_or_buf, orient, ...])\n\nConvert the object to a JSON string.\n\n`Series.to_string`([buf, na_rep, ...])\n\nRender a string representation of the Series.\n\n`Series.to_clipboard`([excel, sep])\n\nCopy object to the system clipboard.\n\n`Series.to_latex`([buf, columns, col_space, ...])\n\nRender object to a LaTeX tabular, longtable, or nested table.\n\n`Series.to_markdown`([buf, mode, index, ...])\n\nPrint Series in Markdown-friendly format.\n\n"}, {"name": "Sparse data structures", "path": "user_guide/sparse", "type": "Manual", "text": "\npandas provides data structures for efficiently storing sparse data. These are\nnot necessarily sparse in the typical \u201cmostly 0\u201d. Rather, you can view these\nobjects as being \u201ccompressed\u201d where any data matching a specific value (`NaN`\n/ missing value, though any value can be chosen, including 0) is omitted. The\ncompressed values are not actually stored in the array.\n\nNotice the dtype, `Sparse[float64, nan]`. The `nan` means that elements in the\narray that are `nan` aren\u2019t actually stored, only the non-`nan` elements are.\nThose non-`nan` elements have a `float64` dtype.\n\nThe sparse objects exist for memory efficiency reasons. Suppose you had a\nlarge, mostly NA `DataFrame`:\n\nAs you can see, the density (% of values that have not been \u201ccompressed\u201d) is\nextremely low. This sparse object takes up much less memory on disk (pickled)\nand in the Python interpreter.\n\nFunctionally, their behavior should be nearly identical to their dense\ncounterparts.\n\n`arrays.SparseArray` is a `ExtensionArray` for storing an array of sparse\nvalues (see dtypes for more on extension arrays). It is a 1-dimensional\nndarray-like object storing only values distinct from the `fill_value`:\n\nA sparse array can be converted to a regular (dense) ndarray with\n`numpy.asarray()`\n\nThe `SparseArray.dtype` property stores two pieces of information\n\nThe dtype of the non-sparse values\n\nThe scalar fill value\n\nA `SparseDtype` may be constructed by passing only a dtype\n\nin which case a default fill value will be used (for NumPy dtypes this is\noften the \u201cmissing\u201d value for that dtype). To override this default an\nexplicit fill value may be passed instead\n\nFinally, the string alias `'Sparse[dtype]'` may be used to specify a sparse\ndtype in many places\n\npandas provides a `.sparse` accessor, similar to `.str` for string data,\n`.cat` for categorical data, and `.dt` for datetime-like data. This namespace\nprovides attributes and methods that are specific to sparse data.\n\nThis accessor is available only on data with `SparseDtype`, and on the\n`Series` class itself for creating a Series with sparse data from a scipy COO\nmatrix with.\n\nNew in version 0.25.0.\n\nA `.sparse` accessor has been added for `DataFrame` as well. See Sparse\naccessor for more.\n\nYou can apply NumPy ufuncs to `SparseArray` and get a `SparseArray` as a\nresult.\n\nThe ufunc is also applied to `fill_value`. This is needed to get the correct\ndense result.\n\nNote\n\n`SparseSeries` and `SparseDataFrame` were removed in pandas 1.0.0. This\nmigration guide is present to aid in migrating from previous versions.\n\nIn older versions of pandas, the `SparseSeries` and `SparseDataFrame` classes\n(documented below) were the preferred way to work with sparse data. With the\nadvent of extension arrays, these subclasses are no longer needed. Their\npurpose is better served by using a regular Series or DataFrame with sparse\nvalues instead.\n\nNote\n\nThere\u2019s no performance or memory penalty to using a Series or DataFrame with\nsparse values, rather than a SparseSeries or SparseDataFrame.\n\nThis section provides some guidance on migrating your code to the new style.\nAs a reminder, you can use the Python warnings module to control warnings. But\nwe recommend modifying your code, rather than ignoring the warning.\n\nConstruction\n\nFrom an array-like, use the regular `Series` or `DataFrame` constructors with\n`SparseArray` values.\n\nFrom a SciPy sparse matrix, use `DataFrame.sparse.from_spmatrix()`,\n\nConversion\n\nFrom sparse to dense, use the `.sparse` accessors\n\nFrom dense to sparse, use `DataFrame.astype()` with a `SparseDtype`.\n\nSparse Properties\n\nSparse-specific properties, like `density`, are available on the `.sparse`\naccessor.\n\nGeneral differences\n\nIn a `SparseDataFrame`, all columns were sparse. A `DataFrame` can have a\nmixture of sparse and dense columns. As a consequence, assigning new columns\nto a `DataFrame` with sparse values will not automatically convert the input\nto be sparse.\n\nInstead, you\u2019ll need to ensure that the values being assigned are sparse\n\nThe `SparseDataFrame.default_kind` and `SparseDataFrame.default_fill_value`\nattributes have no replacement.\n\nUse `DataFrame.sparse.from_spmatrix()` to create a `DataFrame` with sparse\nvalues from a sparse matrix.\n\nNew in version 0.25.0.\n\nAll sparse formats are supported, but matrices that are not in `COOrdinate`\nformat will be converted, copying data as needed. To convert back to sparse\nSciPy matrix in COO format, you can use the `DataFrame.sparse.to_coo()`\nmethod:\n\n`Series.sparse.to_coo()` is implemented for transforming a `Series` with\nsparse values indexed by a `MultiIndex` to a `scipy.sparse.coo_matrix`.\n\nThe method requires a `MultiIndex` with two or more levels.\n\nIn the example below, we transform the `Series` to a sparse representation of\na 2-d array by specifying that the first and second `MultiIndex` levels define\nlabels for the rows and the third and fourth levels define labels for the\ncolumns. We also specify that the column and row labels should be sorted in\nthe final sparse representation.\n\nSpecifying different row and column labels (and not sorting them) yields a\ndifferent sparse matrix:\n\nA convenience method `Series.sparse.from_coo()` is implemented for creating a\n`Series` with sparse values from a `scipy.sparse.coo_matrix`.\n\nThe default behaviour (with `dense_index=False`) simply returns a `Series`\ncontaining only the non-null entries.\n\nSpecifying `dense_index=True` will result in an index that is the Cartesian\nproduct of the row and columns coordinates of the matrix. Note that this will\nconsume a significant amount of memory (relative to `dense_index=False`) if\nthe sparse matrix is large (and sparse) enough.\n\n"}, {"name": "Style", "path": "reference/style", "type": "Style", "text": "\n`Styler` objects are returned by `pandas.DataFrame.style`.\n\n`Styler`(data[, precision, table_styles, ...])\n\nHelps style a DataFrame or Series according to the data with HTML and CSS.\n\n`Styler.from_custom_template`(searchpath[, ...])\n\nFactory function for creating a subclass of `Styler`.\n\n`Styler.env`\n\n`Styler.template_html`\n\n`Styler.template_html_style`\n\n`Styler.template_html_table`\n\n`Styler.template_latex`\n\n`Styler.loader`\n\n`Styler.apply`(func[, axis, subset])\n\nApply a CSS-styling function column-wise, row-wise, or table-wise.\n\n`Styler.applymap`(func[, subset])\n\nApply a CSS-styling function elementwise.\n\n`Styler.apply_index`(func[, axis, level])\n\nApply a CSS-styling function to the index or column headers, level-wise.\n\n`Styler.applymap_index`(func[, axis, level])\n\nApply a CSS-styling function to the index or column headers, elementwise.\n\n`Styler.format`([formatter, subset, na_rep, ...])\n\nFormat the text display value of cells.\n\n`Styler.format_index`([formatter, axis, ...])\n\nFormat the text display value of index labels or column headers.\n\n`Styler.hide`([subset, axis, level, names])\n\nHide the entire index / column headers, or specific rows / columns from\ndisplay.\n\n`Styler.set_td_classes`(classes)\n\nSet the DataFrame of strings added to the `class` attribute of `<td>` HTML\nelements.\n\n`Styler.set_table_styles`([table_styles, ...])\n\nSet the table styles included within the `<style>` HTML element.\n\n`Styler.set_table_attributes`(attributes)\n\nSet the table attributes added to the `<table>` HTML element.\n\n`Styler.set_tooltips`(ttips[, props, css_class])\n\nSet the DataFrame of strings on `Styler` generating `:hover` tooltips.\n\n`Styler.set_caption`(caption)\n\nSet the text added to a `<caption>` HTML element.\n\n`Styler.set_sticky`([axis, pixel_size, levels])\n\nAdd CSS to permanently display the index or column headers in a scrolling\nframe.\n\n`Styler.set_properties`([subset])\n\nSet defined CSS-properties to each `<td>` HTML element within the given\nsubset.\n\n`Styler.set_uuid`(uuid)\n\nSet the uuid applied to `id` attributes of HTML elements.\n\n`Styler.clear`()\n\nReset the `Styler`, removing any previously applied styles.\n\n`Styler.pipe`(func, *args, **kwargs)\n\nApply `func(self, *args, **kwargs)`, and return the result.\n\n`Styler.highlight_null`([null_color, subset, ...])\n\nHighlight missing values with a style.\n\n`Styler.highlight_max`([subset, color, axis, ...])\n\nHighlight the maximum with a style.\n\n`Styler.highlight_min`([subset, color, axis, ...])\n\nHighlight the minimum with a style.\n\n`Styler.highlight_between`([subset, color, ...])\n\nHighlight a defined range with a style.\n\n`Styler.highlight_quantile`([subset, color, ...])\n\nHighlight values defined by a quantile with a style.\n\n`Styler.background_gradient`([cmap, low, ...])\n\nColor the background in a gradient style.\n\n`Styler.text_gradient`([cmap, low, high, ...])\n\nColor the text in a gradient style.\n\n`Styler.bar`([subset, axis, color, cmap, ...])\n\nDraw bar chart in the cell backgrounds.\n\n`Styler.to_html`([buf, table_uuid, ...])\n\nWrite Styler to a file, buffer or string in HTML-CSS format.\n\n`Styler.to_latex`([buf, column_format, ...])\n\nWrite Styler to a file, buffer or string in LaTeX format.\n\n`Styler.to_excel`(excel_writer[, sheet_name, ...])\n\nWrite Styler to an Excel sheet.\n\n`Styler.export`()\n\nExport the styles applied to the current Styler.\n\n`Styler.use`(styles)\n\nSet the styles on the current Styler.\n\n"}, {"name": "Table Visualization", "path": "user_guide/style", "type": "Manual", "text": "\nThis section demonstrates visualization of tabular data using the Styler\nclass. For information on visualization with charting please see Chart\nVisualization. This document is written as a Jupyter Notebook, and can be\nviewed or downloaded here.\n\nStyling should be performed after the data in a DataFrame has been processed.\nThe Styler creates an HTML `<table>` and leverages CSS styling language to\nmanipulate many parameters including colors, fonts, borders, background, etc.\nSee here for more information on styling HTML tables. This allows a lot of\nflexibility out of the box, and even enables web developers to integrate\nDataFrames into their exiting user interface designs.\n\nThe `DataFrame.style` attribute is a property that returns a Styler object. It\nhas a `_repr_html_` method defined on it so they are rendered automatically in\nJupyter Notebook.\n\nThe above output looks very similar to the standard DataFrame HTML\nrepresentation. But the HTML here has already attached some CSS classes to\neach cell, even if we haven\u2019t yet created any styles. We can view these by\ncalling the .to_html() method, which returns the raw HTML as string, which is\nuseful for further processing or adding to a file - read on in More about CSS\nand HTML. Below we will show how we can use these to format the DataFrame to\nbe more communicative. For example how we can build `s`:\n\nBefore adding styles it is useful to show that the Styler can distinguish the\ndisplay value from the actual value, in both datavlaues and index or columns\nheaders. To control the display value, the text is printed in each cell as\nstring, and we can use the .format() and .format_index() methods to manipulate\nthis according to a format spec string or a callable that takes a single value\nand returns a string. It is possible to define this for the whole table, or\nindex, or for individual columns, or MultiIndex levels.\n\nAdditionally, the format function has a precision argument to specifically\nhelp formatting floats, as well as decimal and thousands separators to support\nother locales, an na_rep argument to display missing data, and an escape\nargument to help displaying safe-HTML or safe-LaTeX. The default formatter is\nconfigured to adopt pandas\u2019 `styler.format.precision` option, controllable\nusing `with pd.option_context('format.precision', 2):`\n\nUsing Styler to manipulate the display is a useful feature because maintaining\nthe indexing and datavalues for other purposes gives greater control. You do\nnot have to overwrite your DataFrame to display it how you like. Here is an\nexample of using the formatting functions whilst still relying on the\nunderlying data for indexing and calculations.\n\nThe index and column headers can be completely hidden, as well subselecting\nrows or columns that one wishes to exclude. Both these options are performed\nusing the same methods.\n\nThe index can be hidden from rendering by calling .hide() without any\narguments, which might be useful if your index is integer based. Similarly\ncolumn headers can be hidden by calling .hide(axis=\u201ccolumns\u201d) without any\nfurther arguments.\n\nSpecific rows or columns can be hidden from rendering by calling the same\n.hide() method and passing in a row/column label, a list-like or a slice of\nrow/column labels to for the `subset` argument.\n\nHiding does not change the integer arrangement of CSS classes, e.g. hiding the\nfirst two columns of a DataFrame means the column class indexing will still\nstart at `col2`, since `col0` and `col1` are simply ignored.\n\nWe can update our `Styler` object from before to hide some data and format the\nvalues.\n\nThere are 3 primary methods of adding custom CSS styles to Styler:\n\nUsing .set_table_styles() to control broader areas of the table with specified\ninternal CSS. Although table styles allow the flexibility to add CSS selectors\nand properties controlling all individual parts of the table, they are\nunwieldy for individual cell specifications. Also, note that table styles\ncannot be exported to Excel.\n\nUsing .set_td_classes() to directly link either external CSS classes to your\ndata cells or link the internal CSS classes created by .set_table_styles().\nSee here. These cannot be used on column header rows or indexes, and also\nwon\u2019t export to Excel.\n\nUsing the .apply() and .applymap() functions to add direct internal CSS to\nspecific data cells. See here. As of v1.4.0 there are also methods that work\ndirectly on column header rows or indexes; .apply_index() and\n.applymap_index(). Note that only these methods add styles that will export to\nExcel. These methods work in a similar way to DataFrame.apply() and\nDataFrame.applymap().\n\nTable styles are flexible enough to control all individual parts of the table,\nincluding column headers and indexes. However, they can be unwieldy to type\nfor individual data cells or for any kind of conditional formatting, so we\nrecommend that table styles are used for broad styling, such as entire rows or\ncolumns at a time.\n\nTable styles are also used to control features which can apply to the whole\ntable at once such as creating a generic hover functionality. The `:hover`\npseudo-selector, as well as other pseudo-selectors, can only be used this way.\n\nTo replicate the normal format of CSS selectors and properties (attribute\nvalue pairs), e.g.\n\nthe necessary format to pass styles to .set_table_styles() is as a list of\ndicts, each with a CSS-selector tag and CSS-properties. Properties can either\nbe a list of 2-tuples, or a regular CSS-string, for example:\n\nNext we just add a couple more styling artifacts targeting specific parts of\nthe table. Be careful here, since we are chaining methods we need to\nexplicitly instruct the method not to `overwrite` the existing styles.\n\nAs a convenience method (since version 1.2.0) we can also pass a dict to\n.set_table_styles() which contains row or column keys. Behind the scenes\nStyler just indexes the keys and adds relevant `.col<m>` or `.row<n>` classes\nas necessary to the given CSS selectors.\n\nIf you have designed a website then it is likely you will already have an\nexternal CSS file that controls the styling of table and cell objects within\nit. You may want to use these native files rather than duplicate all the CSS\nin python (and duplicate any maintenance work).\n\nIt is very easy to add a `class` to the main `<table>` using\n.set_table_attributes(). This method can also attach inline styles - read more\nin CSS Hierarchies.\n\nNew in version 1.2.0\n\nThe .set_td_classes() method accepts a DataFrame with matching indices and\ncolumns to the underlying Styler\u2019s DataFrame. That DataFrame will contain\nstrings as css-classes to add to individual data cells: the `<td>` elements of\nthe `<table>`. Rather than use external CSS we will create our classes\ninternally and add them to table style. We will save adding the borders until\nthe section on tooltips.\n\nWe use the following methods to pass your style functions. Both of those\nmethods take a function (and some other keyword arguments) and apply it to the\nDataFrame in a certain way, rendering CSS styles.\n\n.applymap() (elementwise): accepts a function that takes a single value and\nreturns a string with the CSS attribute-value pair.\n\n.apply() (column-/row-/table-wise): accepts a function that takes a Series or\nDataFrame and returns a Series, DataFrame, or numpy array with an identical\nshape where each element is a string with a CSS attribute-value pair. This\nmethod passes each column or row of your DataFrame one-at-a-time or the entire\ntable at once, depending on the `axis` keyword argument. For columnwise use\n`axis=0`, rowwise use `axis=1`, and for the entire table at once use\n`axis=None`.\n\nThis method is powerful for applying multiple, complex logic to data cells. We\ncreate a new DataFrame to demonstrate this.\n\nFor example we can build a function that colors text if it is negative, and\nchain this with a function that partially fades cells of negligible value.\nSince this looks at each element in turn we use `applymap`.\n\nWe can also build a function that highlights the maximum value across rows,\ncols, and the DataFrame all at once. In this case we use `apply`. Below we\nhighlight the maximum in a column.\n\nWe can use the same function across the different axes, highlighting here the\nDataFrame maximum in purple, and row maximums in pink.\n\nThis last example shows how some styles have been overwritten by others. In\ngeneral the most recent style applied is active but you can read more in the\nsection on CSS hierarchies. You can also apply these styles to more granular\nparts of the DataFrame - read more in section on subset slicing.\n\nIt is possible to replicate some of this functionality using just classes but\nit can be more cumbersome. See item 3) of Optimization\n\nDebugging Tip: If you\u2019re having trouble writing your style function, try just\npassing it into `DataFrame.apply`. Internally, `Styler.apply` uses\n`DataFrame.apply` so the result should be the same, and with `DataFrame.apply`\nyou will be able to inspect the CSS string output of your intended function in\neach cell.\n\nSimilar application is acheived for headers by using:\n\n.applymap_index() (elementwise): accepts a function that takes a single value\nand returns a string with the CSS attribute-value pair.\n\n.apply_index() (level-wise): accepts a function that takes a Series and\nreturns a Series, or numpy array with an identical shape where each element is\na string with a CSS attribute-value pair. This method passes each level of\nyour Index one-at-a-time. To style the index use `axis=0` and to style the\ncolumn headers use `axis=1`.\n\nYou can select a `level` of a `MultiIndex` but currently no similar `subset`\napplication is available for these methods.\n\nTable captions can be added with the .set_caption() method. You can use table\nstyles to control the CSS relevant to the caption.\n\nAdding tooltips (since version 1.3.0) can be done using the .set_tooltips()\nmethod in the same way you can add CSS classes to data cells by providing a\nstring based DataFrame with intersecting indices and columns. You don\u2019t have\nto specify a `css_class` name or any css `props` for the tooltips, since there\nare standard defaults, but the option is there if you want more visual\ncontrol.\n\nThe only thing left to do for our table is to add the highlighting borders to\ndraw the audience attention to the tooltips. We will create internal CSS\nclasses as before using table styles. Setting classes always overwrites so we\nneed to make sure we add the previous classes.\n\nThe examples we have shown so far for the `Styler.apply` and `Styler.applymap`\nfunctions have not demonstrated the use of the `subset` argument. This is a\nuseful argument which permits a lot of flexibility: it allows you to apply\nstyles to specific rows or columns, without having to code that logic into\nyour `style` function.\n\nThe value passed to `subset` behaves similar to slicing a DataFrame;\n\nA scalar is treated as a column label\n\nA list (or Series or NumPy array) is treated as multiple column labels\n\nA tuple is treated as `(row_indexer, column_indexer)`\n\nConsider using `pd.IndexSlice` to construct the tuple for the last one. We\nwill create a MultiIndexed DataFrame to demonstrate the functionality.\n\nWe will use subset to highlight the maximum in the third and fourth columns\nwith red text. We will highlight the subset sliced region in yellow.\n\nIf combined with the `IndexSlice` as suggested then it can index across both\ndimensions with greater flexibility.\n\nThis also provides the flexibility to sub select rows when used with the\n`axis=1`.\n\nThere is also scope to provide conditional filtering.\n\nSuppose we want to highlight the maximum across columns 2 and 4 only in the\ncase that the sum of columns 1 and 3 is less than -2.0 (essentially excluding\nrows `(:,'r2')`).\n\nOnly label-based slicing is supported right now, not positional, and not\ncallables.\n\nIf your style function uses a `subset` or `axis` keyword argument, consider\nwrapping your function in a `functools.partial`, partialing out that keyword.\n\nGenerally, for smaller tables and most cases, the rendered HTML does not need\nto be optimized, and we don\u2019t really recommend it. There are two cases where\nit is worth considering:\n\nIf you are rendering and styling a very large HTML table, certain browsers\nhave performance issues.\n\nIf you are using `Styler` to dynamically create part of online user interfaces\nand want to improve network performance.\n\nHere we recommend the following steps to implement:\n\nIgnore the `uuid` and set `cell_ids` to `False`. This will prevent unnecessary\nHTML.\n\nThis is sub-optimal:\n\nThis is better:\n\nUse table styles where possible (e.g. for all cells or rows or columns at a\ntime) since the CSS is nearly always more efficient than other formats.\n\nThis is sub-optimal:\n\nThis is better:\n\nFor large DataFrames where the same style is applied to many cells it can be\nmore efficient to declare the styles as classes and then apply those classes\nto data cells, rather than directly applying styles to cells. It is, however,\nprobably still easier to use the Styler function api when you are not\nconcerned about optimization.\n\nThis is sub-optimal:\n\nThis is better:\n\nTooltips require `cell_ids` to work and they generate extra HTML elements for\nevery data cell.\n\nYou can remove unnecessary HTML, or shorten the default class names by\nreplacing the default css dict. You can read a little more about CSS below.\n\nSome styling functions are common enough that we\u2019ve \u201cbuilt them in\u201d to the\n`Styler`, so you don\u2019t have to write them and apply them yourself. The current\nlist of such functions is:\n\n.highlight_null: for use with identifying missing data.\n\n.highlight_min and .highlight_max: for use with identifying extremeties in\ndata.\n\n.highlight_between and .highlight_quantile: for use with identifying classes\nwithin data.\n\n.background_gradient: a flexible method for highlighting cells based or their,\nor other, values on a numeric scale.\n\n.text_gradient: similar method for highlighting text based on their, or other,\nvalues on a numeric scale.\n\n.bar: to display mini-charts within cell backgrounds.\n\nThe individual documentation on each function often gives more examples of\ntheir arguments.\n\nThis method accepts ranges as float, or NumPy arrays or Series provided the\nindexes match.\n\nUseful for detecting the highest or lowest percentile values\n\nYou can create \u201cheatmaps\u201d with the `background_gradient` and `text_gradient`\nmethods. These require matplotlib, and we\u2019ll use Seaborn to get a nice\ncolormap.\n\n.background_gradient and .text_gradient have a number of keyword arguments to\ncustomise the gradients and colors. See the documentation.\n\nUse `Styler.set_properties` when the style doesn\u2019t actually depend on the\nvalues. This is just a simple wrapper for `.applymap` where the function\nreturns the same properties for all cells.\n\nYou can include \u201cbar charts\u201d in your DataFrame.\n\nAdditional keyword arguments give more control on centering and positioning,\nand you can pass a list of `[color_negative, color_positive]` to highlight\nlower and higher values or a matplotlib colormap.\n\nTo showcase an example here\u2019s how you can change the above with the new\n`align` option, combined with setting `vmin` and `vmax` limits, the `width` of\nthe figure, and underlying css `props` of cells, leaving space to display the\ntext and the bars. We also use `text_gradient` to color the text the same as\nthe bars using a matplotlib colormap (although in this case the visualization\nis probably better without this additional effect).\n\nThe following example aims to give a highlight of the behavior of the new\nalign options:\n\nSay you have a lovely style built up for a DataFrame, and now you want to\napply the same style to a second DataFrame. Export the style with\n`df1.style.export`, and import it on the second DataFrame with `df1.style.set`\n\nNotice that you\u2019re able to share the styles even though they\u2019re data aware.\nThe styles are re-evaluated on the new DataFrame they\u2019ve been `use`d upon.\n\nDataFrame only (use `Series.to_frame().style`)\n\nThe index and columns do not need to be unique, but certain styling functions\ncan only work with unique indexes.\n\nNo large repr, and construction performance isn\u2019t great; although we have some\nHTML optimizations\n\nYou can only apply styles, you can\u2019t insert new HTML entities, except via\nsubclassing.\n\nHere are a few interesting examples.\n\n`Styler` interacts pretty well with widgets. If you\u2019re viewing this online\ninstead of running the notebook yourself, you\u2019re missing out on interactively\nadjusting the color palette.\n\nIf you display a large matrix or DataFrame in a notebook, but you want to\nalways see the column and row headers you can use the .set_sticky method which\nmanipulates the table styles CSS.\n\nIt is also possible to stick MultiIndexes and even only specific levels.\n\nSuppose you have to display HTML within HTML, that can be a bit of pain when\nthe renderer can\u2019t distinguish. You can use the `escape` formatting option to\nhandle this, and even use it within a formatter that contains HTML itself.\n\nSome support (since version 0.20.0) is available for exporting styled\n`DataFrames` to Excel worksheets using the `OpenPyXL` or `XlsxWriter` engines.\nCSS2.2 properties handled include:\n\n`background-color`\n\n`color`\n\n`font-family`\n\n`font-style`\n\n`font-weight`\n\n`text-align`\n\n`text-decoration`\n\n`vertical-align`\n\n`white-space: nowrap`\n\nCurrently broken: `border-style`, `border-width`, `border-color` and their\n{`top`, `right`, `bottom`, `left` variants}\n\nOnly CSS2 named colors and hex colors of the form `#rgb` or `#rrggbb` are\ncurrently supported.\n\nThe following pseudo CSS properties are also available to set excel specific\nstyle properties:\n\n`number-format`\n\nTable level styles, and data cell CSS-classes are not included in the export\nto Excel: individual cells must have their properties mapped by the\n`Styler.apply` and/or `Styler.applymap` methods.\n\nA screenshot of the output:\n\nThere is support (since version 1.3.0) to export `Styler` to LaTeX. The\ndocumentation for the .to_latex method gives further detail and numerous\nexamples.\n\nCascading Style Sheet (CSS) language, which is designed to influence how a\nbrowser renders HTML elements, has its own peculiarities. It never reports\nerrors: it just silently ignores them and doesn\u2019t render your objects how you\nintend so can sometimes be frustrating. Here is a very brief primer on how\n`Styler` creates HTML and interacts with CSS, with advice on common pitfalls\nto avoid.\n\nThe precise structure of the CSS `class` attached to each cell is as follows.\n\nCells with Index and Column names include `index_name` and `level<k>` where\n`k` is its level in a MultiIndex\n\nIndex label cells include\n\n`row_heading`\n\n`level<k>` where `k` is the level in a MultiIndex\n\n`row<m>` where `m` is the numeric position of the row\n\nColumn label cells include\n\n`col_heading`\n\n`level<k>` where `k` is the level in a MultiIndex\n\n`col<n>` where `n` is the numeric position of the column\n\nData cells include\n\n`data`\n\n`row<m>`, where `m` is the numeric position of the cell.\n\n`col<n>`, where `n` is the numeric position of the cell.\n\nBlank cells include `blank`\n\nTrimmed cells include `col_trim` or `row_trim`\n\nThe structure of the `id` is `T_uuid_level<k>_row<m>_col<n>` where `level<k>`\nis used only on headings, and headings will only have either `row<m>` or\n`col<n>` whichever is needed. By default we\u2019ve also prepended each row/column\nidentifier with a UUID unique to each DataFrame so that the style from one\ndoesn\u2019t collide with the styling from another within the same notebook or\npage. You can read more about the use of UUIDs in Optimization.\n\nWe can see example of the HTML by calling the .to_html() method.\n\nThe examples have shown that when CSS styles overlap, the one that comes last\nin the HTML render, takes precedence. So the following yield different\nresults:\n\nThis is only true for CSS rules that are equivalent in hierarchy, or\nimportance. You can read more about CSS specificity here but for our purposes\nit suffices to summarize the key points:\n\nA CSS importance score for each HTML element is derived by starting at zero\nand adding:\n\n1000 for an inline style attribute\n\n100 for each ID\n\n10 for each attribute, class or pseudo-class\n\n1 for each element name or pseudo-element\n\nLet\u2019s use this to describe the action of the following configurations\n\nThis text is red because the generated selector `#T_a_ td` is worth 101 (ID\nplus element), whereas `#T_a_row0_col0` is only worth 100 (ID), so is\nconsidered inferior even though in the HTML it comes after the previous.\n\nIn the above case the text is blue because the selector `#T_b_ .cls-1` is\nworth 110 (ID plus class), which takes precendence.\n\nNow we have created another table style this time the selector `T_c_ td.data`\n(ID plus element plus class) gets bumped up to 111.\n\nIf your style fails to be applied, and its really frustrating, try the\n`!important` trump card.\n\nFinally got that green text after all!\n\nThe core of pandas is, and will remain, its \u201chigh-performance, easy-to-use\ndata structures\u201d. With that in mind, we hope that `DataFrame.style`\naccomplishes two goals\n\nProvide an API that is pleasing to use interactively and is \u201cgood enough\u201d for\nmany tasks\n\nProvide the foundations for dedicated libraries to build on\n\nIf you build a great library on top of this, let us know and we\u2019ll link to it.\n\nIf the default template doesn\u2019t quite suit your needs, you can subclass Styler\nand extend or override the template. We\u2019ll show an example of extending the\ndefault template to insert a custom header before each table.\n\nWe\u2019ll use the following template:\n\nNow that we\u2019ve created a template, we need to set up a subclass of `Styler`\nthat knows about it.\n\nNotice that we include the original loader in our environment\u2019s loader. That\u2019s\nbecause we extend the original template, so the Jinja environment needs to be\nable to find it.\n\nNow we can use that custom styler. It\u2019s `__init__` takes a DataFrame.\n\nOur custom template accepts a `table_title` keyword. We can provide the value\nin the `.to_html` method.\n\nFor convenience, we provide the `Styler.from_custom_template` method that does\nthe same as the custom subclass.\n\nHere\u2019s the template structure for the both the style generation template and\nthe table generation template:\n\nStyle template:\n\nTable template:\n\nSee the template in the GitHub repo for more details.\n\n"}, {"name": "Time deltas", "path": "user_guide/timedeltas", "type": "Manual", "text": "\nTimedeltas are differences in times, expressed in difference units, e.g. days,\nhours, minutes, seconds. They can be both positive and negative.\n\n`Timedelta` is a subclass of `datetime.timedelta`, and behaves in a similar\nmanner, but allows compatibility with `np.timedelta64` types as well as a host\nof custom representation, parsing, and attributes.\n\nYou can construct a `Timedelta` scalar through various arguments, including\nISO 8601 Duration strings.\n\nDateOffsets (`Day, Hour, Minute, Second, Milli, Micro, Nano`) can also be used\nin construction.\n\nFurther, operations among the scalars yield another scalar `Timedelta`.\n\nUsing the top-level `pd.to_timedelta`, you can convert a scalar, array, list,\nor Series from a recognized timedelta format / value into a `Timedelta` type.\nIt will construct Series if the input is a Series, a scalar if the input is\nscalar-like, otherwise it will output a `TimedeltaIndex`.\n\nYou can parse a single string to a Timedelta:\n\nor a list/array of strings:\n\nThe `unit` keyword argument specifies the unit of the Timedelta if the input\nis numeric:\n\nWarning\n\nIf a string or array of strings is passed as an input then the `unit` keyword\nargument will be ignored. If a string without units is passed then the default\nunit of nanoseconds is assumed.\n\npandas represents `Timedeltas` in nanosecond resolution using 64 bit integers.\nAs such, the 64 bit integer limits determine the `Timedelta` limits.\n\nYou can operate on Series/DataFrames and construct `timedelta64[ns]` Series\nthrough subtraction operations on `datetime64[ns]` Series, or `Timestamps`.\n\nOperations with scalars from a `timedelta64[ns]` series:\n\nSeries of timedeltas with `NaT` values are supported:\n\nElements can be set to `NaT` using `np.nan` analogously to datetimes:\n\nOperands can also appear in a reversed order (a singular object operated with\na Series):\n\n`min, max` and the corresponding `idxmin, idxmax` operations are supported on\nframes:\n\n`min, max, idxmin, idxmax` operations are supported on Series as well. A\nscalar result will be a `Timedelta`.\n\nYou can fillna on timedeltas, passing a timedelta to get a particular value.\n\nYou can also negate, multiply and use `abs` on `Timedeltas`:\n\nNumeric reduction operation for `timedelta64[ns]` will return `Timedelta`\nobjects. As usual `NaT` are skipped during evaluation.\n\nTimedelta Series, `TimedeltaIndex`, and `Timedelta` scalars can be converted\nto other \u2018frequencies\u2019 by dividing by another timedelta, or by astyping to a\nspecific timedelta type. These operations yield Series and propagate `NaT` ->\n`nan`. Note that division by the NumPy scalar is true division, while astyping\nis equivalent of floor division.\n\nDividing or multiplying a `timedelta64[ns]` Series by an integer or integer\nSeries yields another `timedelta64[ns]` dtypes Series.\n\nRounded division (floor-division) of a `timedelta64[ns]` Series by a scalar\n`Timedelta` gives a series of integers.\n\nThe mod (%) and divmod operations are defined for `Timedelta` when operating\nwith another timedelta-like or with a numeric argument.\n\nYou can access various components of the `Timedelta` or `TimedeltaIndex`\ndirectly using the attributes `days,seconds,microseconds,nanoseconds`. These\nare identical to the values returned by `datetime.timedelta`, in that, for\nexample, the `.seconds` attribute represents the number of seconds >= 0 and <\n1 day. These are signed according to whether the `Timedelta` is signed.\n\nThese operations can also be directly accessed via the `.dt` property of the\n`Series` as well.\n\nNote\n\nNote that the attributes are NOT the displayed values of the `Timedelta`. Use\n`.components` to retrieve the displayed values.\n\nFor a `Series`:\n\nYou can access the value of the fields for a scalar `Timedelta` directly.\n\nYou can use the `.components` property to access a reduced form of the\ntimedelta. This returns a `DataFrame` indexed similarly to the `Series`. These\nare the displayed values of the `Timedelta`.\n\nYou can convert a `Timedelta` to an ISO 8601 Duration string with the\n`.isoformat` method\n\nTo generate an index with time delta, you can use either the `TimedeltaIndex`\nor the `timedelta_range()` constructor.\n\nUsing `TimedeltaIndex` you can pass string-like, `Timedelta`, `timedelta`, or\n`np.timedelta64` objects. Passing `np.nan/pd.NaT/nat` will represent missing\nvalues.\n\nThe string \u2018infer\u2019 can be passed in order to set the frequency of the index as\nthe inferred frequency upon creation:\n\nSimilar to `date_range()`, you can construct regular ranges of a\n`TimedeltaIndex` using `timedelta_range()`. The default frequency for\n`timedelta_range` is calendar day:\n\nVarious combinations of `start`, `end`, and `periods` can be used with\n`timedelta_range`:\n\nThe `freq` parameter can passed a variety of frequency aliases:\n\nSpecifying `start`, `end`, and `periods` will generate a range of evenly\nspaced timedeltas from `start` to `end` inclusively, with `periods` number of\nelements in the resulting `TimedeltaIndex`:\n\nSimilarly to other of the datetime-like indices, `DatetimeIndex` and\n`PeriodIndex`, you can use `TimedeltaIndex` as the index of pandas objects.\n\nSelections work similarly, with coercion on string-likes and slices:\n\nFurthermore you can use partial string selection and the range will be\ninferred:\n\nFinally, the combination of `TimedeltaIndex` with `DatetimeIndex` allow\ncertain combination operations that are NaT preserving:\n\nSimilarly to frequency conversion on a `Series` above, you can convert these\nindices to yield another Index.\n\nScalars type ops work as well. These can potentially return a different type\nof index.\n\nSimilar to timeseries resampling, we can resample with a `TimedeltaIndex`.\n\n"}, {"name": "Time series / date functionality", "path": "user_guide/timeseries", "type": "Manual", "text": "\npandas contains extensive capabilities and features for working with time\nseries data for all domains. Using the NumPy `datetime64` and `timedelta64`\ndtypes, pandas has consolidated a large number of features from other Python\nlibraries like `scikits.timeseries` as well as created a tremendous amount of\nnew functionality for manipulating time series data.\n\nFor example, pandas supports:\n\nParsing time series information from various sources and formats\n\nGenerate sequences of fixed-frequency dates and time spans\n\nManipulating and converting date times with timezone information\n\nResampling or converting a time series to a particular frequency\n\nPerforming date and time arithmetic with absolute or relative time increments\n\npandas provides a relatively compact and self-contained set of tools for\nperforming the above tasks and more.\n\npandas captures 4 general time related concepts:\n\nDate times: A specific date and time with timezone support. Similar to\n`datetime.datetime` from the standard library.\n\nTime deltas: An absolute time duration. Similar to `datetime.timedelta` from\nthe standard library.\n\nTime spans: A span of time defined by a point in time and its associated\nfrequency.\n\nDate offsets: A relative time duration that respects calendar arithmetic.\nSimilar to `dateutil.relativedelta.relativedelta` from the `dateutil` package.\n\nConcept\n\nScalar Class\n\nArray Class\n\npandas Data Type\n\nPrimary Creation Method\n\nDate times\n\n`Timestamp`\n\n`DatetimeIndex`\n\n`datetime64[ns]` or `datetime64[ns, tz]`\n\n`to_datetime` or `date_range`\n\nTime deltas\n\n`Timedelta`\n\n`TimedeltaIndex`\n\n`timedelta64[ns]`\n\n`to_timedelta` or `timedelta_range`\n\nTime spans\n\n`Period`\n\n`PeriodIndex`\n\n`period[freq]`\n\n`Period` or `period_range`\n\nDate offsets\n\n`DateOffset`\n\n`None`\n\n`None`\n\n`DateOffset`\n\nFor time series data, it\u2019s conventional to represent the time component in the\nindex of a `Series` or `DataFrame` so manipulations can be performed with\nrespect to the time element.\n\nHowever, `Series` and `DataFrame` can directly also support the time component\nas data itself.\n\n`Series` and `DataFrame` have extended data type support and functionality for\n`datetime`, `timedelta` and `Period` data when passed into those constructors.\n`DateOffset` data however will be stored as `object` data.\n\nLastly, pandas represents null date times, time deltas, and time spans as\n`NaT` which is useful for representing missing or null date like values and\nbehaves similar as `np.nan` does for float data.\n\nTimestamped data is the most basic type of time series data that associates\nvalues with points in time. For pandas objects it means using the points in\ntime.\n\nHowever, in many cases it is more natural to associate things like change\nvariables with a time span instead. The span represented by `Period` can be\nspecified explicitly, or inferred from datetime string format.\n\nFor example:\n\n`Timestamp` and `Period` can serve as an index. Lists of `Timestamp` and\n`Period` are automatically coerced to `DatetimeIndex` and `PeriodIndex`\nrespectively.\n\npandas allows you to capture both representations and convert between them.\nUnder the hood, pandas represents timestamps using instances of `Timestamp`\nand sequences of timestamps using instances of `DatetimeIndex`. For regular\ntime spans, pandas uses `Period` objects for scalar values and `PeriodIndex`\nfor sequences of spans. Better support for irregular intervals with arbitrary\nstart and end points are forth-coming in future releases.\n\nTo convert a `Series` or list-like object of date-like objects e.g. strings,\nepochs, or a mixture, you can use the `to_datetime` function. When passed a\n`Series`, this returns a `Series` (with the same index), while a list-like is\nconverted to a `DatetimeIndex`:\n\nIf you use dates which start with the day first (i.e. European style), you can\npass the `dayfirst` flag:\n\nWarning\n\nYou see in the above example that `dayfirst` isn\u2019t strict. If a date can\u2019t be\nparsed with the day being first it will be parsed as if `dayfirst` were False,\nand in the case of parsing delimited date strings (e.g. `31-12-2012`) then a\nwarning will also be raised.\n\nIf you pass a single string to `to_datetime`, it returns a single `Timestamp`.\n`Timestamp` can also accept string input, but it doesn\u2019t accept string parsing\noptions like `dayfirst` or `format`, so use `to_datetime` if these are\nrequired.\n\nYou can also use the `DatetimeIndex` constructor directly:\n\nThe string \u2018infer\u2019 can be passed in order to set the frequency of the index as\nthe inferred frequency upon creation:\n\nIn addition to the required datetime string, a `format` argument can be passed\nto ensure specific parsing. This could also potentially speed up the\nconversion considerably.\n\nFor more information on the choices available when specifying the `format`\noption, see the Python datetime documentation.\n\nYou can also pass a `DataFrame` of integer or string columns to assemble into\na `Series` of `Timestamps`.\n\nYou can pass only the columns that you need to assemble.\n\n`pd.to_datetime` looks for standard designations of the datetime component in\nthe column names, including:\n\nrequired: `year`, `month`, `day`\n\noptional: `hour`, `minute`, `second`, `millisecond`, `microsecond`,\n`nanosecond`\n\nThe default behavior, `errors='raise'`, is to raise when unparsable:\n\nPass `errors='ignore'` to return the original input when unparsable:\n\nPass `errors='coerce'` to convert unparsable data to `NaT` (not a time):\n\npandas supports converting integer or float epoch times to `Timestamp` and\n`DatetimeIndex`. The default unit is nanoseconds, since that is how\n`Timestamp` objects are stored internally. However, epochs are often stored in\nanother `unit` which can be specified. These are computed from the starting\npoint specified by the `origin` parameter.\n\nNote\n\nThe `unit` parameter does not use the same strings as the `format` parameter\nthat was discussed above). The available units are listed on the documentation\nfor `pandas.to_datetime()`.\n\nChanged in version 1.0.0.\n\nConstructing a `Timestamp` or `DatetimeIndex` with an epoch timestamp with the\n`tz` argument specified will raise a ValueError. If you have epochs in wall\ntime in another timezone, you can read the epochs as timezone-naive timestamps\nand then localize to the appropriate timezone:\n\nNote\n\nEpoch times will be rounded to the nearest nanosecond.\n\nWarning\n\nConversion of float epoch times can lead to inaccurate and unexpected results.\nPython floats have about 15 digits precision in decimal. Rounding during\nconversion from float to high precision `Timestamp` is unavoidable. The only\nway to achieve exact precision is to use a fixed-width types (e.g. an int64).\n\nSee also\n\nUsing the origin Parameter\n\nTo invert the operation from above, namely, to convert from a `Timestamp` to a\n\u2018unix\u2019 epoch:\n\nWe subtract the epoch (midnight at January 1, 1970 UTC) and then floor divide\nby the \u201cunit\u201d (1 second).\n\nUsing the `origin` parameter, one can specify an alternative starting point\nfor creation of a `DatetimeIndex`. For example, to use 1960-01-01 as the\nstarting date:\n\nThe default is set at `origin='unix'`, which defaults to `1970-01-01\n00:00:00`. Commonly called \u2018unix epoch\u2019 or POSIX time.\n\nTo generate an index with timestamps, you can use either the `DatetimeIndex`\nor `Index` constructor and pass in a list of datetime objects:\n\nIn practice this becomes very cumbersome because we often need a very long\nindex with a large number of timestamps. If we need timestamps on a regular\nfrequency, we can use the `date_range()` and `bdate_range()` functions to\ncreate a `DatetimeIndex`. The default frequency for `date_range` is a calendar\nday while the default for `bdate_range` is a business day:\n\nConvenience functions like `date_range` and `bdate_range` can utilize a\nvariety of frequency aliases:\n\n`date_range` and `bdate_range` make it easy to generate a range of dates using\nvarious combinations of parameters like `start`, `end`, `periods`, and `freq`.\nThe start and end dates are strictly inclusive, so dates outside of those\nspecified will not be generated:\n\nSpecifying `start`, `end`, and `periods` will generate a range of evenly\nspaced dates from `start` to `end` inclusively, with `periods` number of\nelements in the resulting `DatetimeIndex`:\n\n`bdate_range` can also generate a range of custom frequency dates by using the\n`weekmask` and `holidays` parameters. These parameters will only be used if a\ncustom frequency string is passed.\n\nSee also\n\nCustom business days\n\nSince pandas represents timestamps in nanosecond resolution, the time span\nthat can be represented using a 64-bit integer is limited to approximately 584\nyears:\n\nSee also\n\nRepresenting out-of-bounds spans\n\nOne of the main uses for `DatetimeIndex` is as an index for pandas objects.\nThe `DatetimeIndex` class contains many time series related optimizations:\n\nA large range of dates for various offsets are pre-computed and cached under\nthe hood in order to make generating subsequent date ranges very fast (just\nhave to grab a slice).\n\nFast shifting using the `shift` method on pandas objects.\n\nUnioning of overlapping `DatetimeIndex` objects with the same frequency is\nvery fast (important for fast data alignment).\n\nQuick access to date fields via properties such as `year`, `month`, etc.\n\nRegularization functions like `snap` and very fast `asof` logic.\n\n`DatetimeIndex` objects have all the basic functionality of regular `Index`\nobjects, and a smorgasbord of advanced time series specific methods for easy\nfrequency processing.\n\nSee also\n\nReindexing methods\n\nNote\n\nWhile pandas does not force you to have a sorted date index, some of these\nmethods may have unexpected or incorrect behavior if the dates are unsorted.\n\n`DatetimeIndex` can be used like a regular index and offers all of its\nintelligent functionality like selection, slicing, etc.\n\nDates and strings that parse to timestamps can be passed as indexing\nparameters:\n\nTo provide convenience for accessing longer time series, you can also pass in\nthe year or year and month as strings:\n\nThis type of slicing will work on a `DataFrame` with a `DatetimeIndex` as\nwell. Since the partial string selection is a form of label slicing, the\nendpoints will be included. This would include matching times on an included\ndate:\n\nWarning\n\nIndexing `DataFrame` rows with a single string with getitem (e.g.\n`frame[dtstring]`) is deprecated starting with pandas 1.2.0 (given the\nambiguity whether it is indexing the rows or selecting a column) and will be\nremoved in a future version. The equivalent with `.loc` (e.g.\n`frame.loc[dtstring]`) is still supported.\n\nThis starts on the very first time in the month, and includes the last date\nand time for the month:\n\nThis specifies a stop time that includes all of the times on the last day:\n\nThis specifies an exact stop time (and is not the same as the above):\n\nWe are stopping on the included end-point as it is part of the index:\n\n`DatetimeIndex` partial string indexing also works on a `DataFrame` with a\n`MultiIndex`:\n\nNew in version 0.25.0.\n\nSlicing with string indexing also honors UTC offset.\n\nThe same string used as an indexing parameter can be treated either as a slice\nor as an exact match depending on the resolution of the index. If the string\nis less accurate than the index, it will be treated as a slice, otherwise as\nan exact match.\n\nConsider a `Series` object with a minute resolution index:\n\nA timestamp string less accurate than a minute gives a `Series` object.\n\nA timestamp string with minute resolution (or more accurate), gives a scalar\ninstead, i.e. it is not casted to a slice.\n\nIf index resolution is second, then the minute-accurate timestamp gives a\n`Series`.\n\nIf the timestamp string is treated as a slice, it can be used to index\n`DataFrame` with `.loc[]` as well.\n\nWarning\n\nHowever, if the string is treated as an exact match, the selection in\n`DataFrame`\u2019s `[]` will be column-wise and not row-wise, see Indexing Basics.\nFor example `dft_minute['2011-12-31 23:59']` will raise `KeyError` as\n`'2012-12-31 23:59'` has the same resolution as the index and there is no\ncolumn with such name:\n\nTo always have unambiguous selection, whether the row is treated as a slice or\na single selection, use `.loc`.\n\nNote also that `DatetimeIndex` resolution cannot be less precise than day.\n\nAs discussed in previous section, indexing a `DatetimeIndex` with a partial\nstring depends on the \u201caccuracy\u201d of the period, in other words how specific\nthe interval is in relation to the resolution of the index. In contrast,\nindexing with `Timestamp` or `datetime` objects is exact, because the objects\nhave exact meaning. These also follow the semantics of including both\nendpoints.\n\nThese `Timestamp` and `datetime` objects have exact `hours, minutes,` and\n`seconds`, even though they were not explicitly specified (they are `0`).\n\nWith no defaults.\n\nA `truncate()` convenience function is provided that is similar to slicing.\nNote that `truncate` assumes a 0 value for any unspecified date component in a\n`DatetimeIndex` in contrast to slicing which returns any partially matching\ndates:\n\nEven complicated fancy indexing that breaks the `DatetimeIndex` frequency\nregularity will result in a `DatetimeIndex`, although frequency is lost:\n\nThere are several time/date properties that one can access from `Timestamp` or\na collection of timestamps like a `DatetimeIndex`.\n\nProperty\n\nDescription\n\nyear\n\nThe year of the datetime\n\nmonth\n\nThe month of the datetime\n\nday\n\nThe days of the datetime\n\nhour\n\nThe hour of the datetime\n\nminute\n\nThe minutes of the datetime\n\nsecond\n\nThe seconds of the datetime\n\nmicrosecond\n\nThe microseconds of the datetime\n\nnanosecond\n\nThe nanoseconds of the datetime\n\ndate\n\nReturns datetime.date (does not contain timezone information)\n\ntime\n\nReturns datetime.time (does not contain timezone information)\n\ntimetz\n\nReturns datetime.time as local time with timezone information\n\ndayofyear\n\nThe ordinal day of year\n\nday_of_year\n\nThe ordinal day of year\n\nweekofyear\n\nThe week ordinal of the year\n\nweek\n\nThe week ordinal of the year\n\ndayofweek\n\nThe number of the day of the week with Monday=0, Sunday=6\n\nday_of_week\n\nThe number of the day of the week with Monday=0, Sunday=6\n\nweekday\n\nThe number of the day of the week with Monday=0, Sunday=6\n\nquarter\n\nQuarter of the date: Jan-Mar = 1, Apr-Jun = 2, etc.\n\ndays_in_month\n\nThe number of days in the month of the datetime\n\nis_month_start\n\nLogical indicating if first day of month (defined by frequency)\n\nis_month_end\n\nLogical indicating if last day of month (defined by frequency)\n\nis_quarter_start\n\nLogical indicating if first day of quarter (defined by frequency)\n\nis_quarter_end\n\nLogical indicating if last day of quarter (defined by frequency)\n\nis_year_start\n\nLogical indicating if first day of year (defined by frequency)\n\nis_year_end\n\nLogical indicating if last day of year (defined by frequency)\n\nis_leap_year\n\nLogical indicating if the date belongs to a leap year\n\nFurthermore, if you have a `Series` with datetimelike values, then you can\naccess these properties via the `.dt` accessor, as detailed in the section on\n.dt accessors.\n\nNew in version 1.1.0.\n\nYou may obtain the year, week and day components of the ISO year from the ISO\n8601 standard:\n\nIn the preceding examples, frequency strings (e.g. `'D'`) were used to specify\na frequency that defined:\n\nhow the date times in `DatetimeIndex` were spaced when using `date_range()`\n\nthe frequency of a `Period` or `PeriodIndex`\n\nThese frequency strings map to a `DateOffset` object and its subclasses. A\n`DateOffset` is similar to a `Timedelta` that represents a duration of time\nbut follows specific calendar duration rules. For example, a `Timedelta` day\nwill always increment `datetimes` by 24 hours, while a `DateOffset` day will\nincrement `datetimes` to the same time the next day whether a day represents\n23, 24 or 25 hours due to daylight savings time. However, all `DateOffset`\nsubclasses that are an hour or smaller (`Hour`, `Minute`, `Second`, `Milli`,\n`Micro`, `Nano`) behave like `Timedelta` and respect absolute time.\n\nThe basic `DateOffset` acts similar to `dateutil.relativedelta` (relativedelta\ndocumentation) that shifts a date time by the corresponding calendar duration\nspecified. The arithmetic operator (`+`) can be used to perform the shift.\n\nMost `DateOffsets` have associated frequencies strings, or offset aliases,\nthat can be passed into `freq` keyword arguments. The available date offsets\nand associated frequency strings can be found below:\n\nDate Offset\n\nFrequency String\n\nDescription\n\n`DateOffset`\n\nNone\n\nGeneric offset class, defaults to absolute 24 hours\n\n`BDay` or `BusinessDay`\n\n`'B'`\n\nbusiness day (weekday)\n\n`CDay` or `CustomBusinessDay`\n\n`'C'`\n\ncustom business day\n\n`Week`\n\n`'W'`\n\none week, optionally anchored on a day of the week\n\n`WeekOfMonth`\n\n`'WOM'`\n\nthe x-th day of the y-th week of each month\n\n`LastWeekOfMonth`\n\n`'LWOM'`\n\nthe x-th day of the last week of each month\n\n`MonthEnd`\n\n`'M'`\n\ncalendar month end\n\n`MonthBegin`\n\n`'MS'`\n\ncalendar month begin\n\n`BMonthEnd` or `BusinessMonthEnd`\n\n`'BM'`\n\nbusiness month end\n\n`BMonthBegin` or `BusinessMonthBegin`\n\n`'BMS'`\n\nbusiness month begin\n\n`CBMonthEnd` or `CustomBusinessMonthEnd`\n\n`'CBM'`\n\ncustom business month end\n\n`CBMonthBegin` or `CustomBusinessMonthBegin`\n\n`'CBMS'`\n\ncustom business month begin\n\n`SemiMonthEnd`\n\n`'SM'`\n\n15th (or other day_of_month) and calendar month end\n\n`SemiMonthBegin`\n\n`'SMS'`\n\n15th (or other day_of_month) and calendar month begin\n\n`QuarterEnd`\n\n`'Q'`\n\ncalendar quarter end\n\n`QuarterBegin`\n\n`'QS'`\n\ncalendar quarter begin\n\n`BQuarterEnd`\n\n`'BQ`\n\nbusiness quarter end\n\n`BQuarterBegin`\n\n`'BQS'`\n\nbusiness quarter begin\n\n`FY5253Quarter`\n\n`'REQ'`\n\nretail (aka 52-53 week) quarter\n\n`YearEnd`\n\n`'A'`\n\ncalendar year end\n\n`YearBegin`\n\n`'AS'` or `'BYS'`\n\ncalendar year begin\n\n`BYearEnd`\n\n`'BA'`\n\nbusiness year end\n\n`BYearBegin`\n\n`'BAS'`\n\nbusiness year begin\n\n`FY5253`\n\n`'RE'`\n\nretail (aka 52-53 week) year\n\n`Easter`\n\nNone\n\nEaster holiday\n\n`BusinessHour`\n\n`'BH'`\n\nbusiness hour\n\n`CustomBusinessHour`\n\n`'CBH'`\n\ncustom business hour\n\n`Day`\n\n`'D'`\n\none absolute day\n\n`Hour`\n\n`'H'`\n\none hour\n\n`Minute`\n\n`'T'` or `'min'`\n\none minute\n\n`Second`\n\n`'S'`\n\none second\n\n`Milli`\n\n`'L'` or `'ms'`\n\none millisecond\n\n`Micro`\n\n`'U'` or `'us'`\n\none microsecond\n\n`Nano`\n\n`'N'`\n\none nanosecond\n\n`DateOffsets` additionally have `rollforward()` and `rollback()` methods for\nmoving a date forward or backward respectively to a valid offset date relative\nto the offset. For example, business offsets will roll dates that land on the\nweekends (Saturday and Sunday) forward to Monday since business offsets\noperate on the weekdays.\n\nThese operations preserve time (hour, minute, etc) information by default. To\nreset time to midnight, use `normalize()` before or after applying the\noperation (depending on whether you want the time information included in the\noperation).\n\nSome of the offsets can be \u201cparameterized\u201d when created to result in different\nbehaviors. For example, the `Week` offset for generating weekly data accepts a\n`weekday` parameter which results in the generated dates always lying on a\nparticular day of the week:\n\nThe `normalize` option will be effective for addition and subtraction.\n\nAnother example is parameterizing `YearEnd` with the specific ending month:\n\nOffsets can be used with either a `Series` or `DatetimeIndex` to apply the\noffset to each element.\n\nIf the offset class maps directly to a `Timedelta` (`Day`, `Hour`, `Minute`,\n`Second`, `Micro`, `Milli`, `Nano`) it can be used exactly like a `Timedelta`\n\\- see the Timedelta section for more examples.\n\nNote that some offsets (such as `BQuarterEnd`) do not have a vectorized\nimplementation. They can still be used but may calculate significantly slower\nand will show a `PerformanceWarning`\n\nThe `CDay` or `CustomBusinessDay` class provides a parametric `BusinessDay`\nclass which can be used to create customized business day calendars which\naccount for local holidays and local weekend conventions.\n\nAs an interesting example, let\u2019s look at Egypt where a Friday-Saturday weekend\nis observed.\n\nLet\u2019s map to the weekday names:\n\nHoliday calendars can be used to provide the list of holidays. See the holiday\ncalendar section for more information.\n\nMonthly offsets that respect a certain holiday calendar can be defined in the\nusual way.\n\nNote\n\nThe frequency string \u2018C\u2019 is used to indicate that a CustomBusinessDay\nDateOffset is used, it is important to note that since CustomBusinessDay is a\nparameterised type, instances of CustomBusinessDay may differ and this is not\ndetectable from the \u2018C\u2019 frequency string. The user therefore needs to ensure\nthat the \u2018C\u2019 frequency string is used consistently within the user\u2019s\napplication.\n\nThe `BusinessHour` class provides a business hour representation on\n`BusinessDay`, allowing to use specific start and end times.\n\nBy default, `BusinessHour` uses 9:00 - 17:00 as business hours. Adding\n`BusinessHour` will increment `Timestamp` by hourly frequency. If target\n`Timestamp` is out of business hours, move to the next business hour then\nincrement it. If the result exceeds the business hours end, the remaining\nhours are added to the next business day.\n\nYou can also specify `start` and `end` time by keywords. The argument must be\na `str` with an `hour:minute` representation or a `datetime.time` instance.\nSpecifying seconds, microseconds and nanoseconds as business hour results in\n`ValueError`.\n\nPassing `start` time later than `end` represents midnight business hour. In\nthis case, business hour exceeds midnight and overlap to the next day. Valid\nbusiness hours are distinguished by whether it started from valid\n`BusinessDay`.\n\nApplying `BusinessHour.rollforward` and `rollback` to out of business hours\nresults in the next business hour start or previous day\u2019s end. Different from\nother offsets, `BusinessHour.rollforward` may output different results from\n`apply` by definition.\n\nThis is because one day\u2019s business hour end is equal to next day\u2019s business\nhour start. For example, under the default business hours (9:00 - 17:00),\nthere is no gap (0 minutes) between `2014-08-01 17:00` and `2014-08-04 09:00`.\n\n`BusinessHour` regards Saturday and Sunday as holidays. To use arbitrary\nholidays, you can use `CustomBusinessHour` offset, as explained in the\nfollowing subsection.\n\nThe `CustomBusinessHour` is a mixture of `BusinessHour` and\n`CustomBusinessDay` which allows you to specify arbitrary holidays.\n`CustomBusinessHour` works as the same as `BusinessHour` except that it skips\nspecified custom holidays.\n\nYou can use keyword arguments supported by either `BusinessHour` and\n`CustomBusinessDay`.\n\nA number of string aliases are given to useful common time series frequencies.\nWe will refer to these aliases as offset aliases.\n\nAlias\n\nDescription\n\nB\n\nbusiness day frequency\n\nC\n\ncustom business day frequency\n\nD\n\ncalendar day frequency\n\nW\n\nweekly frequency\n\nM\n\nmonth end frequency\n\nSM\n\nsemi-month end frequency (15th and end of month)\n\nBM\n\nbusiness month end frequency\n\nCBM\n\ncustom business month end frequency\n\nMS\n\nmonth start frequency\n\nSMS\n\nsemi-month start frequency (1st and 15th)\n\nBMS\n\nbusiness month start frequency\n\nCBMS\n\ncustom business month start frequency\n\nQ\n\nquarter end frequency\n\nBQ\n\nbusiness quarter end frequency\n\nQS\n\nquarter start frequency\n\nBQS\n\nbusiness quarter start frequency\n\nA, Y\n\nyear end frequency\n\nBA, BY\n\nbusiness year end frequency\n\nAS, YS\n\nyear start frequency\n\nBAS, BYS\n\nbusiness year start frequency\n\nBH\n\nbusiness hour frequency\n\nH\n\nhourly frequency\n\nT, min\n\nminutely frequency\n\nS\n\nsecondly frequency\n\nL, ms\n\nmilliseconds\n\nU, us\n\nmicroseconds\n\nN\n\nnanoseconds\n\nNote\n\nWhen using the offset aliases above, it should be noted that functions such as\n`date_range()`, `bdate_range()`, will only return timestamps that are in the\ninterval defined by `start_date` and `end_date`. If the `start_date` does not\ncorrespond to the frequency, the returned timestamps will start at the next\nvalid timestamp, same for `end_date`, the returned timestamps will stop at the\nprevious valid timestamp.\n\nFor example, for the offset `MS`, if the `start_date` is not the first of the\nmonth, the returned timestamps will start with the first day of the next\nmonth. If `end_date` is not the first day of a month, the last returned\ntimestamp will be the first day of the corresponding month.\n\nWe can see in the above example `date_range()` and `bdate_range()` will only\nreturn the valid timestamps between the `start_date` and `end_date`. If these\nare not valid timestamps for the given frequency it will roll to the next\nvalue for `start_date` (respectively previous for the `end_date`)\n\nAs we have seen previously, the alias and the offset instance are fungible in\nmost functions:\n\nYou can combine together day and intraday offsets:\n\nFor some frequencies you can specify an anchoring suffix:\n\nAlias\n\nDescription\n\nW-SUN\n\nweekly frequency (Sundays). Same as \u2018W\u2019\n\nW-MON\n\nweekly frequency (Mondays)\n\nW-TUE\n\nweekly frequency (Tuesdays)\n\nW-WED\n\nweekly frequency (Wednesdays)\n\nW-THU\n\nweekly frequency (Thursdays)\n\nW-FRI\n\nweekly frequency (Fridays)\n\nW-SAT\n\nweekly frequency (Saturdays)\n\n(B)Q(S)-DEC\n\nquarterly frequency, year ends in December. Same as \u2018Q\u2019\n\n(B)Q(S)-JAN\n\nquarterly frequency, year ends in January\n\n(B)Q(S)-FEB\n\nquarterly frequency, year ends in February\n\n(B)Q(S)-MAR\n\nquarterly frequency, year ends in March\n\n(B)Q(S)-APR\n\nquarterly frequency, year ends in April\n\n(B)Q(S)-MAY\n\nquarterly frequency, year ends in May\n\n(B)Q(S)-JUN\n\nquarterly frequency, year ends in June\n\n(B)Q(S)-JUL\n\nquarterly frequency, year ends in July\n\n(B)Q(S)-AUG\n\nquarterly frequency, year ends in August\n\n(B)Q(S)-SEP\n\nquarterly frequency, year ends in September\n\n(B)Q(S)-OCT\n\nquarterly frequency, year ends in October\n\n(B)Q(S)-NOV\n\nquarterly frequency, year ends in November\n\n(B)A(S)-DEC\n\nannual frequency, anchored end of December. Same as \u2018A\u2019\n\n(B)A(S)-JAN\n\nannual frequency, anchored end of January\n\n(B)A(S)-FEB\n\nannual frequency, anchored end of February\n\n(B)A(S)-MAR\n\nannual frequency, anchored end of March\n\n(B)A(S)-APR\n\nannual frequency, anchored end of April\n\n(B)A(S)-MAY\n\nannual frequency, anchored end of May\n\n(B)A(S)-JUN\n\nannual frequency, anchored end of June\n\n(B)A(S)-JUL\n\nannual frequency, anchored end of July\n\n(B)A(S)-AUG\n\nannual frequency, anchored end of August\n\n(B)A(S)-SEP\n\nannual frequency, anchored end of September\n\n(B)A(S)-OCT\n\nannual frequency, anchored end of October\n\n(B)A(S)-NOV\n\nannual frequency, anchored end of November\n\nThese can be used as arguments to `date_range`, `bdate_range`, constructors\nfor `DatetimeIndex`, as well as various other timeseries-related functions in\npandas.\n\nFor those offsets that are anchored to the start or end of specific frequency\n(`MonthEnd`, `MonthBegin`, `WeekEnd`, etc), the following rules apply to\nrolling forward and backwards.\n\nWhen `n` is not 0, if the given date is not on an anchor point, it snapped to\nthe next(previous) anchor point, and moved `|n|-1` additional steps forwards\nor backwards.\n\nIf the given date is on an anchor point, it is moved `|n|` points forwards or\nbackwards.\n\nFor the case when `n=0`, the date is not moved if on an anchor point,\notherwise it is rolled forward to the next anchor point.\n\nHolidays and calendars provide a simple way to define holiday rules to be used\nwith `CustomBusinessDay` or in other analysis that requires a predefined set\nof holidays. The `AbstractHolidayCalendar` class provides all the necessary\nmethods to return a list of holidays and only `rules` need to be defined in a\nspecific holiday calendar class. Furthermore, the `start_date` and `end_date`\nclass attributes determine over what date range holidays are generated. These\nshould be overwritten on the `AbstractHolidayCalendar` class to have the range\napply to all calendar subclasses. `USFederalHolidayCalendar` is the only\ncalendar that exists and primarily serves as an example for developing other\ncalendars.\n\nFor holidays that occur on fixed dates (e.g., US Memorial Day or July 4th) an\nobservance rule determines when that holiday is observed if it falls on a\nweekend or some other non-observed day. Defined observance rules are:\n\nRule\n\nDescription\n\nnearest_workday\n\nmove Saturday to Friday and Sunday to Monday\n\nsunday_to_monday\n\nmove Sunday to following Monday\n\nnext_monday_or_tuesday\n\nmove Saturday to Monday and Sunday/Monday to Tuesday\n\nprevious_friday\n\nmove Saturday and Sunday to previous Friday\u201d\n\nnext_monday\n\nmove Saturday and Sunday to following Monday\n\nAn example of how holidays and holiday calendars are defined:\n\nweekday=MO(2) is same as 2 * Week(weekday=2)\n\nUsing this calendar, creating an index or doing offset arithmetic skips\nweekends and holidays (i.e., Memorial Day/July 4th). For example, the below\ndefines a custom business day offset using the `ExampleCalendar`. Like any\nother offset, it can be used to create a `DatetimeIndex` or added to\n`datetime` or `Timestamp` objects.\n\nRanges are defined by the `start_date` and `end_date` class attributes of\n`AbstractHolidayCalendar`. The defaults are shown below.\n\nThese dates can be overwritten by setting the attributes as\ndatetime/Timestamp/string.\n\nEvery calendar class is accessible by name using the `get_calendar` function\nwhich returns a holiday class instance. Any imported calendar class will\nautomatically be available by this function. Also, `HolidayCalendarFactory`\nprovides an easy interface to create calendars that are combinations of\ncalendars or calendars with additional rules.\n\nOne may want to shift or lag the values in a time series back and forward in\ntime. The method for this is `shift()`, which is available on all of the\npandas objects.\n\nThe `shift` method accepts an `freq` argument which can accept a `DateOffset`\nclass or other `timedelta`-like object or also an offset alias.\n\nWhen `freq` is specified, `shift` method changes all the dates in the index\nrather than changing the alignment of the data and the index:\n\nNote that with when `freq` is specified, the leading entry is no longer NaN\nbecause the data is not being realigned.\n\nThe primary function for changing frequencies is the `asfreq()` method. For a\n`DatetimeIndex`, this is basically just a thin, but convenient wrapper around\n`reindex()` which generates a `date_range` and calls `reindex`.\n\n`asfreq` provides a further convenience so you can specify an interpolation\nmethod for any gaps that may appear after the frequency conversion.\n\nRelated to `asfreq` and `reindex` is `fillna()`, which is documented in the\nmissing data section.\n\n`DatetimeIndex` can be converted to an array of Python native\n`datetime.datetime` objects using the `to_pydatetime` method.\n\npandas has a simple, powerful, and efficient functionality for performing\nresampling operations during frequency conversion (e.g., converting secondly\ndata into 5-minutely data). This is extremely common in, but not limited to,\nfinancial applications.\n\n`resample()` is a time-based groupby, followed by a reduction method on each\nof its groups. See some cookbook examples for some advanced strategies.\n\nThe `resample()` method can be used directly from `DataFrameGroupBy` objects,\nsee the groupby docs.\n\nThe `resample` function is very flexible and allows you to specify many\ndifferent parameters to control the frequency conversion and resampling\noperation.\n\nAny function available via dispatching is available as a method of the\nreturned object, including `sum`, `mean`, `std`, `sem`, `max`, `min`,\n`median`, `first`, `last`, `ohlc`:\n\nFor downsampling, `closed` can be set to \u2018left\u2019 or \u2018right\u2019 to specify which\nend of the interval is closed:\n\nParameters like `label` are used to manipulate the resulting labels. `label`\nspecifies whether the result is labeled with the beginning or the end of the\ninterval.\n\nWarning\n\nThe default values for `label` and `closed` is \u2018left\u2019 for all frequency\noffsets except for \u2018M\u2019, \u2018A\u2019, \u2018Q\u2019, \u2018BM\u2019, \u2018BA\u2019, \u2018BQ\u2019, and \u2018W\u2019 which all have a\ndefault of \u2018right\u2019.\n\nThis might unintendedly lead to looking ahead, where the value for a later\ntime is pulled back to a previous time as in the following example with the\n`BusinessDay` frequency:\n\nNotice how the value for Sunday got pulled back to the previous Friday. To get\nthe behavior where the value for Sunday is pushed to Monday, use instead\n\nThe `axis` parameter can be set to 0 or 1 and allows you to resample the\nspecified axis for a `DataFrame`.\n\n`kind` can be set to \u2018timestamp\u2019 or \u2018period\u2019 to convert the resulting index\nto/from timestamp and time span representations. By default `resample` retains\nthe input representation.\n\n`convention` can be set to \u2018start\u2019 or \u2018end\u2019 when resampling period data\n(detail below). It specifies how low frequency periods are converted to higher\nfrequency periods.\n\nFor upsampling, you can specify a way to upsample and the `limit` parameter to\ninterpolate over the gaps that are created:\n\nSparse timeseries are the ones where you have a lot fewer points relative to\nthe amount of time you are looking to resample. Naively upsampling a sparse\nseries can potentially generate lots of intermediate values. When you don\u2019t\nwant to use a method to fill these values, e.g. `fill_method` is `None`, then\nintermediate values will be filled with `NaN`.\n\nSince `resample` is a time-based groupby, the following is a method to\nefficiently resample only the groups that are not all `NaN`.\n\nIf we want to resample to the full range of the series:\n\nWe can instead only resample those groups where we have points as follows:\n\nSimilar to the aggregating API, groupby API, and the window API, a `Resampler`\ncan be selectively resampled.\n\nResampling a `DataFrame`, the default will be to act on all columns with the\nsame function.\n\nWe can select a specific column or columns using standard getitem.\n\nYou can pass a list or dict of functions to do aggregation with, outputting a\n`DataFrame`:\n\nOn a resampled `DataFrame`, you can pass a list of functions to apply to each\ncolumn, which produces an aggregated result with a hierarchical index:\n\nBy passing a dict to `aggregate` you can apply a different aggregation to the\ncolumns of a `DataFrame`:\n\nThe function names can also be strings. In order for a string to be valid it\nmust be implemented on the resampled object:\n\nFurthermore, you can also specify multiple aggregation functions for each\ncolumn separately.\n\nIf a `DataFrame` does not have a datetimelike index, but instead you want to\nresample based on datetimelike column in the frame, it can passed to the `on`\nkeyword.\n\nSimilarly, if you instead want to resample by a datetimelike level of\n`MultiIndex`, its name or location can be passed to the `level` keyword.\n\nWith the `Resampler` object in hand, iterating through the grouped data is\nvery natural and functions similarly to `itertools.groupby()`:\n\nSee Iterating through groups or `Resampler.__iter__` for more.\n\nNew in version 1.1.0.\n\nThe bins of the grouping are adjusted based on the beginning of the day of the\ntime series starting point. This works well with frequencies that are\nmultiples of a day (like `30D`) or that divide a day evenly (like `90s` or\n`1min`). This can create inconsistencies with some frequencies that do not\nmeet this criteria. To change this behavior you can specify a fixed Timestamp\nwith the argument `origin`.\n\nFor example:\n\nHere we can see that, when using `origin` with its default value\n(`'start_day'`), the result after `'2000-10-02 00:00:00'` are not identical\ndepending on the start of time series:\n\nHere we can see that, when setting `origin` to `'epoch'`, the result after\n`'2000-10-02 00:00:00'` are identical depending on the start of time series:\n\nIf needed you can use a custom timestamp for `origin`:\n\nIf needed you can just adjust the bins with an `offset` Timedelta that would\nbe added to the default `origin`. Those two examples are equivalent for this\ntime series:\n\nNote the use of `'start'` for `origin` on the last example. In that case,\n`origin` will be set to the first value of the timeseries.\n\nNew in version 1.3.0.\n\nInstead of adjusting the beginning of bins, sometimes we need to fix the end\nof the bins to make a backward resample with a given `freq`. The backward\nresample sets `closed` to `'right'` by default since the last value should be\nconsidered as the edge point for the last bin.\n\nWe can set `origin` to `'end'`. The value for a specific `Timestamp` index\nstands for the resample result from the current `Timestamp` minus `freq` to\nthe current `Timestamp` with a right close.\n\nBesides, in contrast with the `'start_day'` option, `end_day` is supported.\nThis will set the origin as the ceiling midnight of the largest `Timestamp`.\n\nThe above result uses `2000-10-02 00:29:00` as the last bin\u2019s right edge since\nthe following computation.\n\nRegular intervals of time are represented by `Period` objects in pandas while\nsequences of `Period` objects are collected in a `PeriodIndex`, which can be\ncreated with the convenience function `period_range`.\n\nA `Period` represents a span of time (e.g., a day, a month, a quarter, etc).\nYou can specify the span via `freq` keyword using a frequency alias like\nbelow. Because `freq` represents a span of `Period`, it cannot be negative\nlike \u201c-3D\u201d.\n\nAdding and subtracting integers from periods shifts the period by its own\nfrequency. Arithmetic is not allowed between `Period` with different `freq`\n(span).\n\nIf `Period` freq is daily or higher (`D`, `H`, `T`, `S`, `L`, `U`, `N`),\n`offsets` and `timedelta`-like can be added if the result can have the same\nfreq. Otherwise, `ValueError` will be raised.\n\nIf `Period` has other frequencies, only the same `offsets` can be added.\nOtherwise, `ValueError` will be raised.\n\nTaking the difference of `Period` instances with the same frequency will\nreturn the number of frequency units between them:\n\nRegular sequences of `Period` objects can be collected in a `PeriodIndex`,\nwhich can be constructed using the `period_range` convenience function:\n\nThe `PeriodIndex` constructor can also be used directly:\n\nPassing multiplied frequency outputs a sequence of `Period` which has\nmultiplied span.\n\nIf `start` or `end` are `Period` objects, they will be used as anchor\nendpoints for a `PeriodIndex` with frequency matching that of the\n`PeriodIndex` constructor.\n\nJust like `DatetimeIndex`, a `PeriodIndex` can also be used to index pandas\nobjects:\n\n`PeriodIndex` supports addition and subtraction with the same rule as\n`Period`.\n\n`PeriodIndex` has its own dtype named `period`, refer to Period Dtypes.\n\n`PeriodIndex` has a custom `period` dtype. This is a pandas extension dtype\nsimilar to the timezone aware dtype (`datetime64[ns, tz]`).\n\nThe `period` dtype holds the `freq` attribute and is represented with\n`period[freq]` like `period[D]` or `period[M]`, using frequency strings.\n\nThe `period` dtype can be used in `.astype(...)`. It allows one to change the\n`freq` of a `PeriodIndex` like `.asfreq()` and convert a `DatetimeIndex` to\n`PeriodIndex` like `to_period()`:\n\nPeriodIndex now supports partial string slicing with non-monotonic indexes.\n\nNew in version 1.1.0.\n\nYou can pass in dates and strings to `Series` and `DataFrame` with\n`PeriodIndex`, in the same manner as `DatetimeIndex`. For details, refer to\nDatetimeIndex Partial String Indexing.\n\nPassing a string representing a lower frequency than `PeriodIndex` returns\npartial sliced data.\n\nAs with `DatetimeIndex`, the endpoints will be included in the result. The\nexample below slices data starting from 10:00 to 11:59.\n\nThe frequency of `Period` and `PeriodIndex` can be converted via the `asfreq`\nmethod. Let\u2019s start with the fiscal year 2011, ending in December:\n\nWe can convert it to a monthly frequency. Using the `how` parameter, we can\nspecify whether to return the starting or ending month:\n\nThe shorthands \u2018s\u2019 and \u2018e\u2019 are provided for convenience:\n\nConverting to a \u201csuper-period\u201d (e.g., annual frequency is a super-period of\nquarterly frequency) automatically returns the super-period that includes the\ninput period:\n\nNote that since we converted to an annual frequency that ends the year in\nNovember, the monthly period of December 2011 is actually in the 2012 A-NOV\nperiod.\n\nPeriod conversions with anchored frequencies are particularly useful for\nworking with various quarterly data common to economics, business, and other\nfields. Many organizations define quarters relative to the month in which\ntheir fiscal year starts and ends. Thus, first quarter of 2011 could start in\n2010 or a few months into 2011. Via anchored frequencies, pandas works for all\nquarterly frequencies `Q-JAN` through `Q-DEC`.\n\n`Q-DEC` define regular calendar quarters:\n\n`Q-MAR` defines fiscal year end in March:\n\nTimestamped data can be converted to PeriodIndex-ed data using `to_period` and\nvice-versa using `to_timestamp`:\n\nRemember that \u2018s\u2019 and \u2018e\u2019 can be used to return the timestamps at the start or\nend of the period:\n\nConverting between period and timestamp enables some convenient arithmetic\nfunctions to be used. In the following example, we convert a quarterly\nfrequency with year ending in November to 9am of the end of the month\nfollowing the quarter end:\n\nIf you have data that is outside of the `Timestamp` bounds, see Timestamp\nlimitations, then you can use a `PeriodIndex` and/or `Series` of `Periods` to\ndo computations.\n\nTo convert from an `int64` based YYYYMMDD representation.\n\nThese can easily be converted to a `PeriodIndex`:\n\npandas provides rich support for working with timestamps in different time\nzones using the `pytz` and `dateutil` libraries or `datetime.timezone` objects\nfrom the standard library.\n\nBy default, pandas objects are time zone unaware:\n\nTo localize these dates to a time zone (assign a particular time zone to a\nnaive date), you can use the `tz_localize` method or the `tz` keyword argument\nin `date_range()`, `Timestamp`, or `DatetimeIndex`. You can either pass `pytz`\nor `dateutil` time zone objects or Olson time zone database strings. Olson\ntime zone strings will return `pytz` time zone objects by default. To return\n`dateutil` time zone objects, append `dateutil/` before the string.\n\nIn `pytz` you can find a list of common (and less common) time zones using\n`from pytz import common_timezones, all_timezones`.\n\n`dateutil` uses the OS time zones so there isn\u2019t a fixed list available. For\ncommon zones, the names are the same as `pytz`.\n\nNew in version 0.25.0.\n\nNote that the `UTC` time zone is a special case in `dateutil` and should be\nconstructed explicitly as an instance of `dateutil.tz.tzutc`. You can also\nconstruct other time zones objects explicitly first.\n\nTo convert a time zone aware pandas object from one time zone to another, you\ncan use the `tz_convert` method.\n\nNote\n\nWhen using `pytz` time zones, `DatetimeIndex` will construct a different time\nzone object than a `Timestamp` for the same time zone input. A `DatetimeIndex`\ncan hold a collection of `Timestamp` objects that may have different UTC\noffsets and cannot be succinctly represented by one `pytz` time zone instance\nwhile one `Timestamp` represents one point in time with a specific UTC offset.\n\nWarning\n\nBe wary of conversions between libraries. For some time zones, `pytz` and\n`dateutil` have different definitions of the zone. This is more of a problem\nfor unusual time zones than for \u2018standard\u2019 zones like `US/Eastern`.\n\nWarning\n\nBe aware that a time zone definition across versions of time zone libraries\nmay not be considered equal. This may cause problems when working with stored\ndata that is localized using one version and operated on with a different\nversion. See here for how to handle such a situation.\n\nWarning\n\nFor `pytz` time zones, it is incorrect to pass a time zone object directly\ninto the `datetime.datetime` constructor (e.g., `datetime.datetime(2011, 1, 1,\ntzinfo=pytz.timezone('US/Eastern'))`. Instead, the datetime needs to be\nlocalized using the `localize` method on the `pytz` time zone object.\n\nWarning\n\nBe aware that for times in the future, correct conversion between time zones\n(and UTC) cannot be guaranteed by any time zone library because a timezone\u2019s\noffset from UTC may be changed by the respective government.\n\nWarning\n\nIf you are using dates beyond 2038-01-18, due to current deficiencies in the\nunderlying libraries caused by the year 2038 problem, daylight saving time\n(DST) adjustments to timezone aware dates will not be applied. If and when the\nunderlying libraries are fixed, the DST transitions will be applied.\n\nFor example, for two dates that are in British Summer Time (and so would\nnormally be GMT+1), both the following asserts evaluate as true:\n\nUnder the hood, all timestamps are stored in UTC. Values from a time zone\naware `DatetimeIndex` or `Timestamp` will have their fields (day, hour,\nminute, etc.) localized to the time zone. However, timestamps with the same\nUTC value are still considered to be equal even if they are in different time\nzones:\n\nOperations between `Series` in different time zones will yield UTC `Series`,\naligning the data on the UTC timestamps:\n\nTo remove time zone information, use `tz_localize(None)` or\n`tz_convert(None)`. `tz_localize(None)` will remove the time zone yielding the\nlocal time representation. `tz_convert(None)` will remove the time zone after\nconverting to UTC time.\n\nNew in version 1.1.0.\n\nFor ambiguous times, pandas supports explicitly specifying the keyword-only\nfold argument. Due to daylight saving time, one wall clock time can occur\ntwice when shifting from summer to winter time; fold describes whether the\ndatetime-like corresponds to the first (0) or the second time (1) the wall\nclock hits the ambiguous time. Fold is supported only for constructing from\nnaive `datetime.datetime` (see datetime documentation for details) or from\n`Timestamp` or for constructing from components (see below). Only `dateutil`\ntimezones are supported (see dateutil documentation for `dateutil` methods\nthat deal with ambiguous datetimes) as `pytz` timezones do not support fold\n(see pytz documentation for details on how `pytz` deals with ambiguous\ndatetimes). To localize an ambiguous datetime with `pytz`, please use\n`Timestamp.tz_localize()`. In general, we recommend to rely on\n`Timestamp.tz_localize()` when localizing ambiguous datetimes if you need\ndirect control over how they are handled.\n\n`tz_localize` may not be able to determine the UTC offset of a timestamp\nbecause daylight savings time (DST) in a local time zone causes some times to\noccur twice within one day (\u201cclocks fall back\u201d). The following options are\navailable:\n\n`'raise'`: Raises a `pytz.AmbiguousTimeError` (the default behavior)\n\n`'infer'`: Attempt to determine the correct offset base on the monotonicity of\nthe timestamps\n\n`'NaT'`: Replaces ambiguous times with `NaT`\n\n`bool`: `True` represents a DST time, `False` represents non-DST time. An\narray-like of `bool` values is supported for a sequence of times.\n\nThis will fail as there are ambiguous times (`'11/06/2011 01:00'`)\n\nHandle these ambiguous times by specifying the following.\n\nA DST transition may also shift the local time ahead by 1 hour creating\nnonexistent local times (\u201cclocks spring forward\u201d). The behavior of localizing\na timeseries with nonexistent times can be controlled by the `nonexistent`\nargument. The following options are available:\n\n`'raise'`: Raises a `pytz.NonExistentTimeError` (the default behavior)\n\n`'NaT'`: Replaces nonexistent times with `NaT`\n\n`'shift_forward'`: Shifts nonexistent times forward to the closest real time\n\n`'shift_backward'`: Shifts nonexistent times backward to the closest real time\n\ntimedelta object: Shifts nonexistent times by the timedelta duration\n\nLocalization of nonexistent times will raise an error by default.\n\nTransform nonexistent times to `NaT` or shift the times.\n\nA `Series` with time zone naive values is represented with a dtype of\n`datetime64[ns]`.\n\nA `Series` with a time zone aware values is represented with a dtype of\n`datetime64[ns, tz]` where `tz` is the time zone\n\nBoth of these `Series` time zone information can be manipulated via the `.dt`\naccessor, see the dt accessor section.\n\nFor example, to localize and convert a naive stamp to time zone aware.\n\nTime zone information can also be manipulated using the `astype` method. This\nmethod can convert between different timezone-aware dtypes.\n\nNote\n\nUsing `Series.to_numpy()` on a `Series`, returns a NumPy array of the data.\nNumPy does not currently support time zones (even though it is printing in the\nlocal time zone!), therefore an object array of Timestamps is returned for\ntime zone aware data:\n\nBy converting to an object array of Timestamps, it preserves the time zone\ninformation. For example, when converting back to a Series:\n\nHowever, if you want an actual NumPy `datetime64[ns]` array (with the values\nconverted to UTC) instead of an array of objects, you can specify the `dtype`\nargument:\n\n"}, {"name": "User Guide", "path": "user_guide/index", "type": "Manual", "text": "\nThe User Guide covers all of pandas by topic area. Each of the subsections\nintroduces a topic (such as \u201cworking with missing data\u201d), and discusses how\npandas approaches the problem, with many examples throughout.\n\nUsers brand-new to pandas should start with 10 minutes to pandas.\n\nFor a high level summary of the pandas fundamentals, see Intro to data\nstructures and Essential basic functionality.\n\nFurther information on any specific method can be obtained in the API\nreference.\n\n"}, {"name": "Window", "path": "reference/window", "type": "Window", "text": "\nRolling objects are returned by `.rolling` calls:\n`pandas.DataFrame.rolling()`, `pandas.Series.rolling()`, etc. Expanding\nobjects are returned by `.expanding` calls: `pandas.DataFrame.expanding()`,\n`pandas.Series.expanding()`, etc. ExponentialMovingWindow objects are returned\nby `.ewm` calls: `pandas.DataFrame.ewm()`, `pandas.Series.ewm()`, etc.\n\n`Rolling.count`()\n\nCalculate the rolling count of non NaN observations.\n\n`Rolling.sum`(*args[, engine, engine_kwargs])\n\nCalculate the rolling sum.\n\n`Rolling.mean`(*args[, engine, engine_kwargs])\n\nCalculate the rolling mean.\n\n`Rolling.median`([engine, engine_kwargs])\n\nCalculate the rolling median.\n\n`Rolling.var`([ddof, engine, engine_kwargs])\n\nCalculate the rolling variance.\n\n`Rolling.std`([ddof, engine, engine_kwargs])\n\nCalculate the rolling standard deviation.\n\n`Rolling.min`(*args[, engine, engine_kwargs])\n\nCalculate the rolling minimum.\n\n`Rolling.max`(*args[, engine, engine_kwargs])\n\nCalculate the rolling maximum.\n\n`Rolling.corr`([other, pairwise, ddof])\n\nCalculate the rolling correlation.\n\n`Rolling.cov`([other, pairwise, ddof])\n\nCalculate the rolling sample covariance.\n\n`Rolling.skew`(**kwargs)\n\nCalculate the rolling unbiased skewness.\n\n`Rolling.kurt`(**kwargs)\n\nCalculate the rolling Fisher's definition of kurtosis without bias.\n\n`Rolling.apply`(func[, raw, engine, ...])\n\nCalculate the rolling custom aggregation function.\n\n`Rolling.aggregate`(func, *args, **kwargs)\n\nAggregate using one or more operations over the specified axis.\n\n`Rolling.quantile`(quantile[, interpolation])\n\nCalculate the rolling quantile.\n\n`Rolling.sem`([ddof])\n\nCalculate the rolling standard error of mean.\n\n`Rolling.rank`([method, ascending, pct])\n\nCalculate the rolling rank.\n\n`Window.mean`(*args, **kwargs)\n\nCalculate the rolling weighted window mean.\n\n`Window.sum`(*args, **kwargs)\n\nCalculate the rolling weighted window sum.\n\n`Window.var`([ddof])\n\nCalculate the rolling weighted window variance.\n\n`Window.std`([ddof])\n\nCalculate the rolling weighted window standard deviation.\n\n`Expanding.count`()\n\nCalculate the expanding count of non NaN observations.\n\n`Expanding.sum`(*args[, engine, engine_kwargs])\n\nCalculate the expanding sum.\n\n`Expanding.mean`(*args[, engine, engine_kwargs])\n\nCalculate the expanding mean.\n\n`Expanding.median`([engine, engine_kwargs])\n\nCalculate the expanding median.\n\n`Expanding.var`([ddof, engine, engine_kwargs])\n\nCalculate the expanding variance.\n\n`Expanding.std`([ddof, engine, engine_kwargs])\n\nCalculate the expanding standard deviation.\n\n`Expanding.min`(*args[, engine, engine_kwargs])\n\nCalculate the expanding minimum.\n\n`Expanding.max`(*args[, engine, engine_kwargs])\n\nCalculate the expanding maximum.\n\n`Expanding.corr`([other, pairwise, ddof])\n\nCalculate the expanding correlation.\n\n`Expanding.cov`([other, pairwise, ddof])\n\nCalculate the expanding sample covariance.\n\n`Expanding.skew`(**kwargs)\n\nCalculate the expanding unbiased skewness.\n\n`Expanding.kurt`(**kwargs)\n\nCalculate the expanding Fisher's definition of kurtosis without bias.\n\n`Expanding.apply`(func[, raw, engine, ...])\n\nCalculate the expanding custom aggregation function.\n\n`Expanding.aggregate`(func, *args, **kwargs)\n\nAggregate using one or more operations over the specified axis.\n\n`Expanding.quantile`(quantile[, interpolation])\n\nCalculate the expanding quantile.\n\n`Expanding.sem`([ddof])\n\nCalculate the expanding standard error of mean.\n\n`Expanding.rank`([method, ascending, pct])\n\nCalculate the expanding rank.\n\n`ExponentialMovingWindow.mean`(*args[, ...])\n\nCalculate the ewm (exponential weighted moment) mean.\n\n`ExponentialMovingWindow.sum`(*args[, engine, ...])\n\nCalculate the ewm (exponential weighted moment) sum.\n\n`ExponentialMovingWindow.std`([bias])\n\nCalculate the ewm (exponential weighted moment) standard deviation.\n\n`ExponentialMovingWindow.var`([bias])\n\nCalculate the ewm (exponential weighted moment) variance.\n\n`ExponentialMovingWindow.corr`([other, pairwise])\n\nCalculate the ewm (exponential weighted moment) sample correlation.\n\n`ExponentialMovingWindow.cov`([other, ...])\n\nCalculate the ewm (exponential weighted moment) sample covariance.\n\nBase class for defining custom window boundaries.\n\n`api.indexers.BaseIndexer`([index_array, ...])\n\nBase class for window bounds calculations.\n\n`api.indexers.FixedForwardWindowIndexer`([...])\n\nCreates window boundaries for fixed-length windows that include the current\nrow.\n\n`api.indexers.VariableOffsetWindowIndexer`([...])\n\nCalculate window boundaries based on a non-fixed offset such as a BusinessDay.\n\n"}, {"name": "Windowing Operations", "path": "user_guide/window", "type": "Manual", "text": "\npandas contains a compact set of APIs for performing windowing operations - an\noperation that performs an aggregation over a sliding partition of values. The\nAPI functions similarly to the `groupby` API in that `Series` and `DataFrame`\ncall the windowing method with necessary parameters and then subsequently call\nthe aggregation function.\n\nThe windows are comprised by looking back the length of the window from the\ncurrent observation. The result above can be derived by taking the sum of the\nfollowing windowed partitions of data:\n\npandas supports 4 types of windowing operations:\n\nRolling window: Generic fixed or variable sliding window over the values.\n\nWeighted window: Weighted, non-rectangular window supplied by the\n`scipy.signal` library.\n\nExpanding window: Accumulating window over the values.\n\nExponentially Weighted window: Accumulating and exponentially weighted window\nover the values.\n\nConcept\n\nMethod\n\nReturned Object\n\nSupports time-based windows\n\nSupports chained groupby\n\nSupports table method\n\nSupports online operations\n\nRolling window\n\n`rolling`\n\n`Rolling`\n\nYes\n\nYes\n\nYes (as of version 1.3)\n\nNo\n\nWeighted window\n\n`rolling`\n\n`Window`\n\nNo\n\nNo\n\nNo\n\nNo\n\nExpanding window\n\n`expanding`\n\n`Expanding`\n\nNo\n\nYes\n\nYes (as of version 1.3)\n\nNo\n\nExponentially Weighted window\n\n`ewm`\n\n`ExponentialMovingWindow`\n\nNo\n\nYes (as of version 1.2)\n\nNo\n\nYes (as of version 1.3)\n\nAs noted above, some operations support specifying a window based on a time\noffset:\n\nAdditionally, some methods support chaining a `groupby` operation with a\nwindowing operation which will first group the data by the specified keys and\nthen perform a windowing operation per group.\n\nNote\n\nWindowing operations currently only support numeric data (integer and float)\nand will always return `float64` values.\n\nWarning\n\nSome windowing aggregation, `mean`, `sum`, `var` and `std` methods may suffer\nfrom numerical imprecision due to the underlying windowing algorithms\naccumulating sums. When values differ with magnitude\n\\\\(1/np.finfo(np.double).eps\\\\) this results in truncation. It must be noted,\nthat large values may have an impact on windows, which do not include these\nvalues. Kahan summation is used to compute the rolling sums to preserve\naccuracy as much as possible.\n\nNew in version 1.3.0.\n\nSome windowing operations also support the `method='table'` option in the\nconstructor which performs the windowing operation over an entire `DataFrame`\ninstead of a single column or row at a time. This can provide a useful\nperformance benefit for a `DataFrame` with many columns or rows (with the\ncorresponding `axis` argument) or the ability to utilize other columns during\nthe windowing operation. The `method='table'` option can only be used if\n`engine='numba'` is specified in the corresponding method call.\n\nFor example, a weighted mean calculation can be calculated with `apply()` by\nspecifying a separate column of weights.\n\nNew in version 1.3.\n\nSome windowing operations also support an `online` method after constructing a\nwindowing object which returns a new object that supports passing in new\n`DataFrame` or `Series` objects to continue the windowing calculation with the\nnew values (i.e. online calculations).\n\nThe methods on this new windowing objects must call the aggregation method\nfirst to \u201cprime\u201d the initial state of the online calculation. Then, new\n`DataFrame` or `Series` objects can be passed in the `update` argument to\ncontinue the windowing calculation.\n\nAll windowing operations support a `min_periods` argument that dictates the\nminimum amount of non-`np.nan` values a window must have; otherwise, the\nresulting value is `np.nan`. `min_periods` defaults to 1 for time-based\nwindows and `window` for fixed windows\n\nAdditionally, all windowing operations supports the `aggregate` method for\nreturning a result of multiple aggregations applied to a window.\n\nGeneric rolling windows support specifying windows as a fixed number of\nobservations or variable number of observations based on an offset. If a time\nbased offset is provided, the corresponding time based index must be\nmonotonic.\n\nFor all supported aggregation functions, see Rolling window functions.\n\nBy default the labels are set to the right edge of the window, but a `center`\nkeyword is available so the labels can be set at the center.\n\nThis can also be applied to datetime-like indices.\n\nNew in version 1.3.0.\n\nThe inclusion of the interval endpoints in rolling window calculations can be\nspecified with the `closed` parameter:\n\nValue\n\nBehavior\n\n`'right'`\n\nclose right endpoint\n\n`'left'`\n\nclose left endpoint\n\n`'both'`\n\nclose both endpoints\n\n`'neither'`\n\nopen endpoints\n\nFor example, having the right endpoint open is useful in many problems that\nrequire that there is no contamination from present information back to past\ninformation. This allows the rolling window to compute statistics \u201cup to that\npoint in time\u201d, but not including that point in time.\n\nNew in version 1.0.\n\nIn addition to accepting an integer or offset as a `window` argument,\n`rolling` also accepts a `BaseIndexer` subclass that allows a user to define a\ncustom method for calculating window bounds. The `BaseIndexer` subclass will\nneed to define a `get_window_bounds` method that returns a tuple of two\narrays, the first being the starting indices of the windows and second being\nthe ending indices of the windows. Additionally, `num_values`, `min_periods`,\n`center`, `closed` and will automatically be passed to `get_window_bounds` and\nthe defined method must always accept these arguments.\n\nFor example, if we have the following `DataFrame`\n\nand we want to use an expanding window where `use_expanding` is `True`\notherwise a window of size 1, we can create the following `BaseIndexer`\nsubclass:\n\nYou can view other examples of `BaseIndexer` subclasses here\n\nNew in version 1.1.\n\nOne subclass of note within those examples is the\n`VariableOffsetWindowIndexer` that allows rolling operations over a non-fixed\noffset like a `BusinessDay`.\n\nFor some problems knowledge of the future is available for analysis. For\nexample, this occurs when each data point is a full time series read from an\nexperiment, and the task is to extract underlying conditions. In these cases\nit can be useful to perform forward-looking rolling window computations.\n`FixedForwardWindowIndexer` class is available for this purpose. This\n`BaseIndexer` subclass implements a closed fixed-width forward-looking rolling\nwindow, and we can use it as follows:\n\nWe can also achieve this by using slicing, applying rolling aggregation, and\nthen flipping the result as shown in example below:\n\nThe `apply()` function takes an extra `func` argument and performs generic\nrolling computations. The `func` argument should be a single function that\nproduces a single value from an ndarray input. `raw` specifies whether the\nwindows are cast as `Series` objects (`raw=False`) or ndarray objects\n(`raw=True`).\n\nNew in version 1.0.\n\nAdditionally, `apply()` can leverage Numba if installed as an optional\ndependency. The apply aggregation can be executed using Numba by specifying\n`engine='numba'` and `engine_kwargs` arguments (`raw` must also be set to\n`True`). See enhancing performance with Numba for general usage of the\narguments and performance considerations.\n\nNumba will be applied in potentially two routines:\n\nIf `func` is a standard Python function, the engine will JIT the passed\nfunction. `func` can also be a JITed function in which case the engine will\nnot JIT the function again.\n\nThe engine will JIT the for loop where the apply function is applied to each\nwindow.\n\nThe `engine_kwargs` argument is a dictionary of keyword arguments that will be\npassed into the numba.jit decorator. These keyword arguments will be applied\nto both the passed function (if a standard Python function) and the apply for\nloop over each window.\n\nNew in version 1.3.0.\n\n`mean`, `median`, `max`, `min`, and `sum` also support the `engine` and\n`engine_kwargs` arguments.\n\n`cov()` and `corr()` can compute moving window statistics about two `Series`\nor any combination of `DataFrame`/`Series` or `DataFrame`/`DataFrame`. Here is\nthe behavior in each case:\n\ntwo `Series`: compute the statistic for the pairing.\n\n`DataFrame`/`Series`: compute the statistics for each column of the DataFrame\nwith the passed Series, thus returning a DataFrame.\n\n`DataFrame`/`DataFrame`: by default compute the statistic for matching column\nnames, returning a DataFrame. If the keyword argument `pairwise=True` is\npassed then computes the statistic for each pair of columns, returning a\n`DataFrame` with a `MultiIndex` whose values are the dates in question (see\nthe next section).\n\nFor example:\n\nIn financial data analysis and other fields it\u2019s common to compute covariance\nand correlation matrices for a collection of time series. Often one is also\ninterested in moving-window covariance and correlation matrices. This can be\ndone by passing the `pairwise` keyword argument, which in the case of\n`DataFrame` inputs will yield a MultiIndexed `DataFrame` whose `index` are the\ndates in question. In the case of a single DataFrame argument the `pairwise`\nargument can even be omitted:\n\nNote\n\nMissing values are ignored and each entry is computed using the pairwise\ncomplete observations. Please see the covariance section for caveats\nassociated with this method of calculating covariance and correlation\nmatrices.\n\nThe `win_type` argument in `.rolling` generates a weighted windows that are\ncommonly used in filtering and spectral estimation. `win_type` must be string\nthat corresponds to a scipy.signal window function. Scipy must be installed in\norder to use these windows, and supplementary arguments that the Scipy window\nmethods take must be specified in the aggregation function.\n\nFor all supported aggregation functions, see Weighted window functions.\n\nAn expanding window yields the value of an aggregation statistic with all the\ndata available up to that point in time. Since these calculations are a\nspecial case of rolling statistics, they are implemented in pandas such that\nthe following two calls are equivalent:\n\nFor all supported aggregation functions, see Expanding window functions.\n\nAn exponentially weighted window is similar to an expanding window but with\neach prior point being exponentially weighted down relative to the current\npoint.\n\nIn general, a weighted moving average is calculated as\n\nwhere \\\\(x_t\\\\) is the input, \\\\(y_t\\\\) is the result and the \\\\(w_i\\\\) are\nthe weights.\n\nFor all supported aggregation functions, see Exponentially-weighted window\nfunctions.\n\nThe EW functions support two variants of exponential weights. The default,\n`adjust=True`, uses the weights \\\\(w_i = (1 - \\alpha)^i\\\\) which gives\n\nWhen `adjust=False` is specified, moving averages are calculated as\n\nwhich is equivalent to using weights\n\nNote\n\nThese equations are sometimes written in terms of \\\\(\\alpha' = 1 - \\alpha\\\\),\ne.g.\n\nThe difference between the above two variants arises because we are dealing\nwith series which have finite history. Consider a series of infinite history,\nwith `adjust=True`:\n\nNoting that the denominator is a geometric series with initial term equal to 1\nand a ratio of \\\\(1 - \\alpha\\\\) we have\n\nwhich is the same expression as `adjust=False` above and therefore shows the\nequivalence of the two variants for infinite series. When `adjust=False`, we\nhave \\\\(y_0 = x_0\\\\) and \\\\(y_t = \\alpha x_t + (1 - \\alpha) y_{t-1}\\\\).\nTherefore, there is an assumption that \\\\(x_0\\\\) is not an ordinary value but\nrather an exponentially weighted moment of the infinite series up to that\npoint.\n\nOne must have \\\\(0 < \\alpha \\leq 1\\\\), and while it is possible to pass\n\\\\(\\alpha\\\\) directly, it\u2019s often easier to think about either the span,\ncenter of mass (com) or half-life of an EW moment:\n\nOne must specify precisely one of span, center of mass, half-life and alpha to\nthe EW functions:\n\nSpan corresponds to what is commonly called an \u201cN-day EW moving average\u201d.\n\nCenter of mass has a more physical interpretation and can be thought of in\nterms of span: \\\\(c = (s - 1) / 2\\\\).\n\nHalf-life is the period of time for the exponential weight to reduce to one\nhalf.\n\nAlpha specifies the smoothing factor directly.\n\nNew in version 1.1.0.\n\nYou can also specify `halflife` in terms of a timedelta convertible unit to\nspecify the amount of time it takes for an observation to decay to half its\nvalue when also specifying a sequence of `times`.\n\nThe following formula is used to compute exponentially weighted mean with an\ninput vector of times:\n\nExponentialMovingWindow also has an `ignore_na` argument, which determines how\nintermediate null values affect the calculation of the weights. When\n`ignore_na=False` (the default), weights are calculated based on absolute\npositions, so that intermediate null values affect the result. When\n`ignore_na=True`, weights are calculated by ignoring intermediate null values.\nFor example, assuming `adjust=True`, if `ignore_na=False`, the weighted\naverage of `3, NaN, 5` would be calculated as\n\nWhereas if `ignore_na=True`, the weighted average would be calculated as\n\nThe `var()`, `std()`, and `cov()` functions have a `bias` argument, specifying\nwhether the result should contain biased or unbiased statistics. For example,\nif `bias=True`, `ewmvar(x)` is calculated as `ewmvar(x) = ewma(x**2) -\newma(x)**2`; whereas if `bias=False` (the default), the biased variance\nstatistics are scaled by debiasing factors\n\n(For \\\\(w_i = 1\\\\), this reduces to the usual \\\\(N / (N - 1)\\\\) factor, with\n\\\\(N = t + 1\\\\).) See Weighted Sample Variance on Wikipedia for further\ndetails.\n\n"}, {"name": "Working with missing data", "path": "user_guide/missing_data", "type": "Manual", "text": "\nIn this section, we will discuss missing (also referred to as NA) values in\npandas.\n\nNote\n\nThe choice of using `NaN` internally to denote missing data was largely for\nsimplicity and performance reasons. Starting from pandas 1.0, some optional\ndata types start experimenting with a native `NA` scalar using a mask-based\napproach. See here for more.\n\nSee the cookbook for some advanced strategies.\n\nAs data comes in many shapes and forms, pandas aims to be flexible with regard\nto handling missing data. While `NaN` is the default missing value marker for\nreasons of computational speed and convenience, we need to be able to easily\ndetect this value with data of different types: floating point, integer,\nboolean, and general object. In many cases, however, the Python `None` will\narise and we wish to also consider that \u201cmissing\u201d or \u201cnot available\u201d or \u201cNA\u201d.\n\nNote\n\nIf you want to consider `inf` and `-inf` to be \u201cNA\u201d in computations, you can\nset `pandas.options.mode.use_inf_as_na = True`.\n\nTo make detecting missing values easier (and across different array dtypes),\npandas provides the `isna()` and `notna()` functions, which are also methods\non Series and DataFrame objects:\n\nWarning\n\nOne has to be mindful that in Python (and NumPy), the `nan's` don\u2019t compare\nequal, but `None's` do. Note that pandas/NumPy uses the fact that `np.nan !=\nnp.nan`, and treats `None` like `np.nan`.\n\nSo as compared to above, a scalar equality comparison versus a `None/np.nan`\ndoesn\u2019t provide useful information.\n\nBecause `NaN` is a float, a column of integers with even one missing values is\ncast to floating-point dtype (see Support for integer NA for more). pandas\nprovides a nullable integer array, which can be used by explicitly requesting\nthe dtype:\n\nAlternatively, the string alias `dtype='Int64'` (note the capital `\"I\"`) can\nbe used.\n\nSee Nullable integer data type for more.\n\nFor datetime64[ns] types, `NaT` represents missing values. This is a pseudo-\nnative sentinel value that can be represented by NumPy in a singular dtype\n(datetime64[ns]). pandas objects provide compatibility between `NaT` and\n`NaN`.\n\nYou can insert missing values by simply assigning to containers. The actual\nmissing value used will be chosen based on the dtype.\n\nFor example, numeric containers will always use `NaN` regardless of the\nmissing value type chosen:\n\nLikewise, datetime containers will always use `NaT`.\n\nFor object containers, pandas will use the value given:\n\nMissing values propagate naturally through arithmetic operations between\npandas objects.\n\nThe descriptive statistics and computational methods discussed in the data\nstructure overview (and listed here and here) are all written to account for\nmissing data. For example:\n\nWhen summing data, NA (missing) values will be treated as zero.\n\nIf the data are all NA, the result will be 0.\n\nCumulative methods like `cumsum()` and `cumprod()` ignore NA values by\ndefault, but preserve them in the resulting arrays. To override this behaviour\nand include NA values, use `skipna=False`.\n\nWarning\n\nThis behavior is now standard as of v0.22.0 and is consistent with the default\nin `numpy`; previously sum/prod of all-NA or empty Series/DataFrames would\nreturn NaN. See v0.22.0 whatsnew for more.\n\nThe sum of an empty or all-NA Series or column of a DataFrame is 0.\n\nThe product of an empty or all-NA Series or column of a DataFrame is 1.\n\nNA groups in GroupBy are automatically excluded. This behavior is consistent\nwith R, for example:\n\nSee the groupby section here for more information.\n\npandas objects are equipped with various data manipulation methods for dealing\nwith missing data.\n\n`fillna()` can \u201cfill in\u201d NA values with non-NA data in a couple of ways, which\nwe illustrate:\n\nReplace NA with a scalar value\n\nFill gaps forward or backward\n\nUsing the same filling arguments as reindexing, we can propagate non-NA values\nforward or backward:\n\nLimit the amount of filling\n\nIf we only want consecutive gaps filled up to a certain number of data points,\nwe can use the `limit` keyword:\n\nTo remind you, these are the available filling methods:\n\nMethod\n\nAction\n\npad / ffill\n\nFill values forward\n\nbfill / backfill\n\nFill values backward\n\nWith time series data, using pad/ffill is extremely common so that the \u201clast\nknown value\u201d is available at every time point.\n\n`ffill()` is equivalent to `fillna(method='ffill')` and `bfill()` is\nequivalent to `fillna(method='bfill')`\n\nYou can also fillna using a dict or Series that is alignable. The labels of\nthe dict or index of the Series must match the columns of the frame you wish\nto fill. The use case of this is to fill a DataFrame with the mean of that\ncolumn.\n\nSame result as above, but is aligning the \u2018fill\u2019 value which is a Series in\nthis case.\n\nYou may wish to simply exclude labels from a data set which refer to missing\ndata. To do this, use `dropna()`:\n\nAn equivalent `dropna()` is available for Series. DataFrame.dropna has\nconsiderably more options than Series.dropna, which can be examined in the\nAPI.\n\nBoth Series and DataFrame objects have `interpolate()` that, by default,\nperforms linear interpolation at missing data points.\n\nIndex aware interpolation is available via the `method` keyword:\n\nFor a floating-point index, use `method='values'`:\n\nYou can also interpolate with a DataFrame:\n\nThe `method` argument gives access to fancier interpolation methods. If you\nhave scipy installed, you can pass the name of a 1-d interpolation routine to\n`method`. You\u2019ll want to consult the full scipy interpolation documentation\nand reference guide for details. The appropriate interpolation method will\ndepend on the type of data you are working with.\n\nIf you are dealing with a time series that is growing at an increasing rate,\n`method='quadratic'` may be appropriate.\n\nIf you have values approximating a cumulative distribution function, then\n`method='pchip'` should work well.\n\nTo fill missing values with goal of smooth plotting, consider\n`method='akima'`.\n\nWarning\n\nThese methods require `scipy`.\n\nWhen interpolating via a polynomial or spline approximation, you must also\nspecify the degree or order of the approximation:\n\nCompare several methods:\n\nAnother use case is interpolation at new values. Suppose you have 100\nobservations from some distribution. And let\u2019s suppose that you\u2019re\nparticularly interested in what\u2019s happening around the middle. You can mix\npandas\u2019 `reindex` and `interpolate` methods to interpolate at the new values.\n\nLike other pandas fill methods, `interpolate()` accepts a `limit` keyword\nargument. Use this argument to limit the number of consecutive `NaN` values\nfilled since the last valid observation:\n\nBy default, `NaN` values are filled in a `forward` direction. Use\n`limit_direction` parameter to fill `backward` or from `both` directions.\n\nBy default, `NaN` values are filled whether they are inside (surrounded by)\nexisting valid values, or outside existing valid values. The `limit_area`\nparameter restricts filling to either inside or outside values.\n\nOften times we want to replace arbitrary values with other values.\n\n`replace()` in Series and `replace()` in DataFrame provides an efficient yet\nflexible way to perform such replacements.\n\nFor a Series, you can replace a single value or a list of values by another\nvalue:\n\nYou can replace a list of values by a list of other values:\n\nYou can also specify a mapping dict:\n\nFor a DataFrame, you can specify individual values by column:\n\nInstead of replacing with specified values, you can treat all given values as\nmissing and interpolate over them:\n\nNote\n\nPython strings prefixed with the `r` character such as `r'hello world'` are\nso-called \u201craw\u201d strings. They have different semantics regarding backslashes\nthan strings without this prefix. Backslashes in raw strings will be\ninterpreted as an escaped backslash, e.g., `r'\\' == '\\\\'`. You should read\nabout them if this is unclear.\n\nReplace the \u2018.\u2019 with `NaN` (str -> str):\n\nNow do it with a regular expression that removes surrounding whitespace (regex\n-> regex):\n\nReplace a few different values (list -> list):\n\nlist of regex -> list of regex:\n\nOnly search in column `'b'` (dict -> dict):\n\nSame as the previous example, but use a regular expression for searching\ninstead (dict of regex -> dict):\n\nYou can pass nested dictionaries of regular expressions that use `regex=True`:\n\nAlternatively, you can pass the nested dictionary like so:\n\nYou can also use the group of a regular expression match when replacing (dict\nof regex -> dict of regex), this works for lists as well.\n\nYou can pass a list of regular expressions, of which those that match will be\nreplaced with a scalar (list of regex -> regex).\n\nAll of the regular expression examples can also be passed with the\n`to_replace` argument as the `regex` argument. In this case the `value`\nargument must be passed explicitly by name or `regex` must be a nested\ndictionary. The previous example, in this case, would then be:\n\nThis can be convenient if you do not want to pass `regex=True` every time you\nwant to use a regular expression.\n\nNote\n\nAnywhere in the above `replace` examples that you see a regular expression a\ncompiled regular expression is valid as well.\n\n`replace()` is similar to `fillna()`.\n\nReplacing more than one value is possible by passing a list.\n\nYou can also operate on the DataFrame in place:\n\nWhile pandas supports storing arrays of integer and boolean type, these types\nare not capable of storing missing data. Until we can switch to using a native\nNA type in NumPy, we\u2019ve established some \u201ccasting rules\u201d. When a reindexing\noperation introduces missing data, the Series will be cast according to the\nrules introduced in the table below.\n\ndata type\n\nCast to\n\ninteger\n\nfloat\n\nboolean\n\nobject\n\nfloat\n\nno cast\n\nobject\n\nno cast\n\nFor example:\n\nOrdinarily NumPy will complain if you try to use an object array (even if it\ncontains boolean values) instead of a boolean array to get or set values from\nan ndarray (e.g. selecting values based on some criteria). If a boolean vector\ncontains NAs, an exception will be generated:\n\nHowever, these can be filled in using `fillna()` and it will work fine:\n\npandas provides a nullable integer dtype, but you must explicitly request it\nwhen creating the series or column. Notice that we use a capital \u201cI\u201d in the\n`dtype=\"Int64\"`.\n\nSee Nullable integer data type for more.\n\nWarning\n\nExperimental: the behaviour of `pd.NA` can still change without warning.\n\nNew in version 1.0.0.\n\nStarting from pandas 1.0, an experimental `pd.NA` value (singleton) is\navailable to represent scalar missing values. At this moment, it is used in\nthe nullable integer, boolean and dedicated string data types as the missing\nvalue indicator.\n\nThe goal of `pd.NA` is provide a \u201cmissing\u201d indicator that can be used\nconsistently across data types (instead of `np.nan`, `None` or `pd.NaT`\ndepending on the data type).\n\nFor example, when having missing values in a Series with the nullable integer\ndtype, it will use `pd.NA`:\n\nCurrently, pandas does not yet use those data types by default (when creating\na DataFrame or Series, or when reading in data), so you need to specify the\ndtype explicitly. An easy way to convert to those dtypes is explained here.\n\nIn general, missing values propagate in operations involving `pd.NA`. When one\nof the operands is unknown, the outcome of the operation is also unknown.\n\nFor example, `pd.NA` propagates in arithmetic operations, similarly to\n`np.nan`:\n\nThere are a few special cases when the result is known, even when one of the\noperands is `NA`.\n\nIn equality and comparison operations, `pd.NA` also propagates. This deviates\nfrom the behaviour of `np.nan`, where comparisons with `np.nan` always return\n`False`.\n\nTo check if a value is equal to `pd.NA`, the `isna()` function can be used:\n\nAn exception on this basic propagation rule are reductions (such as the mean\nor the minimum), where pandas defaults to skipping missing values. See above\nfor more.\n\nFor logical operations, `pd.NA` follows the rules of the three-valued logic\n(or Kleene logic, similarly to R, SQL and Julia). This logic means to only\npropagate missing values when it is logically required.\n\nFor example, for the logical \u201cor\u201d operation (`|`), if one of the operands is\n`True`, we already know the result will be `True`, regardless of the other\nvalue (so regardless the missing value would be `True` or `False`). In this\ncase, `pd.NA` does not propagate:\n\nOn the other hand, if one of the operands is `False`, the result depends on\nthe value of the other operand. Therefore, in this case `pd.NA` propagates:\n\nThe behaviour of the logical \u201cand\u201d operation (`&`) can be derived using\nsimilar logic (where now `pd.NA` will not propagate if one of the operands is\nalready `False`):\n\nSince the actual value of an NA is unknown, it is ambiguous to convert NA to a\nboolean value. The following raises an error:\n\nThis also means that `pd.NA` cannot be used in a context where it is evaluated\nto a boolean, such as `if condition: ...` where `condition` can potentially be\n`pd.NA`. In such cases, `isna()` can be used to check for `pd.NA` or\n`condition` being `pd.NA` can be avoided, for example by filling missing\nvalues beforehand.\n\nA similar situation occurs when using Series or DataFrame objects in `if`\nstatements, see Using if/truth statements with pandas.\n\n`pandas.NA` implements NumPy\u2019s `__array_ufunc__` protocol. Most ufuncs work\nwith `NA`, and generally return `NA`:\n\nWarning\n\nCurrently, ufuncs involving an ndarray and `NA` will return an object-dtype\nfilled with NA values.\n\nThe return type here may change to return a different array type in the\nfuture.\n\nSee DataFrame interoperability with NumPy functions for more on ufuncs.\n\nIf you have a DataFrame or Series using traditional types that have missing\ndata represented using `np.nan`, there are convenience methods\n`convert_dtypes()` in Series and `convert_dtypes()` in DataFrame that can\nconvert data to use the newer dtypes for integers, strings and booleans listed\nhere. This is especially helpful after reading in data sets when letting the\nreaders such as `read_csv()` and `read_excel()` infer default dtypes.\n\nIn this example, while the dtypes of all columns are changed, we show the\nresults for the first 10 columns.\n\n"}, {"name": "Working with text data", "path": "user_guide/text", "type": "Manual", "text": "\nNew in version 1.0.0.\n\nThere are two ways to store text data in pandas:\n\n`object` -dtype NumPy array.\n\n`StringDtype` extension type.\n\nWe recommend using `StringDtype` to store text data.\n\nPrior to pandas 1.0, `object` dtype was the only option. This was unfortunate\nfor many reasons:\n\nYou can accidentally store a mixture of strings and non-strings in an `object`\ndtype array. It\u2019s better to have a dedicated dtype.\n\n`object` dtype breaks dtype-specific operations like\n`DataFrame.select_dtypes()`. There isn\u2019t a clear way to select just text while\nexcluding non-text but still object-dtype columns.\n\nWhen reading code, the contents of an `object` dtype array is less clear than\n`'string'`.\n\nCurrently, the performance of `object` dtype arrays of strings and\n`arrays.StringArray` are about the same. We expect future enhancements to\nsignificantly increase the performance and lower the memory overhead of\n`StringArray`.\n\nWarning\n\n`StringArray` is currently considered experimental. The implementation and\nparts of the API may change without warning.\n\nFor backwards-compatibility, `object` dtype remains the default type we infer\na list of strings to\n\nTo explicitly request `string` dtype, specify the `dtype`\n\nOr `astype` after the `Series` or `DataFrame` is created\n\nChanged in version 1.1.0.\n\nYou can also use `StringDtype`/`\"string\"` as the dtype on non-string data and\nit will be converted to `string` dtype:\n\nor convert from existing pandas data:\n\nThese are places where the behavior of `StringDtype` objects differ from\n`object` dtype\n\nFor `StringDtype`, string accessor methods that return numeric output will\nalways return a nullable integer dtype, rather than either int or float dtype,\ndepending on the presence of NA values. Methods returning boolean output will\nreturn a nullable boolean dtype.\n\nBoth outputs are `Int64` dtype. Compare that with object-dtype\n\nWhen NA values are present, the output dtype is float64. Similarly for methods\nreturning boolean values.\n\nSome string methods, like `Series.str.decode()` are not available on\n`StringArray` because `StringArray` only holds strings, not bytes.\n\nIn comparison operations, `arrays.StringArray` and `Series` backed by a\n`StringArray` will return an object with `BooleanDtype`, rather than a `bool`\ndtype object. Missing values in a `StringArray` will propagate in comparison\noperations, rather than always comparing unequal like `numpy.nan`.\n\nEverything else that follows in the rest of this document applies equally to\n`string` and `object` dtype.\n\nSeries and Index are equipped with a set of string processing methods that\nmake it easy to operate on each element of the array. Perhaps most\nimportantly, these methods exclude missing/NA values automatically. These are\naccessed via the `str` attribute and generally have names matching the\nequivalent (scalar) built-in string methods:\n\nThe string methods on Index are especially useful for cleaning up or\ntransforming DataFrame columns. For instance, you may have columns with\nleading or trailing whitespace:\n\nSince `df.columns` is an Index object, we can use the `.str` accessor\n\nThese string methods can then be used to clean up the columns as needed. Here\nwe are removing leading and trailing whitespaces, lower casing all names, and\nreplacing any remaining whitespaces with underscores:\n\nNote\n\nIf you have a `Series` where lots of elements are repeated (i.e. the number of\nunique elements in the `Series` is a lot smaller than the length of the\n`Series`), it can be faster to convert the original `Series` to one of type\n`category` and then use `.str.<method>` or `.dt.<property>` on that. The\nperformance difference comes from the fact that, for `Series` of type\n`category`, the string operations are done on the `.categories` and not on\neach element of the `Series`.\n\nPlease note that a `Series` of type `category` with string `.categories` has\nsome limitations in comparison to `Series` of type string (e.g. you can\u2019t add\nstrings to each other: `s + \" \" + s` won\u2019t work if `s` is a `Series` of type\n`category`). Also, `.str` methods which operate on elements of type `list` are\nnot available on such a `Series`.\n\nWarning\n\nBefore v.0.25.0, the `.str`-accessor did only the most rudimentary type\nchecks. Starting with v.0.25.0, the type of the Series is inferred and the\nallowed types (i.e. strings) are enforced more rigorously.\n\nGenerally speaking, the `.str` accessor is intended to work only on strings.\nWith very few exceptions, other uses are not supported, and may be disabled at\na later point.\n\nMethods like `split` return a Series of lists:\n\nElements in the split lists can be accessed using `get` or `[]` notation:\n\nIt is easy to expand this to return a DataFrame using `expand`.\n\nWhen original `Series` has `StringDtype`, the output columns will all be\n`StringDtype` as well.\n\nIt is also possible to limit the number of splits:\n\n`rsplit` is similar to `split` except it works in the reverse direction, i.e.,\nfrom the end of the string to the beginning of the string:\n\n`replace` optionally uses regular expressions:\n\nWarning\n\nSome caution must be taken when dealing with regular expressions! The current\nbehavior is to treat single character patterns as literal strings, even when\n`regex` is set to `True`. This behavior is deprecated and will be removed in a\nfuture version so that the `regex` keyword is always respected.\n\nChanged in version 1.2.0.\n\nIf you want literal replacement of a string (equivalent to `str.replace()`),\nyou can set the optional `regex` parameter to `False`, rather than escaping\neach character. In this case both `pat` and `repl` must be strings:\n\nThe `replace` method can also take a callable as replacement. It is called on\nevery `pat` using `re.sub()`. The callable should expect one positional\nargument (a regex object) and return a string.\n\nThe `replace` method also accepts a compiled regular expression object from\n`re.compile()` as a pattern. All flags should be included in the compiled\nregular expression object.\n\nIncluding a `flags` argument when calling `replace` with a compiled regular\nexpression object will raise a `ValueError`.\n\n`removeprefix` and `removesuffix` have the same effect as `str.removeprefix`\nand `str.removesuffix` added in Python 3.9\n<https://docs.python.org/3/library/stdtypes.html#str.removeprefix>`__:\n\nNew in version 1.4.0.\n\nThere are several ways to concatenate a `Series` or `Index`, either with\nitself or others, all based on `cat()`, resp. `Index.str.cat`.\n\nThe content of a `Series` (or `Index`) can be concatenated:\n\nIf not specified, the keyword `sep` for the separator defaults to the empty\nstring, `sep=''`:\n\nBy default, missing values are ignored. Using `na_rep`, they can be given a\nrepresentation:\n\nThe first argument to `cat()` can be a list-like object, provided that it\nmatches the length of the calling `Series` (or `Index`).\n\nMissing values on either side will result in missing values in the result as\nwell, unless `na_rep` is specified:\n\nThe parameter `others` can also be two-dimensional. In this case, the number\nor rows must match the lengths of the calling `Series` (or `Index`).\n\nFor concatenation with a `Series` or `DataFrame`, it is possible to align the\nindexes before concatenation by setting the `join`-keyword.\n\nWarning\n\nIf the `join` keyword is not passed, the method `cat()` will currently fall\nback to the behavior before version 0.23.0 (i.e. no alignment), but a\n`FutureWarning` will be raised if any of the involved indexes differ, since\nthis default will change to `join='left'` in a future version.\n\nThe usual options are available for `join` (one of `'left', 'outer', 'inner',\n'right'`). In particular, alignment also means that the different lengths do\nnot need to coincide anymore.\n\nThe same alignment can be used when `others` is a `DataFrame`:\n\nSeveral array-like items (specifically: `Series`, `Index`, and 1-dimensional\nvariants of `np.ndarray`) can be combined in a list-like container (including\niterators, `dict`-views, etc.).\n\nAll elements without an index (e.g. `np.ndarray`) within the passed list-like\nmust match in length to the calling `Series` (or `Index`), but `Series` and\n`Index` may have arbitrary length (as long as alignment is not disabled with\n`join=None`):\n\nIf using `join='right'` on a list-like of `others` that contains different\nindexes, the union of these indexes will be used as the basis for the final\nconcatenation:\n\nYou can use `[]` notation to directly index by position locations. If you\nindex past the end of the string, the result will be a `NaN`.\n\nWarning\n\nBefore version 0.23, argument `expand` of the `extract` method defaulted to\n`False`. When `expand=False`, `expand` returns a `Series`, `Index`, or\n`DataFrame`, depending on the subject and regular expression pattern. When\n`expand=True`, it always returns a `DataFrame`, which is more consistent and\nless confusing from the perspective of a user. `expand=True` has been the\ndefault since version 0.23.0.\n\nThe `extract` method accepts a regular expression with at least one capture\ngroup.\n\nExtracting a regular expression with more than one group returns a DataFrame\nwith one column per group.\n\nElements that do not match return a row filled with `NaN`. Thus, a Series of\nmessy strings can be \u201cconverted\u201d into a like-indexed Series or DataFrame of\ncleaned-up or more useful strings, without necessitating `get()` to access\ntuples or `re.match` objects. The dtype of the result is always object, even\nif no match is found and the result only contains `NaN`.\n\nNamed groups like\n\nand optional groups like\n\ncan also be used. Note that any capture group names in the regular expression\nwill be used for column names; otherwise capture group numbers will be used.\n\nExtracting a regular expression with one group returns a `DataFrame` with one\ncolumn if `expand=True`.\n\nIt returns a Series if `expand=False`.\n\nCalling on an `Index` with a regex with exactly one capture group returns a\n`DataFrame` with one column if `expand=True`.\n\nIt returns an `Index` if `expand=False`.\n\nCalling on an `Index` with a regex with more than one capture group returns a\n`DataFrame` if `expand=True`.\n\nIt raises `ValueError` if `expand=False`.\n\nThe table below summarizes the behavior of `extract(expand=False)` (input\nsubject in first column, number of groups in regex in first row)\n\n1 group\n\n>1 group\n\nIndex\n\nIndex\n\nValueError\n\nSeries\n\nSeries\n\nDataFrame\n\nUnlike `extract` (which returns only the first match),\n\nthe `extractall` method returns every match. The result of `extractall` is\nalways a `DataFrame` with a `MultiIndex` on its rows. The last level of the\n`MultiIndex` is named `match` and indicates the order in the subject.\n\nWhen each subject string in the Series has exactly one match,\n\nthen `extractall(pat).xs(0, level='match')` gives the same result as\n`extract(pat)`.\n\n`Index` also supports `.str.extractall`. It returns a `DataFrame` which has\nthe same result as a `Series.str.extractall` with a default index (starts from\n0).\n\nYou can check whether elements contain a pattern:\n\nOr whether elements match a pattern:\n\nNew in version 1.1.0.\n\nNote\n\nThe distinction between `match`, `fullmatch`, and `contains` is strictness:\n`fullmatch` tests whether the entire string matches the regular expression;\n`match` tests whether there is a match of the regular expression that begins\nat the first character of the string; and `contains` tests whether there is a\nmatch of the regular expression at any position within the string.\n\nThe corresponding functions in the `re` package for these three match modes\nare re.fullmatch, re.match, and re.search, respectively.\n\nMethods like `match`, `fullmatch`, `contains`, `startswith`, and `endswith`\ntake an extra `na` argument so missing values can be considered True or False:\n\nYou can extract dummy variables from string columns. For example if they are\nseparated by a `'|'`:\n\nString `Index` also supports `get_dummies` which returns a `MultiIndex`.\n\nSee also `get_dummies()`.\n\nMethod\n\nDescription\n\n`cat()`\n\nConcatenate strings\n\n`split()`\n\nSplit strings on delimiter\n\n`rsplit()`\n\nSplit strings on delimiter working from the end of the string\n\n`get()`\n\nIndex into each element (retrieve i-th element)\n\n`join()`\n\nJoin strings in each element of the Series with passed separator\n\n`get_dummies()`\n\nSplit strings on the delimiter returning DataFrame of dummy variables\n\n`contains()`\n\nReturn boolean array if each string contains pattern/regex\n\n`replace()`\n\nReplace occurrences of pattern/regex/string with some other string or the\nreturn value of a callable given the occurrence\n\n`removeprefix()`\n\nRemove prefix from string, i.e. only remove if string starts with prefix.\n\n`removesuffix()`\n\nRemove suffix from string, i.e. only remove if string ends with suffix.\n\n`repeat()`\n\nDuplicate values (`s.str.repeat(3)` equivalent to `x * 3`)\n\n`pad()`\n\nAdd whitespace to left, right, or both sides of strings\n\n`center()`\n\nEquivalent to `str.center`\n\n`ljust()`\n\nEquivalent to `str.ljust`\n\n`rjust()`\n\nEquivalent to `str.rjust`\n\n`zfill()`\n\nEquivalent to `str.zfill`\n\n`wrap()`\n\nSplit long strings into lines with length less than a given width\n\n`slice()`\n\nSlice each string in the Series\n\n`slice_replace()`\n\nReplace slice in each string with passed value\n\n`count()`\n\nCount occurrences of pattern\n\n`startswith()`\n\nEquivalent to `str.startswith(pat)` for each element\n\n`endswith()`\n\nEquivalent to `str.endswith(pat)` for each element\n\n`findall()`\n\nCompute list of all occurrences of pattern/regex for each string\n\n`match()`\n\nCall `re.match` on each element, returning matched groups as list\n\n`extract()`\n\nCall `re.search` on each element, returning DataFrame with one row for each\nelement and one column for each regex capture group\n\n`extractall()`\n\nCall `re.findall` on each element, returning DataFrame with one row for each\nmatch and one column for each regex capture group\n\n`len()`\n\nCompute string lengths\n\n`strip()`\n\nEquivalent to `str.strip`\n\n`rstrip()`\n\nEquivalent to `str.rstrip`\n\n`lstrip()`\n\nEquivalent to `str.lstrip`\n\n`partition()`\n\nEquivalent to `str.partition`\n\n`rpartition()`\n\nEquivalent to `str.rpartition`\n\n`lower()`\n\nEquivalent to `str.lower`\n\n`casefold()`\n\nEquivalent to `str.casefold`\n\n`upper()`\n\nEquivalent to `str.upper`\n\n`find()`\n\nEquivalent to `str.find`\n\n`rfind()`\n\nEquivalent to `str.rfind`\n\n`index()`\n\nEquivalent to `str.index`\n\n`rindex()`\n\nEquivalent to `str.rindex`\n\n`capitalize()`\n\nEquivalent to `str.capitalize`\n\n`swapcase()`\n\nEquivalent to `str.swapcase`\n\n`normalize()`\n\nReturn Unicode normal form. Equivalent to `unicodedata.normalize`\n\n`translate()`\n\nEquivalent to `str.translate`\n\n`isalnum()`\n\nEquivalent to `str.isalnum`\n\n`isalpha()`\n\nEquivalent to `str.isalpha`\n\n`isdigit()`\n\nEquivalent to `str.isdigit`\n\n`isspace()`\n\nEquivalent to `str.isspace`\n\n`islower()`\n\nEquivalent to `str.islower`\n\n`isupper()`\n\nEquivalent to `str.isupper`\n\n`istitle()`\n\nEquivalent to `str.istitle`\n\n`isnumeric()`\n\nEquivalent to `str.isnumeric`\n\n`isdecimal()`\n\nEquivalent to `str.isdecimal`\n\n"}]