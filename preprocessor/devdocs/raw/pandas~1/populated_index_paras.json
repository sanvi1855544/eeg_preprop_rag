[{"name": "10 minutes to pandas", "path": "user_guide/10min", "type": "Manual", "text": ["This is a short introduction to pandas, geared mainly for new users. You can see more complex recipes in the Cookbook.", "Customarily, we import as follows:", "See the Intro to data structures section.", "Creating a Series by passing a list of values, letting pandas create a default integer index:", "Creating a DataFrame by passing a NumPy array, with a datetime index and labeled columns:", "Creating a DataFrame by passing a dictionary of objects that can be converted into a series-like structure:", "The columns of the resulting DataFrame have different dtypes:", "If you\u2019re using IPython, tab completion for column names (as well as public attributes) is automatically enabled. Here\u2019s a subset of the attributes that will be completed:", "As you can see, the columns A, B, C, and D are automatically tab completed. E and F are there as well; the rest of the attributes have been truncated for brevity.", "See the Basics section.", "Here is how to view the top and bottom rows of the frame:", "Display the index, columns:", "DataFrame.to_numpy() gives a NumPy representation of the underlying data. Note that this can be an expensive operation when your DataFrame has columns with different data types, which comes down to a fundamental difference between pandas and NumPy: NumPy arrays have one dtype for the entire array, while pandas DataFrames have one dtype per column. When you call DataFrame.to_numpy(), pandas will find the NumPy dtype that can hold all of the dtypes in the DataFrame. This may end up being object, which requires casting every value to a Python object.", "For df, our DataFrame of all floating-point values, DataFrame.to_numpy() is fast and doesn\u2019t require copying data:", "For df2, the DataFrame with multiple dtypes, DataFrame.to_numpy() is relatively expensive:", "Note", "DataFrame.to_numpy() does not include the index or column labels in the output.", "describe() shows a quick statistic summary of your data:", "Transposing your data:", "Sorting by an axis:", "Sorting by values:", "Note", "While standard Python / NumPy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods, .at, .iat, .loc and .iloc.", "See the indexing documentation Indexing and Selecting Data and MultiIndex / Advanced Indexing.", "Selecting a single column, which yields a Series, equivalent to df.A:", "Selecting via [], which slices the rows:", "See more in Selection by Label.", "For getting a cross section using a label:", "Selecting on a multi-axis by label:", "Showing label slicing, both endpoints are included:", "Reduction in the dimensions of the returned object:", "For getting a scalar value:", "For getting fast access to a scalar (equivalent to the prior method):", "See more in Selection by Position.", "Select via the position of the passed integers:", "By integer slices, acting similar to NumPy/Python:", "By lists of integer position locations, similar to the NumPy/Python style:", "For slicing rows explicitly:", "For slicing columns explicitly:", "For getting a value explicitly:", "For getting fast access to a scalar (equivalent to the prior method):", "Using a single column\u2019s values to select data:", "Selecting values from a DataFrame where a boolean condition is met:", "Using the isin() method for filtering:", "Setting a new column automatically aligns the data by the indexes:", "Setting values by label:", "Setting values by position:", "Setting by assigning with a NumPy array:", "The result of the prior setting operations:", "A where operation with setting:", "pandas primarily uses the value np.nan to represent missing data. It is by default not included in computations. See the Missing Data section.", "Reindexing allows you to change/add/delete the index on a specified axis. This returns a copy of the data:", "To drop any rows that have missing data:", "Filling missing data:", "To get the boolean mask where values are nan:", "See the Basic section on Binary Ops.", "Operations in general exclude missing data.", "Performing a descriptive statistic:", "Same operation on the other axis:", "Operating with objects that have different dimensionality and need alignment. In addition, pandas automatically broadcasts along the specified dimension:", "Applying functions to the data:", "See more at Histogramming and Discretization.", "Series is equipped with a set of string processing methods in the str attribute that make it easy to operate on each element of the array, as in the code snippet below. Note that pattern-matching in str generally uses regular expressions by default (and in some cases always uses them). See more at Vectorized String Methods.", "pandas provides various facilities for easily combining together Series and DataFrame objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.", "See the Merging section.", "Concatenating pandas objects together with concat():", "Note", "Adding a column to a DataFrame is relatively fast. However, adding a row requires a copy, and may be expensive. We recommend passing a pre-built list of records to the DataFrame constructor instead of building a DataFrame by iteratively appending records to it.", "SQL style merges. See the Database style joining section.", "Another example that can be given is:", "By \u201cgroup by\u201d we are referring to a process involving one or more of the following steps:", "Splitting the data into groups based on some criteria", "Applying a function to each group independently", "Combining the results into a data structure", "See the Grouping section.", "Grouping and then applying the sum() function to the resulting groups:", "Grouping by multiple columns forms a hierarchical index, and again we can apply the sum() function:", "See the sections on Hierarchical Indexing and Reshaping.", "The stack() method \u201ccompresses\u201d a level in the DataFrame\u2019s columns:", "With a \u201cstacked\u201d DataFrame or Series (having a MultiIndex as the index), the inverse operation of stack() is unstack(), which by default unstacks the last level:", "See the section on Pivot Tables.", "We can produce pivot tables from this data very easily:", "pandas has simple, powerful, and efficient functionality for performing resampling operations during frequency conversion (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications. See the Time Series section.", "Time zone representation:", "Converting to another time zone:", "Converting between time span representations:", "Converting between period and timestamp enables some convenient arithmetic functions to be used. In the following example, we convert a quarterly frequency with year ending in November to 9am of the end of the month following the quarter end:", "pandas can include categorical data in a DataFrame. For full docs, see the categorical introduction and the API documentation.", "Converting the raw grades to a categorical data type:", "Rename the categories to more meaningful names (assigning to Series.cat.categories() is in place!):", "Reorder the categories and simultaneously add the missing categories (methods under Series.cat() return a new Series by default):", "Sorting is per order in the categories, not lexical order:", "Grouping by a categorical column also shows empty categories:", "See the Plotting docs.", "We use the standard convention for referencing the matplotlib API:", "The close() method is used to close a figure window:", "If running under Jupyter Notebook, the plot will appear on plot(). Otherwise use matplotlib.pyplot.show to show it or matplotlib.pyplot.savefig to write it to a file.", "On a DataFrame, the plot() method is a convenience to plot all of the columns with labels:", "Writing to a csv file:", "Reading from a csv file:", "Reading and writing to HDFStores.", "Writing to a HDF5 Store:", "Reading from a HDF5 Store:", "Reading and writing to MS Excel.", "Writing to an excel file:", "Reading from an excel file:", "If you are attempting to perform an operation you might see an exception like:", "See Comparisons for an explanation and what to do.", "See Gotchas as well."]}, {"name": "API reference", "path": "reference/index", "type": "General functions", "text": ["This page gives an overview of all public pandas objects, functions and methods. All classes and functions exposed in pandas.* namespace are public.", "Some subpackages are public which include pandas.errors, pandas.plotting, and pandas.testing. Public functions in pandas.io and pandas.tseries submodules are mentioned in the documentation. pandas.api.types subpackage holds some public functions related to data types in pandas.", "Warning", "The pandas.core, pandas.compat, and pandas.util top-level modules are PRIVATE. Stable functionality in such modules is not guaranteed."]}, {"name": "Categorical data", "path": "user_guide/categorical", "type": "Manual", "text": ["This is an introduction to pandas categorical data type, including a short comparison with R\u2019s factor.", "Categoricals are a pandas data type corresponding to categorical variables in statistics. A categorical variable takes on a limited, and usually fixed, number of possible values (categories; levels in R). Examples are gender, social class, blood type, country affiliation, observation time or rating via Likert scales.", "In contrast to statistical categorical variables, categorical data might have an order (e.g. \u2018strongly agree\u2019 vs \u2018agree\u2019 or \u2018first observation\u2019 vs. \u2018second observation\u2019), but numerical operations (additions, divisions, \u2026) are not possible.", "All values of categorical data are either in categories or np.nan. Order is defined by the order of categories, not lexical order of the values. Internally, the data structure consists of a categories array and an integer array of codes which point to the real value in the categories array.", "The categorical data type is useful in the following cases:", "A string variable consisting of only a few different values. Converting such a string variable to a categorical variable will save some memory, see here.", "The lexical order of a variable is not the same as the logical order (\u201cone\u201d, \u201ctwo\u201d, \u201cthree\u201d). By converting to a categorical and specifying an order on the categories, sorting and min/max will use the logical order instead of the lexical order, see here.", "As a signal to other Python libraries that this column should be treated as a categorical variable (e.g. to use suitable statistical methods or plot types).", "See also the API docs on categoricals.", "Categorical Series or columns in a DataFrame can be created in several ways:", "By specifying dtype=\"category\" when constructing a Series:", "By converting an existing Series or column to a category dtype:", "By using special functions, such as cut(), which groups data into discrete bins. See the example on tiling in the docs.", "By passing a pandas.Categorical object to a Series or assigning it to a DataFrame.", "Categorical data has a specific category dtype:", "Similar to the previous section where a single column was converted to categorical, all columns in a DataFrame can be batch converted to categorical either during or after construction.", "This can be done during construction by specifying dtype=\"category\" in the DataFrame constructor:", "Note that the categories present in each column differ; the conversion is done column by column, so only labels present in a given column are categories:", "Analogously, all columns in an existing DataFrame can be batch converted using DataFrame.astype():", "This conversion is likewise done column by column:", "In the examples above where we passed dtype='category', we used the default behavior:", "Categories are inferred from the data.", "Categories are unordered.", "To control those behaviors, instead of passing 'category', use an instance of CategoricalDtype.", "Similarly, a CategoricalDtype can be used with a DataFrame to ensure that categories are consistent among all columns.", "Note", "To perform table-wise conversion, where all labels in the entire DataFrame are used as categories for each column, the categories parameter can be determined programmatically by categories = pd.unique(df.to_numpy().ravel()).", "If you already have codes and categories, you can use the from_codes() constructor to save the factorize step during normal constructor mode:", "To get back to the original Series or NumPy array, use Series.astype(original_dtype) or np.asarray(categorical):", "Note", "In contrast to R\u2019s factor function, categorical data is not converting input values to strings; categories will end up the same data type as the original values.", "Note", "In contrast to R\u2019s factor function, there is currently no way to assign/change labels at creation time. Use categories to change the categories after creation time.", "A categorical\u2019s type is fully described by", "categories: a sequence of unique values and no missing values", "ordered: a boolean", "This information can be stored in a CategoricalDtype. The categories argument is optional, which implies that the actual categories should be inferred from whatever is present in the data when the pandas.Categorical is created. The categories are assumed to be unordered by default.", "A CategoricalDtype can be used in any place pandas expects a dtype. For example pandas.read_csv(), pandas.DataFrame.astype(), or in the Series constructor.", "Note", "As a convenience, you can use the string 'category' in place of a CategoricalDtype when you want the default behavior of the categories being unordered, and equal to the set values present in the array. In other words, dtype='category' is equivalent to dtype=CategoricalDtype().", "Two instances of CategoricalDtype compare equal whenever they have the same categories and order. When comparing two unordered categoricals, the order of the categories is not considered.", "All instances of CategoricalDtype compare equal to the string 'category'.", "Warning", "Since dtype='category' is essentially CategoricalDtype(None, False), and since all instances CategoricalDtype compare equal to 'category', all instances of CategoricalDtype compare equal to a CategoricalDtype(None, False), regardless of categories or ordered.", "Using describe() on categorical data will produce similar output to a Series or DataFrame of type string.", "Categorical data has a categories and a ordered property, which list their possible values and whether the ordering matters or not. These properties are exposed as s.cat.categories and s.cat.ordered. If you don\u2019t manually specify categories and ordering, they are inferred from the passed arguments.", "It\u2019s also possible to pass in the categories in a specific order:", "Note", "New categorical data are not automatically ordered. You must explicitly pass ordered=True to indicate an ordered Categorical.", "Note", "The result of unique() is not always the same as Series.cat.categories, because Series.unique() has a couple of guarantees, namely that it returns categories in the order of appearance, and it only includes values that are actually present.", "Renaming categories is done by assigning new values to the Series.cat.categories property or by using the rename_categories() method:", "Note", "In contrast to R\u2019s factor, categorical data can have categories of other types than string.", "Note", "Be aware that assigning new categories is an inplace operation, while most other operations under Series.cat per default return a new Series of dtype category.", "Categories must be unique or a ValueError is raised:", "Categories must also not be NaN or a ValueError is raised:", "Appending categories can be done by using the add_categories() method:", "Removing categories can be done by using the remove_categories() method. Values which are removed are replaced by np.nan.:", "Removing unused categories can also be done:", "If you want to do remove and add new categories in one step (which has some speed advantage), or simply set the categories to a predefined scale, use set_categories().", "Note", "Be aware that Categorical.set_categories() cannot know whether some category is omitted intentionally or because it is misspelled or (under Python3) due to a type difference (e.g., NumPy S1 dtype and Python strings). This can result in surprising behaviour!", "If categorical data is ordered (s.cat.ordered == True), then the order of the categories has a meaning and certain operations are possible. If the categorical is unordered, .min()/.max() will raise a TypeError.", "You can set categorical data to be ordered by using as_ordered() or unordered by using as_unordered(). These will by default return a new object.", "Sorting will use the order defined by categories, not any lexical order present on the data type. This is even true for strings and numeric data:", "Reordering the categories is possible via the Categorical.reorder_categories() and the Categorical.set_categories() methods. For Categorical.reorder_categories(), all old categories must be included in the new categories and no new categories are allowed. This will necessarily make the sort order the same as the categories order.", "Note", "Note the difference between assigning new categories and reordering the categories: the first renames categories and therefore the individual values in the Series, but if the first position was sorted last, the renamed value will still be sorted last. Reordering means that the way values are sorted is different afterwards, but not that individual values in the Series are changed.", "Note", "If the Categorical is not ordered, Series.min() and Series.max() will raise TypeError. Numeric operations like +, -, *, / and operations based on them (e.g. Series.median(), which would need to compute the mean between two values if the length of an array is even) do not work and raise a TypeError.", "A categorical dtyped column will participate in a multi-column sort in a similar manner to other columns. The ordering of the categorical is determined by the categories of that column.", "Reordering the categories changes a future sort.", "Comparing categorical data with other objects is possible in three cases:", "Comparing equality (== and !=) to a list-like object (list, Series, array, \u2026) of the same length as the categorical data.", "All comparisons (==, !=, >, >=, <, and <=) of categorical data to another categorical Series, when ordered==True and the categories are the same.", "All comparisons of a categorical data to a scalar.", "All other comparisons, especially \u201cnon-equality\u201d comparisons of two categoricals with different categories or a categorical with any list-like object, will raise a TypeError.", "Note", "Any \u201cnon-equality\u201d comparisons of categorical data with a Series, np.array, list or categorical data with different categories or ordering will raise a TypeError because custom categories ordering could be interpreted in two ways: one with taking into account the ordering and one without.", "Comparing to a categorical with the same categories and ordering or to a scalar works:", "Equality comparisons work with any list-like object of same length and scalars:", "This doesn\u2019t work because the categories are not the same:", "If you want to do a \u201cnon-equality\u201d comparison of a categorical series with a list-like object which is not categorical data, you need to be explicit and convert the categorical data back to the original values:", "When you compare two unordered categoricals with the same categories, the order is not considered:", "Apart from Series.min(), Series.max() and Series.mode(), the following operations are possible with categorical data:", "Series methods like Series.value_counts() will use all categories, even if some categories are not present in the data:", "DataFrame methods like DataFrame.sum() also show \u201cunused\u201d categories.", "Groupby will also show \u201cunused\u201d categories:", "Pivot tables:", "The optimized pandas data access methods .loc, .iloc, .at, and .iat, work as normal. The only difference is the return type (for getting) and that only values already in categories can be assigned.", "If the slicing operation returns either a DataFrame or a column of type Series, the category dtype is preserved.", "An example where the category type is not preserved is if you take one single row: the resulting Series is of dtype object:", "Returning a single item from categorical data will also return the value, not a categorical of length \u201c1\u201d.", "Note", "The is in contrast to R\u2019s factor function, where factor(c(1,2,3))[1] returns a single value factor.", "To get a single value Series of type category, you pass in a list with a single value:", "The accessors .dt and .str will work if the s.cat.categories are of an appropriate type:", "Note", "The returned Series (or DataFrame) is of the same type as if you used the .str.<method> / .dt.<method> on a Series of that type (and not of type category!).", "That means, that the returned values from methods and properties on the accessors of a Series and the returned values from methods and properties on the accessors of this Series transformed to one of type category will be equal:", "Note", "The work is done on the categories and then a new Series is constructed. This has some performance implication if you have a Series of type string, where lots of elements are repeated (i.e. the number of unique elements in the Series is a lot smaller than the length of the Series). In this case it can be faster to convert the original Series to one of type category and use .str.<method> or .dt.<property> on that.", "Setting values in a categorical column (or Series) works as long as the value is included in the categories:", "Setting values by assigning categorical data will also check that the categories match:", "Assigning a Categorical to parts of a column of other types will use the values:", "By default, combining Series or DataFrames which contain the same categories results in category dtype, otherwise results will depend on the dtype of the underlying categories. Merges that result in non-categorical dtypes will likely have higher memory usage. Use .astype or union_categoricals to ensure category results.", "The following table summarizes the results of merging Categoricals:", "arg1", "arg2", "identical", "result", "category", "category", "True", "category", "category (object)", "category (object)", "False", "object (dtype is inferred)", "category (int)", "category (float)", "False", "float (dtype is inferred)", "See also the section on merge dtypes for notes about preserving merge dtypes and performance.", "If you want to combine categoricals that do not necessarily have the same categories, the union_categoricals() function will combine a list-like of categoricals. The new categories will be the union of the categories being combined.", "By default, the resulting categories will be ordered as they appear in the data. If you want the categories to be lexsorted, use sort_categories=True argument.", "union_categoricals also works with the \u201ceasy\u201d case of combining two categoricals of the same categories and order information (e.g. what you could also append for).", "The below raises TypeError because the categories are ordered and not identical.", "Ordered categoricals with different categories or orderings can be combined by using the ignore_ordered=True argument.", "union_categoricals() also works with a CategoricalIndex, or Series containing categorical data, but note that the resulting array will always be a plain Categorical:", "Note", "union_categoricals may recode the integer codes for categories when combining categoricals. This is likely what you want, but if you are relying on the exact numbering of the categories, be aware.", "You can write data that contains category dtypes to a HDFStore. See here for an example and caveats.", "It is also possible to write data to and reading data from Stata format files. See here for an example and caveats.", "Writing to a CSV file will convert the data, effectively removing any information about the categorical (categories and ordering). So if you read back the CSV file you have to convert the relevant columns back to category and assign the right categories and categories ordering.", "The same holds for writing to a SQL database with to_sql.", "pandas primarily uses the value np.nan to represent missing data. It is by default not included in computations. See the Missing Data section.", "Missing values should not be included in the Categorical\u2019s categories, only in the values. Instead, it is understood that NaN is different, and is always a possibility. When working with the Categorical\u2019s codes, missing values will always have a code of -1.", "Methods for working with missing data, e.g. isna(), fillna(), dropna(), all work normally:", "The following differences to R\u2019s factor functions can be observed:", "R\u2019s levels are named categories.", "R\u2019s levels are always of type string, while categories in pandas can be of any dtype.", "It\u2019s not possible to specify labels at creation time. Use s.cat.rename_categories(new_labels) afterwards.", "In contrast to R\u2019s factor function, using categorical data as the sole input to create a new categorical series will not remove unused categories but create a new categorical series which is equal to the passed in one!", "R allows for missing values to be included in its levels (pandas\u2019 categories). pandas does not allow NaN categories, but missing values can still be in the values.", "The memory usage of a Categorical is proportional to the number of categories plus the length of the data. In contrast, an object dtype is a constant times the length of the data.", "Note", "If the number of categories approaches the length of the data, the Categorical will use nearly the same or more memory than an equivalent object dtype representation.", "Currently, categorical data and the underlying Categorical is implemented as a Python object and not as a low-level NumPy array dtype. This leads to some problems.", "NumPy itself doesn\u2019t know about the new dtype:", "Dtype comparisons work:", "To check if a Series contains Categorical data, use hasattr(s, 'cat'):", "Using NumPy functions on a Series of type category should not work as Categoricals are not numeric data (even in the case that .categories is numeric).", "Note", "If such a function works, please file a bug at https://github.com/pandas-dev/pandas!", "pandas currently does not preserve the dtype in apply functions: If you apply along rows you get a Series of object dtype (same as getting a row -> getting one element will return a basic type) and applying along columns will also convert to object. NaN values are unaffected. You can use fillna to handle missing values before applying a function.", "CategoricalIndex is a type of index that is useful for supporting indexing with duplicates. This is a container around a Categorical and allows efficient indexing and storage of an index with a large number of duplicated elements. See the advanced indexing docs for a more detailed explanation.", "Setting the index will create a CategoricalIndex:", "Constructing a Series from a Categorical will not copy the input Categorical. This means that changes to the Series will in most cases change the original Categorical:", "Use copy=True to prevent such a behaviour or simply don\u2019t reuse Categoricals:", "Note", "This also happens in some cases when you supply a NumPy array instead of a Categorical: using an int array (e.g. np.array([1,2,3,4])) will exhibit the same behavior, while using a string array (e.g. np.array([\"a\",\"b\",\"c\",\"a\"])) will not."]}, {"name": "Chart Visualization", "path": "user_guide/visualization", "type": "Manual", "text": ["This section demonstrates visualization through charting. For information on visualization of tabular data please see the section on Table Visualization.", "We use the standard convention for referencing the matplotlib API:", "We provide the basics in pandas to easily create decent looking plots. See the ecosystem section for visualization libraries that go beyond the basics documented here.", "Note", "All calls to np.random are seeded with 123456.", "We will demonstrate the basics, see the cookbook for some advanced strategies.", "The plot method on Series and DataFrame is just a simple wrapper around plt.plot():", "If the index consists of dates, it calls gcf().autofmt_xdate() to try to format the x-axis nicely as per above.", "On DataFrame, plot() is a convenience to plot all of the columns with labels:", "You can plot one column versus another using the x and y keywords in plot():", "Note", "For more formatting and styling options, see formatting below.", "Plotting methods allow for a handful of plot styles other than the default line plot. These methods can be provided as the kind keyword argument to plot(), and include:", "\u2018bar\u2019 or \u2018barh\u2019 for bar plots", "\u2018hist\u2019 for histogram", "\u2018box\u2019 for boxplot", "\u2018kde\u2019 or \u2018density\u2019 for density plots", "\u2018area\u2019 for area plots", "\u2018scatter\u2019 for scatter plots", "\u2018hexbin\u2019 for hexagonal bin plots", "\u2018pie\u2019 for pie plots", "For example, a bar plot can be created the following way:", "You can also create these other plots using the methods DataFrame.plot.<kind> instead of providing the kind keyword argument. This makes it easier to discover plot methods and the specific arguments they use:", "In addition to these kind s, there are the DataFrame.hist(), and DataFrame.boxplot() methods, which use a separate interface.", "Finally, there are several plotting functions in pandas.plotting that take a Series or DataFrame as an argument. These include:", "Scatter Matrix", "Andrews Curves", "Parallel Coordinates", "Lag Plot", "Autocorrelation Plot", "Bootstrap Plot", "RadViz", "Plots may also be adorned with errorbars or tables.", "For labeled, non-time series data, you may wish to produce a bar plot:", "Calling a DataFrame\u2019s plot.bar() method produces a multiple bar plot:", "To produce a stacked bar plot, pass stacked=True:", "To get horizontal bar plots, use the barh method:", "Histograms can be drawn by using the DataFrame.plot.hist() and Series.plot.hist() methods.", "A histogram can be stacked using stacked=True. Bin size can be changed using the bins keyword.", "You can pass other keywords supported by matplotlib hist. For example, horizontal and cumulative histograms can be drawn by orientation='horizontal' and cumulative=True.", "See the hist method and the matplotlib hist documentation for more.", "The existing interface DataFrame.hist to plot histogram still can be used.", "DataFrame.hist() plots the histograms of the columns on multiple subplots:", "The by keyword can be specified to plot grouped histograms:", "In addition, the by keyword can also be specified in DataFrame.plot.hist().", "Changed in version 1.4.0.", "Boxplot can be drawn calling Series.plot.box() and DataFrame.plot.box(), or DataFrame.boxplot() to visualize the distribution of values within each column.", "For instance, here is a boxplot representing five trials of 10 observations of a uniform random variable on [0,1).", "Boxplot can be colorized by passing color keyword. You can pass a dict whose keys are boxes, whiskers, medians and caps. If some keys are missing in the dict, default colors are used for the corresponding artists. Also, boxplot has sym keyword to specify fliers style.", "When you pass other type of arguments via color keyword, it will be directly passed to matplotlib for all the boxes, whiskers, medians and caps colorization.", "The colors are applied to every boxes to be drawn. If you want more complicated colorization, you can get each drawn artists by passing return_type.", "Also, you can pass other keywords supported by matplotlib boxplot. For example, horizontal and custom-positioned boxplot can be drawn by vert=False and positions keywords.", "See the boxplot method and the matplotlib boxplot documentation for more.", "The existing interface DataFrame.boxplot to plot boxplot still can be used.", "You can create a stratified boxplot using the by keyword argument to create groupings. For instance,", "You can also pass a subset of columns to plot, as well as group by multiple columns:", "You could also create groupings with DataFrame.plot.box(), for instance:", "Changed in version 1.4.0.", "In boxplot, the return type can be controlled by the return_type, keyword. The valid choices are {\"axes\", \"dict\", \"both\", None}. Faceting, created by DataFrame.boxplot with the by keyword, will affect the output type as well:", "return_type", "Faceted", "Output type", "None", "No", "axes", "None", "Yes", "2-D ndarray of axes", "'axes'", "No", "axes", "'axes'", "Yes", "Series of axes", "'dict'", "No", "dict of artists", "'dict'", "Yes", "Series of dicts of artists", "'both'", "No", "namedtuple", "'both'", "Yes", "Series of namedtuples", "Groupby.boxplot always returns a Series of return_type.", "The subplots above are split by the numeric columns first, then the value of the g column. Below the subplots are first split by the value of g, then by the numeric columns.", "You can create area plots with Series.plot.area() and DataFrame.plot.area(). Area plots are stacked by default. To produce stacked area plot, each column must be either all positive or all negative values.", "When input data contains NaN, it will be automatically filled by 0. If you want to drop or fill by different values, use dataframe.dropna() or dataframe.fillna() before calling plot.", "To produce an unstacked plot, pass stacked=False. Alpha value is set to 0.5 unless otherwise specified:", "Scatter plot can be drawn by using the DataFrame.plot.scatter() method. Scatter plot requires numeric columns for the x and y axes. These can be specified by the x and y keywords.", "To plot multiple column groups in a single axes, repeat plot method specifying target ax. It is recommended to specify color and label keywords to distinguish each groups.", "The keyword c may be given as the name of a column to provide colors for each point:", "If a categorical column is passed to c, then a discrete colorbar will be produced:", "New in version 1.3.0.", "You can pass other keywords supported by matplotlib scatter. The example below shows a bubble chart using a column of the DataFrame as the bubble size.", "See the scatter method and the matplotlib scatter documentation for more.", "You can create hexagonal bin plots with DataFrame.plot.hexbin(). Hexbin plots can be a useful alternative to scatter plots if your data are too dense to plot each point individually.", "A useful keyword argument is gridsize; it controls the number of hexagons in the x-direction, and defaults to 100. A larger gridsize means more, smaller bins.", "By default, a histogram of the counts around each (x, y) point is computed. You can specify alternative aggregations by passing values to the C and reduce_C_function arguments. C specifies the value at each (x, y) point and reduce_C_function is a function of one argument that reduces all the values in a bin to a single number (e.g. mean, max, sum, std). In this example the positions are given by columns a and b, while the value is given by column z. The bins are aggregated with NumPy\u2019s max function.", "See the hexbin method and the matplotlib hexbin documentation for more.", "You can create a pie plot with DataFrame.plot.pie() or Series.plot.pie(). If your data includes any NaN, they will be automatically filled with 0. A ValueError will be raised if there are any negative values in your data.", "For pie plots it\u2019s best to use square figures, i.e. a figure aspect ratio 1. You can create the figure with equal width and height, or force the aspect ratio to be equal after plotting by calling ax.set_aspect('equal') on the returned axes object.", "Note that pie plot with DataFrame requires that you either specify a target column by the y argument or subplots=True. When y is specified, pie plot of selected column will be drawn. If subplots=True is specified, pie plots for each column are drawn as subplots. A legend will be drawn in each pie plots by default; specify legend=False to hide it.", "You can use the labels and colors keywords to specify the labels and colors of each wedge.", "Warning", "Most pandas plots use the label and color arguments (note the lack of \u201cs\u201d on those). To be consistent with matplotlib.pyplot.pie() you must use labels and colors.", "If you want to hide wedge labels, specify labels=None. If fontsize is specified, the value will be applied to wedge labels. Also, other keywords supported by matplotlib.pyplot.pie() can be used.", "If you pass values whose sum total is less than 1.0, matplotlib draws a semicircle.", "See the matplotlib pie documentation for more.", "pandas tries to be pragmatic about plotting DataFrames or Series that contain missing data. Missing values are dropped, left out, or filled depending on the plot type.", "Plot Type", "NaN Handling", "Line", "Leave gaps at NaNs", "Line (stacked)", "Fill 0\u2019s", "Bar", "Fill 0\u2019s", "Scatter", "Drop NaNs", "Histogram", "Drop NaNs (column-wise)", "Box", "Drop NaNs (column-wise)", "Area", "Fill 0\u2019s", "KDE", "Drop NaNs (column-wise)", "Hexbin", "Drop NaNs", "Pie", "Fill 0\u2019s", "If any of these defaults are not what you want, or if you want to be explicit about how missing values are handled, consider using fillna() or dropna() before plotting.", "These functions can be imported from pandas.plotting and take a Series or DataFrame as an argument.", "You can create a scatter plot matrix using the scatter_matrix method in pandas.plotting:", "You can create density plots using the Series.plot.kde() and DataFrame.plot.kde() methods.", "Andrews curves allow one to plot multivariate data as a large number of curves that are created using the attributes of samples as coefficients for Fourier series, see the Wikipedia entry for more information. By coloring these curves differently for each class it is possible to visualize data clustering. Curves belonging to samples of the same class will usually be closer together and form larger structures.", "Note: The \u201cIris\u201d dataset is available here.", "Parallel coordinates is a plotting technique for plotting multivariate data, see the Wikipedia entry for an introduction. Parallel coordinates allows one to see clusters in data and to estimate other statistics visually. Using parallel coordinates points are represented as connected line segments. Each vertical line represents one attribute. One set of connected line segments represents one data point. Points that tend to cluster will appear closer together.", "Lag plots are used to check if a data set or time series is random. Random data should not exhibit any structure in the lag plot. Non-random structure implies that the underlying data are not random. The lag argument may be passed, and when lag=1 the plot is essentially data[:-1] vs. data[1:].", "Autocorrelation plots are often used for checking randomness in time series. This is done by computing autocorrelations for data values at varying time lags. If time series is random, such autocorrelations should be near zero for any and all time-lag separations. If time series is non-random then one or more of the autocorrelations will be significantly non-zero. The horizontal lines displayed in the plot correspond to 95% and 99% confidence bands. The dashed line is 99% confidence band. See the Wikipedia entry for more about autocorrelation plots.", "Bootstrap plots are used to visually assess the uncertainty of a statistic, such as mean, median, midrange, etc. A random subset of a specified size is selected from a data set, the statistic in question is computed for this subset and the process is repeated a specified number of times. Resulting plots and histograms are what constitutes the bootstrap plot.", "RadViz is a way of visualizing multi-variate data. It is based on a simple spring tension minimization algorithm. Basically you set up a bunch of points in a plane. In our case they are equally spaced on a unit circle. Each point represents a single attribute. You then pretend that each sample in the data set is attached to each of these points by a spring, the stiffness of which is proportional to the numerical value of that attribute (they are normalized to unit interval). The point in the plane, where our sample settles to (where the forces acting on our sample are at an equilibrium) is where a dot representing our sample will be drawn. Depending on which class that sample belongs it will be colored differently. See the R package Radviz for more information.", "Note: The \u201cIris\u201d dataset is available here.", "From version 1.5 and up, matplotlib offers a range of pre-configured plotting styles. Setting the style can be used to easily give plots the general look that you want. Setting the style is as easy as calling matplotlib.style.use(my_plot_style) before creating your plot. For example you could write matplotlib.style.use('ggplot') for ggplot-style plots.", "You can see the various available style names at matplotlib.style.available and it\u2019s very easy to try them out.", "Most plotting methods have a set of keyword arguments that control the layout and formatting of the returned plot:", "For each kind of plot (e.g. line, bar, scatter) any additional arguments keywords are passed along to the corresponding matplotlib function (ax.plot(), ax.bar(), ax.scatter()). These can be used to control additional styling, beyond what pandas provides.", "You may set the legend argument to False to hide the legend, which is shown by default.", "New in version 1.1.0.", "You may set the xlabel and ylabel arguments to give the plot custom labels for x and y axis. By default, pandas will pick up index name as xlabel, while leaving it empty for ylabel.", "You may pass logy to get a log-scale Y axis.", "See also the logx and loglog keyword arguments.", "To plot data on a secondary y-axis, use the secondary_y keyword:", "To plot some columns in a DataFrame, give the column names to the secondary_y keyword:", "Note that the columns plotted on the secondary y-axis is automatically marked with \u201c(right)\u201d in the legend. To turn off the automatic marking, use the mark_right=False keyword:", "Changed in version 1.0.0.", "pandas provides custom formatters for timeseries plots. These change the formatting of the axis labels for dates and times. By default, the custom formatters are applied only to plots created by pandas with DataFrame.plot() or Series.plot(). To have them apply to all plots, including those made by matplotlib, set the option pd.options.plotting.matplotlib.register_converters = True or use pandas.plotting.register_matplotlib_converters().", "pandas includes automatic tick resolution adjustment for regular frequency time-series data. For limited cases where pandas cannot infer the frequency information (e.g., in an externally created twinx), you can choose to suppress this behavior for alignment purposes.", "Here is the default behavior, notice how the x-axis tick labeling is performed:", "Using the x_compat parameter, you can suppress this behavior:", "If you have more than one plot that needs to be suppressed, the use method in pandas.plotting.plot_params can be used in a with statement:", "TimedeltaIndex now uses the native matplotlib tick locator methods, it is useful to call the automatic date tick adjustment from matplotlib for figures whose ticklabels overlap.", "See the autofmt_xdate method and the matplotlib documentation for more.", "Each Series in a DataFrame can be plotted on a different axis with the subplots keyword:", "The layout of subplots can be specified by the layout keyword. It can accept (rows, columns). The layout keyword can be used in hist and boxplot also. If the input is invalid, a ValueError will be raised.", "The number of axes which can be contained by rows x columns specified by layout must be larger than the number of required subplots. If layout can contain more axes than required, blank axes are not drawn. Similar to a NumPy array\u2019s reshape method, you can use -1 for one dimension to automatically calculate the number of rows or columns needed, given the other.", "The above example is identical to using:", "The required number of columns (3) is inferred from the number of series to plot and the given number of rows (2).", "You can pass multiple axes created beforehand as list-like via ax keyword. This allows more complicated layouts. The passed axes must be the same number as the subplots being drawn.", "When multiple axes are passed via the ax keyword, layout, sharex and sharey keywords don\u2019t affect to the output. You should explicitly pass sharex=False and sharey=False, otherwise you will see a warning.", "Another option is passing an ax argument to Series.plot() to plot on a particular axis:", "Plotting with error bars is supported in DataFrame.plot() and Series.plot().", "Horizontal and vertical error bars can be supplied to the xerr and yerr keyword arguments to plot(). The error values can be specified using a variety of formats:", "As a DataFrame or dict of errors with column names matching the columns attribute of the plotting DataFrame or matching the name attribute of the Series.", "As a str indicating which of the columns of plotting DataFrame contain the error values.", "As raw values (list, tuple, or np.ndarray). Must be the same length as the plotting DataFrame/Series.", "Here is an example of one way to easily plot group means with standard deviations from the raw data.", "Asymmetrical error bars are also supported, however raw error values must be provided in this case. For a N length Series, a 2xN array should be provided indicating lower and upper (or left and right) errors. For a MxN DataFrame, asymmetrical errors should be in a Mx2xN array.", "Here is an example of one way to plot the min/max range using asymmetrical error bars.", "Plotting with matplotlib table is now supported in DataFrame.plot() and Series.plot() with a table keyword. The table keyword can accept bool, DataFrame or Series. The simple way to draw a table is to specify table=True. Data will be transposed to meet matplotlib\u2019s default layout.", "Also, you can pass a different DataFrame or Series to the table keyword. The data will be drawn as displayed in print method (not transposed automatically). If required, it should be transposed manually as seen in the example below.", "There also exists a helper function pandas.plotting.table, which creates a table from DataFrame or Series, and adds it to an matplotlib.Axes instance. This function can accept keywords which the matplotlib table has.", "Note: You can get table instances on the axes using axes.tables property for further decorations. See the matplotlib table documentation for more.", "A potential issue when plotting a large number of columns is that it can be difficult to distinguish some series due to repetition in the default colors. To remedy this, DataFrame plotting supports the use of the colormap argument, which accepts either a Matplotlib colormap or a string that is a name of a colormap registered with Matplotlib. A visualization of the default matplotlib colormaps is available here.", "As matplotlib does not directly support colormaps for line-based plots, the colors are selected based on an even spacing determined by the number of columns in the DataFrame. There is no consideration made for background color, so some colormaps will produce lines that are not easily visible.", "To use the cubehelix colormap, we can pass colormap='cubehelix'.", "Alternatively, we can pass the colormap itself:", "Colormaps can also be used other plot types, like bar charts:", "Parallel coordinates charts:", "Andrews curves charts:", "In some situations it may still be preferable or necessary to prepare plots directly with matplotlib, for instance when a certain type of plot or customization is not (yet) supported by pandas. Series and DataFrame objects behave like arrays and can therefore be passed directly to matplotlib functions without explicit casts.", "pandas also automatically registers formatters and locators that recognize date indices, thereby extending date and time support to practically all plot types available in matplotlib. Although this formatting does not provide the same level of refinement you would get when plotting via pandas, it can be faster when plotting a large number of points.", "Starting in version 0.25, pandas can be extended with third-party plotting backends. The main idea is letting users select a plotting backend different than the provided one based on Matplotlib.", "This can be done by passing \u2018backend.module\u2019 as the argument backend in plot function. For example:", "Alternatively, you can also set this option globally, do you don\u2019t need to specify the keyword in each plot call. For example:", "Or:", "This would be more or less equivalent to:", "The backend module can then use other visualization tools (Bokeh, Altair, hvplot,\u2026) to generate the plots. Some libraries implementing a backend for pandas are listed on the ecosystem Visualization page.", "Developers guide can be found at https://pandas.pydata.org/docs/dev/development/extending.html#plotting-backends"]}, {"name": "Computational tools", "path": "user_guide/computation", "type": "Manual", "text": ["Series and DataFrame have a method pct_change() to compute the percent change over a given number of periods (using fill_method to fill NA/null values before computing the percent change).", "Series.cov() can be used to compute covariance between series (excluding missing values).", "Analogously, DataFrame.cov() to compute pairwise covariances among the series in the DataFrame, also excluding NA/null values.", "Note", "Assuming the missing data are missing at random this results in an estimate for the covariance matrix which is unbiased. However, for many applications this estimate may not be acceptable because the estimated covariance matrix is not guaranteed to be positive semi-definite. This could lead to estimated correlations having absolute values which are greater than one, and/or a non-invertible covariance matrix. See Estimation of covariance matrices for more details.", "DataFrame.cov also supports an optional min_periods keyword that specifies the required minimum number of observations for each column pair in order to have a valid result.", "Correlation may be computed using the corr() method. Using the method parameter, several methods for computing correlations are provided:", "Method name", "Description", "pearson (default)", "Standard correlation coefficient", "kendall", "Kendall Tau correlation coefficient", "spearman", "Spearman rank correlation coefficient", "All of these are currently computed using pairwise complete observations. Wikipedia has articles covering the above correlation coefficients:", "Pearson correlation coefficient", "Kendall rank correlation coefficient", "Spearman\u2019s rank correlation coefficient", "Note", "Please see the caveats associated with this method of calculating correlation matrices in the covariance section.", "Note that non-numeric columns will be automatically excluded from the correlation calculation.", "Like cov, corr also supports the optional min_periods keyword:", "The method argument can also be a callable for a generic correlation calculation. In this case, it should be a single function that produces a single value from two ndarray inputs. Suppose we wanted to compute the correlation based on histogram intersection:", "A related method corrwith() is implemented on DataFrame to compute the correlation between like-labeled Series contained in different DataFrame objects.", "The rank() method produces a data ranking with ties being assigned the mean of the ranks (by default) for the group:", "rank() is also a DataFrame method and can rank either the rows (axis=0) or the columns (axis=1). NaN values are excluded from the ranking.", "rank optionally takes a parameter ascending which by default is true; when false, data is reverse-ranked, with larger values assigned a smaller rank.", "rank supports different tie-breaking methods, specified with the method parameter:", "average : average rank of tied group", "min : lowest rank in the group", "max : highest rank in the group", "first : ranks assigned in the order they appear in the array", "See the window operations user guide for an overview of windowing functions."]}, {"name": "Cookbook", "path": "user_guide/cookbook", "type": "Manual", "text": ["This is a repository for short and sweet examples and links for useful pandas recipes. We encourage users to add to this documentation.", "Adding interesting links and/or inline examples to this section is a great First Pull Request.", "Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer.", "pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users.", "These are some neat pandas idioms", "if-then/if-then-else on one column, and assignment to another one or more columns:", "An if-then on one column", "An if-then with assignment to 2 columns:", "Add another line with different logic, to do the -else", "Or use pandas where after you\u2019ve set up a mask", "if-then-else using NumPy\u2019s where()", "Split a frame with a boolean criterion", "Select with multi-column criteria", "\u2026and (without assignment returns a Series)", "\u2026or (without assignment returns a Series)", "\u2026or (with assignment modifies the DataFrame.)", "Select rows with data closest to certain value using argsort", "Dynamically reduce a list of criteria using a binary operators", "One could hard code:", "\u2026Or it can be done with a list of dynamically built criteria", "The indexing docs.", "Using both row labels and value conditionals", "Use loc for label-oriented slicing and iloc positional slicing GH2904", "There are 2 explicit slicing methods, with a third general case", "Positional-oriented (Python slicing style : exclusive of end)", "Label-oriented (Non-Python slicing style : inclusive of end)", "General (Either slicing style : depends on if the slice contains labels or positions)", "Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment.", "Using inverse operator (~) to take the complement of a mask", "Efficiently and dynamically creating new columns using applymap", "Keep other columns when using min() with groupby", "Method 1 : idxmin() to get the index of the minimums", "Method 2 : sort then take first of each", "Notice the same results, with the exception of the index.", "The multindexing docs.", "Creating a MultiIndex from a labeled frame", "Performing arithmetic with a MultiIndex that needs broadcasting", "Slicing a MultiIndex with xs", "To take the cross section of the 1st level and 1st axis the index:", "\u2026and now the 2nd level of the 1st axis.", "Slicing a MultiIndex with xs, method #2", "Setting portions of a MultiIndex with xs", "Sort by specific column or an ordered list of columns, with a MultiIndex", "Partial selection, the need for sortedness GH2995", "Prepending a level to a multiindex", "Flatten Hierarchical columns", "The missing data docs.", "Fill forward a reversed timeseries", "cumsum reset at NaN values", "Using replace with backrefs", "The grouping docs.", "Basic grouping with apply", "Unlike agg, apply\u2019s callable is passed a sub-DataFrame which gives you access to all the columns", "Using get_group", "Apply to different items in a group", "Expanding apply", "Replacing some values with mean of the rest of a group", "Sort groups by aggregated data", "Create multiple aggregated columns", "Create a value counts column and reassign back to the DataFrame", "Shift groups of the values in a column based on the index", "Select row with maximum value from each group", "Grouping like Python\u2019s itertools.groupby", "Alignment and to-date", "Rolling Computation window based on values instead of counts", "Rolling Mean by Time Interval", "Splitting a frame", "Create a list of dataframes, split using a delineation based on logic included in rows.", "The Pivot docs.", "Partial sums and subtotals", "Frequency table like plyr in R", "Plot pandas DataFrame with year over year data", "To create year and month cross tabulation:", "Rolling apply to organize - Turning embedded lists into a MultiIndex frame", "Rolling apply with a DataFrame returning a Series", "Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned", "Rolling apply with a DataFrame returning a Scalar", "Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price)", "Between times", "Using indexer between time", "Constructing a datetime range that excludes weekends and includes only certain times", "Vectorized Lookup", "Aggregation and plotting time series", "Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. How to rearrange a Python pandas DataFrame?", "Dealing with duplicates when reindexing a timeseries to a specified frequency", "Calculate the first day of the month for each entry in a DatetimeIndex", "The Resample docs.", "Using Grouper instead of TimeGrouper for time grouping of values", "Time grouping with some missing values", "Valid frequency arguments to Grouper Timeseries", "Grouping using a MultiIndex", "Using TimeGrouper and another grouping to create subgroups, then apply a custom function GH3791", "Resampling with custom periods", "Resample intraday frame without adding new days", "Resample minute data", "Resample with groupby", "The Join docs.", "Concatenate two dataframes with overlapping index (emulate R rbind)", "Depending on df construction, ignore_index may be needed", "Self Join of a DataFrame GH2996", "How to set the index and join", "KDB like asof join", "Join with a criteria based on the values", "Using searchsorted to merge based on values inside a range", "The Plotting docs.", "Make Matplotlib look like R", "Setting x-axis major and minor labels", "Plotting multiple charts in an IPython Jupyter notebook", "Creating a multi-line plot", "Plotting a heatmap", "Annotate a time-series plot", "Annotate a time-series plot #2", "Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter", "Boxplot for each quartile of a stratifying variable", "Performance comparison of SQL vs HDF5", "The CSV docs", "read_csv in action", "appending to a csv", "Reading a csv chunk-by-chunk", "Reading only certain rows of a csv chunk-by-chunk", "Reading the first few lines of a frame", "Reading a file that is compressed but not by gzip/bz2 (the native compressed formats which read_csv understands). This example shows a WinZipped file, but is a general application of opening the file within a context manager and using that handle to read. See here", "Inferring dtypes from a file", "Dealing with bad lines GH2886", "Write a multi-row index CSV without writing duplicates", "The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat():", "You can use the same approach to read all files matching a pattern. Here is an example using glob:", "Finally, this strategy will work with the other pd.read_*(...) functions described in the io docs.", "Parsing date components in multi-columns is faster with a format", "The SQL docs", "Reading from databases with SQL", "The Excel docs", "Reading from a filelike handle", "Modifying formatting in XlsxWriter output", "Loading only visible sheets GH19842#issuecomment-892150745", "Reading HTML tables from a server that cannot handle the default request header", "The HDFStores docs", "Simple queries with a Timestamp Index", "Managing heterogeneous data using a linked multiple table hierarchy GH3032", "Merging on-disk tables with millions of rows", "Avoiding inconsistencies when writing to a store from multiple processes/threads", "De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. See here", "Creating a store chunk-by-chunk from a csv file", "Appending to a store, while creating a unique index", "Large Data work flows", "Reading in a sequence of files, then providing a global unique index to a store while appending", "Groupby on a HDFStore with low group density", "Groupby on a HDFStore with high group density", "Hierarchical queries on a HDFStore", "Counting with a HDFStore", "Troubleshoot HDFStore exceptions", "Setting min_itemsize with strings", "Using ptrepack to create a completely-sorted-index on a store", "Storing Attributes to a group node", "You can create or load a HDFStore in-memory by passing the driver parameter to PyTables. Changes are only written to disk when the HDFStore is closed.", "pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called main.c compiled with gcc main.c -std=gnu99 on a 64-bit machine,", "the following Python code will read the binary file 'binary.dat' into a pandas DataFrame, where each element of the struct corresponds to a column in the frame:", "Note", "The offsets of the structure elements may be different depending on the architecture of the machine on which the file was created. Using a raw binary file format like this for general data storage is not recommended, as it is not cross platform. We recommended either HDF5 or parquet, both of which are supported by pandas\u2019 IO facilities.", "Numerical integration (sample-based) of a time series", "Often it\u2019s useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from DataFrame.corr(). This can be achieved by passing a boolean mask to where as follows:", "The method argument within DataFrame.corr can accept a callable in addition to the named correlation types. Here we compute the distance correlation matrix for a DataFrame object.", "The Timedeltas docs.", "Using timedeltas", "Adding and subtracting deltas and dates", "Another example", "Values can be set to NaT using np.nan, similar to datetime", "To create a dataframe from every combination of some given values, like R\u2019s expand.grid() function, we can create a dict where the keys are column names and the values are lists of the data values:"]}, {"name": "DataFrame", "path": "reference/frame", "type": "DataFrame", "text": ["DataFrame([data, index, columns, dtype, copy])", "Two-dimensional, size-mutable, potentially heterogeneous tabular data.", "Axes", "DataFrame.index", "The index (row labels) of the DataFrame.", "DataFrame.columns", "The column labels of the DataFrame.", "DataFrame.dtypes", "Return the dtypes in the DataFrame.", "DataFrame.info([verbose, buf, max_cols, ...])", "Print a concise summary of a DataFrame.", "DataFrame.select_dtypes([include, exclude])", "Return a subset of the DataFrame's columns based on the column dtypes.", "DataFrame.values", "Return a Numpy representation of the DataFrame.", "DataFrame.axes", "Return a list representing the axes of the DataFrame.", "DataFrame.ndim", "Return an int representing the number of axes / array dimensions.", "DataFrame.size", "Return an int representing the number of elements in this object.", "DataFrame.shape", "Return a tuple representing the dimensionality of the DataFrame.", "DataFrame.memory_usage([index, deep])", "Return the memory usage of each column in bytes.", "DataFrame.empty", "Indicator whether Series/DataFrame is empty.", "DataFrame.set_flags(*[, copy, ...])", "Return a new object with updated flags.", "DataFrame.astype(dtype[, copy, errors])", "Cast a pandas object to a specified dtype dtype.", "DataFrame.convert_dtypes([infer_objects, ...])", "Convert columns to best possible dtypes using dtypes supporting pd.NA.", "DataFrame.infer_objects()", "Attempt to infer better dtypes for object columns.", "DataFrame.copy([deep])", "Make a copy of this object's indices and data.", "DataFrame.bool()", "Return the bool of a single element Series or DataFrame.", "DataFrame.head([n])", "Return the first n rows.", "DataFrame.at", "Access a single value for a row/column label pair.", "DataFrame.iat", "Access a single value for a row/column pair by integer position.", "DataFrame.loc", "Access a group of rows and columns by label(s) or a boolean array.", "DataFrame.iloc", "Purely integer-location based indexing for selection by position.", "DataFrame.insert(loc, column, value[, ...])", "Insert column into DataFrame at specified location.", "DataFrame.__iter__()", "Iterate over info axis.", "DataFrame.items()", "Iterate over (column name, Series) pairs.", "DataFrame.iteritems()", "Iterate over (column name, Series) pairs.", "DataFrame.keys()", "Get the 'info axis' (see Indexing for more).", "DataFrame.iterrows()", "Iterate over DataFrame rows as (index, Series) pairs.", "DataFrame.itertuples([index, name])", "Iterate over DataFrame rows as namedtuples.", "DataFrame.lookup(row_labels, col_labels)", "(DEPRECATED) Label-based \"fancy indexing\" function for DataFrame.", "DataFrame.pop(item)", "Return item and drop from frame.", "DataFrame.tail([n])", "Return the last n rows.", "DataFrame.xs(key[, axis, level, drop_level])", "Return cross-section from the Series/DataFrame.", "DataFrame.get(key[, default])", "Get item from object for given key (ex: DataFrame column).", "DataFrame.isin(values)", "Whether each element in the DataFrame is contained in values.", "DataFrame.where(cond[, other, inplace, ...])", "Replace values where the condition is False.", "DataFrame.mask(cond[, other, inplace, axis, ...])", "Replace values where the condition is True.", "DataFrame.query(expr[, inplace])", "Query the columns of a DataFrame with a boolean expression.", "For more information on .at, .iat, .loc, and .iloc, see the indexing documentation.", "DataFrame.add(other[, axis, level, fill_value])", "Get Addition of dataframe and other, element-wise (binary operator add).", "DataFrame.sub(other[, axis, level, fill_value])", "Get Subtraction of dataframe and other, element-wise (binary operator sub).", "DataFrame.mul(other[, axis, level, fill_value])", "Get Multiplication of dataframe and other, element-wise (binary operator mul).", "DataFrame.div(other[, axis, level, fill_value])", "Get Floating division of dataframe and other, element-wise (binary operator truediv).", "DataFrame.truediv(other[, axis, level, ...])", "Get Floating division of dataframe and other, element-wise (binary operator truediv).", "DataFrame.floordiv(other[, axis, level, ...])", "Get Integer division of dataframe and other, element-wise (binary operator floordiv).", "DataFrame.mod(other[, axis, level, fill_value])", "Get Modulo of dataframe and other, element-wise (binary operator mod).", "DataFrame.pow(other[, axis, level, fill_value])", "Get Exponential power of dataframe and other, element-wise (binary operator pow).", "DataFrame.dot(other)", "Compute the matrix multiplication between the DataFrame and other.", "DataFrame.radd(other[, axis, level, fill_value])", "Get Addition of dataframe and other, element-wise (binary operator radd).", "DataFrame.rsub(other[, axis, level, fill_value])", "Get Subtraction of dataframe and other, element-wise (binary operator rsub).", "DataFrame.rmul(other[, axis, level, fill_value])", "Get Multiplication of dataframe and other, element-wise (binary operator rmul).", "DataFrame.rdiv(other[, axis, level, fill_value])", "Get Floating division of dataframe and other, element-wise (binary operator rtruediv).", "DataFrame.rtruediv(other[, axis, level, ...])", "Get Floating division of dataframe and other, element-wise (binary operator rtruediv).", "DataFrame.rfloordiv(other[, axis, level, ...])", "Get Integer division of dataframe and other, element-wise (binary operator rfloordiv).", "DataFrame.rmod(other[, axis, level, fill_value])", "Get Modulo of dataframe and other, element-wise (binary operator rmod).", "DataFrame.rpow(other[, axis, level, fill_value])", "Get Exponential power of dataframe and other, element-wise (binary operator rpow).", "DataFrame.lt(other[, axis, level])", "Get Less than of dataframe and other, element-wise (binary operator lt).", "DataFrame.gt(other[, axis, level])", "Get Greater than of dataframe and other, element-wise (binary operator gt).", "DataFrame.le(other[, axis, level])", "Get Less than or equal to of dataframe and other, element-wise (binary operator le).", "DataFrame.ge(other[, axis, level])", "Get Greater than or equal to of dataframe and other, element-wise (binary operator ge).", "DataFrame.ne(other[, axis, level])", "Get Not equal to of dataframe and other, element-wise (binary operator ne).", "DataFrame.eq(other[, axis, level])", "Get Equal to of dataframe and other, element-wise (binary operator eq).", "DataFrame.combine(other, func[, fill_value, ...])", "Perform column-wise combine with another DataFrame.", "DataFrame.combine_first(other)", "Update null elements with value in the same location in other.", "DataFrame.apply(func[, axis, raw, ...])", "Apply a function along an axis of the DataFrame.", "DataFrame.applymap(func[, na_action])", "Apply a function to a Dataframe elementwise.", "DataFrame.pipe(func, *args, **kwargs)", "Apply chainable functions that expect Series or DataFrames.", "DataFrame.agg([func, axis])", "Aggregate using one or more operations over the specified axis.", "DataFrame.aggregate([func, axis])", "Aggregate using one or more operations over the specified axis.", "DataFrame.transform(func[, axis])", "Call func on self producing a DataFrame with the same axis shape as self.", "DataFrame.groupby([by, axis, level, ...])", "Group DataFrame using a mapper or by a Series of columns.", "DataFrame.rolling(window[, min_periods, ...])", "Provide rolling window calculations.", "DataFrame.expanding([min_periods, center, ...])", "Provide expanding window calculations.", "DataFrame.ewm([com, span, halflife, alpha, ...])", "Provide exponentially weighted (EW) calculations.", "DataFrame.abs()", "Return a Series/DataFrame with absolute numeric value of each element.", "DataFrame.all([axis, bool_only, skipna, level])", "Return whether all elements are True, potentially over an axis.", "DataFrame.any([axis, bool_only, skipna, level])", "Return whether any element is True, potentially over an axis.", "DataFrame.clip([lower, upper, axis, inplace])", "Trim values at input threshold(s).", "DataFrame.corr([method, min_periods])", "Compute pairwise correlation of columns, excluding NA/null values.", "DataFrame.corrwith(other[, axis, drop, method])", "Compute pairwise correlation.", "DataFrame.count([axis, level, numeric_only])", "Count non-NA cells for each column or row.", "DataFrame.cov([min_periods, ddof])", "Compute pairwise covariance of columns, excluding NA/null values.", "DataFrame.cummax([axis, skipna])", "Return cumulative maximum over a DataFrame or Series axis.", "DataFrame.cummin([axis, skipna])", "Return cumulative minimum over a DataFrame or Series axis.", "DataFrame.cumprod([axis, skipna])", "Return cumulative product over a DataFrame or Series axis.", "DataFrame.cumsum([axis, skipna])", "Return cumulative sum over a DataFrame or Series axis.", "DataFrame.describe([percentiles, include, ...])", "Generate descriptive statistics.", "DataFrame.diff([periods, axis])", "First discrete difference of element.", "DataFrame.eval(expr[, inplace])", "Evaluate a string describing operations on DataFrame columns.", "DataFrame.kurt([axis, skipna, level, ...])", "Return unbiased kurtosis over requested axis.", "DataFrame.kurtosis([axis, skipna, level, ...])", "Return unbiased kurtosis over requested axis.", "DataFrame.mad([axis, skipna, level])", "Return the mean absolute deviation of the values over the requested axis.", "DataFrame.max([axis, skipna, level, ...])", "Return the maximum of the values over the requested axis.", "DataFrame.mean([axis, skipna, level, ...])", "Return the mean of the values over the requested axis.", "DataFrame.median([axis, skipna, level, ...])", "Return the median of the values over the requested axis.", "DataFrame.min([axis, skipna, level, ...])", "Return the minimum of the values over the requested axis.", "DataFrame.mode([axis, numeric_only, dropna])", "Get the mode(s) of each element along the selected axis.", "DataFrame.pct_change([periods, fill_method, ...])", "Percentage change between the current and a prior element.", "DataFrame.prod([axis, skipna, level, ...])", "Return the product of the values over the requested axis.", "DataFrame.product([axis, skipna, level, ...])", "Return the product of the values over the requested axis.", "DataFrame.quantile([q, axis, numeric_only, ...])", "Return values at the given quantile over requested axis.", "DataFrame.rank([axis, method, numeric_only, ...])", "Compute numerical data ranks (1 through n) along axis.", "DataFrame.round([decimals])", "Round a DataFrame to a variable number of decimal places.", "DataFrame.sem([axis, skipna, level, ddof, ...])", "Return unbiased standard error of the mean over requested axis.", "DataFrame.skew([axis, skipna, level, ...])", "Return unbiased skew over requested axis.", "DataFrame.sum([axis, skipna, level, ...])", "Return the sum of the values over the requested axis.", "DataFrame.std([axis, skipna, level, ddof, ...])", "Return sample standard deviation over requested axis.", "DataFrame.var([axis, skipna, level, ddof, ...])", "Return unbiased variance over requested axis.", "DataFrame.nunique([axis, dropna])", "Count number of distinct elements in specified axis.", "DataFrame.value_counts([subset, normalize, ...])", "Return a Series containing counts of unique rows in the DataFrame.", "DataFrame.add_prefix(prefix)", "Prefix labels with string prefix.", "DataFrame.add_suffix(suffix)", "Suffix labels with string suffix.", "DataFrame.align(other[, join, axis, level, ...])", "Align two objects on their axes with the specified join method.", "DataFrame.at_time(time[, asof, axis])", "Select values at particular time of day (e.g., 9:30AM).", "DataFrame.between_time(start_time, end_time)", "Select values between particular times of the day (e.g., 9:00-9:30 AM).", "DataFrame.drop([labels, axis, index, ...])", "Drop specified labels from rows or columns.", "DataFrame.drop_duplicates([subset, keep, ...])", "Return DataFrame with duplicate rows removed.", "DataFrame.duplicated([subset, keep])", "Return boolean Series denoting duplicate rows.", "DataFrame.equals(other)", "Test whether two objects contain the same elements.", "DataFrame.filter([items, like, regex, axis])", "Subset the dataframe rows or columns according to the specified index labels.", "DataFrame.first(offset)", "Select initial periods of time series data based on a date offset.", "DataFrame.head([n])", "Return the first n rows.", "DataFrame.idxmax([axis, skipna])", "Return index of first occurrence of maximum over requested axis.", "DataFrame.idxmin([axis, skipna])", "Return index of first occurrence of minimum over requested axis.", "DataFrame.last(offset)", "Select final periods of time series data based on a date offset.", "DataFrame.reindex([labels, index, columns, ...])", "Conform Series/DataFrame to new index with optional filling logic.", "DataFrame.reindex_like(other[, method, ...])", "Return an object with matching indices as other object.", "DataFrame.rename([mapper, index, columns, ...])", "Alter axes labels.", "DataFrame.rename_axis([mapper, index, ...])", "Set the name of the axis for the index or columns.", "DataFrame.reset_index([level, drop, ...])", "Reset the index, or a level of it.", "DataFrame.sample([n, frac, replace, ...])", "Return a random sample of items from an axis of object.", "DataFrame.set_axis(labels[, axis, inplace])", "Assign desired index to given axis.", "DataFrame.set_index(keys[, drop, append, ...])", "Set the DataFrame index using existing columns.", "DataFrame.tail([n])", "Return the last n rows.", "DataFrame.take(indices[, axis, is_copy])", "Return the elements in the given positional indices along an axis.", "DataFrame.truncate([before, after, axis, copy])", "Truncate a Series or DataFrame before and after some index value.", "DataFrame.backfill([axis, inplace, limit, ...])", "Synonym for DataFrame.fillna() with method='bfill'.", "DataFrame.bfill([axis, inplace, limit, downcast])", "Synonym for DataFrame.fillna() with method='bfill'.", "DataFrame.dropna([axis, how, thresh, ...])", "Remove missing values.", "DataFrame.ffill([axis, inplace, limit, downcast])", "Synonym for DataFrame.fillna() with method='ffill'.", "DataFrame.fillna([value, method, axis, ...])", "Fill NA/NaN values using the specified method.", "DataFrame.interpolate([method, axis, limit, ...])", "Fill NaN values using an interpolation method.", "DataFrame.isna()", "Detect missing values.", "DataFrame.isnull()", "DataFrame.isnull is an alias for DataFrame.isna.", "DataFrame.notna()", "Detect existing (non-missing) values.", "DataFrame.notnull()", "DataFrame.notnull is an alias for DataFrame.notna.", "DataFrame.pad([axis, inplace, limit, downcast])", "Synonym for DataFrame.fillna() with method='ffill'.", "DataFrame.replace([to_replace, value, ...])", "Replace values given in to_replace with value.", "DataFrame.droplevel(level[, axis])", "Return Series/DataFrame with requested index / column level(s) removed.", "DataFrame.pivot([index, columns, values])", "Return reshaped DataFrame organized by given index / column values.", "DataFrame.pivot_table([values, index, ...])", "Create a spreadsheet-style pivot table as a DataFrame.", "DataFrame.reorder_levels(order[, axis])", "Rearrange index levels using input order.", "DataFrame.sort_values(by[, axis, ascending, ...])", "Sort by the values along either axis.", "DataFrame.sort_index([axis, level, ...])", "Sort object by labels (along an axis).", "DataFrame.nlargest(n, columns[, keep])", "Return the first n rows ordered by columns in descending order.", "DataFrame.nsmallest(n, columns[, keep])", "Return the first n rows ordered by columns in ascending order.", "DataFrame.swaplevel([i, j, axis])", "Swap levels i and j in a MultiIndex.", "DataFrame.stack([level, dropna])", "Stack the prescribed level(s) from columns to index.", "DataFrame.unstack([level, fill_value])", "Pivot a level of the (necessarily hierarchical) index labels.", "DataFrame.swapaxes(axis1, axis2[, copy])", "Interchange axes and swap values axes appropriately.", "DataFrame.melt([id_vars, value_vars, ...])", "Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.", "DataFrame.explode(column[, ignore_index])", "Transform each element of a list-like to a row, replicating index values.", "DataFrame.squeeze([axis])", "Squeeze 1 dimensional axis objects into scalars.", "DataFrame.to_xarray()", "Return an xarray object from the pandas object.", "DataFrame.T", "DataFrame.transpose(*args[, copy])", "Transpose index and columns.", "DataFrame.append(other[, ignore_index, ...])", "Append rows of other to the end of caller, returning a new object.", "DataFrame.assign(**kwargs)", "Assign new columns to a DataFrame.", "DataFrame.compare(other[, align_axis, ...])", "Compare to another DataFrame and show the differences.", "DataFrame.join(other[, on, how, lsuffix, ...])", "Join columns of another DataFrame.", "DataFrame.merge(right[, how, on, left_on, ...])", "Merge DataFrame or named Series objects with a database-style join.", "DataFrame.update(other[, join, overwrite, ...])", "Modify in place using non-NA values from another DataFrame.", "DataFrame.asfreq(freq[, method, how, ...])", "Convert time series to specified frequency.", "DataFrame.asof(where[, subset])", "Return the last row(s) without any NaNs before where.", "DataFrame.shift([periods, freq, axis, ...])", "Shift index by desired number of periods with an optional time freq.", "DataFrame.slice_shift([periods, axis])", "(DEPRECATED) Equivalent to shift without copying data.", "DataFrame.tshift([periods, freq, axis])", "(DEPRECATED) Shift the time index, using the index's frequency if available.", "DataFrame.first_valid_index()", "Return index for first non-NA value or None, if no NA value is found.", "DataFrame.last_valid_index()", "Return index for last non-NA value or None, if no NA value is found.", "DataFrame.resample(rule[, axis, closed, ...])", "Resample time-series data.", "DataFrame.to_period([freq, axis, copy])", "Convert DataFrame from DatetimeIndex to PeriodIndex.", "DataFrame.to_timestamp([freq, how, axis, copy])", "Cast to DatetimeIndex of timestamps, at beginning of period.", "DataFrame.tz_convert(tz[, axis, level, copy])", "Convert tz-aware axis to target time zone.", "DataFrame.tz_localize(tz[, axis, level, ...])", "Localize tz-naive index of a Series or DataFrame to target time zone.", "Flags refer to attributes of the pandas object. Properties of the dataset (like the date is was recorded, the URL it was accessed from, etc.) should be stored in DataFrame.attrs.", "Flags(obj, *, allows_duplicate_labels)", "Flags that apply to pandas objects.", "DataFrame.attrs is a dictionary for storing global metadata for this DataFrame.", "Warning", "DataFrame.attrs is considered experimental and may change without warning.", "DataFrame.attrs", "Dictionary of global attributes of this dataset.", "DataFrame.plot is both a callable method and a namespace attribute for specific plotting methods of the form DataFrame.plot.<kind>.", "DataFrame.plot([x, y, kind, ax, ....])", "DataFrame plotting accessor and method", "DataFrame.plot.area([x, y])", "Draw a stacked area plot.", "DataFrame.plot.bar([x, y])", "Vertical bar plot.", "DataFrame.plot.barh([x, y])", "Make a horizontal bar plot.", "DataFrame.plot.box([by])", "Make a box plot of the DataFrame columns.", "DataFrame.plot.density([bw_method, ind])", "Generate Kernel Density Estimate plot using Gaussian kernels.", "DataFrame.plot.hexbin(x, y[, C, ...])", "Generate a hexagonal binning plot.", "DataFrame.plot.hist([by, bins])", "Draw one histogram of the DataFrame's columns.", "DataFrame.plot.kde([bw_method, ind])", "Generate Kernel Density Estimate plot using Gaussian kernels.", "DataFrame.plot.line([x, y])", "Plot Series or DataFrame as lines.", "DataFrame.plot.pie(**kwargs)", "Generate a pie plot.", "DataFrame.plot.scatter(x, y[, s, c])", "Create a scatter plot with varying marker point size and color.", "DataFrame.boxplot([column, by, ax, ...])", "Make a box plot from DataFrame columns.", "DataFrame.hist([column, by, grid, ...])", "Make a histogram of the DataFrame's columns.", "Sparse-dtype specific methods and attributes are provided under the DataFrame.sparse accessor.", "DataFrame.sparse.density", "Ratio of non-sparse points to total (dense) data points.", "DataFrame.sparse.from_spmatrix(data[, ...])", "Create a new DataFrame from a scipy sparse matrix.", "DataFrame.sparse.to_coo()", "Return the contents of the frame as a sparse SciPy COO matrix.", "DataFrame.sparse.to_dense()", "Convert a DataFrame with sparse values to dense.", "DataFrame.from_dict(data[, orient, dtype, ...])", "Construct DataFrame from dict of array-like or dicts.", "DataFrame.from_records(data[, index, ...])", "Convert structured or record ndarray to DataFrame.", "DataFrame.to_parquet([path, engine, ...])", "Write a DataFrame to the binary parquet format.", "DataFrame.to_pickle(path[, compression, ...])", "Pickle (serialize) object to file.", "DataFrame.to_csv([path_or_buf, sep, na_rep, ...])", "Write object to a comma-separated values (csv) file.", "DataFrame.to_hdf(path_or_buf, key[, mode, ...])", "Write the contained data to an HDF5 file using HDFStore.", "DataFrame.to_sql(name, con[, schema, ...])", "Write records stored in a DataFrame to a SQL database.", "DataFrame.to_dict([orient, into])", "Convert the DataFrame to a dictionary.", "DataFrame.to_excel(excel_writer[, ...])", "Write object to an Excel sheet.", "DataFrame.to_json([path_or_buf, orient, ...])", "Convert the object to a JSON string.", "DataFrame.to_html([buf, columns, col_space, ...])", "Render a DataFrame as an HTML table.", "DataFrame.to_feather(path, **kwargs)", "Write a DataFrame to the binary Feather format.", "DataFrame.to_latex([buf, columns, ...])", "Render object to a LaTeX tabular, longtable, or nested table.", "DataFrame.to_stata(path[, convert_dates, ...])", "Export DataFrame object to Stata dta format.", "DataFrame.to_gbq(destination_table[, ...])", "Write a DataFrame to a Google BigQuery table.", "DataFrame.to_records([index, column_dtypes, ...])", "Convert DataFrame to a NumPy record array.", "DataFrame.to_string([buf, columns, ...])", "Render a DataFrame to a console-friendly tabular output.", "DataFrame.to_clipboard([excel, sep])", "Copy object to the system clipboard.", "DataFrame.to_markdown([buf, mode, index, ...])", "Print DataFrame in Markdown-friendly format.", "DataFrame.style", "Returns a Styler object."]}, {"name": "Date offsets", "path": "reference/offset_frequency", "type": "Data offsets", "text": ["DateOffset", "Standard kind of date increment used for a date range.", "DateOffset.freqstr", "DateOffset.kwds", "DateOffset.name", "DateOffset.nanos", "DateOffset.normalize", "DateOffset.rule_code", "DateOffset.n", "DateOffset.is_month_start", "DateOffset.is_month_end", "DateOffset.apply", "DateOffset.apply_index(other)", "DateOffset.copy", "DateOffset.isAnchored", "DateOffset.onOffset", "DateOffset.is_anchored", "DateOffset.is_on_offset", "DateOffset.__call__(*args, **kwargs)", "Call self as a function.", "DateOffset.is_month_start", "DateOffset.is_month_end", "DateOffset.is_quarter_start", "DateOffset.is_quarter_end", "DateOffset.is_year_start", "DateOffset.is_year_end", "BusinessDay", "DateOffset subclass representing possibly n business days.", "Alias:", "BDay", "alias of pandas._libs.tslibs.offsets.BusinessDay", "BusinessDay.freqstr", "BusinessDay.kwds", "BusinessDay.name", "BusinessDay.nanos", "BusinessDay.normalize", "BusinessDay.rule_code", "BusinessDay.n", "BusinessDay.weekmask", "BusinessDay.holidays", "BusinessDay.calendar", "BusinessDay.apply", "BusinessDay.apply_index(other)", "BusinessDay.copy", "BusinessDay.isAnchored", "BusinessDay.onOffset", "BusinessDay.is_anchored", "BusinessDay.is_on_offset", "BusinessDay.__call__(*args, **kwargs)", "Call self as a function.", "BusinessDay.is_month_start", "BusinessDay.is_month_end", "BusinessDay.is_quarter_start", "BusinessDay.is_quarter_end", "BusinessDay.is_year_start", "BusinessDay.is_year_end", "BusinessHour", "DateOffset subclass representing possibly n business hours.", "BusinessHour.freqstr", "BusinessHour.kwds", "BusinessHour.name", "BusinessHour.nanos", "BusinessHour.normalize", "BusinessHour.rule_code", "BusinessHour.n", "BusinessHour.start", "BusinessHour.end", "BusinessHour.weekmask", "BusinessHour.holidays", "BusinessHour.calendar", "BusinessHour.apply", "BusinessHour.apply_index(other)", "BusinessHour.copy", "BusinessHour.isAnchored", "BusinessHour.onOffset", "BusinessHour.is_anchored", "BusinessHour.is_on_offset", "BusinessHour.__call__(*args, **kwargs)", "Call self as a function.", "BusinessHour.is_month_start", "BusinessHour.is_month_end", "BusinessHour.is_quarter_start", "BusinessHour.is_quarter_end", "BusinessHour.is_year_start", "BusinessHour.is_year_end", "CustomBusinessDay", "DateOffset subclass representing custom business days excluding holidays.", "Alias:", "CDay", "alias of pandas._libs.tslibs.offsets.CustomBusinessDay", "CustomBusinessDay.freqstr", "CustomBusinessDay.kwds", "CustomBusinessDay.name", "CustomBusinessDay.nanos", "CustomBusinessDay.normalize", "CustomBusinessDay.rule_code", "CustomBusinessDay.n", "CustomBusinessDay.weekmask", "CustomBusinessDay.calendar", "CustomBusinessDay.holidays", "CustomBusinessDay.apply_index", "CustomBusinessDay.apply", "CustomBusinessDay.copy", "CustomBusinessDay.isAnchored", "CustomBusinessDay.onOffset", "CustomBusinessDay.is_anchored", "CustomBusinessDay.is_on_offset", "CustomBusinessDay.__call__(*args, **kwargs)", "Call self as a function.", "CustomBusinessDay.is_month_start", "CustomBusinessDay.is_month_end", "CustomBusinessDay.is_quarter_start", "CustomBusinessDay.is_quarter_end", "CustomBusinessDay.is_year_start", "CustomBusinessDay.is_year_end", "CustomBusinessHour", "DateOffset subclass representing possibly n custom business days.", "CustomBusinessHour.freqstr", "CustomBusinessHour.kwds", "CustomBusinessHour.name", "CustomBusinessHour.nanos", "CustomBusinessHour.normalize", "CustomBusinessHour.rule_code", "CustomBusinessHour.n", "CustomBusinessHour.weekmask", "CustomBusinessHour.calendar", "CustomBusinessHour.holidays", "CustomBusinessHour.start", "CustomBusinessHour.end", "CustomBusinessHour.apply", "CustomBusinessHour.apply_index(other)", "CustomBusinessHour.copy", "CustomBusinessHour.isAnchored", "CustomBusinessHour.onOffset", "CustomBusinessHour.is_anchored", "CustomBusinessHour.is_on_offset", "CustomBusinessHour.__call__(*args, **kwargs)", "Call self as a function.", "CustomBusinessHour.is_month_start", "CustomBusinessHour.is_month_end", "CustomBusinessHour.is_quarter_start", "CustomBusinessHour.is_quarter_end", "CustomBusinessHour.is_year_start", "CustomBusinessHour.is_year_end", "MonthEnd", "DateOffset of one month end.", "MonthEnd.freqstr", "MonthEnd.kwds", "MonthEnd.name", "MonthEnd.nanos", "MonthEnd.normalize", "MonthEnd.rule_code", "MonthEnd.n", "MonthEnd.apply", "MonthEnd.apply_index(other)", "MonthEnd.copy", "MonthEnd.isAnchored", "MonthEnd.onOffset", "MonthEnd.is_anchored", "MonthEnd.is_on_offset", "MonthEnd.__call__(*args, **kwargs)", "Call self as a function.", "MonthEnd.is_month_start", "MonthEnd.is_month_end", "MonthEnd.is_quarter_start", "MonthEnd.is_quarter_end", "MonthEnd.is_year_start", "MonthEnd.is_year_end", "MonthBegin", "DateOffset of one month at beginning.", "MonthBegin.freqstr", "MonthBegin.kwds", "MonthBegin.name", "MonthBegin.nanos", "MonthBegin.normalize", "MonthBegin.rule_code", "MonthBegin.n", "MonthBegin.apply", "MonthBegin.apply_index(other)", "MonthBegin.copy", "MonthBegin.isAnchored", "MonthBegin.onOffset", "MonthBegin.is_anchored", "MonthBegin.is_on_offset", "MonthBegin.__call__(*args, **kwargs)", "Call self as a function.", "MonthBegin.is_month_start", "MonthBegin.is_month_end", "MonthBegin.is_quarter_start", "MonthBegin.is_quarter_end", "MonthBegin.is_year_start", "MonthBegin.is_year_end", "BusinessMonthEnd", "DateOffset increments between the last business day of the month.", "Alias:", "BMonthEnd", "alias of pandas._libs.tslibs.offsets.BusinessMonthEnd", "BusinessMonthEnd.freqstr", "BusinessMonthEnd.kwds", "BusinessMonthEnd.name", "BusinessMonthEnd.nanos", "BusinessMonthEnd.normalize", "BusinessMonthEnd.rule_code", "BusinessMonthEnd.n", "BusinessMonthEnd.apply", "BusinessMonthEnd.apply_index(other)", "BusinessMonthEnd.copy", "BusinessMonthEnd.isAnchored", "BusinessMonthEnd.onOffset", "BusinessMonthEnd.is_anchored", "BusinessMonthEnd.is_on_offset", "BusinessMonthEnd.__call__(*args, **kwargs)", "Call self as a function.", "BusinessMonthEnd.is_month_start", "BusinessMonthEnd.is_month_end", "BusinessMonthEnd.is_quarter_start", "BusinessMonthEnd.is_quarter_end", "BusinessMonthEnd.is_year_start", "BusinessMonthEnd.is_year_end", "BusinessMonthBegin", "DateOffset of one month at the first business day.", "Alias:", "BMonthBegin", "alias of pandas._libs.tslibs.offsets.BusinessMonthBegin", "BusinessMonthBegin.freqstr", "BusinessMonthBegin.kwds", "BusinessMonthBegin.name", "BusinessMonthBegin.nanos", "BusinessMonthBegin.normalize", "BusinessMonthBegin.rule_code", "BusinessMonthBegin.n", "BusinessMonthBegin.apply", "BusinessMonthBegin.apply_index(other)", "BusinessMonthBegin.copy", "BusinessMonthBegin.isAnchored", "BusinessMonthBegin.onOffset", "BusinessMonthBegin.is_anchored", "BusinessMonthBegin.is_on_offset", "BusinessMonthBegin.__call__(*args, **kwargs)", "Call self as a function.", "BusinessMonthBegin.is_month_start", "BusinessMonthBegin.is_month_end", "BusinessMonthBegin.is_quarter_start", "BusinessMonthBegin.is_quarter_end", "BusinessMonthBegin.is_year_start", "BusinessMonthBegin.is_year_end", "CustomBusinessMonthEnd", "Attributes", "Alias:", "CBMonthEnd", "alias of pandas._libs.tslibs.offsets.CustomBusinessMonthEnd", "CustomBusinessMonthEnd.freqstr", "CustomBusinessMonthEnd.kwds", "CustomBusinessMonthEnd.m_offset", "CustomBusinessMonthEnd.name", "CustomBusinessMonthEnd.nanos", "CustomBusinessMonthEnd.normalize", "CustomBusinessMonthEnd.rule_code", "CustomBusinessMonthEnd.n", "CustomBusinessMonthEnd.weekmask", "CustomBusinessMonthEnd.calendar", "CustomBusinessMonthEnd.holidays", "CustomBusinessMonthEnd.apply", "CustomBusinessMonthEnd.apply_index(other)", "CustomBusinessMonthEnd.copy", "CustomBusinessMonthEnd.isAnchored", "CustomBusinessMonthEnd.onOffset", "CustomBusinessMonthEnd.is_anchored", "CustomBusinessMonthEnd.is_on_offset", "CustomBusinessMonthEnd.__call__(*args, **kwargs)", "Call self as a function.", "CustomBusinessMonthEnd.is_month_start", "CustomBusinessMonthEnd.is_month_end", "CustomBusinessMonthEnd.is_quarter_start", "CustomBusinessMonthEnd.is_quarter_end", "CustomBusinessMonthEnd.is_year_start", "CustomBusinessMonthEnd.is_year_end", "CustomBusinessMonthBegin", "Attributes", "Alias:", "CBMonthBegin", "alias of pandas._libs.tslibs.offsets.CustomBusinessMonthBegin", "CustomBusinessMonthBegin.freqstr", "CustomBusinessMonthBegin.kwds", "CustomBusinessMonthBegin.m_offset", "CustomBusinessMonthBegin.name", "CustomBusinessMonthBegin.nanos", "CustomBusinessMonthBegin.normalize", "CustomBusinessMonthBegin.rule_code", "CustomBusinessMonthBegin.n", "CustomBusinessMonthBegin.weekmask", "CustomBusinessMonthBegin.calendar", "CustomBusinessMonthBegin.holidays", "CustomBusinessMonthBegin.apply", "CustomBusinessMonthBegin.apply_index(other)", "CustomBusinessMonthBegin.copy", "CustomBusinessMonthBegin.isAnchored", "CustomBusinessMonthBegin.onOffset", "CustomBusinessMonthBegin.is_anchored", "CustomBusinessMonthBegin.is_on_offset", "CustomBusinessMonthBegin.__call__(*args, ...)", "Call self as a function.", "CustomBusinessMonthBegin.is_month_start", "CustomBusinessMonthBegin.is_month_end", "CustomBusinessMonthBegin.is_quarter_start", "CustomBusinessMonthBegin.is_quarter_end", "CustomBusinessMonthBegin.is_year_start", "CustomBusinessMonthBegin.is_year_end", "SemiMonthEnd", "Two DateOffset's per month repeating on the last day of the month and day_of_month.", "SemiMonthEnd.freqstr", "SemiMonthEnd.kwds", "SemiMonthEnd.name", "SemiMonthEnd.nanos", "SemiMonthEnd.normalize", "SemiMonthEnd.rule_code", "SemiMonthEnd.n", "SemiMonthEnd.day_of_month", "SemiMonthEnd.apply", "SemiMonthEnd.apply_index(other)", "SemiMonthEnd.copy", "SemiMonthEnd.isAnchored", "SemiMonthEnd.onOffset", "SemiMonthEnd.is_anchored", "SemiMonthEnd.is_on_offset", "SemiMonthEnd.__call__(*args, **kwargs)", "Call self as a function.", "SemiMonthEnd.is_month_start", "SemiMonthEnd.is_month_end", "SemiMonthEnd.is_quarter_start", "SemiMonthEnd.is_quarter_end", "SemiMonthEnd.is_year_start", "SemiMonthEnd.is_year_end", "SemiMonthBegin", "Two DateOffset's per month repeating on the first day of the month and day_of_month.", "SemiMonthBegin.freqstr", "SemiMonthBegin.kwds", "SemiMonthBegin.name", "SemiMonthBegin.nanos", "SemiMonthBegin.normalize", "SemiMonthBegin.rule_code", "SemiMonthBegin.n", "SemiMonthBegin.day_of_month", "SemiMonthBegin.apply", "SemiMonthBegin.apply_index(other)", "SemiMonthBegin.copy", "SemiMonthBegin.isAnchored", "SemiMonthBegin.onOffset", "SemiMonthBegin.is_anchored", "SemiMonthBegin.is_on_offset", "SemiMonthBegin.__call__(*args, **kwargs)", "Call self as a function.", "SemiMonthBegin.is_month_start", "SemiMonthBegin.is_month_end", "SemiMonthBegin.is_quarter_start", "SemiMonthBegin.is_quarter_end", "SemiMonthBegin.is_year_start", "SemiMonthBegin.is_year_end", "Week", "Weekly offset.", "Week.freqstr", "Week.kwds", "Week.name", "Week.nanos", "Week.normalize", "Week.rule_code", "Week.n", "Week.weekday", "Week.apply", "Week.apply_index(other)", "Week.copy", "Week.isAnchored", "Week.onOffset", "Week.is_anchored", "Week.is_on_offset", "Week.__call__(*args, **kwargs)", "Call self as a function.", "Week.is_month_start", "Week.is_month_end", "Week.is_quarter_start", "Week.is_quarter_end", "Week.is_year_start", "Week.is_year_end", "WeekOfMonth", "Describes monthly dates like \"the Tuesday of the 2nd week of each month\".", "WeekOfMonth.freqstr", "WeekOfMonth.kwds", "WeekOfMonth.name", "WeekOfMonth.nanos", "WeekOfMonth.normalize", "WeekOfMonth.rule_code", "WeekOfMonth.n", "WeekOfMonth.week", "WeekOfMonth.apply", "WeekOfMonth.apply_index(other)", "WeekOfMonth.copy", "WeekOfMonth.isAnchored", "WeekOfMonth.onOffset", "WeekOfMonth.is_anchored", "WeekOfMonth.is_on_offset", "WeekOfMonth.__call__(*args, **kwargs)", "Call self as a function.", "WeekOfMonth.weekday", "WeekOfMonth.is_month_start", "WeekOfMonth.is_month_end", "WeekOfMonth.is_quarter_start", "WeekOfMonth.is_quarter_end", "WeekOfMonth.is_year_start", "WeekOfMonth.is_year_end", "LastWeekOfMonth", "Describes monthly dates in last week of month like \"the last Tuesday of each month\".", "LastWeekOfMonth.freqstr", "LastWeekOfMonth.kwds", "LastWeekOfMonth.name", "LastWeekOfMonth.nanos", "LastWeekOfMonth.normalize", "LastWeekOfMonth.rule_code", "LastWeekOfMonth.n", "LastWeekOfMonth.weekday", "LastWeekOfMonth.week", "LastWeekOfMonth.apply", "LastWeekOfMonth.apply_index(other)", "LastWeekOfMonth.copy", "LastWeekOfMonth.isAnchored", "LastWeekOfMonth.onOffset", "LastWeekOfMonth.is_anchored", "LastWeekOfMonth.is_on_offset", "LastWeekOfMonth.__call__(*args, **kwargs)", "Call self as a function.", "LastWeekOfMonth.is_month_start", "LastWeekOfMonth.is_month_end", "LastWeekOfMonth.is_quarter_start", "LastWeekOfMonth.is_quarter_end", "LastWeekOfMonth.is_year_start", "LastWeekOfMonth.is_year_end", "BQuarterEnd", "DateOffset increments between the last business day of each Quarter.", "BQuarterEnd.freqstr", "BQuarterEnd.kwds", "BQuarterEnd.name", "BQuarterEnd.nanos", "BQuarterEnd.normalize", "BQuarterEnd.rule_code", "BQuarterEnd.n", "BQuarterEnd.startingMonth", "BQuarterEnd.apply", "BQuarterEnd.apply_index(other)", "BQuarterEnd.copy", "BQuarterEnd.isAnchored", "BQuarterEnd.onOffset", "BQuarterEnd.is_anchored", "BQuarterEnd.is_on_offset", "BQuarterEnd.__call__(*args, **kwargs)", "Call self as a function.", "BQuarterEnd.is_month_start", "BQuarterEnd.is_month_end", "BQuarterEnd.is_quarter_start", "BQuarterEnd.is_quarter_end", "BQuarterEnd.is_year_start", "BQuarterEnd.is_year_end", "BQuarterBegin", "DateOffset increments between the first business day of each Quarter.", "BQuarterBegin.freqstr", "BQuarterBegin.kwds", "BQuarterBegin.name", "BQuarterBegin.nanos", "BQuarterBegin.normalize", "BQuarterBegin.rule_code", "BQuarterBegin.n", "BQuarterBegin.startingMonth", "BQuarterBegin.apply", "BQuarterBegin.apply_index(other)", "BQuarterBegin.copy", "BQuarterBegin.isAnchored", "BQuarterBegin.onOffset", "BQuarterBegin.is_anchored", "BQuarterBegin.is_on_offset", "BQuarterBegin.__call__(*args, **kwargs)", "Call self as a function.", "BQuarterBegin.is_month_start", "BQuarterBegin.is_month_end", "BQuarterBegin.is_quarter_start", "BQuarterBegin.is_quarter_end", "BQuarterBegin.is_year_start", "BQuarterBegin.is_year_end", "QuarterEnd", "DateOffset increments between Quarter end dates.", "QuarterEnd.freqstr", "QuarterEnd.kwds", "QuarterEnd.name", "QuarterEnd.nanos", "QuarterEnd.normalize", "QuarterEnd.rule_code", "QuarterEnd.n", "QuarterEnd.startingMonth", "QuarterEnd.apply", "QuarterEnd.apply_index(other)", "QuarterEnd.copy", "QuarterEnd.isAnchored", "QuarterEnd.onOffset", "QuarterEnd.is_anchored", "QuarterEnd.is_on_offset", "QuarterEnd.__call__(*args, **kwargs)", "Call self as a function.", "QuarterEnd.is_month_start", "QuarterEnd.is_month_end", "QuarterEnd.is_quarter_start", "QuarterEnd.is_quarter_end", "QuarterEnd.is_year_start", "QuarterEnd.is_year_end", "QuarterBegin", "DateOffset increments between Quarter start dates.", "QuarterBegin.freqstr", "QuarterBegin.kwds", "QuarterBegin.name", "QuarterBegin.nanos", "QuarterBegin.normalize", "QuarterBegin.rule_code", "QuarterBegin.n", "QuarterBegin.startingMonth", "QuarterBegin.apply", "QuarterBegin.apply_index(other)", "QuarterBegin.copy", "QuarterBegin.isAnchored", "QuarterBegin.onOffset", "QuarterBegin.is_anchored", "QuarterBegin.is_on_offset", "QuarterBegin.__call__(*args, **kwargs)", "Call self as a function.", "QuarterBegin.is_month_start", "QuarterBegin.is_month_end", "QuarterBegin.is_quarter_start", "QuarterBegin.is_quarter_end", "QuarterBegin.is_year_start", "QuarterBegin.is_year_end", "BYearEnd", "DateOffset increments between the last business day of the year.", "BYearEnd.freqstr", "BYearEnd.kwds", "BYearEnd.name", "BYearEnd.nanos", "BYearEnd.normalize", "BYearEnd.rule_code", "BYearEnd.n", "BYearEnd.month", "BYearEnd.apply", "BYearEnd.apply_index(other)", "BYearEnd.copy", "BYearEnd.isAnchored", "BYearEnd.onOffset", "BYearEnd.is_anchored", "BYearEnd.is_on_offset", "BYearEnd.__call__(*args, **kwargs)", "Call self as a function.", "BYearEnd.is_month_start", "BYearEnd.is_month_end", "BYearEnd.is_quarter_start", "BYearEnd.is_quarter_end", "BYearEnd.is_year_start", "BYearEnd.is_year_end", "BYearBegin", "DateOffset increments between the first business day of the year.", "BYearBegin.freqstr", "BYearBegin.kwds", "BYearBegin.name", "BYearBegin.nanos", "BYearBegin.normalize", "BYearBegin.rule_code", "BYearBegin.n", "BYearBegin.month", "BYearBegin.apply", "BYearBegin.apply_index(other)", "BYearBegin.copy", "BYearBegin.isAnchored", "BYearBegin.onOffset", "BYearBegin.is_anchored", "BYearBegin.is_on_offset", "BYearBegin.__call__(*args, **kwargs)", "Call self as a function.", "BYearBegin.is_month_start", "BYearBegin.is_month_end", "BYearBegin.is_quarter_start", "BYearBegin.is_quarter_end", "BYearBegin.is_year_start", "BYearBegin.is_year_end", "YearEnd", "DateOffset increments between calendar year ends.", "YearEnd.freqstr", "YearEnd.kwds", "YearEnd.name", "YearEnd.nanos", "YearEnd.normalize", "YearEnd.rule_code", "YearEnd.n", "YearEnd.month", "YearEnd.apply", "YearEnd.apply_index(other)", "YearEnd.copy", "YearEnd.isAnchored", "YearEnd.onOffset", "YearEnd.is_anchored", "YearEnd.is_on_offset", "YearEnd.__call__(*args, **kwargs)", "Call self as a function.", "YearEnd.is_month_start", "YearEnd.is_month_end", "YearEnd.is_quarter_start", "YearEnd.is_quarter_end", "YearEnd.is_year_start", "YearEnd.is_year_end", "YearBegin", "DateOffset increments between calendar year begin dates.", "YearBegin.freqstr", "YearBegin.kwds", "YearBegin.name", "YearBegin.nanos", "YearBegin.normalize", "YearBegin.rule_code", "YearBegin.n", "YearBegin.month", "YearBegin.apply", "YearBegin.apply_index(other)", "YearBegin.copy", "YearBegin.isAnchored", "YearBegin.onOffset", "YearBegin.is_anchored", "YearBegin.is_on_offset", "YearBegin.__call__(*args, **kwargs)", "Call self as a function.", "YearBegin.is_month_start", "YearBegin.is_month_end", "YearBegin.is_quarter_start", "YearBegin.is_quarter_end", "YearBegin.is_year_start", "YearBegin.is_year_end", "FY5253", "Describes 52-53 week fiscal year.", "FY5253.freqstr", "FY5253.kwds", "FY5253.name", "FY5253.nanos", "FY5253.normalize", "FY5253.rule_code", "FY5253.n", "FY5253.startingMonth", "FY5253.variation", "FY5253.weekday", "FY5253.apply", "FY5253.apply_index(other)", "FY5253.copy", "FY5253.get_rule_code_suffix", "FY5253.get_year_end", "FY5253.isAnchored", "FY5253.onOffset", "FY5253.is_anchored", "FY5253.is_on_offset", "FY5253.__call__(*args, **kwargs)", "Call self as a function.", "FY5253.is_month_start", "FY5253.is_month_end", "FY5253.is_quarter_start", "FY5253.is_quarter_end", "FY5253.is_year_start", "FY5253.is_year_end", "FY5253Quarter", "DateOffset increments between business quarter dates for 52-53 week fiscal year (also known as a 4-4-5 calendar).", "FY5253Quarter.freqstr", "FY5253Quarter.kwds", "FY5253Quarter.name", "FY5253Quarter.nanos", "FY5253Quarter.normalize", "FY5253Quarter.rule_code", "FY5253Quarter.n", "FY5253Quarter.qtr_with_extra_week", "FY5253Quarter.startingMonth", "FY5253Quarter.variation", "FY5253Quarter.weekday", "FY5253Quarter.apply", "FY5253Quarter.apply_index(other)", "FY5253Quarter.copy", "FY5253Quarter.get_rule_code_suffix", "FY5253Quarter.get_weeks", "FY5253Quarter.isAnchored", "FY5253Quarter.onOffset", "FY5253Quarter.is_anchored", "FY5253Quarter.is_on_offset", "FY5253Quarter.year_has_extra_week", "FY5253Quarter.__call__(*args, **kwargs)", "Call self as a function.", "FY5253Quarter.is_month_start", "FY5253Quarter.is_month_end", "FY5253Quarter.is_quarter_start", "FY5253Quarter.is_quarter_end", "FY5253Quarter.is_year_start", "FY5253Quarter.is_year_end", "Easter", "DateOffset for the Easter holiday using logic defined in dateutil.", "Easter.freqstr", "Easter.kwds", "Easter.name", "Easter.nanos", "Easter.normalize", "Easter.rule_code", "Easter.n", "Easter.apply", "Easter.apply_index(other)", "Easter.copy", "Easter.isAnchored", "Easter.onOffset", "Easter.is_anchored", "Easter.is_on_offset", "Easter.__call__(*args, **kwargs)", "Call self as a function.", "Easter.is_month_start", "Easter.is_month_end", "Easter.is_quarter_start", "Easter.is_quarter_end", "Easter.is_year_start", "Easter.is_year_end", "Tick", "Attributes", "Tick.delta", "Tick.freqstr", "Tick.kwds", "Tick.name", "Tick.nanos", "Tick.normalize", "Tick.rule_code", "Tick.n", "Tick.copy", "Tick.isAnchored", "Tick.onOffset", "Tick.is_anchored", "Tick.is_on_offset", "Tick.__call__(*args, **kwargs)", "Call self as a function.", "Tick.apply", "Tick.apply_index(other)", "Tick.is_month_start", "Tick.is_month_end", "Tick.is_quarter_start", "Tick.is_quarter_end", "Tick.is_year_start", "Tick.is_year_end", "Day", "Attributes", "Day.delta", "Day.freqstr", "Day.kwds", "Day.name", "Day.nanos", "Day.normalize", "Day.rule_code", "Day.n", "Day.copy", "Day.isAnchored", "Day.onOffset", "Day.is_anchored", "Day.is_on_offset", "Day.__call__(*args, **kwargs)", "Call self as a function.", "Day.apply", "Day.apply_index(other)", "Day.is_month_start", "Day.is_month_end", "Day.is_quarter_start", "Day.is_quarter_end", "Day.is_year_start", "Day.is_year_end", "Hour", "Attributes", "Hour.delta", "Hour.freqstr", "Hour.kwds", "Hour.name", "Hour.nanos", "Hour.normalize", "Hour.rule_code", "Hour.n", "Hour.copy", "Hour.isAnchored", "Hour.onOffset", "Hour.is_anchored", "Hour.is_on_offset", "Hour.__call__(*args, **kwargs)", "Call self as a function.", "Hour.apply", "Hour.apply_index(other)", "Hour.is_month_start", "Hour.is_month_end", "Hour.is_quarter_start", "Hour.is_quarter_end", "Hour.is_year_start", "Hour.is_year_end", "Minute", "Attributes", "Minute.delta", "Minute.freqstr", "Minute.kwds", "Minute.name", "Minute.nanos", "Minute.normalize", "Minute.rule_code", "Minute.n", "Minute.copy", "Minute.isAnchored", "Minute.onOffset", "Minute.is_anchored", "Minute.is_on_offset", "Minute.__call__(*args, **kwargs)", "Call self as a function.", "Minute.apply", "Minute.apply_index(other)", "Minute.is_month_start", "Minute.is_month_end", "Minute.is_quarter_start", "Minute.is_quarter_end", "Minute.is_year_start", "Minute.is_year_end", "Second", "Attributes", "Second.delta", "Second.freqstr", "Second.kwds", "Second.name", "Second.nanos", "Second.normalize", "Second.rule_code", "Second.n", "Second.copy", "Second.isAnchored", "Second.onOffset", "Second.is_anchored", "Second.is_on_offset", "Second.__call__(*args, **kwargs)", "Call self as a function.", "Second.apply", "Second.apply_index(other)", "Second.is_month_start", "Second.is_month_end", "Second.is_quarter_start", "Second.is_quarter_end", "Second.is_year_start", "Second.is_year_end", "Milli", "Attributes", "Milli.delta", "Milli.freqstr", "Milli.kwds", "Milli.name", "Milli.nanos", "Milli.normalize", "Milli.rule_code", "Milli.n", "Milli.copy", "Milli.isAnchored", "Milli.onOffset", "Milli.is_anchored", "Milli.is_on_offset", "Milli.__call__(*args, **kwargs)", "Call self as a function.", "Milli.apply", "Milli.apply_index(other)", "Milli.is_month_start", "Milli.is_month_end", "Milli.is_quarter_start", "Milli.is_quarter_end", "Milli.is_year_start", "Milli.is_year_end", "Micro", "Attributes", "Micro.delta", "Micro.freqstr", "Micro.kwds", "Micro.name", "Micro.nanos", "Micro.normalize", "Micro.rule_code", "Micro.n", "Micro.copy", "Micro.isAnchored", "Micro.onOffset", "Micro.is_anchored", "Micro.is_on_offset", "Micro.__call__(*args, **kwargs)", "Call self as a function.", "Micro.apply", "Micro.apply_index(other)", "Micro.is_month_start", "Micro.is_month_end", "Micro.is_quarter_start", "Micro.is_quarter_end", "Micro.is_year_start", "Micro.is_year_end", "Nano", "Attributes", "Nano.delta", "Nano.freqstr", "Nano.kwds", "Nano.name", "Nano.nanos", "Nano.normalize", "Nano.rule_code", "Nano.n", "Nano.copy", "Nano.isAnchored", "Nano.onOffset", "Nano.is_anchored", "Nano.is_on_offset", "Nano.__call__(*args, **kwargs)", "Call self as a function.", "Nano.apply", "Nano.apply_index(other)", "Nano.is_month_start", "Nano.is_month_end", "Nano.is_quarter_start", "Nano.is_quarter_end", "Nano.is_year_start", "Nano.is_year_end"]}, {"name": "Duplicate Labels", "path": "user_guide/duplicates", "type": "Manual", "text": ["Index objects are not required to be unique; you can have duplicate row or column labels. This may be a bit confusing at first. If you\u2019re familiar with SQL, you know that row labels are similar to a primary key on a table, and you would never want duplicates in a SQL table. But one of pandas\u2019 roles is to clean messy, real-world data before it goes to some downstream system. And real-world data has duplicates, even in fields that are supposed to be unique.", "This section describes how duplicate labels change the behavior of certain operations, and how prevent duplicates from arising during operations, or to detect them if they do.", "Some pandas methods (Series.reindex() for example) just don\u2019t work with duplicates present. The output can\u2019t be determined, and so pandas raises.", "Other methods, like indexing, can give very surprising results. Typically indexing with a scalar will reduce dimensionality. Slicing a DataFrame with a scalar will return a Series. Slicing a Series with a scalar will return a scalar. But with duplicates, this isn\u2019t the case.", "We have duplicates in the columns. If we slice 'B', we get back a Series", "But slicing 'A' returns a DataFrame", "This applies to row labels as well", "You can check whether an Index (storing the row or column labels) is unique with Index.is_unique:", "Note", "Checking whether an index is unique is somewhat expensive for large datasets. pandas does cache this result, so re-checking on the same index is very fast.", "Index.duplicated() will return a boolean ndarray indicating whether a label is repeated.", "Which can be used as a boolean filter to drop duplicate rows.", "If you need additional logic to handle duplicate labels, rather than just dropping the repeats, using groupby() on the index is a common trick. For example, we\u2019ll resolve duplicates by taking the average of all rows with the same label.", "New in version 1.2.0.", "As noted above, handling duplicates is an important feature when reading in raw data. That said, you may want to avoid introducing duplicates as part of a data processing pipeline (from methods like pandas.concat(), rename(), etc.). Both Series and DataFrame disallow duplicate labels by calling .set_flags(allows_duplicate_labels=False). (the default is to allow them). If there are duplicate labels, an exception will be raised.", "This applies to both row and column labels for a DataFrame", "This attribute can be checked or set with allows_duplicate_labels, which indicates whether that object can have duplicate labels.", "DataFrame.set_flags() can be used to return a new DataFrame with attributes like allows_duplicate_labels set to some value", "The new DataFrame returned is a view on the same data as the old DataFrame. Or the property can just be set directly on the same object", "When processing raw, messy data you might initially read in the messy data (which potentially has duplicate labels), deduplicate, and then disallow duplicates going forward, to ensure that your data pipeline doesn\u2019t introduce duplicates.", "Setting allows_duplicate_labels=True on a Series or DataFrame with duplicate labels or performing an operation that introduces duplicate labels on a Series or DataFrame that disallows duplicates will raise an errors.DuplicateLabelError.", "This error message contains the labels that are duplicated, and the numeric positions of all the duplicates (including the \u201coriginal\u201d) in the Series or DataFrame", "In general, disallowing duplicates is \u201csticky\u201d. It\u2019s preserved through operations.", "Warning", "This is an experimental feature. Currently, many methods fail to propagate the allows_duplicate_labels value. In future versions it is expected that every method taking or returning one or more DataFrame or Series objects will propagate allows_duplicate_labels."]}, {"name": "Enhancing performance", "path": "user_guide/enhancingperf", "type": "Manual", "text": ["In this part of the tutorial, we will investigate how to speed up certain functions operating on pandas DataFrames using three different techniques: Cython, Numba and pandas.eval(). We will see a speed improvement of ~200 when we use Cython and Numba on a test function operating row-wise on the DataFrame. Using pandas.eval() we will speed up a sum by an order of ~2.", "Note", "In addition to following the steps in this tutorial, users interested in enhancing performance are highly encouraged to install the recommended dependencies for pandas. These dependencies are often not installed by default, but will offer speed improvements if present.", "For many use cases writing pandas in pure Python and NumPy is sufficient. In some computationally heavy applications however, it can be possible to achieve sizable speed-ups by offloading work to cython.", "This tutorial assumes you have refactored as much as possible in Python, for example by trying to remove for-loops and making use of NumPy vectorization. It\u2019s always worth optimising in Python first.", "This tutorial walks through a \u201ctypical\u201d process of cythonizing a slow computation. We use an example from the Cython documentation but in the context of pandas. Our final cythonized solution is around 100 times faster than the pure Python solution.", "We have a DataFrame to which we want to apply a function row-wise.", "Here\u2019s the function in pure Python:", "We achieve our result by using apply (row-wise):", "But clearly this isn\u2019t fast enough for us. Let\u2019s take a look and see where the time is spent during this operation (limited to the most time consuming four calls) using the prun ipython magic function:", "By far the majority of time is spend inside either integrate_f or f, hence we\u2019ll concentrate our efforts cythonizing these two functions.", "First we\u2019re going to need to import the Cython magic function to IPython:", "Now, let\u2019s simply copy our functions over to Cython as is (the suffix is here to distinguish between function versions):", "Note", "If you\u2019re having trouble pasting the above into your ipython, you may need to be using bleeding edge IPython for paste to play well with cell magics.", "Already this has shaved a third off, not too bad for a simple copy and paste.", "We get another huge improvement simply by providing type information:", "Now, we\u2019re talking! It\u2019s now over ten times faster than the original Python implementation, and we haven\u2019t really modified the code. Let\u2019s have another look at what\u2019s eating up time:", "It\u2019s calling series\u2026 a lot! It\u2019s creating a Series from each row, and get-ting from both the index and the series (three times for each row). Function calls are expensive in Python, so maybe we could minimize these by cythonizing the apply part.", "Note", "We are now passing ndarrays into the Cython function, fortunately Cython plays very nicely with NumPy.", "The implementation is simple, it creates an array of zeros and loops over the rows, applying our integrate_f_typed, and putting this in the zeros array.", "Warning", "You can not pass a Series directly as a ndarray typed parameter to a Cython function. Instead pass the actual ndarray using the Series.to_numpy(). The reason is that the Cython definition is specific to an ndarray and not the passed Series.", "So, do not do this:", "But rather, use Series.to_numpy() to get the underlying ndarray:", "Note", "Loops like this would be extremely slow in Python, but in Cython looping over NumPy arrays is fast.", "We\u2019ve gotten another big improvement. Let\u2019s check again where the time is spent:", "As one might expect, the majority of the time is now spent in apply_integrate_f, so if we wanted to make anymore efficiencies we must continue to concentrate our efforts here.", "There is still hope for improvement. Here\u2019s an example of using some more advanced Cython techniques:", "Even faster, with the caveat that a bug in our Cython code (an off-by-one error, for example) might cause a segfault because memory access isn\u2019t checked. For more about boundscheck and wraparound, see the Cython docs on compiler directives.", "An alternative to statically compiling Cython code is to use a dynamic just-in-time (JIT) compiler with Numba.", "Numba allows you to write a pure Python function which can be JIT compiled to native machine instructions, similar in performance to C, C++ and Fortran, by decorating your function with @jit.", "Numba works by generating optimized machine code using the LLVM compiler infrastructure at import time, runtime, or statically (using the included pycc tool). Numba supports compilation of Python to run on either CPU or GPU hardware and is designed to integrate with the Python scientific software stack.", "Note", "The @jit compilation will add overhead to the runtime of the function, so performance benefits may not be realized especially when using small data sets. Consider caching your function to avoid compilation overhead each time your function is run.", "Numba can be used in 2 ways with pandas:", "Specify the engine=\"numba\" keyword in select pandas methods", "Define your own Python function decorated with @jit and pass the underlying NumPy array of Series or Dataframe (using to_numpy()) into the function", "If Numba is installed, one can specify engine=\"numba\" in select pandas methods to execute the method using Numba. Methods that support engine=\"numba\" will also have an engine_kwargs keyword that accepts a dictionary that allows one to specify \"nogil\", \"nopython\" and \"parallel\" keys with boolean values to pass into the @jit decorator. If engine_kwargs is not specified, it defaults to {\"nogil\": False, \"nopython\": True, \"parallel\": False} unless otherwise specified.", "In terms of performance, the first time a function is run using the Numba engine will be slow as Numba will have some function compilation overhead. However, the JIT compiled functions are cached, and subsequent calls will be fast. In general, the Numba engine is performant with a larger amount of data points (e.g. 1+ million).", "A custom Python function decorated with @jit can be used with pandas objects by passing their NumPy array representations with to_numpy().", "In this example, using Numba was faster than Cython.", "Numba can also be used to write vectorized functions that do not require the user to explicitly loop over the observations of a vector; a vectorized function will be applied to each row automatically. Consider the following example of doubling each observation:", "Numba is best at accelerating functions that apply numerical functions to NumPy arrays. If you try to @jit a function that contains unsupported Python or NumPy code, compilation will revert object mode which will mostly likely not speed up your function. If you would prefer that Numba throw an error if it cannot compile a function in a way that speeds up your code, pass Numba the argument nopython=True (e.g. @jit(nopython=True)). For more on troubleshooting Numba modes, see the Numba troubleshooting page.", "Using parallel=True (e.g. @jit(parallel=True)) may result in a SIGABRT if the threading layer leads to unsafe behavior. You can first specify a safe threading layer before running a JIT function with parallel=True.", "Generally if the you encounter a segfault (SIGSEGV) while using Numba, please report the issue to the Numba issue tracker.", "The top-level function pandas.eval() implements expression evaluation of Series and DataFrame objects.", "Note", "To benefit from using eval() you need to install numexpr. See the recommended dependencies section for more details.", "The point of using eval() for expression evaluation rather than plain Python is two-fold: 1) large DataFrame objects are evaluated more efficiently and 2) large arithmetic and boolean expressions are evaluated all at once by the underlying engine (by default numexpr is used for evaluation).", "Note", "You should not use eval() for simple expressions or for expressions involving small DataFrames. In fact, eval() is many orders of magnitude slower for smaller expressions/objects than plain ol\u2019 Python. A good rule of thumb is to only use eval() when you have a DataFrame with more than 10,000 rows.", "eval() supports all arithmetic expressions supported by the engine in addition to some extensions available only in pandas.", "Note", "The larger the frame and the larger the expression the more speedup you will see from using eval().", "These operations are supported by pandas.eval():", "Arithmetic operations except for the left shift (<<) and right shift (>>) operators, e.g., df + 2 * pi / s ** 4 % 42 - the_golden_ratio", "Comparison operations, including chained comparisons, e.g., 2 < df < df2", "Boolean operations, e.g., df < df2 and df3 < df4 or not df_bool", "list and tuple literals, e.g., [1, 2] or (1, 2)", "Attribute access, e.g., df.a", "Subscript expressions, e.g., df[0]", "Simple variable evaluation, e.g., pd.eval(\"df\") (this is not very useful)", "Math functions: sin, cos, exp, log, expm1, log1p, sqrt, sinh, cosh, tanh, arcsin, arccos, arctan, arccosh, arcsinh, arctanh, abs, arctan2 and log10.", "This Python syntax is not allowed:", "Expressions", "Function calls other than math functions.", "is/is not operations", "if expressions", "lambda expressions", "list/set/dict comprehensions", "Literal dict and set expressions", "yield expressions", "Generator expressions", "Boolean expressions consisting of only scalar values", "Statements", "Neither simple nor compound statements are allowed. This includes things like for, while, and if.", "pandas.eval() works well with expressions containing large arrays.", "First let\u2019s create a few decent-sized arrays to play with:", "Now let\u2019s compare adding them together using plain ol\u2019 Python versus eval():", "Now let\u2019s do the same thing but with comparisons:", "eval() also works with unaligned pandas objects:", "Note", "Operations such as", "should be performed in Python. An exception will be raised if you try to perform any boolean/bitwise operations with scalar operands that are not of type bool or np.bool_. Again, you should perform these kinds of operations in plain Python.", "In addition to the top level pandas.eval() function you can also evaluate an expression in the \u201ccontext\u201d of a DataFrame.", "Any expression that is a valid pandas.eval() expression is also a valid DataFrame.eval() expression, with the added benefit that you don\u2019t have to prefix the name of the DataFrame to the column(s) you\u2019re interested in evaluating.", "In addition, you can perform assignment of columns within an expression. This allows for formulaic evaluation. The assignment target can be a new column name or an existing column name, and it must be a valid Python identifier.", "The inplace keyword determines whether this assignment will performed on the original DataFrame or return a copy with the new column.", "When inplace is set to False, the default, a copy of the DataFrame with the new or modified columns is returned and the original frame is unchanged.", "As a convenience, multiple assignments can be performed by using a multi-line string.", "The equivalent in standard Python would be", "The query method has a inplace keyword which determines whether the query modifies the original frame.", "You must explicitly reference any local variable that you want to use in an expression by placing the @ character in front of the name. For example,", "If you don\u2019t prefix the local variable with @, pandas will raise an exception telling you the variable is undefined.", "When using DataFrame.eval() and DataFrame.query(), this allows you to have a local variable and a DataFrame column with the same name in an expression.", "With pandas.eval() you cannot use the @ prefix at all, because it isn\u2019t defined in that context. pandas will let you know this if you try to use @ in a top-level call to pandas.eval(). For example,", "In this case, you should simply refer to the variables like you would in standard Python.", "There are two different parsers and two different engines you can use as the backend.", "The default 'pandas' parser allows a more intuitive syntax for expressing query-like operations (comparisons, conjunctions and disjunctions). In particular, the precedence of the & and | operators is made equal to the precedence of the corresponding boolean operations and and or.", "For example, the above conjunction can be written without parentheses. Alternatively, you can use the 'python' parser to enforce strict Python semantics.", "The same expression can be \u201canded\u201d together with the word and as well:", "The and and or operators here have the same precedence that they would in vanilla Python.", "There\u2019s also the option to make eval() operate identical to plain ol\u2019 Python.", "Note", "Using the 'python' engine is generally not useful, except for testing other evaluation engines against it. You will achieve no performance benefits using eval() with engine='python' and in fact may incur a performance hit.", "You can see this by using pandas.eval() with the 'python' engine. It is a bit slower (not by much) than evaluating the same expression in Python", "eval() is intended to speed up certain kinds of operations. In particular, those operations involving complex expressions with large DataFrame/Series objects should see a significant performance benefit. Here is a plot showing the running time of pandas.eval() as function of the size of the frame involved in the computation. The two lines are two different engines.", "Note", "Operations with smallish objects (around 15k-20k rows) are faster using plain Python:", "This plot was created using a DataFrame with 3 columns each containing floating point values generated using numpy.random.randn().", "Expressions that would result in an object dtype or involve datetime operations (because of NaT) must be evaluated in Python space. The main reason for this behavior is to maintain backwards compatibility with versions of NumPy < 1.7. In those versions of NumPy a call to ndarray.astype(str) will truncate any strings that are more than 60 characters in length. Second, we can\u2019t pass object arrays to numexpr thus string comparisons must be evaluated in Python space.", "The upshot is that this only applies to object-dtype expressions. So, if you have an expression\u2013for example", "the numeric part of the comparison (nums == 1) will be evaluated by numexpr.", "In general, DataFrame.query()/pandas.eval() will evaluate the subexpressions that can be evaluated by numexpr and those that must be evaluated in Python space transparently to the user. This is done by inferring the result type of an expression from its arguments and operators."]}, {"name": "Essential basic functionality", "path": "user_guide/basics", "type": "Manual", "text": ["Here we discuss a lot of the essential functionality common to the pandas data structures. To begin, let\u2019s create some example objects like we did in the 10 minutes to pandas section:", "To view a small sample of a Series or DataFrame object, use the head() and tail() methods. The default number of elements to display is five, but you may pass a custom number.", "pandas objects have a number of attributes enabling you to access the metadata", "shape: gives the axis dimensions of the object, consistent with ndarray", "Series: index (only axis)", "DataFrame: index (rows) and columns", "Note, these attributes can be safely assigned to!", "pandas objects (Index, Series, DataFrame) can be thought of as containers for arrays, which hold the actual data and do the actual computation. For many types, the underlying array is a numpy.ndarray. However, pandas and 3rd party libraries may extend NumPy\u2019s type system to add support for custom arrays (see dtypes).", "To get the actual data inside a Index or Series, use the .array property", "array will always be an ExtensionArray. The exact details of what an ExtensionArray is and why pandas uses them are a bit beyond the scope of this introduction. See dtypes for more.", "If you know you need a NumPy array, use to_numpy() or numpy.asarray().", "When the Series or Index is backed by an ExtensionArray, to_numpy() may involve copying data and coercing values. See dtypes for more.", "to_numpy() gives some control over the dtype of the resulting numpy.ndarray. For example, consider datetimes with timezones. NumPy doesn\u2019t have a dtype to represent timezone-aware datetimes, so there are two possibly useful representations:", "An object-dtype numpy.ndarray with Timestamp objects, each with the correct tz", "A datetime64[ns] -dtype numpy.ndarray, where the values have been converted to UTC and the timezone discarded", "Timezones may be preserved with dtype=object", "Or thrown away with dtype='datetime64[ns]'", "Getting the \u201craw data\u201d inside a DataFrame is possibly a bit more complex. When your DataFrame only has a single data type for all the columns, DataFrame.to_numpy() will return the underlying data:", "If a DataFrame contains homogeneously-typed data, the ndarray can actually be modified in-place, and the changes will be reflected in the data structure. For heterogeneous data (e.g. some of the DataFrame\u2019s columns are not all the same dtype), this will not be the case. The values attribute itself, unlike the axis labels, cannot be assigned to.", "Note", "When working with heterogeneous data, the dtype of the resulting ndarray will be chosen to accommodate all of the data involved. For example, if strings are involved, the result will be of object dtype. If there are only floats and integers, the resulting array will be of float dtype.", "In the past, pandas recommended Series.values or DataFrame.values for extracting the data from a Series or DataFrame. You\u2019ll still find references to these in old code bases and online. Going forward, we recommend avoiding .values and using .array or .to_numpy(). .values has the following drawbacks:", "When your Series contains an extension type, it\u2019s unclear whether Series.values returns a NumPy array or the extension array. Series.array will always return an ExtensionArray, and will never copy data. Series.to_numpy() will always return a NumPy array, potentially at the cost of copying / coercing values.", "When your DataFrame contains a mixture of data types, DataFrame.values may involve copying data and coercing values to a common dtype, a relatively expensive operation. DataFrame.to_numpy(), being a method, makes it clearer that the returned NumPy array may not be a view on the same data in the DataFrame.", "pandas has support for accelerating certain types of binary numerical and boolean operations using the numexpr library and the bottleneck libraries.", "These libraries are especially useful when dealing with large data sets, and provide large speedups. numexpr uses smart chunking, caching, and multiple cores. bottleneck is a set of specialized cython routines that are especially fast when dealing with arrays that have nans.", "Here is a sample (using 100 column x 100,000 row DataFrames):", "Operation", "0.11.0 (ms)", "Prior Version (ms)", "Ratio to Prior", "df1 > df2", "13.32", "125.35", "0.1063", "df1 * df2", "21.71", "36.63", "0.5928", "df1 + df2", "22.04", "36.50", "0.6039", "You are highly encouraged to install both libraries. See the section Recommended Dependencies for more installation info.", "These are both enabled to be used by default, you can control this by setting the options:", "With binary operations between pandas data structures, there are two key points of interest:", "Broadcasting behavior between higher- (e.g. DataFrame) and lower-dimensional (e.g. Series) objects.", "Missing data in computations.", "We will demonstrate how to manage these issues independently, though they can be handled simultaneously.", "DataFrame has the methods add(), sub(), mul(), div() and related functions radd(), rsub(), \u2026 for carrying out binary operations. For broadcasting behavior, Series input is of primary interest. Using these functions, you can use to either match on the index or columns via the axis keyword:", "Furthermore you can align a level of a MultiIndexed DataFrame with a Series.", "Series and Index also support the divmod() builtin. This function takes the floor division and modulo operation at the same time returning a two-tuple of the same type as the left hand side. For example:", "We can also do elementwise divmod():", "In Series and DataFrame, the arithmetic functions have the option of inputting a fill_value, namely a value to substitute when at most one of the values at a location are missing. For example, when adding two DataFrame objects, you may wish to treat NaN as 0 unless both DataFrames are missing that value, in which case the result will be NaN (you can later replace NaN with some other value using fillna if you wish).", "Series and DataFrame have the binary comparison methods eq, ne, lt, gt, le, and ge whose behavior is analogous to the binary arithmetic operations described above:", "These operations produce a pandas object of the same type as the left-hand-side input that is of dtype bool. These boolean objects can be used in indexing operations, see the section on Boolean indexing.", "You can apply the reductions: empty, any(), all(), and bool() to provide a way to summarize a boolean result.", "You can reduce to a final boolean value.", "You can test if a pandas object is empty, via the empty property.", "To evaluate single-element pandas objects in a boolean context, use the method bool():", "Warning", "You might be tempted to do the following:", "Or", "These will both raise errors, as you are trying to compare multiple values.:", "See gotchas for a more detailed discussion.", "Often you may find that there is more than one way to compute the same result. As a simple example, consider df + df and df * 2. To test that these two computations produce the same result, given the tools shown above, you might imagine using (df + df == df * 2).all(). But in fact, this expression is False:", "Notice that the boolean DataFrame df + df == df * 2 contains some False values! This is because NaNs do not compare as equals:", "So, NDFrames (such as Series and DataFrames) have an equals() method for testing equality, with NaNs in corresponding locations treated as equal.", "Note that the Series or DataFrame index needs to be in the same order for equality to be True:", "You can conveniently perform element-wise comparisons when comparing a pandas data structure with a scalar value:", "pandas also handles element-wise comparisons between different array-like objects of the same length:", "Trying to compare Index or Series objects of different lengths will raise a ValueError:", "Note that this is different from the NumPy behavior where a comparison can be broadcast:", "or it can return False if broadcasting can not be done:", "A problem occasionally arising is the combination of two similar data sets where values in one are preferred over the other. An example would be two data series representing a particular economic indicator where one is considered to be of \u201chigher quality\u201d. However, the lower quality series might extend further back in history or have more complete data coverage. As such, we would like to combine two DataFrame objects where missing values in one DataFrame are conditionally filled with like-labeled values from the other DataFrame. The function implementing this operation is combine_first(), which we illustrate:", "The combine_first() method above calls the more general DataFrame.combine(). This method takes another DataFrame and a combiner function, aligns the input DataFrame and then passes the combiner function pairs of Series (i.e., columns whose names are the same).", "So, for instance, to reproduce combine_first() as above:", "There exists a large number of methods for computing descriptive statistics and other related operations on Series, DataFrame. Most of these are aggregations (hence producing a lower-dimensional result) like sum(), mean(), and quantile(), but some of them, like cumsum() and cumprod(), produce an object of the same size. Generally speaking, these methods take an axis argument, just like ndarray.{sum, std, \u2026}, but the axis can be specified by name or integer:", "Series: no axis argument needed", "DataFrame: \u201cindex\u201d (axis=0, default), \u201ccolumns\u201d (axis=1)", "For example:", "All such methods have a skipna option signaling whether to exclude missing data (True by default):", "Combined with the broadcasting / arithmetic behavior, one can describe various statistical procedures, like standardization (rendering data zero mean and standard deviation of 1), very concisely:", "Note that methods like cumsum() and cumprod() preserve the location of NaN values. This is somewhat different from expanding() and rolling() since NaN behavior is furthermore dictated by a min_periods parameter.", "Here is a quick reference summary table of common functions. Each also takes an optional level parameter which applies only if the object has a hierarchical index.", "Function", "Description", "count", "Number of non-NA observations", "sum", "Sum of values", "mean", "Mean of values", "mad", "Mean absolute deviation", "median", "Arithmetic median of values", "min", "Minimum", "max", "Maximum", "mode", "Mode", "abs", "Absolute Value", "prod", "Product of values", "std", "Bessel-corrected sample standard deviation", "var", "Unbiased variance", "sem", "Standard error of the mean", "skew", "Sample skewness (3rd moment)", "kurt", "Sample kurtosis (4th moment)", "quantile", "Sample quantile (value at %)", "cumsum", "Cumulative sum", "cumprod", "Cumulative product", "cummax", "Cumulative maximum", "cummin", "Cumulative minimum", "Note that by chance some NumPy methods, like mean, std, and sum, will exclude NAs on Series input by default:", "Series.nunique() will return the number of unique non-NA values in a Series:", "There is a convenient describe() function which computes a variety of summary statistics about a Series or the columns of a DataFrame (excluding NAs of course):", "You can select specific percentiles to include in the output:", "By default, the median is always included.", "For a non-numerical Series object, describe() will give a simple summary of the number of unique values and most frequently occurring values:", "Note that on a mixed-type DataFrame object, describe() will restrict the summary to include only numerical columns or, if none are, only categorical columns:", "This behavior can be controlled by providing a list of types as include/exclude arguments. The special value all can also be used:", "That feature relies on select_dtypes. Refer to there for details about accepted inputs.", "The idxmin() and idxmax() functions on Series and DataFrame compute the index labels with the minimum and maximum corresponding values:", "When there are multiple rows (or columns) matching the minimum or maximum value, idxmin() and idxmax() return the first matching index:", "Note", "idxmin and idxmax are called argmin and argmax in NumPy.", "The value_counts() Series method and top-level function computes a histogram of a 1D array of values. It can also be used as a function on regular arrays:", "New in version 1.1.0.", "The value_counts() method can be used to count combinations across multiple columns. By default all columns are used but a subset can be selected using the subset argument.", "Similarly, you can get the most frequently occurring value(s), i.e. the mode, of the values in a Series or DataFrame:", "Continuous values can be discretized using the cut() (bins based on values) and qcut() (bins based on sample quantiles) functions:", "qcut() computes sample quantiles. For example, we could slice up some normally distributed data into equal-size quartiles like so:", "We can also pass infinite values to define the bins:", "To apply your own or another library\u2019s functions to pandas objects, you should be aware of the three methods below. The appropriate method to use depends on whether your function expects to operate on an entire DataFrame or Series, row- or column-wise, or elementwise.", "Tablewise Function Application: pipe()", "Row or Column-wise Function Application: apply()", "Aggregation API: agg() and transform()", "Applying Elementwise Functions: applymap()", "DataFrames and Series can be passed into functions. However, if the function needs to be called in a chain, consider using the pipe() method.", "First some setup:", "extract_city_name and add_country_name are functions taking and returning DataFrames.", "Now compare the following:", "Is equivalent to:", "pandas encourages the second style, which is known as method chaining. pipe makes it easy to use your own or another library\u2019s functions in method chains, alongside pandas\u2019 methods.", "In the example above, the functions extract_city_name and add_country_name each expected a DataFrame as the first positional argument. What if the function you wish to apply takes its data as, say, the second argument? In this case, provide pipe with a tuple of (callable, data_keyword). .pipe will route the DataFrame to the argument specified in the tuple.", "For example, we can fit a regression using statsmodels. Their API expects a formula first and a DataFrame as the second argument, data. We pass in the function, keyword pair (sm.ols, 'data') to pipe:", "The pipe method is inspired by unix pipes and more recently dplyr and magrittr, which have introduced the popular (%>%) (read pipe) operator for R. The implementation of pipe here is quite clean and feels right at home in Python. We encourage you to view the source code of pipe().", "Arbitrary functions can be applied along the axes of a DataFrame using the apply() method, which, like the descriptive statistics methods, takes an optional axis argument:", "The apply() method will also dispatch on a string method name.", "The return type of the function passed to apply() affects the type of the final output from DataFrame.apply for the default behaviour:", "If the applied function returns a Series, the final output is a DataFrame. The columns match the index of the Series returned by the applied function.", "If the applied function returns any other type, the final output is a Series.", "This default behaviour can be overridden using the result_type, which accepts three options: reduce, broadcast, and expand. These will determine how list-likes return values expand (or not) to a DataFrame.", "apply() combined with some cleverness can be used to answer many questions about a data set. For example, suppose we wanted to extract the date where the maximum value for each column occurred:", "You may also pass additional arguments and keyword arguments to the apply() method. For instance, consider the following function you would like to apply:", "You may then apply this function as follows:", "Another useful feature is the ability to pass Series methods to carry out some Series operation on each column or row:", "Finally, apply() takes an argument raw which is False by default, which converts each row or column into a Series before applying the function. When set to True, the passed function will instead receive an ndarray object, which has positive performance implications if you do not need the indexing functionality.", "The aggregation API allows one to express possibly multiple aggregation operations in a single concise way. This API is similar across pandas objects, see groupby API, the window API, and the resample API. The entry point for aggregation is DataFrame.aggregate(), or the alias DataFrame.agg().", "We will use a similar starting frame from above:", "Using a single function is equivalent to apply(). You can also pass named methods as strings. These will return a Series of the aggregated output:", "Single aggregations on a Series this will return a scalar value:", "You can pass multiple aggregation arguments as a list. The results of each of the passed functions will be a row in the resulting DataFrame. These are naturally named from the aggregation function.", "Multiple functions yield multiple rows:", "On a Series, multiple functions return a Series, indexed by the function names:", "Passing a lambda function will yield a <lambda> named row:", "Passing a named function will yield that name for the row:", "Passing a dictionary of column names to a scalar or a list of scalars, to DataFrame.agg allows you to customize which functions are applied to which columns. Note that the results are not in any particular order, you can use an OrderedDict instead to guarantee ordering.", "Passing a list-like will generate a DataFrame output. You will get a matrix-like output of all of the aggregators. The output will consist of all unique functions. Those that are not noted for a particular column will be NaN:", "Deprecated since version 1.4.0: Attempting to determine which columns cannot be aggregated and silently dropping them from the results is deprecated and will be removed in a future version. If any porition of the columns or operations provided fail, the call to .agg will raise.", "When presented with mixed dtypes that cannot aggregate, .agg will only take the valid aggregations. This is similar to how .groupby.agg works.", "With .agg() it is possible to easily create a custom describe function, similar to the built in describe function.", "The transform() method returns an object that is indexed the same (same size) as the original. This API allows you to provide multiple operations at the same time rather than one-by-one. Its API is quite similar to the .agg API.", "We create a frame similar to the one used in the above sections.", "Transform the entire frame. .transform() allows input functions as: a NumPy function, a string function name or a user defined function.", "Here transform() received a single function; this is equivalent to a ufunc application.", "Passing a single function to .transform() with a Series will yield a single Series in return.", "Passing multiple functions will yield a column MultiIndexed DataFrame. The first level will be the original frame column names; the second level will be the names of the transforming functions.", "Passing multiple functions to a Series will yield a DataFrame. The resulting column names will be the transforming functions.", "Passing a dict of functions will allow selective transforming per column.", "Passing a dict of lists will generate a MultiIndexed DataFrame with these selective transforms.", "Since not all functions can be vectorized (accept NumPy arrays and return another array or value), the methods applymap() on DataFrame and analogously map() on Series accept any Python function taking a single value and returning a single value. For example:", "Series.map() has an additional feature; it can be used to easily \u201clink\u201d or \u201cmap\u201d values defined by a secondary series. This is closely related to merging/joining functionality:", "reindex() is the fundamental data alignment method in pandas. It is used to implement nearly all other features relying on label-alignment functionality. To reindex means to conform the data to match a given set of labels along a particular axis. This accomplishes several things:", "Reorders the existing data to match a new set of labels", "Inserts missing value (NA) markers in label locations where no data for that label existed", "If specified, fill data for missing labels using logic (highly relevant to working with time series data)", "Here is a simple example:", "Here, the f label was not contained in the Series and hence appears as NaN in the result.", "With a DataFrame, you can simultaneously reindex the index and columns:", "You may also use reindex with an axis keyword:", "Note that the Index objects containing the actual axis labels can be shared between objects. So if we have a Series and a DataFrame, the following can be done:", "This means that the reindexed Series\u2019s index is the same Python object as the DataFrame\u2019s index.", "DataFrame.reindex() also supports an \u201caxis-style\u201d calling convention, where you specify a single labels argument and the axis it applies to.", "See also", "MultiIndex / Advanced Indexing is an even more concise way of doing reindexing.", "Note", "When writing performance-sensitive code, there is a good reason to spend some time becoming a reindexing ninja: many operations are faster on pre-aligned data. Adding two unaligned DataFrames internally triggers a reindexing step. For exploratory analysis you will hardly notice the difference (because reindex has been heavily optimized), but when CPU cycles matter sprinkling a few explicit reindex calls here and there can have an impact.", "You may wish to take an object and reindex its axes to be labeled the same as another object. While the syntax for this is straightforward albeit verbose, it is a common enough operation that the reindex_like() method is available to make this simpler:", "The align() method is the fastest way to simultaneously align two objects. It supports a join argument (related to joining and merging):", "join='outer': take the union of the indexes (default)", "join='left': use the calling object\u2019s index", "join='right': use the passed object\u2019s index", "join='inner': intersect the indexes", "It returns a tuple with both of the reindexed Series:", "For DataFrames, the join method will be applied to both the index and the columns by default:", "You can also pass an axis option to only align on the specified axis:", "If you pass a Series to DataFrame.align(), you can choose to align both objects either on the DataFrame\u2019s index or columns using the axis argument:", "reindex() takes an optional parameter method which is a filling method chosen from the following table:", "Method", "Action", "pad / ffill", "Fill values forward", "bfill / backfill", "Fill values backward", "nearest", "Fill from the nearest index value", "We illustrate these fill methods on a simple Series:", "These methods require that the indexes are ordered increasing or decreasing.", "Note that the same result could have been achieved using fillna (except for method='nearest') or interpolate:", "reindex() will raise a ValueError if the index is not monotonically increasing or decreasing. fillna() and interpolate() will not perform any checks on the order of the index.", "The limit and tolerance arguments provide additional control over filling while reindexing. Limit specifies the maximum count of consecutive matches:", "In contrast, tolerance specifies the maximum distance between the index and indexer values:", "Notice that when used on a DatetimeIndex, TimedeltaIndex or PeriodIndex, tolerance will coerced into a Timedelta if possible. This allows you to specify tolerance with appropriate strings.", "A method closely related to reindex is the drop() function. It removes a set of labels from an axis:", "Note that the following also works, but is a bit less obvious / clean:", "The rename() method allows you to relabel an axis based on some mapping (a dict or Series) or an arbitrary function.", "If you pass a function, it must return a value when called with any of the labels (and must produce a set of unique values). A dict or Series can also be used:", "If the mapping doesn\u2019t include a column/index label, it isn\u2019t renamed. Note that extra labels in the mapping don\u2019t throw an error.", "DataFrame.rename() also supports an \u201caxis-style\u201d calling convention, where you specify a single mapper and the axis to apply that mapping to.", "The rename() method also provides an inplace named parameter that is by default False and copies the underlying data. Pass inplace=True to rename the data in place.", "Finally, rename() also accepts a scalar or list-like for altering the Series.name attribute.", "The methods DataFrame.rename_axis() and Series.rename_axis() allow specific names of a MultiIndex to be changed (as opposed to the labels).", "The behavior of basic iteration over pandas objects depends on the type. When iterating over a Series, it is regarded as array-like, and basic iteration produces the values. DataFrames follow the dict-like convention of iterating over the \u201ckeys\u201d of the objects.", "In short, basic iteration (for i in object) produces:", "Series: values", "DataFrame: column labels", "Thus, for example, iterating over a DataFrame gives you the column names:", "pandas objects also have the dict-like items() method to iterate over the (key, value) pairs.", "To iterate over the rows of a DataFrame, you can use the following methods:", "iterrows(): Iterate over the rows of a DataFrame as (index, Series) pairs. This converts the rows to Series objects, which can change the dtypes and has some performance implications.", "itertuples(): Iterate over the rows of a DataFrame as namedtuples of the values. This is a lot faster than iterrows(), and is in most cases preferable to use to iterate over the values of a DataFrame.", "Warning", "Iterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed and can be avoided with one of the following approaches:", "Look for a vectorized solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing, \u2026", "When you have a function that cannot work on the full DataFrame/Series at once, it is better to use apply() instead of iterating over the values. See the docs on function application.", "If you need to do iterative manipulations on the values but performance is important, consider writing the inner loop with cython or numba. See the enhancing performance section for some examples of this approach.", "Warning", "You should never modify something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect!", "For example, in the following case setting the value has no effect:", "Consistent with the dict-like interface, items() iterates through key-value pairs:", "Series: (index, scalar value) pairs", "DataFrame: (column, Series) pairs", "For example:", "iterrows() allows you to iterate through the rows of a DataFrame as Series objects. It returns an iterator yielding each index value along with a Series containing the data in each row:", "Note", "Because iterrows() returns a Series for each row, it does not preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). For example,", "All values in row, returned as a Series, are now upcasted to floats, also the original integer value in column x:", "To preserve dtypes while iterating over the rows, it is better to use itertuples() which returns namedtuples of the values and which is generally much faster than iterrows().", "For instance, a contrived way to transpose the DataFrame would be:", "The itertuples() method will return an iterator yielding a namedtuple for each row in the DataFrame. The first element of the tuple will be the row\u2019s corresponding index value, while the remaining values are the row values.", "For instance:", "This method does not convert the row to a Series object; it merely returns the values inside a namedtuple. Therefore, itertuples() preserves the data type of the values and is generally faster as iterrows().", "Note", "The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. With a large number of columns (>255), regular tuples are returned.", "Series has an accessor to succinctly return datetime like properties for the values of the Series, if it is a datetime/period like Series. This will return a Series, indexed like the existing Series.", "This enables nice expressions like this:", "You can easily produces tz aware transformations:", "You can also chain these types of operations:", "You can also format datetime values as strings with Series.dt.strftime() which supports the same format as the standard strftime().", "The .dt accessor works for period and timedelta dtypes.", "Note", "Series.dt will raise a TypeError if you access with a non-datetime-like values.", "Series is equipped with a set of string processing methods that make it easy to operate on each element of the array. Perhaps most importantly, these methods exclude missing/NA values automatically. These are accessed via the Series\u2019s str attribute and generally have names matching the equivalent (scalar) built-in string methods. For example:", "Powerful pattern-matching methods are provided as well, but note that pattern-matching generally uses regular expressions by default (and in some cases always uses them).", "Note", "Prior to pandas 1.0, string methods were only available on object -dtype Series. pandas 1.0 added the StringDtype which is dedicated to strings. See Text data types for more.", "Please see Vectorized String Methods for a complete description.", "pandas supports three kinds of sorting: sorting by index labels, sorting by column values, and sorting by a combination of both.", "The Series.sort_index() and DataFrame.sort_index() methods are used to sort a pandas object by its index levels.", "New in version 1.1.0.", "Sorting by index also supports a key parameter that takes a callable function to apply to the index being sorted. For MultiIndex objects, the key is applied per-level to the levels specified by level.", "For information on key sorting by value, see value sorting.", "The Series.sort_values() method is used to sort a Series by its values. The DataFrame.sort_values() method is used to sort a DataFrame by its column or row values. The optional by parameter to DataFrame.sort_values() may used to specify one or more columns to use to determine the sorted order.", "The by parameter can take a list of column names, e.g.:", "These methods have special treatment of NA values via the na_position argument:", "New in version 1.1.0.", "Sorting also supports a key parameter that takes a callable function to apply to the values being sorted.", "key will be given the Series of values and should return a Series or array of the same shape with the transformed values. For DataFrame objects, the key is applied per column, so the key should still expect a Series and return a Series, e.g.", "The name or type of each column can be used to apply different functions to different columns.", "Strings passed as the by parameter to DataFrame.sort_values() may refer to either columns or index level names.", "Sort by \u2018second\u2019 (index) and \u2018A\u2019 (column)", "Note", "If a string matches both a column name and an index level name then a warning is issued and the column takes precedence. This will result in an ambiguity error in a future version.", "Series has the searchsorted() method, which works similarly to numpy.ndarray.searchsorted().", "Series has the nsmallest() and nlargest() methods which return the smallest or largest \\(n\\) values. For a large Series this can be much faster than sorting the entire Series and calling head(n) on the result.", "DataFrame also has the nlargest and nsmallest methods.", "You must be explicit about sorting when the column is a MultiIndex, and fully specify all levels to by.", "The copy() method on pandas objects copies the underlying data (though not the axis indexes, since they are immutable) and returns a new object. Note that it is seldom necessary to copy objects. For example, there are only a handful of ways to alter a DataFrame in-place:", "Inserting, deleting, or modifying a column.", "Assigning to the index or columns attributes.", "For homogeneous data, directly modifying the values via the values attribute or advanced indexing.", "To be clear, no pandas method has the side effect of modifying your data; almost every method returns a new object, leaving the original object untouched. If the data is modified, it is because you did so explicitly.", "For the most part, pandas uses NumPy arrays and dtypes for Series or individual columns of a DataFrame. NumPy provides support for float, int, bool, timedelta64[ns] and datetime64[ns] (note that NumPy does not support timezone-aware datetimes).", "pandas and third-party libraries extend NumPy\u2019s type system in a few places. This section describes the extensions pandas has made internally. See Extension types for how to write your own extension that works with pandas. See Extension data types for a list of third-party libraries that have implemented an extension.", "The following table lists all of pandas extension types. For methods requiring dtype arguments, strings can be specified as indicated. See the respective documentation sections for more on each type.", "Kind of Data", "Data Type", "Scalar", "Array", "String Aliases", "tz-aware datetime", "DatetimeTZDtype", "Timestamp", "arrays.DatetimeArray", "'datetime64[ns, <tz>]'", "Categorical", "CategoricalDtype", "(none)", "Categorical", "'category'", "period (time spans)", "PeriodDtype", "Period", "arrays.PeriodArray 'Period[<freq>]'", "'period[<freq>]',", "sparse", "SparseDtype", "(none)", "arrays.SparseArray", "'Sparse', 'Sparse[int]', 'Sparse[float]'", "intervals", "IntervalDtype", "Interval", "arrays.IntervalArray", "'interval', 'Interval', 'Interval[<numpy_dtype>]', 'Interval[datetime64[ns, <tz>]]', 'Interval[timedelta64[<freq>]]'", "nullable integer", "Int64Dtype, \u2026", "(none)", "arrays.IntegerArray", "'Int8', 'Int16', 'Int32', 'Int64', 'UInt8', 'UInt16', 'UInt32', 'UInt64'", "Strings", "StringDtype", "str", "arrays.StringArray", "'string'", "Boolean (with NA)", "BooleanDtype", "bool", "arrays.BooleanArray", "'boolean'", "pandas has two ways to store strings.", "object dtype, which can hold any Python object, including strings.", "StringDtype, which is dedicated to strings.", "Generally, we recommend using StringDtype. See Text data types for more.", "Finally, arbitrary objects may be stored using the object dtype, but should be avoided to the extent possible (for performance and interoperability with other libraries and methods. See object conversion).", "A convenient dtypes attribute for DataFrame returns a Series with the data type of each column.", "On a Series object, use the dtype attribute.", "If a pandas object contains data with multiple dtypes in a single column, the dtype of the column will be chosen to accommodate all of the data types (object is the most general).", "The number of columns of each type in a DataFrame can be found by calling DataFrame.dtypes.value_counts().", "Numeric dtypes will propagate and can coexist in DataFrames. If a dtype is passed (either directly via the dtype keyword, a passed ndarray, or a passed Series), then it will be preserved in DataFrame operations. Furthermore, different numeric dtypes will NOT be combined. The following example will give you a taste.", "By default integer types are int64 and float types are float64, regardless of platform (32-bit or 64-bit). The following will all result in int64 dtypes.", "Note that Numpy will choose platform-dependent types when creating arrays. The following WILL result in int32 on 32-bit platform.", "Types can potentially be upcasted when combined with other types, meaning they are promoted from the current type (e.g. int to float).", "DataFrame.to_numpy() will return the lower-common-denominator of the dtypes, meaning the dtype that can accommodate ALL of the types in the resulting homogeneous dtyped NumPy array. This can force some upcasting.", "You can use the astype() method to explicitly convert dtypes from one to another. These will by default return a copy, even if the dtype was unchanged (pass copy=False to change this behavior). In addition, they will raise an exception if the astype operation is invalid.", "Upcasting is always according to the NumPy rules. If two different dtypes are involved in an operation, then the more general one will be used as the result of the operation.", "Convert a subset of columns to a specified type using astype().", "Convert certain columns to a specific dtype by passing a dict to astype().", "Note", "When trying to convert a subset of columns to a specified type using astype() and loc(), upcasting occurs.", "loc() tries to fit in what we are assigning to the current dtypes, while [] will overwrite them taking the dtype from the right hand side. Therefore the following piece of code produces the unintended result.", "pandas offers various functions to try to force conversion of types from the object dtype to other types. In cases where the data is already of the correct type, but stored in an object array, the DataFrame.infer_objects() and Series.infer_objects() methods can be used to soft convert to the correct type.", "Because the data was transposed the original inference stored all columns as object, which infer_objects will correct.", "The following functions are available for one dimensional object arrays or scalars to perform hard conversion of objects to a specified type:", "to_numeric() (conversion to numeric dtypes)", "to_datetime() (conversion to datetime objects)", "to_timedelta() (conversion to timedelta objects)", "To force a conversion, we can pass in an errors argument, which specifies how pandas should deal with elements that cannot be converted to desired dtype or object. By default, errors='raise', meaning that any errors encountered will be raised during the conversion process. However, if errors='coerce', these errors will be ignored and pandas will convert problematic elements to pd.NaT (for datetime and timedelta) or np.nan (for numeric). This might be useful if you are reading in data which is mostly of the desired dtype (e.g. numeric, datetime), but occasionally has non-conforming elements intermixed that you want to represent as missing:", "The errors parameter has a third option of errors='ignore', which will simply return the passed in data if it encounters any errors with the conversion to a desired data type:", "In addition to object conversion, to_numeric() provides another argument downcast, which gives the option of downcasting the newly (or already) numeric data to a smaller dtype, which can conserve memory:", "As these methods apply only to one-dimensional arrays, lists or scalars; they cannot be used directly on multi-dimensional objects such as DataFrames. However, with apply(), we can \u201capply\u201d the function over each column efficiently:", "Performing selection operations on integer type data can easily upcast the data to floating. The dtype of the input data will be preserved in cases where nans are not introduced. See also Support for integer NA.", "While float dtypes are unchanged.", "The select_dtypes() method implements subsetting of columns based on their dtype.", "First, let\u2019s create a DataFrame with a slew of different dtypes:", "And the dtypes:", "select_dtypes() has two parameters include and exclude that allow you to say \u201cgive me the columns with these dtypes\u201d (include) and/or \u201cgive the columns without these dtypes\u201d (exclude).", "For example, to select bool columns:", "You can also pass the name of a dtype in the NumPy dtype hierarchy:", "select_dtypes() also works with generic dtypes as well.", "For example, to select all numeric and boolean columns while excluding unsigned integers:", "To select string columns you must use the object dtype:", "To see all the child dtypes of a generic dtype like numpy.number you can define a function that returns a tree of child dtypes:", "All NumPy dtypes are subclasses of numpy.generic:", "Note", "pandas also defines the types category, and datetime64[ns, tz], which are not integrated into the normal NumPy hierarchy and won\u2019t show up with the above function."]}, {"name": "Extensions", "path": "reference/extensions", "type": "Extensions", "text": ["These are primarily intended for library authors looking to extend pandas objects.", "api.extensions.register_extension_dtype(cls)", "Register an ExtensionType with pandas as class decorator.", "api.extensions.register_dataframe_accessor(name)", "Register a custom accessor on DataFrame objects.", "api.extensions.register_series_accessor(name)", "Register a custom accessor on Series objects.", "api.extensions.register_index_accessor(name)", "Register a custom accessor on Index objects.", "api.extensions.ExtensionDtype()", "A custom data type, to be paired with an ExtensionArray.", "api.extensions.ExtensionArray()", "Abstract base class for custom 1-D array types.", "arrays.PandasArray(values[, copy])", "A pandas ExtensionArray for NumPy data.", "Additionally, we have some utility methods for ensuring your object behaves correctly.", "api.indexers.check_array_indexer(array, indexer)", "Check if indexer is a valid array indexer for array.", "The sentinel pandas.api.extensions.no_default is used as the default value in some methods. Use an is comparison to check if the user provides a non-default value."]}, {"name": "Frequently Asked Questions (FAQ)", "path": "user_guide/gotchas", "type": "Manual", "text": ["The memory usage of a DataFrame (including the index) is shown when calling the info(). A configuration option, display.memory_usage (see the list of options), specifies if the DataFrame\u2019s memory usage will be displayed when invoking the df.info() method.", "For example, the memory usage of the DataFrame below is shown when calling info():", "The + symbol indicates that the true memory usage could be higher, because pandas does not count the memory used by values in columns with dtype=object.", "Passing memory_usage='deep' will enable a more accurate memory usage report, accounting for the full usage of the contained objects. This is optional as it can be expensive to do this deeper introspection.", "By default the display option is set to True but can be explicitly overridden by passing the memory_usage argument when invoking df.info().", "The memory usage of each column can be found by calling the memory_usage() method. This returns a Series with an index represented by column names and memory usage of each column shown in bytes. For the DataFrame above, the memory usage of each column and the total memory usage can be found with the memory_usage method:", "By default the memory usage of the DataFrame\u2019s index is shown in the returned Series, the memory usage of the index can be suppressed by passing the index=False argument:", "The memory usage displayed by the info() method utilizes the memory_usage() method to determine the memory usage of a DataFrame while also formatting the output in human-readable units (base-2 representation; i.e. 1KB = 1024 bytes).", "See also Categorical Memory Usage.", "pandas follows the NumPy convention of raising an error when you try to convert something to a bool. This happens in an if-statement or when using the boolean operations: and, or, and not. It is not clear what the result of the following code should be:", "Should it be True because it\u2019s not zero-length, or False because there are False values? It is unclear, so instead, pandas raises a ValueError:", "You need to explicitly choose what you want to do with the DataFrame, e.g. use any(), all() or empty(). Alternatively, you might want to compare if the pandas object is None:", "Below is how to check if any of the values are True:", "To evaluate single-element pandas objects in a boolean context, use the method bool():", "Bitwise boolean operators like == and != return a boolean Series, which is almost always what you want anyways.", "See boolean comparisons for more examples.", "Using the Python in operator on a Series tests for membership in the index, not membership among the values.", "If this behavior is surprising, keep in mind that using in on a Python dictionary tests keys, not values, and Series are dict-like. To test for membership in the values, use the method isin():", "For DataFrames, likewise, in applies to the column axis, testing for membership in the list of column names.", "This section applies to pandas methods that take a UDF. In particular, the methods .apply, .aggregate, .transform, and .filter.", "It is a general rule in programming that one should not mutate a container while it is being iterated over. Mutation will invalidate the iterator, causing unexpected behavior. Consider the example:", "One probably would have expected that the result would be [1, 3, 5]. When using a pandas method that takes a UDF, internally pandas is often iterating over the DataFrame or other pandas object. Therefore, if the UDF mutates (changes) the DataFrame, unexpected behavior can arise.", "Here is a similar example with DataFrame.apply():", "To resolve this issue, one can make a copy so that the mutation does not apply to the container being iterated over.", "For lack of NA (missing) support from the ground up in NumPy and Python in general, we were given the difficult choice between either:", "A masked array solution: an array of data and an array of boolean values indicating whether a value is there or is missing.", "Using a special sentinel value, bit pattern, or set of sentinel values to denote NA across the dtypes.", "For many reasons we chose the latter. After years of production use it has proven, at least in my opinion, to be the best decision given the state of affairs in NumPy and Python in general. The special value NaN (Not-A-Number) is used everywhere as the NA value, and there are API functions isna and notna which can be used across the dtypes to detect NA values.", "However, it comes with it a couple of trade-offs which I most certainly have not ignored.", "In the absence of high performance NA support being built into NumPy from the ground up, the primary casualty is the ability to represent NAs in integer arrays. For example:", "This trade-off is made largely for memory and performance reasons, and also so that the resulting Series continues to be \u201cnumeric\u201d.", "If you need to represent integers with possibly missing values, use one of the nullable-integer extension dtypes provided by pandas", "Int8Dtype", "Int16Dtype", "Int32Dtype", "Int64Dtype", "See Nullable integer data type for more.", "When introducing NAs into an existing Series or DataFrame via reindex() or some other means, boolean and integer types will be promoted to a different dtype in order to store the NAs. The promotions are summarized in this table:", "Typeclass", "Promotion dtype for storing NAs", "floating", "no change", "object", "no change", "integer", "cast to float64", "boolean", "cast to object", "While this may seem like a heavy trade-off, I have found very few cases where this is an issue in practice i.e. storing values greater than 2**53. Some explanation for the motivation is in the next section.", "Many people have suggested that NumPy should simply emulate the NA support present in the more domain-specific statistical programming language R. Part of the reason is the NumPy type hierarchy:", "Typeclass", "Dtypes", "numpy.floating", "float16, float32, float64, float128", "numpy.integer", "int8, int16, int32, int64", "numpy.unsignedinteger", "uint8, uint16, uint32, uint64", "numpy.object_", "object_", "numpy.bool_", "bool_", "numpy.character", "string_, unicode_", "The R language, by contrast, only has a handful of built-in data types: integer, numeric (floating-point), character, and boolean. NA types are implemented by reserving special bit patterns for each type to be used as the missing value. While doing this with the full NumPy type hierarchy would be possible, it would be a more substantial trade-off (especially for the 8- and 16-bit data types) and implementation undertaking.", "An alternate approach is that of using masked arrays. A masked array is an array of data with an associated boolean mask denoting whether each value should be considered NA or not. I am personally not in love with this approach as I feel that overall it places a fairly heavy burden on the user and the library implementer. Additionally, it exacts a fairly high performance cost when working with numerical data compared with the simple approach of using NaN. Thus, I have chosen the Pythonic \u201cpracticality beats purity\u201d approach and traded integer NA capability for a much simpler approach of using a special value in float and object arrays to denote NA, and promoting integer arrays to floating when NAs must be introduced.", "For Series and DataFrame objects, var() normalizes by N-1 to produce unbiased estimates of the sample variance, while NumPy\u2019s var normalizes by N, which measures the variance of the sample. Note that cov() normalizes by N-1 in both pandas and NumPy.", "As of pandas 0.11, pandas is not 100% thread safe. The known issues relate to the copy() method. If you are doing a lot of copying of DataFrame objects shared among threads, we recommend holding locks inside the threads where the data copying occurs.", "See this link for more information.", "Occasionally you may have to deal with data that were created on a machine with a different byte order than the one on which you are running Python. A common symptom of this issue is an error like:", "To deal with this issue you should convert the underlying NumPy array to the native system byte order before passing it to Series or DataFrame constructors using something similar to the following:", "See the NumPy documentation on byte order for more details."]}, {"name": "General functions", "path": "reference/general_functions", "type": "Input/output", "text": ["melt(frame[, id_vars, value_vars, var_name, ...])", "Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.", "pivot(data[, index, columns, values])", "Return reshaped DataFrame organized by given index / column values.", "pivot_table(data[, values, index, columns, ...])", "Create a spreadsheet-style pivot table as a DataFrame.", "crosstab(index, columns[, values, rownames, ...])", "Compute a simple cross tabulation of two (or more) factors.", "cut(x, bins[, right, labels, retbins, ...])", "Bin values into discrete intervals.", "qcut(x, q[, labels, retbins, precision, ...])", "Quantile-based discretization function.", "merge(left, right[, how, on, left_on, ...])", "Merge DataFrame or named Series objects with a database-style join.", "merge_ordered(left, right[, on, left_on, ...])", "Perform a merge for ordered data with optional filling/interpolation.", "merge_asof(left, right[, on, left_on, ...])", "Perform a merge by key distance.", "concat(objs[, axis, join, ignore_index, ...])", "Concatenate pandas objects along a particular axis with optional set logic along the other axes.", "get_dummies(data[, prefix, prefix_sep, ...])", "Convert categorical variable into dummy/indicator variables.", "factorize(values[, sort, na_sentinel, size_hint])", "Encode the object as an enumerated type or categorical variable.", "unique(values)", "Return unique values based on a hash table.", "wide_to_long(df, stubnames, i, j[, sep, suffix])", "Unpivot a DataFrame from wide to long format.", "isna(obj)", "Detect missing values for an array-like object.", "isnull(obj)", "Detect missing values for an array-like object.", "notna(obj)", "Detect non-missing values for an array-like object.", "notnull(obj)", "Detect non-missing values for an array-like object.", "to_numeric(arg[, errors, downcast])", "Convert argument to a numeric type.", "to_datetime(arg[, errors, dayfirst, ...])", "Convert argument to datetime.", "to_timedelta(arg[, unit, errors])", "Convert argument to timedelta.", "date_range([start, end, periods, freq, tz, ...])", "Return a fixed frequency DatetimeIndex.", "bdate_range([start, end, periods, freq, tz, ...])", "Return a fixed frequency DatetimeIndex, with business day as the default frequency.", "period_range([start, end, periods, freq, name])", "Return a fixed frequency PeriodIndex.", "timedelta_range([start, end, periods, freq, ...])", "Return a fixed frequency TimedeltaIndex, with day as the default frequency.", "infer_freq(index[, warn])", "Infer the most likely frequency given the input index.", "interval_range([start, end, periods, freq, ...])", "Return a fixed frequency IntervalIndex.", "eval(expr[, parser, engine, truediv, ...])", "Evaluate a Python expression as a string using various backends.", "util.hash_array(vals[, encoding, hash_key, ...])", "Given a 1d array, return an array of deterministic integers.", "util.hash_pandas_object(obj[, index, ...])", "Return a data hash of the Index/Series/DataFrame.", "test([extra_args])", "Run the pandas test suite using pytest."]}, {"name": "General utility functions", "path": "reference/general_utility_functions", "type": "Input/output", "text": ["describe_option(pat[, _print_desc])", "Prints the description for one or more registered options.", "reset_option(pat)", "Reset one or more options to their default value.", "get_option(pat)", "Retrieves the value of the specified option.", "set_option(pat, value)", "Sets the value of the specified option.", "option_context(*args)", "Context manager to temporarily set options in the with statement context.", "testing.assert_frame_equal(left, right[, ...])", "Check that left and right DataFrame are equal.", "testing.assert_series_equal(left, right[, ...])", "Check that left and right Series are equal.", "testing.assert_index_equal(left, right[, ...])", "Check that left and right Index are equal.", "testing.assert_extension_array_equal(left, right)", "Check that left and right ExtensionArrays are equal.", "errors.AbstractMethodError(class_instance[, ...])", "Raise this error instead of NotImplementedError for abstract methods while keeping compatibility with Python 2 and Python 3.", "errors.AccessorRegistrationWarning", "Warning for attribute conflicts in accessor registration.", "errors.DtypeWarning", "Warning raised when reading different dtypes in a column from a file.", "errors.DuplicateLabelError", "Error raised when an operation would introduce duplicate labels.", "errors.EmptyDataError", "Exception that is thrown in pd.read_csv (by both the C and Python engines) when empty data or header is encountered.", "errors.InvalidIndexError", "Exception raised when attempting to use an invalid index key.", "errors.IntCastingNaNError", "Raised when attempting an astype operation on an array with NaN to an integer dtype.", "errors.MergeError", "Error raised when problems arise during merging due to problems with input data.", "errors.NullFrequencyError", "Error raised when a null freq attribute is used in an operation that needs a non-null frequency, particularly DatetimeIndex.shift, TimedeltaIndex.shift, PeriodIndex.shift.", "errors.NumbaUtilError", "Error raised for unsupported Numba engine routines.", "errors.OptionError", "Exception for pandas.options, backwards compatible with KeyError checks.", "errors.OutOfBoundsDatetime", "errors.OutOfBoundsTimedelta", "Raised when encountering a timedelta value that cannot be represented as a timedelta64[ns].", "errors.ParserError", "Exception that is raised by an error encountered in parsing file contents.", "errors.ParserWarning", "Warning raised when reading a file that doesn't use the default 'c' parser.", "errors.PerformanceWarning", "Warning raised when there is a possible performance impact.", "errors.UnsortedIndexError", "Error raised when attempting to get a slice of a MultiIndex, and the index has not been lexsorted.", "errors.UnsupportedFunctionCall", "Exception raised when attempting to call a numpy function on a pandas object, but that function is not supported by the object e.g.", "api.types.union_categoricals(to_union[, ...])", "Combine list-like of Categorical-like, unioning categories.", "api.types.infer_dtype", "Efficiently infer the type of a passed val, or list-like array of values.", "api.types.pandas_dtype(dtype)", "Convert input into a pandas only dtype object or a numpy dtype object.", "api.types.is_bool_dtype(arr_or_dtype)", "Check whether the provided array or dtype is of a boolean dtype.", "api.types.is_categorical_dtype(arr_or_dtype)", "Check whether an array-like or dtype is of the Categorical dtype.", "api.types.is_complex_dtype(arr_or_dtype)", "Check whether the provided array or dtype is of a complex dtype.", "api.types.is_datetime64_any_dtype(arr_or_dtype)", "Check whether the provided array or dtype is of the datetime64 dtype.", "api.types.is_datetime64_dtype(arr_or_dtype)", "Check whether an array-like or dtype is of the datetime64 dtype.", "api.types.is_datetime64_ns_dtype(arr_or_dtype)", "Check whether the provided array or dtype is of the datetime64[ns] dtype.", "api.types.is_datetime64tz_dtype(arr_or_dtype)", "Check whether an array-like or dtype is of a DatetimeTZDtype dtype.", "api.types.is_extension_type(arr)", "(DEPRECATED) Check whether an array-like is of a pandas extension class instance.", "api.types.is_extension_array_dtype(arr_or_dtype)", "Check if an object is a pandas extension array type.", "api.types.is_float_dtype(arr_or_dtype)", "Check whether the provided array or dtype is of a float dtype.", "api.types.is_int64_dtype(arr_or_dtype)", "Check whether the provided array or dtype is of the int64 dtype.", "api.types.is_integer_dtype(arr_or_dtype)", "Check whether the provided array or dtype is of an integer dtype.", "api.types.is_interval_dtype(arr_or_dtype)", "Check whether an array-like or dtype is of the Interval dtype.", "api.types.is_numeric_dtype(arr_or_dtype)", "Check whether the provided array or dtype is of a numeric dtype.", "api.types.is_object_dtype(arr_or_dtype)", "Check whether an array-like or dtype is of the object dtype.", "api.types.is_period_dtype(arr_or_dtype)", "Check whether an array-like or dtype is of the Period dtype.", "api.types.is_signed_integer_dtype(arr_or_dtype)", "Check whether the provided array or dtype is of a signed integer dtype.", "api.types.is_string_dtype(arr_or_dtype)", "Check whether the provided array or dtype is of the string dtype.", "api.types.is_timedelta64_dtype(arr_or_dtype)", "Check whether an array-like or dtype is of the timedelta64 dtype.", "api.types.is_timedelta64_ns_dtype(arr_or_dtype)", "Check whether the provided array or dtype is of the timedelta64[ns] dtype.", "api.types.is_unsigned_integer_dtype(arr_or_dtype)", "Check whether the provided array or dtype is of an unsigned integer dtype.", "api.types.is_sparse(arr)", "Check whether an array-like is a 1-D pandas sparse array.", "api.types.is_dict_like(obj)", "Check if the object is dict-like.", "api.types.is_file_like(obj)", "Check if the object is a file-like object.", "api.types.is_list_like", "Check if the object is list-like.", "api.types.is_named_tuple(obj)", "Check if the object is a named tuple.", "api.types.is_iterator", "Check if the object is an iterator.", "api.types.is_bool", "Return True if given object is boolean.", "api.types.is_categorical(arr)", "Check whether an array-like is a Categorical instance.", "api.types.is_complex", "Return True if given object is complex.", "api.types.is_float", "Return True if given object is float.", "api.types.is_hashable(obj)", "Return True if hash(obj) will succeed, False otherwise.", "api.types.is_integer", "Return True if given object is integer.", "api.types.is_interval", "api.types.is_number(obj)", "Check if the object is a number.", "api.types.is_re(obj)", "Check if the object is a regex pattern instance.", "api.types.is_re_compilable(obj)", "Check if the object can be compiled into a regex pattern instance.", "api.types.is_scalar", "Return True if given object is scalar.", "show_versions([as_json])", "Provide useful information, important for bug reports."]}, {"name": "Group by: split-apply-combine", "path": "user_guide/groupby", "type": "Manual", "text": ["By \u201cgroup by\u201d we are referring to a process involving one or more of the following steps:", "Splitting the data into groups based on some criteria.", "Applying a function to each group independently.", "Combining the results into a data structure.", "Out of these, the split step is the most straightforward. In fact, in many situations we may wish to split the data set into groups and do something with those groups. In the apply step, we might wish to do one of the following:", "Aggregation: compute a summary statistic (or statistics) for each group. Some examples:", "Compute group sums or means.", "Compute group sizes / counts.", "Transformation: perform some group-specific computations and return a like-indexed object. Some examples:", "Standardize data (zscore) within a group.", "Filling NAs within groups with a value derived from each group.", "Filtration: discard some groups, according to a group-wise computation that evaluates True or False. Some examples:", "Discard data that belongs to groups with only a few members.", "Filter out data based on the group sum or mean.", "Some combination of the above: GroupBy will examine the results of the apply step and try to return a sensibly combined result if it doesn\u2019t fit into either of the above two categories.", "Since the set of object instance methods on pandas data structures are generally rich and expressive, we often simply want to invoke, say, a DataFrame function on each group. The name GroupBy should be quite familiar to those who have used a SQL-based tool (or itertools), in which you can write code like:", "We aim to make operations like this natural and easy to express using pandas. We\u2019ll address each area of GroupBy functionality then provide some non-trivial examples / use cases.", "See the cookbook for some advanced strategies.", "pandas objects can be split on any of their axes. The abstract definition of grouping is to provide a mapping of labels to group names. To create a GroupBy object (more on what the GroupBy object is later), you may do the following:", "The mapping can be specified many different ways:", "A Python function, to be called on each of the axis labels.", "A list or NumPy array of the same length as the selected axis.", "A dict or Series, providing a label -> group name mapping.", "For DataFrame objects, a string indicating either a column name or an index level name to be used to group.", "df.groupby('A') is just syntactic sugar for df.groupby(df['A']).", "A list of any of the above things.", "Collectively we refer to the grouping objects as the keys. For example, consider the following DataFrame:", "Note", "A string passed to groupby may refer to either a column or an index level. If a string matches both a column name and an index level name, a ValueError will be raised.", "On a DataFrame, we obtain a GroupBy object by calling groupby(). We could naturally group by either the A or B columns, or both:", "If we also have a MultiIndex on columns A and B, we can group by all but the specified columns", "These will split the DataFrame on its index (rows). We could also split by the columns:", "pandas Index objects support duplicate values. If a non-unique index is used as the group key in a groupby operation, all values for the same index value will be considered to be in one group and thus the output of aggregation functions will only contain unique index values:", "Note that no splitting occurs until it\u2019s needed. Creating the GroupBy object only verifies that you\u2019ve passed a valid mapping.", "Note", "Many kinds of complicated data manipulations can be expressed in terms of GroupBy operations (though can\u2019t be guaranteed to be the most efficient). You can get quite creative with the label mapping functions.", "By default the group keys are sorted during the groupby operation. You may however pass sort=False for potential speedups:", "Note that groupby will preserve the order in which observations are sorted within each group. For example, the groups created by groupby() below are in the order they appeared in the original DataFrame:", "New in version 1.1.0.", "By default NA values are excluded from group keys during the groupby operation. However, in case you want to include NA values in group keys, you could pass dropna=False to achieve it.", "The default setting of dropna argument is True which means NA are not included in group keys.", "The groups attribute is a dict whose keys are the computed unique groups and corresponding values being the axis labels belonging to each group. In the above example we have:", "Calling the standard Python len function on the GroupBy object just returns the length of the groups dict, so it is largely just a convenience:", "GroupBy will tab complete column names (and other attributes):", "With hierarchically-indexed data, it\u2019s quite natural to group by one of the levels of the hierarchy.", "Let\u2019s create a Series with a two-level MultiIndex.", "We can then group by one of the levels in s.", "If the MultiIndex has names specified, these can be passed instead of the level number:", "Grouping with multiple levels is supported.", "Index level names may be supplied as keys.", "More on the sum function and aggregation later.", "A DataFrame may be grouped by a combination of columns and index levels by specifying the column names as strings and the index levels as pd.Grouper objects.", "The following example groups df by the second index level and the A column.", "Index levels may also be specified by name.", "Index level names may be specified as keys directly to groupby.", "Once you have created the GroupBy object from a DataFrame, you might want to do something different for each of the columns. Thus, using [] similar to getting a column from a DataFrame, you can do:", "This is mainly syntactic sugar for the alternative and much more verbose:", "Additionally this method avoids recomputing the internal grouping information derived from the passed key.", "With the GroupBy object in hand, iterating through the grouped data is very natural and functions similarly to itertools.groupby():", "In the case of grouping by multiple keys, the group name will be a tuple:", "See Iterating through groups.", "A single group can be selected using get_group():", "Or for an object grouped on multiple columns:", "Once the GroupBy object has been created, several methods are available to perform a computation on the grouped data. These operations are similar to the aggregating API, window API, and resample API.", "An obvious one is aggregation via the aggregate() or equivalently agg() method:", "As you can see, the result of the aggregation will have the group names as the new index along the grouped axis. In the case of multiple keys, the result is a MultiIndex by default, though this can be changed by using the as_index option:", "Note that you could use the reset_index DataFrame function to achieve the same result as the column names are stored in the resulting MultiIndex:", "Another simple aggregation example is to compute the size of each group. This is included in GroupBy as the size method. It returns a Series whose index are the group names and whose values are the sizes of each group.", "Another aggregation example is to compute the number of unique values of each group. This is similar to the value_counts function, except that it only counts unique values.", "Note", "Aggregation functions will not return the groups that you are aggregating over if they are named columns, when as_index=True, the default. The grouped columns will be the indices of the returned object.", "Passing as_index=False will return the groups that you are aggregating over, if they are named columns.", "Aggregating functions are the ones that reduce the dimension of the returned objects. Some common aggregating functions are tabulated below:", "Function", "Description", "mean()", "Compute mean of groups", "sum()", "Compute sum of group values", "size()", "Compute group sizes", "count()", "Compute count of group", "std()", "Standard deviation of groups", "var()", "Compute variance of groups", "sem()", "Standard error of the mean of groups", "describe()", "Generates descriptive statistics", "first()", "Compute first of group values", "last()", "Compute last of group values", "nth()", "Take nth value, or a subset if n is a list", "min()", "Compute min of group values", "max()", "Compute max of group values", "The aggregating functions above will exclude NA values. Any function which reduces a Series to a scalar value is an aggregation function and will work, a trivial example is df.groupby('A').agg(lambda ser: 1). Note that nth() can act as a reducer or a filter, see here.", "With grouped Series you can also pass a list or dict of functions to do aggregation with, outputting a DataFrame:", "On a grouped DataFrame, you can pass a list of functions to apply to each column, which produces an aggregated result with a hierarchical index:", "The resulting aggregations are named for the functions themselves. If you need to rename, then you can add in a chained operation for a Series like this:", "For a grouped DataFrame, you can rename in a similar manner:", "Note", "In general, the output column names should be unique. You can\u2019t apply the same function (or two functions with the same name) to the same column.", "pandas does allow you to provide multiple lambdas. In this case, pandas will mangle the name of the (nameless) lambda functions, appending _<i> to each subsequent lambda.", "New in version 0.25.0.", "To support column-specific aggregation with control over the output column names, pandas accepts the special syntax in GroupBy.agg(), known as \u201cnamed aggregation\u201d, where", "The keywords are the output column names", "The values are tuples whose first element is the column to select and the second element is the aggregation to apply to that column. pandas provides the pandas.NamedAgg namedtuple with the fields ['column', 'aggfunc'] to make it clearer what the arguments are. As usual, the aggregation can be a callable or a string alias.", "pandas.NamedAgg is just a namedtuple. Plain tuples are allowed as well.", "If your desired output column names are not valid Python keywords, construct a dictionary and unpack the keyword arguments", "Additional keyword arguments are not passed through to the aggregation functions. Only pairs of (column, aggfunc) should be passed as **kwargs. If your aggregation functions requires additional arguments, partially apply them with functools.partial().", "Note", "For Python 3.5 and earlier, the order of **kwargs in a functions was not preserved. This means that the output column ordering would not be consistent. To ensure consistent ordering, the keys (and so output columns) will always be sorted for Python 3.5.", "Named aggregation is also valid for Series groupby aggregations. In this case there\u2019s no column selection, so the values are just the functions.", "By passing a dict to aggregate you can apply a different aggregation to the columns of a DataFrame:", "The function names can also be strings. In order for a string to be valid it must be either implemented on GroupBy or available via dispatching:", "Some common aggregations, currently only sum, mean, std, and sem, have optimized Cython implementations:", "Of course sum and mean are implemented on pandas objects, so the above code would work even without the special versions via dispatching (see below).", "Users can also provide their own functions for custom aggregations. When aggregating with a User-Defined Function (UDF), the UDF should not mutate the provided Series, see Mutating with User Defined Function (UDF) methods for more information.", "The resulting dtype will reflect that of the aggregating function. If the results from different groups have different dtypes, then a common dtype will be determined in the same way as DataFrame construction.", "The transform method returns an object that is indexed the same (same size) as the one being grouped. The transform function must:", "Return a result that is either the same size as the group chunk or broadcastable to the size of the group chunk (e.g., a scalar, grouped.transform(lambda x: x.iloc[-1])).", "Operate column-by-column on the group chunk. The transform is applied to the first group chunk using chunk.apply.", "Not perform in-place operations on the group chunk. Group chunks should be treated as immutable, and changes to a group chunk may produce unexpected results. For example, when using fillna, inplace must be False (grouped.transform(lambda x: x.fillna(inplace=False))).", "(Optionally) operates on the entire group chunk. If this is supported, a fast path is used starting from the second chunk.", "Similar to Aggregations with User-Defined Functions, the resulting dtype will reflect that of the transformation function. If the results from different groups have different dtypes, then a common dtype will be determined in the same way as DataFrame construction.", "Suppose we wished to standardize the data within each group:", "We would expect the result to now have mean 0 and standard deviation 1 within each group, which we can easily check:", "We can also visually compare the original and transformed data sets.", "Transformation functions that have lower dimension outputs are broadcast to match the shape of the input array.", "Alternatively, the built-in methods could be used to produce the same outputs.", "Another common data transform is to replace missing data with the group mean.", "We can verify that the group means have not changed in the transformed data and that the transformed data contains no NAs.", "Note", "Some functions will automatically transform the input when applied to a GroupBy object, but returning an object of the same shape as the original. Passing as_index=False will not affect these transformation methods.", "For example: fillna, ffill, bfill, shift..", "It is possible to use resample(), expanding() and rolling() as methods on groupbys.", "The example below will apply the rolling() method on the samples of the column B based on the groups of column A.", "The expanding() method will accumulate a given operation (sum() in the example) for all the members of each particular group.", "Suppose you want to use the resample() method to get a daily frequency in each group of your dataframe and wish to complete the missing values with the ffill() method.", "The filter method returns a subset of the original object. Suppose we want to take only elements that belong to groups with a group sum greater than 2.", "The argument of filter must be a function that, applied to the group as a whole, returns True or False.", "Another useful operation is filtering out elements that belong to groups with only a couple members.", "Alternatively, instead of dropping the offending groups, we can return a like-indexed objects where the groups that do not pass the filter are filled with NaNs.", "For DataFrames with multiple columns, filters should explicitly specify a column as the filter criterion.", "Note", "Some functions when applied to a groupby object will act as a filter on the input, returning a reduced shape of the original (and potentially eliminating groups), but with the index unchanged. Passing as_index=False will not affect these transformation methods.", "For example: head, tail.", "When doing an aggregation or transformation, you might just want to call an instance method on each data group. This is pretty easy to do by passing lambda functions:", "But, it\u2019s rather verbose and can be untidy if you need to pass additional arguments. Using a bit of metaprogramming cleverness, GroupBy now has the ability to \u201cdispatch\u201d method calls to the groups:", "What is actually happening here is that a function wrapper is being generated. When invoked, it takes any passed arguments and invokes the function with any arguments on each group (in the above example, the std function). The results are then combined together much in the style of agg and transform (it actually uses apply to infer the gluing, documented next). This enables some operations to be carried out rather succinctly:", "In this example, we chopped the collection of time series into yearly chunks then independently called fillna on the groups.", "The nlargest and nsmallest methods work on Series style groupbys:", "Some operations on the grouped data might not fit into either the aggregate or transform categories. Or, you may simply want GroupBy to infer how to combine the results. For these, use the apply function, which can be substituted for both aggregate and transform in many standard use cases. However, apply can handle some exceptional use cases, for example:", "The dimension of the returned result can also change:", "apply on a Series can operate on a returned value from the applied function, that is itself a series, and possibly upcast the result to a DataFrame:", "Note", "apply can act as a reducer, transformer, or filter function, depending on exactly what is passed to it. So depending on the path taken, and exactly what you are grouping. Thus the grouped columns(s) may be included in the output as well as set the indices.", "Similar to Aggregations with User-Defined Functions, the resulting dtype will reflect that of the apply function. If the results from different groups have different dtypes, then a common dtype will be determined in the same way as DataFrame construction.", "New in version 1.1.", "If Numba is installed as an optional dependency, the transform and aggregate methods support engine='numba' and engine_kwargs arguments. See enhancing performance with Numba for general usage of the arguments and performance considerations.", "The function signature must start with values, index exactly as the data belonging to each group will be passed into values, and the group index will be passed into index.", "Warning", "When using engine='numba', there will be no \u201cfall back\u201d behavior internally. The group data and group index will be passed as NumPy arrays to the JITed user defined function, and no alternative execution attempts will be tried.", "Again consider the example DataFrame we\u2019ve been looking at:", "Suppose we wish to compute the standard deviation grouped by the A column. There is a slight problem, namely that we don\u2019t care about the data in column B. We refer to this as a \u201cnuisance\u201d column. If the passed aggregation function can\u2019t be applied to some columns, the troublesome columns will be (silently) dropped. Thus, this does not pose any problems:", "Note that df.groupby('A').colname.std(). is more efficient than df.groupby('A').std().colname, so if the result of an aggregation function is only interesting over one column (here colname), it may be filtered before applying the aggregation function.", "Note", "Any object column, also if it contains numerical values such as Decimal objects, is considered as a \u201cnuisance\u201d columns. They are excluded from aggregate functions automatically in groupby.", "If you do wish to include decimal or object columns in an aggregation with other non-nuisance data types, you must do so explicitly.", "When using a Categorical grouper (as a single grouper, or as part of multiple groupers), the observed keyword controls whether to return a cartesian product of all possible groupers values (observed=False) or only those that are observed groupers (observed=True).", "Show all values:", "Show only the observed values:", "The returned dtype of the grouped will always include all of the categories that were grouped.", "If there are any NaN or NaT values in the grouping key, these will be automatically excluded. In other words, there will never be an \u201cNA group\u201d or \u201cNaT group\u201d. This was not the case in older versions of pandas, but users were generally discarding the NA group anyway (and supporting it was an implementation headache).", "Categorical variables represented as instance of pandas\u2019s Categorical class can be used as group keys. If so, the order of the levels will be preserved:", "You may need to specify a bit more data to properly group. You can use the pd.Grouper to provide this local control.", "Groupby a specific column with the desired frequency. This is like resampling.", "You have an ambiguous specification in that you have a named index and a column that could be potential groupers.", "Just like for a DataFrame or Series you can call head and tail on a groupby:", "This shows the first or last n rows from each group.", "To select from a DataFrame or Series the nth item, use nth(). This is a reduction method, and will return a single row (or no row) per group if you pass an int for n:", "If you want to select the nth not-null item, use the dropna kwarg. For a DataFrame this should be either 'any' or 'all' just like you would pass to dropna:", "As with other methods, passing as_index=False, will achieve a filtration, which returns the grouped row.", "You can also select multiple rows from each group by specifying multiple nth values as a list of ints.", "To see the order in which each row appears within its group, use the cumcount method:", "To see the ordering of the groups (as opposed to the order of rows within a group given by cumcount) you can use ngroup().", "Note that the numbers given to the groups match the order in which the groups would be seen when iterating over the groupby object, not the order they are first observed.", "Groupby also works with some plotting methods. For example, suppose we suspect that some features in a DataFrame may differ by group, in this case, the values in column 1 where the group is \u201cB\u201d are 3 higher on average.", "We can easily visualize this with a boxplot:", "The result of calling boxplot is a dictionary whose keys are the values of our grouping column g (\u201cA\u201d and \u201cB\u201d). The values of the resulting dictionary can be controlled by the return_type keyword of boxplot. See the visualization documentation for more.", "Warning", "For historical reasons, df.groupby(\"g\").boxplot() is not equivalent to df.boxplot(by=\"g\"). See here for an explanation.", "Similar to the functionality provided by DataFrame and Series, functions that take GroupBy objects can be chained together using a pipe method to allow for a cleaner, more readable syntax. To read about .pipe in general terms, see here.", "Combining .groupby and .pipe is often useful when you need to reuse GroupBy objects.", "As an example, imagine having a DataFrame with columns for stores, products, revenue and quantity sold. We\u2019d like to do a groupwise calculation of prices (i.e. revenue/quantity) per store and per product. We could do this in a multi-step operation, but expressing it in terms of piping can make the code more readable. First we set the data:", "Now, to find prices per store/product, we can simply do:", "Piping can also be expressive when you want to deliver a grouped object to some arbitrary function, for example:", "where mean takes a GroupBy object and finds the mean of the Revenue and Quantity columns respectively for each Store-Product combination. The mean function can be any function that takes in a GroupBy object; the .pipe will pass the GroupBy object as a parameter into the function you specify.", "Regroup columns of a DataFrame according to their sum, and sum the aggregated ones.", "By using ngroup(), we can extract information about the groups in a way similar to factorize() (as described further in the reshaping API) but which applies naturally to multiple columns of mixed type and different sources. This can be useful as an intermediate categorical-like step in processing, when the relationships between the group rows are more important than their content, or as input to an algorithm which only accepts the integer encoding. (For more information about support in pandas for full categorical data, see the Categorical introduction and the API documentation.)", "Resampling produces new hypothetical samples (resamples) from already existing observed data or from a model that generates data. These new samples are similar to the pre-existing samples.", "In order to resample to work on indices that are non-datetimelike, the following procedure can be utilized.", "In the following examples, df.index // 5 returns a binary array which is used to determine what gets selected for the groupby operation.", "Note", "The below example shows how we can downsample by consolidation of samples into fewer samples. Here by using df.index // 5, we are aggregating the samples in bins. By applying std() function, we aggregate the information contained in many samples into a small subset of values which is their standard deviation thereby reducing the number of samples.", "Group DataFrame columns, compute a set of metrics and return a named Series. The Series name is used as the name for the column index. This is especially useful in conjunction with reshaping operations such as stacking in which the column index name will be used as the name of the inserted column:"]}, {"name": "GroupBy", "path": "reference/groupby", "type": "GroupBy", "text": ["GroupBy objects are returned by groupby calls: pandas.DataFrame.groupby(), pandas.Series.groupby(), etc.", "GroupBy.__iter__()", "Groupby iterator.", "GroupBy.groups", "Dict {group name -> group labels}.", "GroupBy.indices", "Dict {group name -> group indices}.", "GroupBy.get_group(name[, obj])", "Construct DataFrame from group with provided name.", "Grouper(*args, **kwargs)", "A Grouper allows the user to specify a groupby instruction for an object.", "GroupBy.apply(func, *args, **kwargs)", "Apply function func group-wise and combine the results together.", "GroupBy.agg(func, *args, **kwargs)", "SeriesGroupBy.aggregate([func, engine, ...])", "Aggregate using one or more operations over the specified axis.", "DataFrameGroupBy.aggregate([func, engine, ...])", "Aggregate using one or more operations over the specified axis.", "SeriesGroupBy.transform(func, *args[, ...])", "Call function producing a like-indexed Series on each group and return a Series having the same indexes as the original object filled with the transformed values.", "DataFrameGroupBy.transform(func, *args[, ...])", "Call function producing a like-indexed DataFrame on each group and return a DataFrame having the same indexes as the original object filled with the transformed values.", "GroupBy.pipe(func, *args, **kwargs)", "Apply a function func with arguments to this GroupBy object and return the function's result.", "GroupBy.all([skipna])", "Return True if all values in the group are truthful, else False.", "GroupBy.any([skipna])", "Return True if any value in the group is truthful, else False.", "GroupBy.bfill([limit])", "Backward fill the values.", "GroupBy.backfill([limit])", "Backward fill the values.", "GroupBy.count()", "Compute count of group, excluding missing values.", "GroupBy.cumcount([ascending])", "Number each item in each group from 0 to the length of that group - 1.", "GroupBy.cummax([axis])", "Cumulative max for each group.", "GroupBy.cummin([axis])", "Cumulative min for each group.", "GroupBy.cumprod([axis])", "Cumulative product for each group.", "GroupBy.cumsum([axis])", "Cumulative sum for each group.", "GroupBy.ffill([limit])", "Forward fill the values.", "GroupBy.first([numeric_only, min_count])", "Compute first of group values.", "GroupBy.head([n])", "Return first n rows of each group.", "GroupBy.last([numeric_only, min_count])", "Compute last of group values.", "GroupBy.max([numeric_only, min_count])", "Compute max of group values.", "GroupBy.mean([numeric_only, engine, ...])", "Compute mean of groups, excluding missing values.", "GroupBy.median([numeric_only])", "Compute median of groups, excluding missing values.", "GroupBy.min([numeric_only, min_count])", "Compute min of group values.", "GroupBy.ngroup([ascending])", "Number each group from 0 to the number of groups - 1.", "GroupBy.nth(n[, dropna])", "Take the nth row from each group if n is an int, otherwise a subset of rows.", "GroupBy.ohlc()", "Compute open, high, low and close values of a group, excluding missing values.", "GroupBy.pad([limit])", "Forward fill the values.", "GroupBy.prod([numeric_only, min_count])", "Compute prod of group values.", "GroupBy.rank([method, ascending, na_option, ...])", "Provide the rank of values within each group.", "GroupBy.pct_change([periods, fill_method, ...])", "Calculate pct_change of each value to previous entry in group.", "GroupBy.size()", "Compute group sizes.", "GroupBy.sem([ddof])", "Compute standard error of the mean of groups, excluding missing values.", "GroupBy.std([ddof, engine, engine_kwargs])", "Compute standard deviation of groups, excluding missing values.", "GroupBy.sum([numeric_only, min_count, ...])", "Compute sum of group values.", "GroupBy.var([ddof, engine, engine_kwargs])", "Compute variance of groups, excluding missing values.", "GroupBy.tail([n])", "Return last n rows of each group.", "The following methods are available in both SeriesGroupBy and DataFrameGroupBy objects, but may differ slightly, usually in that the DataFrameGroupBy version usually permits the specification of an axis argument, and often an argument indicating whether to restrict application to columns of a specific data type.", "DataFrameGroupBy.all([skipna])", "Return True if all values in the group are truthful, else False.", "DataFrameGroupBy.any([skipna])", "Return True if any value in the group is truthful, else False.", "DataFrameGroupBy.backfill([limit])", "Backward fill the values.", "DataFrameGroupBy.bfill([limit])", "Backward fill the values.", "DataFrameGroupBy.corr", "Compute pairwise correlation of columns, excluding NA/null values.", "DataFrameGroupBy.count()", "Compute count of group, excluding missing values.", "DataFrameGroupBy.cov", "Compute pairwise covariance of columns, excluding NA/null values.", "DataFrameGroupBy.cumcount([ascending])", "Number each item in each group from 0 to the length of that group - 1.", "DataFrameGroupBy.cummax([axis])", "Cumulative max for each group.", "DataFrameGroupBy.cummin([axis])", "Cumulative min for each group.", "DataFrameGroupBy.cumprod([axis])", "Cumulative product for each group.", "DataFrameGroupBy.cumsum([axis])", "Cumulative sum for each group.", "DataFrameGroupBy.describe(**kwargs)", "Generate descriptive statistics.", "DataFrameGroupBy.diff", "First discrete difference of element.", "DataFrameGroupBy.ffill([limit])", "Forward fill the values.", "DataFrameGroupBy.fillna", "Fill NA/NaN values using the specified method.", "DataFrameGroupBy.filter(func[, dropna])", "Return a copy of a DataFrame excluding filtered elements.", "DataFrameGroupBy.hist", "Make a histogram of the DataFrame's columns.", "DataFrameGroupBy.idxmax([axis, skipna])", "Return index of first occurrence of maximum over requested axis.", "DataFrameGroupBy.idxmin([axis, skipna])", "Return index of first occurrence of minimum over requested axis.", "DataFrameGroupBy.mad", "Return the mean absolute deviation of the values over the requested axis.", "DataFrameGroupBy.nunique([dropna])", "Return DataFrame with counts of unique elements in each position.", "DataFrameGroupBy.pad([limit])", "Forward fill the values.", "DataFrameGroupBy.pct_change([periods, ...])", "Calculate pct_change of each value to previous entry in group.", "DataFrameGroupBy.plot", "Class implementing the .plot attribute for groupby objects.", "DataFrameGroupBy.quantile([q, interpolation])", "Return group values at the given quantile, a la numpy.percentile.", "DataFrameGroupBy.rank([method, ascending, ...])", "Provide the rank of values within each group.", "DataFrameGroupBy.resample(rule, *args, **kwargs)", "Provide resampling when using a TimeGrouper.", "DataFrameGroupBy.sample([n, frac, replace, ...])", "Return a random sample of items from each group.", "DataFrameGroupBy.shift([periods, freq, ...])", "Shift each group by periods observations.", "DataFrameGroupBy.size()", "Compute group sizes.", "DataFrameGroupBy.skew", "Return unbiased skew over requested axis.", "DataFrameGroupBy.take", "Return the elements in the given positional indices along an axis.", "DataFrameGroupBy.tshift", "(DEPRECATED) Shift the time index, using the index's frequency if available.", "DataFrameGroupBy.value_counts([subset, ...])", "Return a Series or DataFrame containing counts of unique rows.", "The following methods are available only for SeriesGroupBy objects.", "SeriesGroupBy.hist", "Draw histogram of the input series using matplotlib.", "SeriesGroupBy.nlargest([n, keep])", "Return the largest n elements.", "SeriesGroupBy.nsmallest([n, keep])", "Return the smallest n elements.", "SeriesGroupBy.nunique([dropna])", "Return number of unique elements in the group.", "SeriesGroupBy.unique", "Return unique values of Series object.", "SeriesGroupBy.value_counts([normalize, ...])", "SeriesGroupBy.is_monotonic_increasing", "Alias for is_monotonic.", "SeriesGroupBy.is_monotonic_decreasing", "Return boolean if values in the object are monotonic_decreasing.", "The following methods are available only for DataFrameGroupBy objects.", "DataFrameGroupBy.corrwith", "Compute pairwise correlation.", "DataFrameGroupBy.boxplot([subplots, column, ...])", "Make box plots from DataFrameGroupBy data."]}, {"name": "Index objects", "path": "reference/indexing", "type": "Index Objects", "text": ["Many of these methods or variants thereof are available on the objects that contain an index (Series/DataFrame) and those should most likely be used before calling these methods directly.", "Index([data, dtype, copy, name, tupleize_cols])", "Immutable sequence used for indexing and alignment.", "Index.values", "Return an array representing the data in the Index.", "Index.is_monotonic", "Alias for is_monotonic_increasing.", "Index.is_monotonic_increasing", "Return if the index is monotonic increasing (only equal or increasing) values.", "Index.is_monotonic_decreasing", "Return if the index is monotonic decreasing (only equal or decreasing) values.", "Index.is_unique", "Return if the index has unique values.", "Index.has_duplicates", "Check if the Index has duplicate values.", "Index.hasnans", "Return True if there are any NaNs.", "Index.dtype", "Return the dtype object of the underlying data.", "Index.inferred_type", "Return a string of the type inferred from the values.", "Index.is_all_dates", "Whether or not the index values only consist of dates.", "Index.shape", "Return a tuple of the shape of the underlying data.", "Index.name", "Return Index or MultiIndex name.", "Index.names", "Index.nbytes", "Return the number of bytes in the underlying data.", "Index.ndim", "Number of dimensions of the underlying data, by definition 1.", "Index.size", "Return the number of elements in the underlying data.", "Index.empty", "Index.T", "Return the transpose, which is by definition self.", "Index.memory_usage([deep])", "Memory usage of the values.", "Index.all(*args, **kwargs)", "Return whether all elements are Truthy.", "Index.any(*args, **kwargs)", "Return whether any element is Truthy.", "Index.argmin([axis, skipna])", "Return int position of the smallest value in the Series.", "Index.argmax([axis, skipna])", "Return int position of the largest value in the Series.", "Index.copy([name, deep, dtype, names])", "Make a copy of this object.", "Index.delete(loc)", "Make new Index with passed location(-s) deleted.", "Index.drop(labels[, errors])", "Make new Index with passed list of labels deleted.", "Index.drop_duplicates([keep])", "Return Index with duplicate values removed.", "Index.duplicated([keep])", "Indicate duplicate index values.", "Index.equals(other)", "Determine if two Index object are equal.", "Index.factorize([sort, na_sentinel])", "Encode the object as an enumerated type or categorical variable.", "Index.identical(other)", "Similar to equals, but checks that object attributes and types are also equal.", "Index.insert(loc, item)", "Make new Index inserting new item at location.", "Index.is_(other)", "More flexible, faster check like is but that works through views.", "Index.is_boolean()", "Check if the Index only consists of booleans.", "Index.is_categorical()", "Check if the Index holds categorical data.", "Index.is_floating()", "Check if the Index is a floating type.", "Index.is_integer()", "Check if the Index only consists of integers.", "Index.is_interval()", "Check if the Index holds Interval objects.", "Index.is_mixed()", "Check if the Index holds data with mixed data types.", "Index.is_numeric()", "Check if the Index only consists of numeric data.", "Index.is_object()", "Check if the Index is of the object dtype.", "Index.min([axis, skipna])", "Return the minimum value of the Index.", "Index.max([axis, skipna])", "Return the maximum value of the Index.", "Index.reindex(target[, method, level, ...])", "Create index with target's values.", "Index.rename(name[, inplace])", "Alter Index or MultiIndex name.", "Index.repeat(repeats[, axis])", "Repeat elements of a Index.", "Index.where(cond[, other])", "Replace values where the condition is False.", "Index.take(indices[, axis, allow_fill, ...])", "Return a new Index of the values selected by the indices.", "Index.putmask(mask, value)", "Return a new Index of the values set with the mask.", "Index.unique([level])", "Return unique values in the index.", "Index.nunique([dropna])", "Return number of unique elements in the object.", "Index.value_counts([normalize, sort, ...])", "Return a Series containing counts of unique values.", "Index.set_names(names[, level, inplace])", "Set Index or MultiIndex name.", "Index.droplevel([level])", "Return index with requested level(s) removed.", "Index.fillna([value, downcast])", "Fill NA/NaN values with the specified value.", "Index.dropna([how])", "Return Index without NA/NaN values.", "Index.isna()", "Detect missing values.", "Index.notna()", "Detect existing (non-missing) values.", "Index.astype(dtype[, copy])", "Create an Index with values cast to dtypes.", "Index.item()", "Return the first element of the underlying data as a Python scalar.", "Index.map(mapper[, na_action])", "Map values using an input mapping or function.", "Index.ravel([order])", "Return an ndarray of the flattened values of the underlying data.", "Index.to_list()", "Return a list of the values.", "Index.to_native_types([slicer])", "(DEPRECATED) Format specified values of self and return them.", "Index.to_series([index, name])", "Create a Series with both index and values equal to the index keys.", "Index.to_frame([index, name])", "Create a DataFrame with a column containing the Index.", "Index.view([cls])", "Index.argsort(*args, **kwargs)", "Return the integer indices that would sort the index.", "Index.searchsorted(value[, side, sorter])", "Find indices where elements should be inserted to maintain order.", "Index.sort_values([return_indexer, ...])", "Return a sorted copy of the index.", "Index.shift([periods, freq])", "Shift index by desired number of time frequency increments.", "Index.append(other)", "Append a collection of Index options together.", "Index.join(other[, how, level, ...])", "Compute join_index and indexers to conform data structures to the new index.", "Index.intersection(other[, sort])", "Form the intersection of two Index objects.", "Index.union(other[, sort])", "Form the union of two Index objects.", "Index.difference(other[, sort])", "Return a new Index with elements of index not in other.", "Index.symmetric_difference(other[, ...])", "Compute the symmetric difference of two Index objects.", "Index.asof(label)", "Return the label from the index, or, if not present, the previous one.", "Index.asof_locs(where, mask)", "Return the locations (indices) of labels in the index.", "Index.get_indexer(target[, method, limit, ...])", "Compute indexer and mask for new index given the current index.", "Index.get_indexer_for(target)", "Guaranteed return of an indexer even when non-unique.", "Index.get_indexer_non_unique(target)", "Compute indexer and mask for new index given the current index.", "Index.get_level_values(level)", "Return an Index of values for requested level.", "Index.get_loc(key[, method, tolerance])", "Get integer location, slice or boolean mask for requested label.", "Index.get_slice_bound(label, side[, kind])", "Calculate slice bound that corresponds to given label.", "Index.get_value(series, key)", "Fast lookup of value from 1-dimensional ndarray.", "Index.isin(values[, level])", "Return a boolean array where the index values are in values.", "Index.slice_indexer([start, end, step, kind])", "Compute the slice indexer for input labels and step.", "Index.slice_locs([start, end, step, kind])", "Compute slice locations for input labels.", "RangeIndex([start, stop, step, dtype, copy, ...])", "Immutable Index implementing a monotonic integer range.", "Int64Index([data, dtype, copy, name])", "(DEPRECATED) Immutable sequence used for indexing and alignment.", "UInt64Index([data, dtype, copy, name])", "(DEPRECATED) Immutable sequence used for indexing and alignment.", "Float64Index([data, dtype, copy, name])", "(DEPRECATED) Immutable sequence used for indexing and alignment.", "RangeIndex.start", "The value of the start parameter (0 if this was not supplied).", "RangeIndex.stop", "The value of the stop parameter.", "RangeIndex.step", "The value of the step parameter (1 if this was not supplied).", "RangeIndex.from_range(data[, name, dtype])", "Create RangeIndex from a range object.", "CategoricalIndex([data, categories, ...])", "Index based on an underlying Categorical.", "CategoricalIndex.codes", "The category codes of this categorical.", "CategoricalIndex.categories", "The categories of this categorical.", "CategoricalIndex.ordered", "Whether the categories have an ordered relationship.", "CategoricalIndex.rename_categories(*args, ...)", "Rename categories.", "CategoricalIndex.reorder_categories(*args, ...)", "Reorder categories as specified in new_categories.", "CategoricalIndex.add_categories(*args, **kwargs)", "Add new categories.", "CategoricalIndex.remove_categories(*args, ...)", "Remove the specified categories.", "CategoricalIndex.remove_unused_categories(...)", "Remove categories which are not used.", "CategoricalIndex.set_categories(*args, **kwargs)", "Set the categories to the specified new_categories.", "CategoricalIndex.as_ordered(*args, **kwargs)", "Set the Categorical to be ordered.", "CategoricalIndex.as_unordered(*args, **kwargs)", "Set the Categorical to be unordered.", "CategoricalIndex.map(mapper)", "Map values using input an input mapping or function.", "CategoricalIndex.equals(other)", "Determine if two CategoricalIndex objects contain the same elements.", "IntervalIndex(data[, closed, dtype, copy, ...])", "Immutable index of intervals that are closed on the same side.", "IntervalIndex.from_arrays(left, right[, ...])", "Construct from two arrays defining the left and right bounds.", "IntervalIndex.from_tuples(data[, closed, ...])", "Construct an IntervalIndex from an array-like of tuples.", "IntervalIndex.from_breaks(breaks[, closed, ...])", "Construct an IntervalIndex from an array of splits.", "IntervalIndex.left", "IntervalIndex.right", "IntervalIndex.mid", "IntervalIndex.closed", "Whether the intervals are closed on the left-side, right-side, both or neither.", "IntervalIndex.length", "IntervalIndex.values", "Return an array representing the data in the Index.", "IntervalIndex.is_empty", "Indicates if an interval is empty, meaning it contains no points.", "IntervalIndex.is_non_overlapping_monotonic", "Return True if the IntervalArray is non-overlapping (no Intervals share points) and is either monotonic increasing or monotonic decreasing, else False.", "IntervalIndex.is_overlapping", "Return True if the IntervalIndex has overlapping intervals, else False.", "IntervalIndex.get_loc(key[, method, tolerance])", "Get integer location, slice or boolean mask for requested label.", "IntervalIndex.get_indexer(target[, method, ...])", "Compute indexer and mask for new index given the current index.", "IntervalIndex.set_closed(*args, **kwargs)", "Return an IntervalArray identical to the current one, but closed on the specified side.", "IntervalIndex.contains(*args, **kwargs)", "Check elementwise if the Intervals contain the value.", "IntervalIndex.overlaps(*args, **kwargs)", "Check elementwise if an Interval overlaps the values in the IntervalArray.", "IntervalIndex.to_tuples(*args, **kwargs)", "Return an ndarray of tuples of the form (left, right).", "MultiIndex([levels, codes, sortorder, ...])", "A multi-level, or hierarchical, index object for pandas objects.", "IndexSlice", "Create an object to more easily perform multi-index slicing.", "MultiIndex.from_arrays(arrays[, sortorder, ...])", "Convert arrays to MultiIndex.", "MultiIndex.from_tuples(tuples[, sortorder, ...])", "Convert list of tuples to MultiIndex.", "MultiIndex.from_product(iterables[, ...])", "Make a MultiIndex from the cartesian product of multiple iterables.", "MultiIndex.from_frame(df[, sortorder, names])", "Make a MultiIndex from a DataFrame.", "MultiIndex.names", "Names of levels in MultiIndex.", "MultiIndex.levels", "MultiIndex.codes", "MultiIndex.nlevels", "Integer number of levels in this MultiIndex.", "MultiIndex.levshape", "A tuple with the length of each level.", "MultiIndex.dtypes", "Return the dtypes as a Series for the underlying MultiIndex.", "MultiIndex.set_levels(levels[, level, ...])", "Set new levels on MultiIndex.", "MultiIndex.set_codes(codes[, level, ...])", "Set new codes on MultiIndex.", "MultiIndex.to_flat_index()", "Convert a MultiIndex to an Index of Tuples containing the level values.", "MultiIndex.to_frame([index, name])", "Create a DataFrame with the levels of the MultiIndex as columns.", "MultiIndex.sortlevel([level, ascending, ...])", "Sort MultiIndex at the requested level.", "MultiIndex.droplevel([level])", "Return index with requested level(s) removed.", "MultiIndex.swaplevel([i, j])", "Swap level i with level j.", "MultiIndex.reorder_levels(order)", "Rearrange levels using input order.", "MultiIndex.remove_unused_levels()", "Create new MultiIndex from current that removes unused levels.", "MultiIndex.get_loc(key[, method])", "Get location for a label or a tuple of labels.", "MultiIndex.get_locs(seq)", "Get location for a sequence of labels.", "MultiIndex.get_loc_level(key[, level, ...])", "Get location and sliced index for requested label(s)/level(s).", "MultiIndex.get_indexer(target[, method, ...])", "Compute indexer and mask for new index given the current index.", "MultiIndex.get_level_values(level)", "Return vector of label values for requested level.", "DatetimeIndex([data, freq, tz, normalize, ...])", "Immutable ndarray-like of datetime64 data.", "DatetimeIndex.year", "The year of the datetime.", "DatetimeIndex.month", "The month as January=1, December=12.", "DatetimeIndex.day", "The day of the datetime.", "DatetimeIndex.hour", "The hours of the datetime.", "DatetimeIndex.minute", "The minutes of the datetime.", "DatetimeIndex.second", "The seconds of the datetime.", "DatetimeIndex.microsecond", "The microseconds of the datetime.", "DatetimeIndex.nanosecond", "The nanoseconds of the datetime.", "DatetimeIndex.date", "Returns numpy array of python datetime.date objects.", "DatetimeIndex.time", "Returns numpy array of datetime.time objects.", "DatetimeIndex.timetz", "Returns numpy array of datetime.time objects with timezone information.", "DatetimeIndex.dayofyear", "The ordinal day of the year.", "DatetimeIndex.day_of_year", "The ordinal day of the year.", "DatetimeIndex.weekofyear", "(DEPRECATED) The week ordinal of the year.", "DatetimeIndex.week", "(DEPRECATED) The week ordinal of the year.", "DatetimeIndex.dayofweek", "The day of the week with Monday=0, Sunday=6.", "DatetimeIndex.day_of_week", "The day of the week with Monday=0, Sunday=6.", "DatetimeIndex.weekday", "The day of the week with Monday=0, Sunday=6.", "DatetimeIndex.quarter", "The quarter of the date.", "DatetimeIndex.tz", "Return the timezone.", "DatetimeIndex.freq", "Return the frequency object if it is set, otherwise None.", "DatetimeIndex.freqstr", "Return the frequency object as a string if its set, otherwise None.", "DatetimeIndex.is_month_start", "Indicates whether the date is the first day of the month.", "DatetimeIndex.is_month_end", "Indicates whether the date is the last day of the month.", "DatetimeIndex.is_quarter_start", "Indicator for whether the date is the first day of a quarter.", "DatetimeIndex.is_quarter_end", "Indicator for whether the date is the last day of a quarter.", "DatetimeIndex.is_year_start", "Indicate whether the date is the first day of a year.", "DatetimeIndex.is_year_end", "Indicate whether the date is the last day of the year.", "DatetimeIndex.is_leap_year", "Boolean indicator if the date belongs to a leap year.", "DatetimeIndex.inferred_freq", "Tries to return a string representing a frequency guess, generated by infer_freq.", "DatetimeIndex.indexer_at_time(time[, asof])", "Return index locations of values at particular time of day (e.g.", "DatetimeIndex.indexer_between_time(...[, ...])", "Return index locations of values between particular times of day (e.g., 9:00-9:30AM).", "DatetimeIndex.normalize(*args, **kwargs)", "Convert times to midnight.", "DatetimeIndex.strftime(*args, **kwargs)", "Convert to Index using specified date_format.", "DatetimeIndex.snap([freq])", "Snap time stamps to nearest occurring frequency.", "DatetimeIndex.tz_convert(tz)", "Convert tz-aware Datetime Array/Index from one time zone to another.", "DatetimeIndex.tz_localize(tz[, ambiguous, ...])", "Localize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index.", "DatetimeIndex.round(*args, **kwargs)", "Perform round operation on the data to the specified freq.", "DatetimeIndex.floor(*args, **kwargs)", "Perform floor operation on the data to the specified freq.", "DatetimeIndex.ceil(*args, **kwargs)", "Perform ceil operation on the data to the specified freq.", "DatetimeIndex.month_name(*args, **kwargs)", "Return the month names of the DateTimeIndex with specified locale.", "DatetimeIndex.day_name(*args, **kwargs)", "Return the day names of the DateTimeIndex with specified locale.", "DatetimeIndex.to_period(*args, **kwargs)", "Cast to PeriodArray/Index at a particular frequency.", "DatetimeIndex.to_perioddelta(freq)", "Calculate TimedeltaArray of difference between index values and index converted to PeriodArray at specified freq.", "DatetimeIndex.to_pydatetime(*args, **kwargs)", "Return Datetime Array/Index as object ndarray of datetime.datetime objects.", "DatetimeIndex.to_series([keep_tz, index, name])", "Create a Series with both index and values equal to the index keys useful with map for returning an indexer based on an index.", "DatetimeIndex.to_frame([index, name])", "Create a DataFrame with a column containing the Index.", "DatetimeIndex.mean(*args, **kwargs)", "Return the mean value of the Array.", "DatetimeIndex.std(*args, **kwargs)", "Return sample standard deviation over requested axis.", "TimedeltaIndex([data, unit, freq, closed, ...])", "Immutable ndarray of timedelta64 data, represented internally as int64, and which can be boxed to timedelta objects.", "TimedeltaIndex.days", "Number of days for each element.", "TimedeltaIndex.seconds", "Number of seconds (>= 0 and less than 1 day) for each element.", "TimedeltaIndex.microseconds", "Number of microseconds (>= 0 and less than 1 second) for each element.", "TimedeltaIndex.nanoseconds", "Number of nanoseconds (>= 0 and less than 1 microsecond) for each element.", "TimedeltaIndex.components", "Return a dataframe of the components (days, hours, minutes, seconds, milliseconds, microseconds, nanoseconds) of the Timedeltas.", "TimedeltaIndex.inferred_freq", "Tries to return a string representing a frequency guess, generated by infer_freq.", "TimedeltaIndex.to_pytimedelta(*args, **kwargs)", "Return Timedelta Array/Index as object ndarray of datetime.timedelta objects.", "TimedeltaIndex.to_series([index, name])", "Create a Series with both index and values equal to the index keys.", "TimedeltaIndex.round(*args, **kwargs)", "Perform round operation on the data to the specified freq.", "TimedeltaIndex.floor(*args, **kwargs)", "Perform floor operation on the data to the specified freq.", "TimedeltaIndex.ceil(*args, **kwargs)", "Perform ceil operation on the data to the specified freq.", "TimedeltaIndex.to_frame([index, name])", "Create a DataFrame with a column containing the Index.", "TimedeltaIndex.mean(*args, **kwargs)", "Return the mean value of the Array.", "PeriodIndex([data, ordinal, freq, dtype, ...])", "Immutable ndarray holding ordinal values indicating regular periods in time.", "PeriodIndex.day", "The days of the period.", "PeriodIndex.dayofweek", "The day of the week with Monday=0, Sunday=6.", "PeriodIndex.day_of_week", "The day of the week with Monday=0, Sunday=6.", "PeriodIndex.dayofyear", "The ordinal day of the year.", "PeriodIndex.day_of_year", "The ordinal day of the year.", "PeriodIndex.days_in_month", "The number of days in the month.", "PeriodIndex.daysinmonth", "The number of days in the month.", "PeriodIndex.end_time", "PeriodIndex.freq", "Return the frequency object if it is set, otherwise None.", "PeriodIndex.freqstr", "Return the frequency object as a string if its set, otherwise None.", "PeriodIndex.hour", "The hour of the period.", "PeriodIndex.is_leap_year", "Logical indicating if the date belongs to a leap year.", "PeriodIndex.minute", "The minute of the period.", "PeriodIndex.month", "The month as January=1, December=12.", "PeriodIndex.quarter", "The quarter of the date.", "PeriodIndex.qyear", "PeriodIndex.second", "The second of the period.", "PeriodIndex.start_time", "PeriodIndex.week", "The week ordinal of the year.", "PeriodIndex.weekday", "The day of the week with Monday=0, Sunday=6.", "PeriodIndex.weekofyear", "The week ordinal of the year.", "PeriodIndex.year", "The year of the period.", "PeriodIndex.asfreq([freq, how])", "Convert the PeriodArray to the specified frequency freq.", "PeriodIndex.strftime(*args, **kwargs)", "Convert to Index using specified date_format.", "PeriodIndex.to_timestamp([freq, how])", "Cast to DatetimeArray/Index."]}, {"name": "Indexing and selecting data", "path": "user_guide/indexing", "type": "Manual", "text": ["The axis labeling information in pandas objects serves many purposes:", "Identifies data (i.e. provides metadata) using known indicators, important for analysis, visualization, and interactive console display.", "Enables automatic and explicit data alignment.", "Allows intuitive getting and setting of subsets of the data set.", "In this section, we will focus on the final point: namely, how to slice, dice, and generally get and set subsets of pandas objects. The primary focus will be on Series and DataFrame as they have received more development attention in this area.", "Note", "The Python and NumPy indexing operators [] and attribute operator . provide quick and easy access to pandas data structures across a wide range of use cases. This makes interactive work intuitive, as there\u2019s little new to learn if you already know how to deal with Python dictionaries and NumPy arrays. However, since the type of the data to be accessed isn\u2019t known in advance, directly using standard operators has some optimization limits. For production code, we recommended that you take advantage of the optimized pandas data access methods exposed in this chapter.", "Warning", "Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called chained assignment and should be avoided. See Returning a View versus Copy.", "See the MultiIndex / Advanced Indexing for MultiIndex and more advanced indexing documentation.", "See the cookbook for some advanced strategies.", "Object selection has had a number of user-requested additions in order to support more explicit location based indexing. pandas now supports three types of multi-axis indexing.", ".loc is primarily label based, but may also be used with a boolean array. .loc will raise KeyError when the items are not found. Allowed inputs are:", "A single label, e.g. 5 or 'a' (Note that 5 is interpreted as a label of the index. This use is not an integer position along the index.).", "A list or array of labels ['a', 'b', 'c'].", "A slice object with labels 'a':'f' (Note that contrary to usual Python slices, both the start and the stop are included, when present in the index! See Slicing with labels and Endpoints are inclusive.)", "A boolean array (any NA values will be treated as False).", "A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above).", "See more at Selection by Label.", ".iloc is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array. .iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing. (this conforms with Python/NumPy slice semantics). Allowed inputs are:", "An integer e.g. 5.", "A list or array of integers [4, 3, 0].", "A slice object with ints 1:7.", "A boolean array (any NA values will be treated as False).", "A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above).", "See more at Selection by Position, Advanced Indexing and Advanced Hierarchical.", ".loc, .iloc, and also [] indexing can accept a callable as indexer. See more at Selection By Callable.", "Getting values from an object with multi-axes selection uses the following notation (using .loc as an example, but the following applies to .iloc as well). Any of the axes accessors may be the null slice :. Axes left out of the specification are assumed to be :, e.g. p.loc['a'] is equivalent to p.loc['a', :, :].", "Object Type", "Indexers", "Series", "s.loc[indexer]", "DataFrame", "df.loc[row_indexer,column_indexer]", "As mentioned when introducing the data structures in the last section, the primary function of indexing with [] (a.k.a. __getitem__ for those familiar with implementing class behavior in Python) is selecting out lower-dimensional slices. The following table shows return type values when indexing pandas objects with []:", "Object Type", "Selection", "Return Value Type", "Series", "series[label]", "scalar value", "DataFrame", "frame[colname]", "Series corresponding to colname", "Here we construct a simple time series data set to use for illustrating the indexing functionality:", "Note", "None of the indexing functionality is time series specific unless specifically stated.", "Thus, as per above, we have the most basic indexing using []:", "You can pass a list of columns to [] to select columns in that order. If a column is not contained in the DataFrame, an exception will be raised. Multiple columns can also be set in this manner:", "You may find this useful for applying a transform (in-place) to a subset of the columns.", "Warning", "pandas aligns all AXES when setting Series and DataFrame from .loc, and .iloc.", "This will not modify df because the column alignment is before value assignment.", "The correct way to swap column values is by using raw values:", "You may access an index on a Series or column on a DataFrame directly as an attribute:", "Warning", "You can use this access only if the index element is a valid Python identifier, e.g. s.1 is not allowed. See here for an explanation of valid identifiers.", "The attribute will not be available if it conflicts with an existing method name, e.g. s.min is not allowed, but s['min'] is possible.", "Similarly, the attribute will not be available if it conflicts with any of the following list: index, major_axis, minor_axis, items.", "In any of these cases, standard indexing will still work, e.g. s['1'], s['min'], and s['index'] will access the corresponding element or column.", "If you are using the IPython environment, you may also use tab-completion to see these accessible attributes.", "You can also assign a dict to a row of a DataFrame:", "You can use attribute access to modify an existing element of a Series or column of a DataFrame, but be careful; if you try to use attribute access to create a new column, it creates a new attribute rather than a new column. In 0.21.0 and later, this will raise a UserWarning:", "The most robust and consistent way of slicing ranges along arbitrary axes is described in the Selection by Position section detailing the .iloc method. For now, we explain the semantics of slicing using the [] operator.", "With Series, the syntax works exactly as with an ndarray, returning a slice of the values and the corresponding labels:", "Note that setting works as well:", "With DataFrame, slicing inside of [] slices the rows. This is provided largely as a convenience since it is such a common operation.", "Warning", "Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called chained assignment and should be avoided. See Returning a View versus Copy.", "Warning", ".loc is strict when you present slicers that are not compatible (or convertible) with the index type. For example using integers in a DatetimeIndex. These will raise a TypeError.", "String likes in slicing can be convertible to the type of the index and lead to natural slicing.", "Warning", "Changed in version 1.0.0.", "pandas will raise a KeyError if indexing with a list with missing labels. See list-like Using loc with missing keys in a list is Deprecated.", "pandas provides a suite of methods in order to have purely label based indexing. This is a strict inclusion based protocol. Every label asked for must be in the index, or a KeyError will be raised. When slicing, both the start bound AND the stop bound are included, if present in the index. Integers are valid labels, but they refer to the label and not the position.", "The .loc attribute is the primary access method. The following are valid inputs:", "A single label, e.g. 5 or 'a' (Note that 5 is interpreted as a label of the index. This use is not an integer position along the index.).", "A list or array of labels ['a', 'b', 'c'].", "A slice object with labels 'a':'f' (Note that contrary to usual Python slices, both the start and the stop are included, when present in the index! See Slicing with labels.", "A boolean array.", "A callable, see Selection By Callable.", "Note that setting works as well:", "With a DataFrame:", "Accessing via label slices:", "For getting a cross section using a label (equivalent to df.xs('a')):", "For getting values with a boolean array:", "NA values in a boolean array propagate as False:", "Changed in version 1.0.2.", "For getting a value explicitly:", "When using .loc with slices, if both the start and the stop labels are present in the index, then elements located between the two (including them) are returned:", "If at least one of the two is absent, but the index is sorted, and can be compared against start and stop labels, then slicing will still work as expected, by selecting labels which rank between the two:", "However, if at least one of the two is absent and the index is not sorted, an error will be raised (since doing otherwise would be computationally expensive, as well as potentially ambiguous for mixed type indexes). For instance, in the above example, s.loc[1:6] would raise KeyError.", "For the rationale behind this behavior, see Endpoints are inclusive.", "Also, if the index has duplicate labels and either the start or the stop label is duplicated, an error will be raised. For instance, in the above example, s.loc[2:5] would raise a KeyError.", "For more information about duplicate labels, see Duplicate Labels.", "Warning", "Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called chained assignment and should be avoided. See Returning a View versus Copy.", "pandas provides a suite of methods in order to get purely integer based indexing. The semantics follow closely Python and NumPy slicing. These are 0-based indexing. When slicing, the start bound is included, while the upper bound is excluded. Trying to use a non-integer, even a valid label will raise an IndexError.", "The .iloc attribute is the primary access method. The following are valid inputs:", "An integer e.g. 5.", "A list or array of integers [4, 3, 0].", "A slice object with ints 1:7.", "A boolean array.", "A callable, see Selection By Callable.", "Note that setting works as well:", "With a DataFrame:", "Select via integer slicing:", "Select via integer list:", "For getting a cross section using an integer position (equiv to df.xs(1)):", "Out of range slice indexes are handled gracefully just as in Python/NumPy.", "Note that using slices that go out of bounds can result in an empty axis (e.g. an empty DataFrame being returned).", "A single indexer that is out of bounds will raise an IndexError. A list of indexers where any element is out of bounds will raise an IndexError.", ".loc, .iloc, and also [] indexing can accept a callable as indexer. The callable must be a function with one argument (the calling Series or DataFrame) that returns valid output for indexing.", "You can use callable indexing in Series.", "Using these methods / indexers, you can chain data selection operations without using a temporary variable.", "If you wish to get the 0th and the 2nd elements from the index in the \u2018A\u2019 column, you can do:", "This can also be expressed using .iloc, by explicitly getting locations on the indexers, and using positional indexing to select things.", "For getting multiple indexers, using .get_indexer:", "Warning", "Changed in version 1.0.0.", "Using .loc or [] with a list with one or more missing labels will no longer reindex, in favor of .reindex.", "In prior versions, using .loc[list-of-labels] would work as long as at least 1 of the keys was found (otherwise it would raise a KeyError). This behavior was changed and will now raise a KeyError if at least one label is missing. The recommended alternative is to use .reindex().", "For example.", "Selection with all keys found is unchanged.", "Previous behavior", "Current behavior", "The idiomatic way to achieve selecting potentially not-found elements is via .reindex(). See also the section on reindexing.", "Alternatively, if you want to select only valid keys, the following is idiomatic and efficient; it is guaranteed to preserve the dtype of the selection.", "Having a duplicated index will raise for a .reindex():", "Generally, you can intersect the desired labels with the current axis, and then reindex.", "However, this would still raise if your resulting index is duplicated.", "A random selection of rows or columns from a Series or DataFrame with the sample() method. The method will sample rows by default, and accepts a specific number of rows/columns to return, or a fraction of rows.", "By default, sample will return each row at most once, but one can also sample with replacement using the replace option:", "By default, each row has an equal probability of being selected, but if you want rows to have different probabilities, you can pass the sample function sampling weights as weights. These weights can be a list, a NumPy array, or a Series, but they must be of the same length as the object you are sampling. Missing values will be treated as a weight of zero, and inf values are not allowed. If weights do not sum to 1, they will be re-normalized by dividing all weights by the sum of the weights. For example:", "When applied to a DataFrame, you can use a column of the DataFrame as sampling weights (provided you are sampling rows and not columns) by simply passing the name of the column as a string.", "sample also allows users to sample columns instead of rows using the axis argument.", "Finally, one can also set a seed for sample\u2019s random number generator using the random_state argument, which will accept either an integer (as a seed) or a NumPy RandomState object.", "The .loc/[] operations can perform enlargement when setting a non-existent key for that axis.", "In the Series case this is effectively an appending operation.", "A DataFrame can be enlarged on either axis via .loc.", "This is like an append operation on the DataFrame.", "Since indexing with [] must handle a lot of cases (single-label access, slicing, boolean indexing, etc.), it has a bit of overhead in order to figure out what you\u2019re asking for. If you only want to access a scalar value, the fastest way is to use the at and iat methods, which are implemented on all of the data structures.", "Similarly to loc, at provides label based scalar lookups, while, iat provides integer based lookups analogously to iloc", "You can also set using these same indexers.", "at may enlarge the object in-place as above if the indexer is missing.", "Another common operation is the use of boolean vectors to filter the data. The operators are: | for or, & for and, and ~ for not. These must be grouped by using parentheses, since by default Python will evaluate an expression such as df['A'] > 2 & df['B'] < 3 as df['A'] > (2 & df['B']) < 3, while the desired evaluation order is (df['A'] > 2) & (df['B'] < 3).", "Using a boolean vector to index a Series works exactly as in a NumPy ndarray:", "You may select rows from a DataFrame using a boolean vector the same length as the DataFrame\u2019s index (for example, something derived from one of the columns of the DataFrame):", "List comprehensions and the map method of Series can also be used to produce more complex criteria:", "With the choice methods Selection by Label, Selection by Position, and Advanced Indexing you may select along more than one axis using boolean vectors combined with other indexing expressions.", "Warning", "iloc supports two kinds of boolean indexing. If the indexer is a boolean Series, an error will be raised. For instance, in the following example, df.iloc[s.values, 1] is ok. The boolean indexer is an array. But df.iloc[s, 1] would raise ValueError.", "Consider the isin() method of Series, which returns a boolean vector that is true wherever the Series elements exist in the passed list. This allows you to select rows where one or more columns have values you want:", "The same method is available for Index objects and is useful for the cases when you don\u2019t know which of the sought labels are in fact present:", "In addition to that, MultiIndex allows selecting a separate level to use in the membership check:", "DataFrame also has an isin() method. When calling isin, pass a set of values as either an array or dict. If values is an array, isin returns a DataFrame of booleans that is the same shape as the original DataFrame, with True wherever the element is in the sequence of values.", "Oftentimes you\u2019ll want to match certain values with certain columns. Just make values a dict where the key is the column, and the value is a list of items you want to check for.", "To return the DataFrame of booleans where the values are not in the original DataFrame, use the ~ operator:", "Combine DataFrame\u2019s isin with the any() and all() methods to quickly select subsets of your data that meet a given criteria. To select a row where each column meets its own criterion:", "Selecting values from a Series with a boolean vector generally returns a subset of the data. To guarantee that selection output has the same shape as the original data, you can use the where method in Series and DataFrame.", "To return only the selected rows:", "To return a Series of the same shape as the original:", "Selecting values from a DataFrame with a boolean criterion now also preserves input data shape. where is used under the hood as the implementation. The code below is equivalent to df.where(df < 0).", "In addition, where takes an optional other argument for replacement of values where the condition is False, in the returned copy.", "You may wish to set values based on some boolean criteria. This can be done intuitively like so:", "By default, where returns a modified copy of the data. There is an optional parameter inplace so that the original data can be modified without creating a copy:", "Note", "The signature for DataFrame.where() differs from numpy.where(). Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2).", "Alignment", "Furthermore, where aligns the input boolean condition (ndarray or DataFrame), such that partial selection with setting is possible. This is analogous to partial setting via .loc (but on the contents rather than the axis labels).", "Where can also accept axis and level parameters to align the input when performing the where.", "This is equivalent to (but faster than) the following.", "where can accept a callable as condition and other arguments. The function must be with one argument (the calling Series or DataFrame) and that returns valid output as condition and other argument.", "mask() is the inverse boolean operation of where.", "An alternative to where() is to use numpy.where(). Combined with setting a new column, you can use it to enlarge a DataFrame where the values are determined conditionally.", "Consider you have two choices to choose from in the following DataFrame. And you want to set a new column color to \u2018green\u2019 when the second column has \u2018Z\u2019. You can do the following:", "If you have multiple conditions, you can use numpy.select() to achieve that. Say corresponding to three conditions there are three choice of colors, with a fourth color as a fallback, you can do the following.", "DataFrame objects have a query() method that allows selection using an expression.", "You can get the value of the frame where column b has values between the values of columns a and c. For example:", "Do the same thing but fall back on a named index if there is no column with the name a.", "If instead you don\u2019t want to or cannot name your index, you can use the name index in your query expression:", "Note", "If the name of your index overlaps with a column name, the column name is given precedence. For example,", "You can still use the index in a query expression by using the special identifier \u2018index\u2019:", "If for some reason you have a column named index, then you can refer to the index as ilevel_0 as well, but at this point you should consider renaming your columns to something less ambiguous.", "You can also use the levels of a DataFrame with a MultiIndex as if they were columns in the frame:", "If the levels of the MultiIndex are unnamed, you can refer to them using special names:", "The convention is ilevel_0, which means \u201cindex level 0\u201d for the 0th level of the index.", "A use case for query() is when you have a collection of DataFrame objects that have a subset of column names (or index levels/names) in common. You can pass the same query to both frames without having to specify which frame you\u2019re interested in querying", "Full numpy-like syntax:", "Slightly nicer by removing the parentheses (comparison operators bind tighter than & and |):", "Use English instead of symbols:", "Pretty close to how you might write it on paper:", "query() also supports special use of Python\u2019s in and not in comparison operators, providing a succinct syntax for calling the isin method of a Series or DataFrame.", "You can combine this with other expressions for very succinct queries:", "Note", "Note that in and not in are evaluated in Python, since numexpr has no equivalent of this operation. However, only the in/not in expression itself is evaluated in vanilla Python. For example, in the expression", "(b + c + d) is evaluated by numexpr and then the in operation is evaluated in plain Python. In general, any operations that can be evaluated using numexpr will be.", "Comparing a list of values to a column using ==/!= works similarly to in/not in.", "You can negate boolean expressions with the word not or the ~ operator.", "Of course, expressions can be arbitrarily complex too:", "DataFrame.query() using numexpr is slightly faster than Python for large frames.", "Note", "You will only see the performance benefits of using the numexpr engine with DataFrame.query() if your frame has more than approximately 200,000 rows.", "This plot was created using a DataFrame with 3 columns each containing floating point values generated using numpy.random.randn().", "If you want to identify and remove duplicate rows in a DataFrame, there are two methods that will help: duplicated and drop_duplicates. Each takes as an argument the columns to use to identify duplicated rows.", "duplicated returns a boolean vector whose length is the number of rows, and which indicates whether a row is duplicated.", "drop_duplicates removes duplicate rows.", "By default, the first observed row of a duplicate set is considered unique, but each method has a keep parameter to specify targets to be kept.", "keep='first' (default): mark / drop duplicates except for the first occurrence.", "keep='last': mark / drop duplicates except for the last occurrence.", "keep=False: mark / drop all duplicates.", "Also, you can pass a list of columns to identify duplications.", "To drop duplicates by index value, use Index.duplicated then perform slicing. The same set of options are available for the keep parameter.", "Each of Series or DataFrame have a get method which can return a default value.", "Sometimes you want to extract a set of values given a sequence of row labels and column labels, this can be achieved by pandas.factorize and NumPy indexing. For instance:", "Formerly this could be achieved with the dedicated DataFrame.lookup method which was deprecated in version 1.2.0.", "The pandas Index class and its subclasses can be viewed as implementing an ordered multiset. Duplicates are allowed. However, if you try to convert an Index object with duplicate entries into a set, an exception will be raised.", "Index also provides the infrastructure necessary for lookups, data alignment, and reindexing. The easiest way to create an Index directly is to pass a list or other sequence to Index:", "You can also pass a name to be stored in the index:", "The name, if set, will be shown in the console display:", "Indexes are \u201cmostly immutable\u201d, but it is possible to set and change their name attribute. You can use the rename, set_names to set these attributes directly, and they default to returning a copy.", "See Advanced Indexing for usage of MultiIndexes.", "set_names, set_levels, and set_codes also take an optional level argument", "The two main operations are union and intersection. Difference is provided via the .difference() method.", "Also available is the symmetric_difference operation, which returns elements that appear in either idx1 or idx2, but not in both. This is equivalent to the Index created by idx1.difference(idx2).union(idx2.difference(idx1)), with duplicates dropped.", "Note", "The resulting index from a set operation will be sorted in ascending order.", "When performing Index.union() between indexes with different dtypes, the indexes must be cast to a common dtype. Typically, though not always, this is object dtype. The exception is when performing a union between integer and float data. In this case, the integer values are converted to float", "Important", "Even though Index can hold missing values (NaN), it should be avoided if you do not want any unexpected results. For example, some operations exclude missing values implicitly.", "Index.fillna fills missing values with specified scalar value.", "Occasionally you will load or create a data set into a DataFrame and want to add an index after you\u2019ve already done so. There are a couple of different ways.", "DataFrame has a set_index() method which takes a column name (for a regular Index) or a list of column names (for a MultiIndex). To create a new, re-indexed DataFrame:", "The append keyword option allow you to keep the existing index and append the given columns to a MultiIndex:", "Other options in set_index allow you not drop the index columns or to add the index in-place (without creating a new object):", "As a convenience, there is a new function on DataFrame called reset_index() which transfers the index values into the DataFrame\u2019s columns and sets a simple integer index. This is the inverse operation of set_index().", "The output is more similar to a SQL table or a record array. The names for the columns derived from the index are the ones stored in the names attribute.", "You can use the level keyword to remove only a portion of the index:", "reset_index takes an optional parameter drop which if true simply discards the index, instead of putting index values in the DataFrame\u2019s columns.", "If you create an index yourself, you can just assign it to the index field:", "When setting values in a pandas object, care must be taken to avoid what is called chained indexing. Here is an example.", "Compare these two access methods:", "These both yield the same results, so which should you use? It is instructive to understand the order of operations on these and why method 2 (.loc) is much preferred over method 1 (chained []).", "dfmi['one'] selects the first level of the columns and returns a DataFrame that is singly-indexed. Then another Python operation dfmi_with_one['second'] selects the series indexed by 'second'. This is indicated by the variable dfmi_with_one because pandas sees these operations as separate events. e.g. separate calls to __getitem__, so it has to treat them as linear operations, they happen one after another.", "Contrast this to df.loc[:,('one','second')] which passes a nested tuple of (slice(None),('one','second')) to a single call to __getitem__. This allows pandas to deal with this as a single entity. Furthermore this order of operations can be significantly faster, and allows one to index both axes if so desired.", "The problem in the previous section is just a performance issue. What\u2019s up with the SettingWithCopy warning? We don\u2019t usually throw warnings around when you do something that might cost a few extra milliseconds!", "But it turns out that assigning to the product of chained indexing has inherently unpredictable results. To see this, think about how the Python interpreter executes this code:", "But this code is handled differently:", "See that __getitem__ in there? Outside of simple cases, it\u2019s very hard to predict whether it will return a view or a copy (it depends on the memory layout of the array, about which pandas makes no guarantees), and therefore whether the __setitem__ will modify dfmi or a temporary object that gets thrown out immediately afterward. That\u2019s what SettingWithCopy is warning you about!", "Note", "You may be wondering whether we should be concerned about the loc property in the first example. But dfmi.loc is guaranteed to be dfmi itself with modified indexing behavior, so dfmi.loc.__getitem__ / dfmi.loc.__setitem__ operate on dfmi directly. Of course, dfmi.loc.__getitem__(idx) may be a view or a copy of dfmi.", "Sometimes a SettingWithCopy warning will arise at times when there\u2019s no obvious chained indexing going on. These are the bugs that SettingWithCopy is designed to catch! pandas is probably trying to warn you that you\u2019ve done this:", "Yikes!", "When you use chained indexing, the order and type of the indexing operation partially determine whether the result is a slice into the original object, or a copy of the slice.", "pandas has the SettingWithCopyWarning because assigning to a copy of a slice is frequently not intentional, but a mistake caused by chained indexing returning a copy where a slice was expected.", "If you would like pandas to be more or less trusting about assignment to a chained indexing expression, you can set the option mode.chained_assignment to one of these values:", "'warn', the default, means a SettingWithCopyWarning is printed.", "'raise' means pandas will raise a SettingWithCopyException you have to deal with.", "None will suppress the warnings entirely.", "This however is operating on a copy and will not work.", "A chained assignment can also crop up in setting in a mixed dtype frame.", "Note", "These setting rules apply to all of .loc/.iloc.", "The following is the recommended access method using .loc for multiple items (using mask) and a single item using a fixed index:", "The following can work at times, but it is not guaranteed to, and therefore should be avoided:", "Last, the subsequent example will not work at all, and so should be avoided:", "Warning", "The chained assignment warnings / exceptions are aiming to inform the user of a possibly invalid assignment. There may be false positives; situations where a chained assignment is inadvertently reported."]}, {"name": "Input/output", "path": "reference/io", "type": "Input/output", "text": ["read_pickle(filepath_or_buffer[, ...])", "Load pickled pandas object (or any object) from file.", "DataFrame.to_pickle(path[, compression, ...])", "Pickle (serialize) object to file.", "read_table(filepath_or_buffer[, sep, ...])", "Read general delimited file into DataFrame.", "read_csv(filepath_or_buffer[, sep, ...])", "Read a comma-separated values (csv) file into DataFrame.", "DataFrame.to_csv([path_or_buf, sep, na_rep, ...])", "Write object to a comma-separated values (csv) file.", "read_fwf(filepath_or_buffer[, colspecs, ...])", "Read a table of fixed-width formatted lines into DataFrame.", "read_clipboard([sep])", "Read text from clipboard and pass to read_csv.", "DataFrame.to_clipboard([excel, sep])", "Copy object to the system clipboard.", "read_excel(io[, sheet_name, header, names, ...])", "Read an Excel file into a pandas DataFrame.", "DataFrame.to_excel(excel_writer[, ...])", "Write object to an Excel sheet.", "ExcelFile.parse([sheet_name, header, names, ...])", "Parse specified sheet(s) into a DataFrame.", "Styler.to_excel(excel_writer[, sheet_name, ...])", "Write Styler to an Excel sheet.", "ExcelWriter(path[, engine, date_format, ...])", "Class for writing DataFrame objects into excel sheets.", "read_json([path_or_buf, orient, typ, dtype, ...])", "Convert a JSON string to pandas object.", "json_normalize(data[, record_path, meta, ...])", "Normalize semi-structured JSON data into a flat table.", "DataFrame.to_json([path_or_buf, orient, ...])", "Convert the object to a JSON string.", "build_table_schema(data[, index, ...])", "Create a Table schema from data.", "read_html(io[, match, flavor, header, ...])", "Read HTML tables into a list of DataFrame objects.", "DataFrame.to_html([buf, columns, col_space, ...])", "Render a DataFrame as an HTML table.", "Styler.to_html([buf, table_uuid, ...])", "Write Styler to a file, buffer or string in HTML-CSS format.", "read_xml(path_or_buffer[, xpath, ...])", "Read XML document into a DataFrame object.", "DataFrame.to_xml([path_or_buffer, index, ...])", "Render a DataFrame to an XML document.", "DataFrame.to_latex([buf, columns, ...])", "Render object to a LaTeX tabular, longtable, or nested table.", "Styler.to_latex([buf, column_format, ...])", "Write Styler to a file, buffer or string in LaTeX format.", "read_hdf(path_or_buf[, key, mode, errors, ...])", "Read from the store, close it if we opened it.", "HDFStore.put(key, value[, format, index, ...])", "Store object in HDFStore.", "HDFStore.append(key, value[, format, axes, ...])", "Append to Table in file.", "HDFStore.get(key)", "Retrieve pandas object stored in file.", "HDFStore.select(key[, where, start, stop, ...])", "Retrieve pandas object stored in file, optionally based on where criteria.", "HDFStore.info()", "Print detailed information on the store.", "HDFStore.keys([include])", "Return a list of keys corresponding to objects stored in HDFStore.", "HDFStore.groups()", "Return a list of all the top-level nodes.", "HDFStore.walk([where])", "Walk the pytables group hierarchy for pandas objects.", "Warning", "One can store a subclass of DataFrame or Series to HDF5, but the type of the subclass is lost upon storing.", "read_feather(path[, columns, use_threads, ...])", "Load a feather-format object from the file path.", "DataFrame.to_feather(path, **kwargs)", "Write a DataFrame to the binary Feather format.", "read_parquet(path[, engine, columns, ...])", "Load a parquet object from the file path, returning a DataFrame.", "DataFrame.to_parquet([path, engine, ...])", "Write a DataFrame to the binary parquet format.", "read_orc(path[, columns])", "Load an ORC object from the file path, returning a DataFrame.", "read_sas(filepath_or_buffer[, format, ...])", "Read SAS files stored as either XPORT or SAS7BDAT format files.", "read_spss(path[, usecols, convert_categoricals])", "Load an SPSS file from the file path, returning a DataFrame.", "read_sql_table(table_name, con[, schema, ...])", "Read SQL database table into a DataFrame.", "read_sql_query(sql, con[, index_col, ...])", "Read SQL query into a DataFrame.", "read_sql(sql, con[, index_col, ...])", "Read SQL query or database table into a DataFrame.", "DataFrame.to_sql(name, con[, schema, ...])", "Write records stored in a DataFrame to a SQL database.", "read_gbq(query[, project_id, index_col, ...])", "Load data from Google BigQuery.", "read_stata(filepath_or_buffer[, ...])", "Read Stata file into DataFrame.", "DataFrame.to_stata(path[, convert_dates, ...])", "Export DataFrame object to Stata dta format.", "StataReader.data_label", "Return data label of Stata file.", "StataReader.value_labels()", "Return a dict, associating each variable name a dict, associating each value its corresponding label.", "StataReader.variable_labels()", "Return variable labels as a dict, associating each variable name with corresponding label.", "StataWriter.write_file()", "Export DataFrame object to Stata dta format."]}, {"name": "Intro to data structures", "path": "user_guide/dsintro", "type": "Manual", "text": ["We\u2019ll start with a quick, non-comprehensive overview of the fundamental data structures in pandas to get you started. The fundamental behavior about data types, indexing, and axis labeling / alignment apply across all of the objects. To get started, import NumPy and load pandas into your namespace:", "Here is a basic tenet to keep in mind: data alignment is intrinsic. The link between labels and data will not be broken unless done so explicitly by you.", "We\u2019ll give a brief intro to the data structures, then consider all of the broad categories of functionality and methods in separate sections.", "Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index. The basic method to create a Series is to call:", "Here, data can be many different things:", "a Python dict", "an ndarray", "a scalar value (like 5)", "The passed index is a list of axis labels. Thus, this separates into a few cases depending on what data is:", "From ndarray", "If data is an ndarray, index must be the same length as data. If no index is passed, one will be created having values [0, ..., len(data) - 1].", "Note", "pandas supports non-unique index values. If an operation that does not support duplicate index values is attempted, an exception will be raised at that time. The reason for being lazy is nearly all performance-based (there are many instances in computations, like parts of GroupBy, where the index is not used).", "From dict", "Series can be instantiated from dicts:", "Note", "When the data is a dict, and an index is not passed, the Series index will be ordered by the dict\u2019s insertion order, if you\u2019re using Python version >= 3.6 and pandas version >= 0.23.", "If you\u2019re using Python < 3.6 or pandas < 0.23, and an index is not passed, the Series index will be the lexically ordered list of dict keys.", "In the example above, if you were on a Python version lower than 3.6 or a pandas version lower than 0.23, the Series would be ordered by the lexical order of the dict keys (i.e. ['a', 'b', 'c'] rather than ['b', 'a', 'c']).", "If an index is passed, the values in data corresponding to the labels in the index will be pulled out.", "Note", "NaN (not a number) is the standard missing data marker used in pandas.", "From scalar value", "If data is a scalar value, an index must be provided. The value will be repeated to match the length of index.", "Series acts very similarly to a ndarray, and is a valid argument to most NumPy functions. However, operations such as slicing will also slice the index.", "Note", "We will address array-based indexing like s[[4, 3, 1]] in section on indexing.", "Like a NumPy array, a pandas Series has a dtype.", "This is often a NumPy dtype. However, pandas and 3rd-party libraries extend NumPy\u2019s type system in a few places, in which case the dtype would be an ExtensionDtype. Some examples within pandas are Categorical data and Nullable integer data type. See dtypes for more.", "If you need the actual array backing a Series, use Series.array.", "Accessing the array can be useful when you need to do some operation without the index (to disable automatic alignment, for example).", "Series.array will always be an ExtensionArray. Briefly, an ExtensionArray is a thin wrapper around one or more concrete arrays like a numpy.ndarray. pandas knows how to take an ExtensionArray and store it in a Series or a column of a DataFrame. See dtypes for more.", "While Series is ndarray-like, if you need an actual ndarray, then use Series.to_numpy().", "Even if the Series is backed by a ExtensionArray, Series.to_numpy() will return a NumPy ndarray.", "A Series is like a fixed-size dict in that you can get and set values by index label:", "If a label is not contained, an exception is raised:", "Using the get method, a missing label will return None or specified default:", "See also the section on attribute access.", "When working with raw NumPy arrays, looping through value-by-value is usually not necessary. The same is true when working with Series in pandas. Series can also be passed into most NumPy methods expecting an ndarray.", "A key difference between Series and ndarray is that operations between Series automatically align the data based on label. Thus, you can write computations without giving consideration to whether the Series involved have the same labels.", "The result of an operation between unaligned Series will have the union of the indexes involved. If a label is not found in one Series or the other, the result will be marked as missing NaN. Being able to write code without doing any explicit data alignment grants immense freedom and flexibility in interactive data analysis and research. The integrated data alignment features of the pandas data structures set pandas apart from the majority of related tools for working with labeled data.", "Note", "In general, we chose to make the default result of operations between differently indexed objects yield the union of the indexes in order to avoid loss of information. Having an index label, though the data is missing, is typically important information as part of a computation. You of course have the option of dropping labels with missing data via the dropna function.", "Series can also have a name attribute:", "The Series name will be assigned automatically in many cases, in particular when taking 1D slices of DataFrame as you will see below.", "You can rename a Series with the pandas.Series.rename() method.", "Note that s and s2 refer to different objects.", "DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It is generally the most commonly used pandas object. Like Series, DataFrame accepts many different kinds of input:", "Dict of 1D ndarrays, lists, dicts, or Series", "2-D numpy.ndarray", "Structured or record ndarray", "A Series", "Another DataFrame", "Along with the data, you can optionally pass index (row labels) and columns (column labels) arguments. If you pass an index and / or columns, you are guaranteeing the index and / or columns of the resulting DataFrame. Thus, a dict of Series plus a specific index will discard all data not matching up to the passed index.", "If axis labels are not passed, they will be constructed from the input data based on common sense rules.", "Note", "When the data is a dict, and columns is not specified, the DataFrame columns will be ordered by the dict\u2019s insertion order, if you are using Python version >= 3.6 and pandas >= 0.23.", "If you are using Python < 3.6 or pandas < 0.23, and columns is not specified, the DataFrame columns will be the lexically ordered list of dict keys.", "The resulting index will be the union of the indexes of the various Series. If there are any nested dicts, these will first be converted to Series. If no columns are passed, the columns will be the ordered list of dict keys.", "The row and column labels can be accessed respectively by accessing the index and columns attributes:", "Note", "When a particular set of columns is passed along with a dict of data, the passed columns override the keys in the dict.", "The ndarrays must all be the same length. If an index is passed, it must clearly also be the same length as the arrays. If no index is passed, the result will be range(n), where n is the array length.", "This case is handled identically to a dict of arrays.", "Note", "DataFrame is not intended to work exactly like a 2-dimensional NumPy ndarray.", "You can automatically create a MultiIndexed frame by passing a tuples dictionary.", "The result will be a DataFrame with the same index as the input Series, and with one column whose name is the original name of the Series (only if no other column name provided).", "The field names of the first namedtuple in the list determine the columns of the DataFrame. The remaining namedtuples (or tuples) are simply unpacked and their values are fed into the rows of the DataFrame. If any of those tuples is shorter than the first namedtuple then the later columns in the corresponding row are marked as missing values. If any are longer than the first namedtuple, a ValueError is raised.", "New in version 1.1.0.", "Data Classes as introduced in PEP557, can be passed into the DataFrame constructor. Passing a list of dataclasses is equivalent to passing a list of dictionaries.", "Please be aware, that all values in the list should be dataclasses, mixing types in the list would result in a TypeError.", "Missing data", "Much more will be said on this topic in the Missing data section. To construct a DataFrame with missing data, we use np.nan to represent missing values. Alternatively, you may pass a numpy.MaskedArray as the data argument to the DataFrame constructor, and its masked entries will be considered missing.", "DataFrame.from_dict", "DataFrame.from_dict takes a dict of dicts or a dict of array-like sequences and returns a DataFrame. It operates like the DataFrame constructor except for the orient parameter which is 'columns' by default, but which can be set to 'index' in order to use the dict keys as row labels.", "If you pass orient='index', the keys will be the row labels. In this case, you can also pass the desired column names:", "DataFrame.from_records", "DataFrame.from_records takes a list of tuples or an ndarray with structured dtype. It works analogously to the normal DataFrame constructor, except that the resulting DataFrame index may be a specific field of the structured dtype. For example:", "You can treat a DataFrame semantically like a dict of like-indexed Series objects. Getting, setting, and deleting columns works with the same syntax as the analogous dict operations:", "Columns can be deleted or popped like with a dict:", "When inserting a scalar value, it will naturally be propagated to fill the column:", "When inserting a Series that does not have the same index as the DataFrame, it will be conformed to the DataFrame\u2019s index:", "You can insert raw ndarrays but their length must match the length of the DataFrame\u2019s index.", "By default, columns get inserted at the end. The insert function is available to insert at a particular location in the columns:", "Inspired by dplyr\u2019s mutate verb, DataFrame has an assign() method that allows you to easily create new columns that are potentially derived from existing columns.", "In the example above, we inserted a precomputed value. We can also pass in a function of one argument to be evaluated on the DataFrame being assigned to.", "assign always returns a copy of the data, leaving the original DataFrame untouched.", "Passing a callable, as opposed to an actual value to be inserted, is useful when you don\u2019t have a reference to the DataFrame at hand. This is common when using assign in a chain of operations. For example, we can limit the DataFrame to just those observations with a Sepal Length greater than 5, calculate the ratio, and plot:", "Since a function is passed in, the function is computed on the DataFrame being assigned to. Importantly, this is the DataFrame that\u2019s been filtered to those rows with sepal length greater than 5. The filtering happens first, and then the ratio calculations. This is an example where we didn\u2019t have a reference to the filtered DataFrame available.", "The function signature for assign is simply **kwargs. The keys are the column names for the new fields, and the values are either a value to be inserted (for example, a Series or NumPy array), or a function of one argument to be called on the DataFrame. A copy of the original DataFrame is returned, with the new values inserted.", "Starting with Python 3.6 the order of **kwargs is preserved. This allows for dependent assignment, where an expression later in **kwargs can refer to a column created earlier in the same assign().", "In the second expression, x['C'] will refer to the newly created column, that\u2019s equal to dfa['A'] + dfa['B'].", "The basics of indexing are as follows:", "Operation", "Syntax", "Result", "Select column", "df[col]", "Series", "Select row by label", "df.loc[label]", "Series", "Select row by integer location", "df.iloc[loc]", "Series", "Slice rows", "df[5:10]", "DataFrame", "Select rows by boolean vector", "df[bool_vec]", "DataFrame", "Row selection, for example, returns a Series whose index is the columns of the DataFrame:", "For a more exhaustive treatment of sophisticated label-based indexing and slicing, see the section on indexing. We will address the fundamentals of reindexing / conforming to new sets of labels in the section on reindexing.", "Data alignment between DataFrame objects automatically align on both the columns and the index (row labels). Again, the resulting object will have the union of the column and row labels.", "When doing an operation between DataFrame and Series, the default behavior is to align the Series index on the DataFrame columns, thus broadcasting row-wise. For example:", "For explicit control over the matching and broadcasting behavior, see the section on flexible binary operations.", "Operations with scalars are just as you would expect:", "Boolean operators work as well:", "To transpose, access the T attribute (also the transpose function), similar to an ndarray:", "Elementwise NumPy ufuncs (log, exp, sqrt, \u2026) and various other NumPy functions can be used with no issues on Series and DataFrame, assuming the data within are numeric:", "DataFrame is not intended to be a drop-in replacement for ndarray as its indexing semantics and data model are quite different in places from an n-dimensional array.", "Series implements __array_ufunc__, which allows it to work with NumPy\u2019s universal functions.", "The ufunc is applied to the underlying array in a Series.", "Changed in version 0.25.0: When multiple Series are passed to a ufunc, they are aligned before performing the operation.", "Like other parts of the library, pandas will automatically align labeled inputs as part of a ufunc with multiple inputs. For example, using numpy.remainder() on two Series with differently ordered labels will align before the operation.", "As usual, the union of the two indices is taken, and non-overlapping values are filled with missing values.", "When a binary ufunc is applied to a Series and Index, the Series implementation takes precedence and a Series is returned.", "NumPy ufuncs are safe to apply to Series backed by non-ndarray arrays, for example arrays.SparseArray (see Sparse calculation). If possible, the ufunc is applied without converting the underlying data to an ndarray.", "Very large DataFrames will be truncated to display them in the console. You can also get a summary using info(). (Here I am reading a CSV version of the baseball dataset from the plyr R package):", "However, using to_string will return a string representation of the DataFrame in tabular form, though it won\u2019t always fit the console width:", "Wide DataFrames will be printed across multiple rows by default:", "You can change how much to print on a single row by setting the display.width option:", "You can adjust the max width of the individual columns by setting display.max_colwidth", "You can also disable this feature via the expand_frame_repr option. This will print the table in one block.", "If a DataFrame column label is a valid Python variable name, the column can be accessed like an attribute:", "The columns are also connected to the IPython completion mechanism so they can be tab-completed:"]}, {"name": "IO tools (text, CSV, HDF5, \u2026)", "path": "user_guide/io", "type": "Manual", "text": ["The pandas I/O API is a set of top level reader functions accessed like pandas.read_csv() that generally return a pandas object. The corresponding writer functions are object methods that are accessed like DataFrame.to_csv(). Below is a table containing available readers and writers.", "Format Type", "Data Description", "Reader", "Writer", "text", "CSV", "read_csv", "to_csv", "text", "Fixed-Width Text File", "read_fwf", "text", "JSON", "read_json", "to_json", "text", "HTML", "read_html", "to_html", "text", "LaTeX", "Styler.to_latex", "text", "XML", "read_xml", "to_xml", "text", "Local clipboard", "read_clipboard", "to_clipboard", "binary", "MS Excel", "read_excel", "to_excel", "binary", "OpenDocument", "read_excel", "binary", "HDF5 Format", "read_hdf", "to_hdf", "binary", "Feather Format", "read_feather", "to_feather", "binary", "Parquet Format", "read_parquet", "to_parquet", "binary", "ORC Format", "read_orc", "binary", "Stata", "read_stata", "to_stata", "binary", "SAS", "read_sas", "binary", "SPSS", "read_spss", "binary", "Python Pickle Format", "read_pickle", "to_pickle", "SQL", "SQL", "read_sql", "to_sql", "SQL", "Google BigQuery", "read_gbq", "to_gbq", "Here is an informal performance comparison for some of these IO methods.", "Note", "For examples that use the StringIO class, make sure you import it with from io import StringIO for Python 3.", "The workhorse function for reading text files (a.k.a. flat files) is read_csv(). See the cookbook for some advanced strategies.", "read_csv() accepts the following common arguments:", "Either a path to a file (a str, pathlib.Path, or py:py._path.local.LocalPath), URL (including http, ftp, and S3 locations), or any object with a read() method (such as an open file or StringIO).", "Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python\u2019s builtin sniffer tool, csv.Sniffer. In addition, separators longer than 1 character and different from '\\s+' will be interpreted as regular expressions and will also force the use of the Python parsing engine. Note that regex delimiters are prone to ignoring quoted data. Regex example: '\\\\r\\\\t'.", "Alternative argument name for sep.", "Specifies whether or not whitespace (e.g. ' ' or '\\t') will be used as the delimiter. Equivalent to setting sep='\\s+'. If this option is set to True, nothing should be passed in for the delimiter parameter.", "Row number(s) to use as the column names, and the start of the data. Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first line of the file, if column names are passed explicitly then the behavior is identical to header=None. Explicitly pass header=0 to be able to replace existing names.", "The header can be a list of ints that specify row locations for a MultiIndex on the columns e.g. [0,1,3]. Intervening rows that are not specified will be skipped (e.g. 2 in this example is skipped). Note that this parameter ignores commented lines and empty lines if skip_blank_lines=True, so header=0 denotes the first line of data rather than the first line of the file.", "List of column names to use. If file contains no header row, then you should explicitly pass header=None. Duplicates in this list are not allowed.", "Column(s) to use as the row labels of the DataFrame, either given as string name or column index. If a sequence of int / str is given, a MultiIndex is used.", "Note: index_col=False can be used to force pandas to not use the first column as the index, e.g. when you have a malformed file with delimiters at the end of each line.", "The default value of None instructs pandas to guess. If the number of fields in the column header row is equal to the number of fields in the body of the data file, then a default index is used. If it is larger, then the first columns are used as index so that the remaining number of fields in the body are equal to the number of fields in the header.", "The first row after the header is used to determine the number of columns, which will go into the index. If the subsequent rows contain less columns than the first row, they are filled with NaN.", "This can be avoided through usecols. This ensures that the columns are taken as is and the trailing data are ignored.", "Return a subset of the columns. If list-like, all elements must either be positional (i.e. integer indices into the document columns) or strings that correspond to column names provided either by the user in names or inferred from the document header row(s). If names are given, the document header row(s) are not taken into account. For example, a valid list-like usecols parameter would be [0, 1, 2] or ['foo', 'bar', 'baz'].", "Element order is ignored, so usecols=[0, 1] is the same as [1, 0]. To instantiate a DataFrame from data with element order preserved use pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']] for columns in ['foo', 'bar'] order or pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']] for ['bar', 'foo'] order.", "If callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to True:", "Using this parameter results in much faster parsing time and lower memory usage when using the c engine. The Python engine loads the data first before deciding which columns to drop.", "If the parsed data only contains one column then return a Series.", "Deprecated since version 1.4.0: Append .squeeze(\"columns\") to the call to {func_name} to squeeze the data.", "Prefix to add to column numbers when no header, e.g. \u2018X\u2019 for X0, X1, \u2026", "Deprecated since version 1.4.0: Use a list comprehension on the DataFrame\u2019s columns after calling read_csv.", "Duplicate columns will be specified as \u2018X\u2019, \u2018X.1\u2019\u2026\u2019X.N\u2019, rather than \u2018X\u2019\u2026\u2019X\u2019. Passing in False will cause data to be overwritten if there are duplicate names in the columns.", "Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32} (unsupported with engine='python'). Use str or object together with suitable na_values settings to preserve and not interpret dtype.", "Parser engine to use. The C and pyarrow engines are faster, while the python engine is currently more feature-complete. Multithreading is currently only supported by the pyarrow engine.", "New in version 1.4.0: The \u201cpyarrow\u201d engine was added as an experimental engine, and some features are unsupported, or may not work correctly, with this engine.", "Dict of functions for converting values in certain columns. Keys can either be integers or column labels.", "Values to consider as True.", "Values to consider as False.", "Skip spaces after delimiter.", "Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.", "If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise:", "Number of lines at bottom of file to skip (unsupported with engine=\u2019c\u2019).", "Number of rows of file to read. Useful for reading pieces of large files.", "Internally process the file in chunks, resulting in lower memory use while parsing, but possibly mixed type inference. To ensure no mixed types either set False, or specify the type with the dtype parameter. Note that the entire file is read into a single DataFrame regardless, use the chunksize or iterator parameter to return the data in chunks. (Only valid with C parser)", "If a filepath is provided for filepath_or_buffer, map the file object directly onto memory and access the data directly from there. Using this option can improve performance because there is no longer any I/O overhead.", "Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. See na values const below for a list of the values interpreted as NaN by default.", "Whether or not to include the default NaN values when parsing the data. Depending on whether na_values is passed in, the behavior is as follows:", "If keep_default_na is True, and na_values are specified, na_values is appended to the default NaN values used for parsing.", "If keep_default_na is True, and na_values are not specified, only the default NaN values are used for parsing.", "If keep_default_na is False, and na_values are specified, only the NaN values specified na_values are used for parsing.", "If keep_default_na is False, and na_values are not specified, no strings will be parsed as NaN.", "Note that if na_filter is passed in as False, the keep_default_na and na_values parameters will be ignored.", "Detect missing value markers (empty strings and the value of na_values). In data without any NAs, passing na_filter=False can improve the performance of reading a large file.", "Indicate number of NA values placed in non-numeric columns.", "If True, skip over blank lines rather than interpreting as NaN values.", "If True -> try parsing the index.", "If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column.", "If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column.", "If {'foo': [1, 3]} -> parse columns 1, 3 as date and call result \u2018foo\u2019. A fast-path exists for iso8601-formatted dates.", "If True and parse_dates is enabled for a column, attempt to infer the datetime format to speed up the processing.", "If True and parse_dates specifies combining multiple columns then keep the original columns.", "Function to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments.", "DD/MM format dates, international and European format.", "If True, use a cache of unique, converted dates to apply the datetime conversion. May produce significant speed-up when parsing duplicate date strings, especially ones with timezone offsets.", "New in version 0.25.0.", "Return TextFileReader object for iteration or getting chunks with get_chunk().", "Return TextFileReader object for iteration. See iterating and chunking below.", "For on-the-fly decompression of on-disk data. If \u2018infer\u2019, then use gzip, bz2, zip, xz, or zstandard if filepath_or_buffer is path-like ending in \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, \u2018.zst\u2019, respectively, and no decompression otherwise. If using \u2018zip\u2019, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.", "Changed in version 1.1.0: dict option extended to support gzip and bz2.", "Changed in version 1.2.0: Previous versions forwarded dict entries for \u2018gzip\u2019 to gzip.open.", "Thousands separator.", "Character to recognize as decimal point. E.g. use ',' for European data.", "Specifies which converter the C engine should use for floating-point values. The options are None for the ordinary converter, high for the high-precision converter, and round_trip for the round-trip converter.", "Character to break file into lines. Only valid with C parser.", "The character used to denote the start and end of a quoted item. Quoted items can include the delimiter and it will be ignored.", "Control field quoting behavior per csv.QUOTE_* constants. Use one of QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).", "When quotechar is specified and quoting is not QUOTE_NONE, indicate whether or not to interpret two consecutive quotechar elements inside a field as a single quotechar element.", "One-character string used to escape delimiter when quoting is QUOTE_NONE.", "Indicates remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines (as long as skip_blank_lines=True), fully commented lines are ignored by the parameter header but not by skiprows. For example, if comment='#', parsing \u2018#empty\\na,b,c\\n1,2,3\u2019 with header=0 will result in \u2018a,b,c\u2019 being treated as the header.", "Encoding to use for UTF when reading/writing (e.g. 'utf-8'). List of Python standard encodings.", "If provided, this parameter will override values (default or not) for the following parameters: delimiter, doublequote, escapechar, skipinitialspace, quotechar, and quoting. If it is necessary to override values, a ParserWarning will be issued. See csv.Dialect documentation for more details.", "Lines with too many fields (e.g. a csv line with too many commas) will by default cause an exception to be raised, and no DataFrame will be returned. If False, then these \u201cbad lines\u201d will dropped from the DataFrame that is returned. See bad lines below.", "Deprecated since version 1.3.0: The on_bad_lines parameter should be used instead to specify behavior upon encountering a bad line instead.", "If error_bad_lines is False, and warn_bad_lines is True, a warning for each \u201cbad line\u201d will be output.", "Deprecated since version 1.3.0: The on_bad_lines parameter should be used instead to specify behavior upon encountering a bad line instead.", "Specifies what to do upon encountering a bad line (a line with too many fields). Allowed values are :", "\u2018error\u2019, raise an ParserError when a bad line is encountered.", "\u2018warn\u2019, print a warning when a bad line is encountered and skip that line.", "\u2018skip\u2019, skip bad lines without raising or warning when they are encountered.", "New in version 1.3.0.", "You can indicate the data type for the whole DataFrame or individual columns:", "Fortunately, pandas offers more than one way to ensure that your column(s) contain only one dtype. If you\u2019re unfamiliar with these concepts, you can see here to learn more about dtypes, and here to learn more about object conversion in pandas.", "For instance, you can use the converters argument of read_csv():", "Or you can use the to_numeric() function to coerce the dtypes after reading in the data,", "which will convert all valid parsing to floats, leaving the invalid parsing as NaN.", "Ultimately, how you deal with reading in columns containing mixed dtypes depends on your specific needs. In the case above, if you wanted to NaN out the data anomalies, then to_numeric() is probably your best option. However, if you wanted for all the data to be coerced, no matter the type, then using the converters argument of read_csv() would certainly be worth trying.", "Note", "In some cases, reading in abnormal data with columns containing mixed dtypes will result in an inconsistent dataset. If you rely on pandas to infer the dtypes of your columns, the parsing engine will go and infer the dtypes for different chunks of the data, rather than the whole dataset at once. Consequently, you can end up with column(s) with mixed dtypes. For example,", "will result with mixed_df containing an int dtype for certain chunks of the column, and str for others due to the mixed dtypes from the data that was read in. It is important to note that the overall column will be marked with a dtype of object, which is used for columns with mixed dtypes.", "Categorical columns can be parsed directly by specifying dtype='category' or dtype=CategoricalDtype(categories, ordered).", "Individual columns can be parsed as a Categorical using a dict specification:", "Specifying dtype='category' will result in an unordered Categorical whose categories are the unique values observed in the data. For more control on the categories and order, create a CategoricalDtype ahead of time, and pass that for that column\u2019s dtype.", "When using dtype=CategoricalDtype, \u201cunexpected\u201d values outside of dtype.categories are treated as missing values.", "This matches the behavior of Categorical.set_categories().", "Note", "With dtype='category', the resulting categories will always be parsed as strings (object dtype). If the categories are numeric they can be converted using the to_numeric() function, or as appropriate, another converter such as to_datetime().", "When dtype is a CategoricalDtype with homogeneous categories ( all numeric, all datetimes, etc.), the conversion is done automatically.", "A file may or may not have a header row. pandas assumes the first row should be used as the column names:", "By specifying the names argument in conjunction with header you can indicate other names to use and whether or not to throw away the header row (if any):", "If the header is in a row other than the first, pass the row number to header. This will skip the preceding rows:", "Note", "Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first non-blank line of the file, if column names are passed explicitly then the behavior is identical to header=None.", "If the file or header contains duplicate names, pandas will by default distinguish between them so as to prevent overwriting data:", "There is no more duplicate data because mangle_dupe_cols=True by default, which modifies a series of duplicate columns \u2018X\u2019, \u2026, \u2018X\u2019 to become \u2018X\u2019, \u2018X.1\u2019, \u2026, \u2018X.N\u2019. If mangle_dupe_cols=False, duplicate data can arise:", "To prevent users from encountering this problem with duplicate data, a ValueError exception is raised if mangle_dupe_cols != True:", "The usecols argument allows you to select any subset of the columns in a file, either using the column names, position numbers or a callable:", "The usecols argument can also be used to specify which columns not to use in the final result:", "In this case, the callable is specifying that we exclude the \u201ca\u201d and \u201cc\u201d columns from the output.", "If the comment parameter is specified, then completely commented lines will be ignored. By default, completely blank lines will be ignored as well.", "If skip_blank_lines=False, then read_csv will not ignore blank lines:", "Warning", "The presence of ignored lines might create ambiguities involving line numbers; the parameter header uses row numbers (ignoring commented/empty lines), while skiprows uses line numbers (including commented/empty lines):", "If both header and skiprows are specified, header will be relative to the end of skiprows. For example:", "Sometimes comments or meta data may be included in a file:", "By default, the parser includes the comments in the output:", "We can suppress the comments using the comment keyword:", "The encoding argument should be used for encoded unicode data, which will result in byte strings being decoded to unicode in the result:", "Some formats which encode all characters as multiple bytes, like UTF-16, won\u2019t parse correctly at all without specifying the encoding. Full list of Python standard encodings.", "If a file has one more column of data than the number of column names, the first column will be used as the DataFrame\u2019s row names:", "Ordinarily, you can achieve this behavior using the index_col option.", "There are some exception cases when a file has been prepared with delimiters at the end of each data line, confusing the parser. To explicitly disable the index column inference and discard the last column, pass index_col=False:", "If a subset of data is being parsed using the usecols option, the index_col specification is based on that subset, not the original data.", "To better facilitate working with datetime data, read_csv() uses the keyword arguments parse_dates and date_parser to allow users to specify a variety of columns and date/time formats to turn the input text data into datetime objects.", "The simplest case is to just pass in parse_dates=True:", "It is often the case that we may want to store date and time data separately, or store various date fields separately. the parse_dates keyword can be used to specify a combination of columns to parse the dates and/or times from.", "You can specify a list of column lists to parse_dates, the resulting date columns will be prepended to the output (so as to not affect the existing column order) and the new column names will be the concatenation of the component column names:", "By default the parser removes the component date columns, but you can choose to retain them via the keep_date_col keyword:", "Note that if you wish to combine multiple columns into a single date column, a nested list must be used. In other words, parse_dates=[1, 2] indicates that the second and third columns should each be parsed as separate date columns while parse_dates=[[1, 2]] means the two columns should be parsed into a single column.", "You can also use a dict to specify custom name columns:", "It is important to remember that if multiple text columns are to be parsed into a single date column, then a new column is prepended to the data. The index_col specification is based off of this new set of columns rather than the original data columns:", "Note", "If a column or index contains an unparsable date, the entire column or index will be returned unaltered as an object data type. For non-standard datetime parsing, use to_datetime() after pd.read_csv.", "Note", "read_csv has a fast_path for parsing datetime strings in iso8601 format, e.g \u201c2000-01-01T00:01:02+00:00\u201d and similar variations. If you can arrange for your data to store datetimes in this format, load times will be significantly faster, ~20x has been observed.", "Finally, the parser allows you to specify a custom date_parser function to take full advantage of the flexibility of the date parsing API:", "pandas will try to call the date_parser function in three different ways. If an exception is raised, the next one is tried:", "date_parser is first called with one or more arrays as arguments, as defined using parse_dates (e.g., date_parser(['2013', '2013'], ['1', '2'])).", "If #1 fails, date_parser is called with all the columns concatenated row-wise into a single array (e.g., date_parser(['2013 1', '2013 2'])).", "Note that performance-wise, you should try these methods of parsing dates in order:", "Try to infer the format using infer_datetime_format=True (see section below).", "If you know the format, use pd.to_datetime(): date_parser=lambda x: pd.to_datetime(x, format=...).", "If you have a really non-standard format, use a custom date_parser function. For optimal performance, this should be vectorized, i.e., it should accept arrays as arguments.", "pandas cannot natively represent a column or index with mixed timezones. If your CSV file contains columns with a mixture of timezones, the default result will be an object-dtype column with strings, even with parse_dates.", "To parse the mixed-timezone values as a datetime column, pass a partially-applied to_datetime() with utc=True as the date_parser.", "If you have parse_dates enabled for some or all of your columns, and your datetime strings are all formatted the same way, you may get a large speed up by setting infer_datetime_format=True. If set, pandas will attempt to guess the format of your datetime strings, and then use a faster means of parsing the strings. 5-10x parsing speeds have been observed. pandas will fallback to the usual parsing if either the format cannot be guessed or the format that was guessed cannot properly parse the entire column of strings. So in general, infer_datetime_format should not have any negative consequences if enabled.", "Here are some examples of datetime strings that can be guessed (All representing December 30th, 2011 at 00:00:00):", "\u201c20111230\u201d", "\u201c2011/12/30\u201d", "\u201c20111230 00:00:00\u201d", "\u201c12/30/2011 00:00:00\u201d", "\u201c30/Dec/2011 00:00:00\u201d", "\u201c30/December/2011 00:00:00\u201d", "Note that infer_datetime_format is sensitive to dayfirst. With dayfirst=True, it will guess \u201c01/12/2011\u201d to be December 1st. With dayfirst=False (default) it will guess \u201c01/12/2011\u201d to be January 12th.", "While US date formats tend to be MM/DD/YYYY, many international formats use DD/MM/YYYY instead. For convenience, a dayfirst keyword is provided:", "New in version 1.2.0.", "df.to_csv(..., mode=\"wb\") allows writing a CSV to a file object opened binary mode. In most cases, it is not necessary to specify mode as Pandas will auto-detect whether the file object is opened in text or binary mode.", "The parameter float_precision can be specified in order to use a specific floating-point converter during parsing with the C engine. The options are the ordinary converter, the high-precision converter, and the round-trip converter (which is guaranteed to round-trip values after writing to a file). For example:", "For large numbers that have been written with a thousands separator, you can set the thousands keyword to a string of length 1 so that integers will be parsed correctly:", "By default, numbers with a thousands separator will be parsed as strings:", "The thousands keyword allows integers to be parsed correctly:", "To control which values are parsed as missing values (which are signified by NaN), specify a string in na_values. If you specify a list of strings, then all values in it are considered to be missing values. If you specify a number (a float, like 5.0 or an integer like 5), the corresponding equivalent values will also imply a missing value (in this case effectively [5.0, 5] are recognized as NaN).", "To completely override the default values that are recognized as missing, specify keep_default_na=False.", "The default NaN recognized values are ['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A N/A', '#N/A', 'N/A',\n'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN', '-NaN', 'nan', '-nan', ''].", "Let us consider some examples:", "In the example above 5 and 5.0 will be recognized as NaN, in addition to the defaults. A string will first be interpreted as a numerical 5, then as a NaN.", "Above, only an empty field will be recognized as NaN.", "Above, both NA and 0 as strings are NaN.", "The default values, in addition to the string \"Nope\" are recognized as NaN.", "inf like values will be parsed as np.inf (positive infinity), and -inf as -np.inf (negative infinity). These will ignore the case of the value, meaning Inf, will also be parsed as np.inf.", "Using the squeeze keyword, the parser will return output with a single column as a Series:", "Deprecated since version 1.4.0: Users should append .squeeze(\"columns\") to the DataFrame returned by read_csv instead.", "The common values True, False, TRUE, and FALSE are all recognized as boolean. Occasionally you might want to recognize other values as being boolean. To do this, use the true_values and false_values options as follows:", "Some files may have malformed lines with too few fields or too many. Lines with too few fields will have NA values filled in the trailing fields. Lines with too many fields will raise an error by default:", "You can elect to skip bad lines:", "Or pass a callable function to handle the bad line if engine=\"python\". The bad line will be a list of strings that was split by the sep:", "You can also use the usecols parameter to eliminate extraneous column data that appear in some lines but not others:", "In case you want to keep all data including the lines with too many fields, you can specify a sufficient number of names. This ensures that lines with not enough fields are filled with NaN.", "The dialect keyword gives greater flexibility in specifying the file format. By default it uses the Excel dialect but you can specify either the dialect name or a csv.Dialect instance.", "Suppose you had data with unenclosed quotes:", "By default, read_csv uses the Excel dialect and treats the double quote as the quote character, which causes it to fail when it finds a newline before it finds the closing double quote.", "We can get around this using dialect:", "All of the dialect options can be specified separately by keyword arguments:", "Another common dialect option is skipinitialspace, to skip any whitespace after a delimiter:", "The parsers make every attempt to \u201cdo the right thing\u201d and not be fragile. Type inference is a pretty big deal. If a column can be coerced to integer dtype without altering the contents, the parser will do so. Any non-numeric columns will come through as object dtype as with the rest of pandas objects.", "Quotes (and other escape characters) in embedded fields can be handled in any number of ways. One way is to use backslashes; to properly parse this data, you should pass the escapechar option:", "While read_csv() reads delimited data, the read_fwf() function works with data files that have known and fixed column widths. The function parameters to read_fwf are largely the same as read_csv with two extra parameters, and a different usage of the delimiter parameter:", "colspecs: A list of pairs (tuples) giving the extents of the fixed-width fields of each line as half-open intervals (i.e., [from, to[ ). String value \u2018infer\u2019 can be used to instruct the parser to try detecting the column specifications from the first 100 rows of the data. Default behavior, if not specified, is to infer.", "widths: A list of field widths which can be used instead of \u2018colspecs\u2019 if the intervals are contiguous.", "delimiter: Characters to consider as filler characters in the fixed-width file. Can be used to specify the filler character of the fields if it is not spaces (e.g., \u2018~\u2019).", "Consider a typical fixed-width data file:", "In order to parse this file into a DataFrame, we simply need to supply the column specifications to the read_fwf function along with the file name:", "Note how the parser automatically picks column names X.<column number> when header=None argument is specified. Alternatively, you can supply just the column widths for contiguous columns:", "The parser will take care of extra white spaces around the columns so it\u2019s ok to have extra separation between the columns in the file.", "By default, read_fwf will try to infer the file\u2019s colspecs by using the first 100 rows of the file. It can do it only in cases when the columns are aligned and correctly separated by the provided delimiter (default delimiter is whitespace).", "read_fwf supports the dtype parameter for specifying the types of parsed columns to be different from the inferred type.", "Consider a file with one less entry in the header than the number of data column:", "In this special case, read_csv assumes that the first column is to be used as the index of the DataFrame:", "Note that the dates weren\u2019t automatically parsed. In that case you would need to do as before:", "Suppose you have data indexed by two columns:", "The index_col argument to read_csv can take a list of column numbers to turn multiple columns into a MultiIndex for the index of the returned object:", "By specifying list of row locations for the header argument, you can read in a MultiIndex for the columns. Specifying non-consecutive rows will skip the intervening rows.", "read_csv is also able to interpret a more common format of multi-columns indices.", "Note: If an index_col is not specified (e.g. you don\u2019t have an index, or wrote it with df.to_csv(..., index=False), then any names on the columns index will be lost.", "read_csv is capable of inferring delimited (not necessarily comma-separated) files, as pandas uses the csv.Sniffer class of the csv module. For this, you have to specify sep=None.", "It\u2019s best to use concat() to combine multiple files. See the cookbook for an example.", "Suppose you wish to iterate through a (potentially very large) file lazily rather than reading the entire file into memory, such as the following:", "By specifying a chunksize to read_csv, the return value will be an iterable object of type TextFileReader:", "Changed in version 1.2: read_csv/json/sas return a context-manager when iterating through a file.", "Specifying iterator=True will also return the TextFileReader object:", "Pandas currently supports three engines, the C engine, the python engine, and an experimental pyarrow engine (requires the pyarrow package). In general, the pyarrow engine is fastest on larger workloads and is equivalent in speed to the C engine on most other workloads. The python engine tends to be slower than the pyarrow and C engines on most workloads. However, the pyarrow engine is much less robust than the C engine, which lacks a few features compared to the Python engine.", "Where possible, pandas uses the C parser (specified as engine='c'), but it may fall back to Python if C-unsupported options are specified.", "Currently, options unsupported by the C and pyarrow engines include:", "sep other than a single character (e.g. regex separators)", "skipfooter", "sep=None with delim_whitespace=False", "Specifying any of the above options will produce a ParserWarning unless the python engine is selected explicitly using engine='python'.", "Options that are unsupported by the pyarrow engine which are not covered by the list above include:", "float_precision", "chunksize", "comment", "nrows", "thousands", "memory_map", "dialect", "warn_bad_lines", "error_bad_lines", "on_bad_lines", "delim_whitespace", "quoting", "lineterminator", "converters", "decimal", "iterator", "dayfirst", "infer_datetime_format", "verbose", "skipinitialspace", "low_memory", "Specifying these options with engine='pyarrow' will raise a ValueError.", "You can pass in a URL to read or write remote files to many of pandas\u2019 IO functions - the following example shows reading a CSV file:", "New in version 1.3.0.", "A custom header can be sent alongside HTTP(s) requests by passing a dictionary of header key value mappings to the storage_options keyword argument as shown below:", "All URLs which are not local files or HTTP(s) are handled by fsspec, if installed, and its various filesystem implementations (including Amazon S3, Google Cloud, SSH, FTP, webHDFS\u2026). Some of these implementations will require additional packages to be installed, for example S3 URLs require the s3fs library:", "When dealing with remote storage systems, you might need extra configuration with environment variables or config files in special locations. For example, to access data in your S3 bucket, you will need to define credentials in one of the several ways listed in the S3Fs documentation. The same is true for several of the storage backends, and you should follow the links at fsimpl1 for implementations built into fsspec and fsimpl2 for those not included in the main fsspec distribution.", "You can also pass parameters directly to the backend driver. For example, if you do not have S3 credentials, you can still access public data by specifying an anonymous connection, such as", "New in version 1.2.0.", "fsspec also allows complex URLs, for accessing data in compressed archives, local caching of files, and more. To locally cache the above example, you would modify the call to", "where we specify that the \u201canon\u201d parameter is meant for the \u201cs3\u201d part of the implementation, not to the caching implementation. Note that this caches to a temporary directory for the duration of the session only, but you can also specify a permanent store.", "The Series and DataFrame objects have an instance method to_csv which allows storing the contents of the object as a comma-separated-values file. The function takes a number of arguments. Only the first is required.", "path_or_buf: A string path to the file to write or a file object. If a file object it must be opened with newline=''", "sep : Field delimiter for the output file (default \u201c,\u201d)", "na_rep: A string representation of a missing value (default \u2018\u2019)", "float_format: Format string for floating point numbers", "columns: Columns to write (default None)", "header: Whether to write out the column names (default True)", "index: whether to write row (index) names (default True)", "index_label: Column label(s) for index column(s) if desired. If None (default), and header and index are True, then the index names are used. (A sequence should be given if the DataFrame uses MultiIndex).", "mode : Python write mode, default \u2018w\u2019", "encoding: a string representing the encoding to use if the contents are non-ASCII, for Python versions prior to 3", "line_terminator: Character sequence denoting line end (default os.linesep)", "quoting: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL). Note that if you have set a float_format then floats are converted to strings and csv.QUOTE_NONNUMERIC will treat them as non-numeric", "quotechar: Character used to quote fields (default \u2018\u201d\u2019)", "doublequote: Control quoting of quotechar in fields (default True)", "escapechar: Character used to escape sep and quotechar when appropriate (default None)", "chunksize: Number of rows to write at a time", "date_format: Format string for datetime objects", "The DataFrame object has an instance method to_string which allows control over the string representation of the object. All arguments are optional:", "buf default None, for example a StringIO object", "columns default None, which columns to write", "col_space default None, minimum width of each column.", "na_rep default NaN, representation of NA value", "formatters default None, a dictionary (by column) of functions each of which takes a single argument and returns a formatted string", "float_format default None, a function which takes a single (float) argument and returns a formatted string; to be applied to floats in the DataFrame.", "sparsify default True, set to False for a DataFrame with a hierarchical index to print every MultiIndex key at each row.", "index_names default True, will print the names of the indices", "index default True, will print the index (ie, row labels)", "header default True, will print the column labels", "justify default left, will print column headers left- or right-justified", "The Series object also has a to_string method, but with only the buf, na_rep, float_format arguments. There is also a length argument which, if set to True, will additionally output the length of the Series.", "Read and write JSON format files and strings.", "A Series or DataFrame can be converted to a valid JSON string. Use to_json with optional parameters:", "path_or_buf : the pathname or buffer to write the output This can be None in which case a JSON string is returned", "orient :", "default is index", "allowed values are {split, records, index}", "default is columns", "allowed values are {split, records, index, columns, values, table}", "The format of the JSON string", "split", "dict like {index -> [index], columns -> [columns], data -> [values]}", "records", "list like [{column -> value}, \u2026 , {column -> value}]", "index", "dict like {index -> {column -> value}}", "columns", "dict like {column -> {index -> value}}", "values", "just the values array", "table", "adhering to the JSON Table Schema", "date_format : string, type of date conversion, \u2018epoch\u2019 for timestamp, \u2018iso\u2019 for ISO8601.", "double_precision : The number of decimal places to use when encoding floating point values, default 10.", "force_ascii : force encoded string to be ASCII, default True.", "date_unit : The time unit to encode to, governs timestamp and ISO8601 precision. One of \u2018s\u2019, \u2018ms\u2019, \u2018us\u2019 or \u2018ns\u2019 for seconds, milliseconds, microseconds and nanoseconds respectively. Default \u2018ms\u2019.", "default_handler : The handler to call if an object cannot otherwise be converted to a suitable format for JSON. Takes a single argument, which is the object to convert, and returns a serializable object.", "lines : If records orient, then will write each record per line as json.", "Note NaN\u2019s, NaT\u2019s and None will be converted to null and datetime objects will be converted based on the date_format and date_unit parameters.", "There are a number of different options for the format of the resulting JSON file / string. Consider the following DataFrame and Series:", "Column oriented (the default for DataFrame) serializes the data as nested JSON objects with column labels acting as the primary index:", "Index oriented (the default for Series) similar to column oriented but the index labels are now primary:", "Record oriented serializes the data to a JSON array of column -> value records, index labels are not included. This is useful for passing DataFrame data to plotting libraries, for example the JavaScript library d3.js:", "Value oriented is a bare-bones option which serializes to nested JSON arrays of values only, column and index labels are not included:", "Split oriented serializes to a JSON object containing separate entries for values, index and columns. Name is also included for Series:", "Table oriented serializes to the JSON Table Schema, allowing for the preservation of metadata including but not limited to dtypes and index names.", "Note", "Any orient option that encodes to a JSON object will not preserve the ordering of index and column labels during round-trip serialization. If you wish to preserve label ordering use the split option as it uses ordered containers.", "Writing in ISO date format:", "Writing in ISO date format, with microseconds:", "Epoch timestamps, in seconds:", "Writing to a file, with a date index and a date column:", "If the JSON serializer cannot handle the container contents directly it will fall back in the following manner:", "if the dtype is unsupported (e.g. np.complex_) then the default_handler, if provided, will be called for each value, otherwise an exception is raised.", "if an object is unsupported it will attempt the following:", "check if the object has defined a toDict method and call it. A toDict method should return a dict which will then be JSON serialized.", "invoke the default_handler if one was provided.", "convert the object to a dict by traversing its contents. However this will often fail with an OverflowError or give unexpected results.", "In general the best approach for unsupported objects or dtypes is to provide a default_handler. For example:", "can be dealt with by specifying a simple default_handler:", "Reading a JSON string to pandas object can take a number of parameters. The parser will try to parse a DataFrame if typ is not supplied or is None. To explicitly force Series parsing, pass typ=series", "filepath_or_buffer : a VALID JSON string or file handle / StringIO. The string could be a URL. Valid URL schemes include http, ftp, S3, and file. For file URLs, a host is expected. For instance, a local file could be file ://localhost/path/to/table.json", "typ : type of object to recover (series or frame), default \u2018frame\u2019", "orient :", "default is index", "allowed values are {split, records, index}", "default is columns", "allowed values are {split, records, index, columns, values, table}", "The format of the JSON string", "split", "dict like {index -> [index], columns -> [columns], data -> [values]}", "records", "list like [{column -> value}, \u2026 , {column -> value}]", "index", "dict like {index -> {column -> value}}", "columns", "dict like {column -> {index -> value}}", "values", "just the values array", "table", "adhering to the JSON Table Schema", "dtype : if True, infer dtypes, if a dict of column to dtype, then use those, if False, then don\u2019t infer dtypes at all, default is True, apply only to the data.", "convert_axes : boolean, try to convert the axes to the proper dtypes, default is True", "convert_dates : a list of columns to parse for dates; If True, then try to parse date-like columns, default is True.", "keep_default_dates : boolean, default True. If parsing dates, then parse the default date-like columns.", "numpy : direct decoding to NumPy arrays. default is False; Supports numeric data only, although labels may be non-numeric. Also note that the JSON ordering MUST be the same for each term if numpy=True.", "precise_float : boolean, default False. Set to enable usage of higher precision (strtod) function when decoding string to double values. Default (False) is to use fast but less precise builtin functionality.", "date_unit : string, the timestamp unit to detect if converting dates. Default None. By default the timestamp precision will be detected, if this is not desired then pass one of \u2018s\u2019, \u2018ms\u2019, \u2018us\u2019 or \u2018ns\u2019 to force timestamp precision to seconds, milliseconds, microseconds or nanoseconds respectively.", "lines : reads file as one json object per line.", "encoding : The encoding to use to decode py3 bytes.", "chunksize : when used in combination with lines=True, return a JsonReader which reads in chunksize lines per iteration.", "The parser will raise one of ValueError/TypeError/AssertionError if the JSON is not parseable.", "If a non-default orient was used when encoding to JSON be sure to pass the same option here so that decoding produces sensible results, see Orient Options for an overview.", "The default of convert_axes=True, dtype=True, and convert_dates=True will try to parse the axes, and all of the data into appropriate types, including dates. If you need to override specific dtypes, pass a dict to dtype. convert_axes should only be set to False if you need to preserve string-like numbers (e.g. \u20181\u2019, \u20182\u2019) in an axes.", "Note", "Large integer values may be converted to dates if convert_dates=True and the data and / or column labels appear \u2018date-like\u2019. The exact threshold depends on the date_unit specified. \u2018date-like\u2019 means that the column label meets one of the following criteria:", "it ends with '_at'", "it ends with '_time'", "it begins with 'timestamp'", "it is 'modified'", "it is 'date'", "Warning", "When reading JSON data, automatic coercing into dtypes has some quirks:", "an index can be reconstructed in a different order from serialization, that is, the returned order is not guaranteed to be the same as before serialization", "a column that was float data will be converted to integer if it can be done safely, e.g. a column of 1.", "bool columns will be converted to integer on reconstruction", "Thus there are times where you may want to specify specific dtypes via the dtype keyword argument.", "Reading from a JSON string:", "Reading from a file:", "Don\u2019t convert any data (but still convert axes and dates):", "Specify dtypes for conversion:", "Preserve string indices:", "Dates written in nanoseconds need to be read back in nanoseconds:", "Note", "This param has been deprecated as of version 1.0.0 and will raise a FutureWarning.", "This supports numeric data only. Index and columns labels may be non-numeric, e.g. strings, dates etc.", "If numpy=True is passed to read_json an attempt will be made to sniff an appropriate dtype during deserialization and to subsequently decode directly to NumPy arrays, bypassing the need for intermediate Python objects.", "This can provide speedups if you are deserialising a large amount of numeric data:", "The speedup is less noticeable for smaller datasets:", "Warning", "Direct NumPy decoding makes a number of assumptions and may fail or produce unexpected output if these assumptions are not satisfied:", "data is numeric.", "data is uniform. The dtype is sniffed from the first value decoded. A ValueError may be raised, or incorrect output may be produced if this condition is not satisfied.", "labels are ordered. Labels are only read from the first container, it is assumed that each subsequent row / column has been encoded in the same order. This should be satisfied if the data was encoded using to_json but may not be the case if the JSON is from another source.", "pandas provides a utility function to take a dict or list of dicts and normalize this semi-structured data into a flat table.", "The max_level parameter provides more control over which level to end normalization. With max_level=1 the following snippet normalizes until 1st nesting level of the provided dict.", "pandas is able to read and write line-delimited json files that are common in data processing pipelines using Hadoop or Spark.", "For line-delimited json files, pandas can also return an iterator which reads in chunksize lines at a time. This can be useful for large files or to read from a stream.", "Table Schema is a spec for describing tabular datasets as a JSON object. The JSON includes information on the field names, types, and other attributes. You can use the orient table to build a JSON string with two fields, schema and data.", "The schema field contains the fields key, which itself contains a list of column name to type pairs, including the Index or MultiIndex (see below for a list of types). The schema field also contains a primaryKey field if the (Multi)index is unique.", "The second field, data, contains the serialized data with the records orient. The index is included, and any datetimes are ISO 8601 formatted, as required by the Table Schema spec.", "The full list of types supported are described in the Table Schema spec. This table shows the mapping from pandas types:", "pandas type", "Table Schema type", "int64", "integer", "float64", "number", "bool", "boolean", "datetime64[ns]", "datetime", "timedelta64[ns]", "duration", "categorical", "any", "object", "str", "A few notes on the generated table schema:", "The schema object contains a pandas_version field. This contains the version of pandas\u2019 dialect of the schema, and will be incremented with each revision.", "All dates are converted to UTC when serializing. Even timezone naive values, which are treated as UTC with an offset of 0.", "datetimes with a timezone (before serializing), include an additional field tz with the time zone name (e.g. 'US/Central').", "Periods are converted to timestamps before serialization, and so have the same behavior of being converted to UTC. In addition, periods will contain and additional field freq with the period\u2019s frequency, e.g. 'A-DEC'.", "Categoricals use the any type and an enum constraint listing the set of possible values. Additionally, an ordered field is included:", "A primaryKey field, containing an array of labels, is included if the index is unique:", "The primaryKey behavior is the same with MultiIndexes, but in this case the primaryKey is an array:", "The default naming roughly follows these rules:", "For series, the object.name is used. If that\u2019s none, then the name is values", "For DataFrames, the stringified version of the column name is used", "For Index (not MultiIndex), index.name is used, with a fallback to index if that is None.", "For MultiIndex, mi.names is used. If any level has no name, then level_<i> is used.", "read_json also accepts orient='table' as an argument. This allows for the preservation of metadata such as dtypes and index names in a round-trippable manner.", "Please note that the literal string \u2018index\u2019 as the name of an Index is not round-trippable, nor are any names beginning with 'level_' within a MultiIndex. These are used by default in DataFrame.to_json() to indicate missing values and the subsequent read cannot distinguish the intent.", "When using orient='table' along with user-defined ExtensionArray, the generated schema will contain an additional extDtype key in the respective fields element. This extra key is not standard but does enable JSON roundtrips for extension types (e.g. read_json(df.to_json(orient=\"table\"), orient=\"table\")).", "The extDtype key carries the name of the extension, if you have properly registered the ExtensionDtype, pandas will use said name to perform a lookup into the registry and re-convert the serialized data into your custom dtype.", "Warning", "We highly encourage you to read the HTML Table Parsing gotchas below regarding the issues surrounding the BeautifulSoup4/html5lib/lxml parsers.", "The top-level read_html() function can accept an HTML string/file/URL and will parse HTML tables into list of pandas DataFrames. Let\u2019s look at a few examples.", "Note", "read_html returns a list of DataFrame objects, even if there is only a single table contained in the HTML content.", "Read a URL with no options:", "Note", "The data from the above URL changes every Monday so the resulting data above and the data below may be slightly different.", "Read in the content of the file from the above URL and pass it to read_html as a string:", "You can even pass in an instance of StringIO if you so desire:", "Note", "The following examples are not run by the IPython evaluator due to the fact that having so many network-accessing functions slows down the documentation build. If you spot an error or an example that doesn\u2019t run, please do not hesitate to report it over on pandas GitHub issues page.", "Read a URL and match a table that contains specific text:", "Specify a header row (by default <th> or <td> elements located within a <thead> are used to form the column index, if multiple rows are contained within <thead> then a MultiIndex is created); if specified, the header row is taken from the data minus the parsed header elements (<th> elements).", "Specify an index column:", "Specify a number of rows to skip:", "Specify a number of rows to skip using a list (range works as well):", "Specify an HTML attribute:", "Specify values that should be converted to NaN:", "Specify whether to keep the default set of NaN values:", "Specify converters for columns. This is useful for numerical text data that has leading zeros. By default columns that are numerical are cast to numeric types and the leading zeros are lost. To avoid this, we can convert these columns to strings.", "Use some combination of the above:", "Read in pandas to_html output (with some loss of floating point precision):", "The lxml backend will raise an error on a failed parse if that is the only parser you provide. If you only have a single parser you can provide just a string, but it is considered good practice to pass a list with one string if, for example, the function expects a sequence of strings. You may use:", "Or you could pass flavor='lxml' without a list:", "However, if you have bs4 and html5lib installed and pass None or ['lxml',\n'bs4'] then the parse will most likely succeed. Note that as soon as a parse succeeds, the function will return.", "DataFrame objects have an instance method to_html which renders the contents of the DataFrame as an HTML table. The function arguments are as in the method to_string described above.", "Note", "Not all of the possible options for DataFrame.to_html are shown here for brevity\u2019s sake. See to_html() for the full set of options.", "HTML:", "The columns argument will limit the columns shown:", "HTML:", "float_format takes a Python callable to control the precision of floating point values:", "HTML:", "bold_rows will make the row labels bold by default, but you can turn that off:", "The classes argument provides the ability to give the resulting HTML table CSS classes. Note that these classes are appended to the existing 'dataframe' class.", "The render_links argument provides the ability to add hyperlinks to cells that contain URLs.", "HTML:", "Finally, the escape argument allows you to control whether the \u201c<\u201d, \u201c>\u201d and \u201c&\u201d characters escaped in the resulting HTML (by default it is True). So to get the HTML without escaped characters pass escape=False", "Escaped:", "Not escaped:", "Note", "Some browsers may not show a difference in the rendering of the previous two HTML tables.", "There are some versioning issues surrounding the libraries that are used to parse HTML tables in the top-level pandas io function read_html.", "Issues with lxml", "Benefits", "lxml is very fast.", "lxml requires Cython to install correctly.", "Drawbacks", "lxml does not make any guarantees about the results of its parse unless it is given strictly valid markup.", "In light of the above, we have chosen to allow you, the user, to use the lxml backend, but this backend will use html5lib if lxml fails to parse", "It is therefore highly recommended that you install both BeautifulSoup4 and html5lib, so that you will still get a valid result (provided everything else is valid) even if lxml fails.", "Issues with BeautifulSoup4 using lxml as a backend", "The above issues hold here as well since BeautifulSoup4 is essentially just a wrapper around a parser backend.", "Issues with BeautifulSoup4 using html5lib as a backend", "Benefits", "html5lib is far more lenient than lxml and consequently deals with real-life markup in a much saner way rather than just, e.g., dropping an element without notifying you.", "html5lib generates valid HTML5 markup from invalid markup automatically. This is extremely important for parsing HTML tables, since it guarantees a valid document. However, that does NOT mean that it is \u201ccorrect\u201d, since the process of fixing markup does not have a single definition.", "html5lib is pure Python and requires no additional build steps beyond its own installation.", "Drawbacks", "The biggest drawback to using html5lib is that it is slow as molasses. However consider the fact that many tables on the web are not big enough for the parsing algorithm runtime to matter. It is more likely that the bottleneck will be in the process of reading the raw text from the URL over the web, i.e., IO (input-output). For very large tables, this might not be true.", "New in version 1.3.0.", "Currently there are no methods to read from LaTeX, only output methods.", "Note", "DataFrame and Styler objects currently have a to_latex method. We recommend using the Styler.to_latex() method over DataFrame.to_latex() due to the former\u2019s greater flexibility with conditional styling, and the latter\u2019s possible future deprecation.", "Review the documentation for Styler.to_latex, which gives examples of conditional styling and explains the operation of its keyword arguments.", "For simple application the following pattern is sufficient.", "To format values before output, chain the Styler.format method.", "New in version 1.3.0.", "The top-level read_xml() function can accept an XML string/file/URL and will parse nodes and attributes into a pandas DataFrame.", "Note", "Since there is no standard XML structure where design types can vary in many ways, read_xml works best with flatter, shallow versions. If an XML document is deeply nested, use the stylesheet feature to transform XML into a flatter version.", "Let\u2019s look at a few examples.", "Read an XML string:", "Read a URL with no options:", "Read in the content of the \u201cbooks.xml\u201d file and pass it to read_xml as a string:", "Read in the content of the \u201cbooks.xml\u201d as instance of StringIO or BytesIO and pass it to read_xml:", "Even read XML from AWS S3 buckets such as Python Software Foundation\u2019s IRS 990 Form:", "With lxml as default parser, you access the full-featured XML library that extends Python\u2019s ElementTree API. One powerful tool is ability to query nodes selectively or conditionally with more expressive XPath:", "Specify only elements or only attributes to parse:", "XML documents can have namespaces with prefixes and default namespaces without prefixes both of which are denoted with a special attribute xmlns. In order to parse by node under a namespace context, xpath must reference a prefix.", "For example, below XML contains a namespace with prefix, doc, and URI at https://example.com. In order to parse doc:row nodes, namespaces must be used.", "Similarly, an XML document can have a default namespace without prefix. Failing to assign a temporary prefix will return no nodes and raise a ValueError. But assigning any temporary name to correct URI allows parsing by nodes.", "However, if XPath does not reference node names such as default, /*, then namespaces is not required.", "With lxml as parser, you can flatten nested XML documents with an XSLT script which also can be string/file/URL types. As background, XSLT is a special-purpose language written in a special XML file that can transform original XML documents into other XML, HTML, even text (CSV, JSON, etc.) using an XSLT processor.", "For example, consider this somewhat nested structure of Chicago \u201cL\u201d Rides where station and rides elements encapsulate data in their own sections. With below XSLT, lxml can transform original nested document into a flatter output (as shown below for demonstration) for easier parse into DataFrame:", "New in version 1.3.0.", "DataFrame objects have an instance method to_xml which renders the contents of the DataFrame as an XML document.", "Note", "This method does not support special properties of XML including DTD, CData, XSD schemas, processing instructions, comments, and others. Only namespaces at the root level is supported. However, stylesheet allows design changes after initial output.", "Let\u2019s look at a few examples.", "Write an XML without options:", "Write an XML with new root and row name:", "Write an attribute-centric XML:", "Write a mix of elements and attributes:", "Any DataFrames with hierarchical columns will be flattened for XML element names with levels delimited by underscores:", "Write an XML with default namespace:", "Write an XML with namespace prefix:", "Write an XML without declaration or pretty print:", "Write an XML and transform with stylesheet:", "All XML documents adhere to W3C specifications. Both etree and lxml parsers will fail to parse any markup document that is not well-formed or follows XML syntax rules. Do be aware HTML is not an XML document unless it follows XHTML specs. However, other popular markup types including KML, XAML, RSS, MusicML, MathML are compliant XML schemas.", "For above reason, if your application builds XML prior to pandas operations, use appropriate DOM libraries like etree and lxml to build the necessary document and not by string concatenation or regex adjustments. Always remember XML is a special text file with markup rules.", "With very large XML files (several hundred MBs to GBs), XPath and XSLT can become memory-intensive operations. Be sure to have enough available RAM for reading and writing to large XML files (roughly about 5 times the size of text).", "Because XSLT is a programming language, use it with caution since such scripts can pose a security risk in your environment and can run large or infinite recursive operations. Always test scripts on small fragments before full run.", "The etree parser supports all functionality of both read_xml and to_xml except for complex XPath and any XSLT. Though limited in features, etree is still a reliable and capable parser and tree builder. Its performance may trail lxml to a certain degree for larger files but relatively unnoticeable on small to medium size files.", "The read_excel() method can read Excel 2007+ (.xlsx) files using the openpyxl Python module. Excel 2003 (.xls) files can be read using xlrd. Binary Excel (.xlsb) files can be read using pyxlsb. The to_excel() instance method is used for saving a DataFrame to Excel. Generally the semantics are similar to working with csv data. See the cookbook for some advanced strategies.", "Warning", "The xlwt package for writing old-style .xls excel files is no longer maintained. The xlrd package is now only for reading old-style .xls files.", "Before pandas 1.3.0, the default argument engine=None to read_excel() would result in using the xlrd engine in many cases, including new Excel 2007+ (.xlsx) files. pandas will now default to using the openpyxl engine.", "It is strongly encouraged to install openpyxl to read Excel 2007+ (.xlsx) files. Please do not report issues when using ``xlrd`` to read ``.xlsx`` files. This is no longer supported, switch to using openpyxl instead.", "Attempting to use the the xlwt engine will raise a FutureWarning unless the option io.excel.xls.writer is set to \"xlwt\". While this option is now deprecated and will also raise a FutureWarning, it can be globally set and the warning suppressed. Users are recommended to write .xlsx files using the openpyxl engine instead.", "In the most basic use-case, read_excel takes a path to an Excel file, and the sheet_name indicating which sheet to parse.", "To facilitate working with multiple sheets from the same file, the ExcelFile class can be used to wrap the file and can be passed into read_excel There will be a performance benefit for reading multiple sheets as the file is read into memory only once.", "The ExcelFile class can also be used as a context manager.", "The sheet_names property will generate a list of the sheet names in the file.", "The primary use-case for an ExcelFile is parsing multiple sheets with different parameters:", "Note that if the same parsing parameters are used for all sheets, a list of sheet names can simply be passed to read_excel with no loss in performance.", "ExcelFile can also be called with a xlrd.book.Book object as a parameter. This allows the user to control how the excel file is read. For example, sheets can be loaded on demand by calling xlrd.open_workbook() with on_demand=True.", "Note", "The second argument is sheet_name, not to be confused with ExcelFile.sheet_names.", "Note", "An ExcelFile\u2019s attribute sheet_names provides access to a list of sheets.", "The arguments sheet_name allows specifying the sheet or sheets to read.", "The default value for sheet_name is 0, indicating to read the first sheet", "Pass a string to refer to the name of a particular sheet in the workbook.", "Pass an integer to refer to the index of a sheet. Indices follow Python convention, beginning at 0.", "Pass a list of either strings or integers, to return a dictionary of specified sheets.", "Pass a None to return a dictionary of all available sheets.", "Using the sheet index:", "Using all default values:", "Using None to get all sheets:", "Using a list to get multiple sheets:", "read_excel can read more than one sheet, by setting sheet_name to either a list of sheet names, a list of sheet positions, or None to read all sheets. Sheets can be specified by sheet index or sheet name, using an integer or string, respectively.", "read_excel can read a MultiIndex index, by passing a list of columns to index_col and a MultiIndex column by passing a list of rows to header. If either the index or columns have serialized level names those will be read in as well by specifying the rows/columns that make up the levels.", "For example, to read in a MultiIndex index without names:", "If the index has level names, they will parsed as well, using the same parameters.", "If the source file has both MultiIndex index and columns, lists specifying each should be passed to index_col and header:", "It is often the case that users will insert columns to do temporary computations in Excel and you may not want to read in those columns. read_excel takes a usecols keyword to allow you to specify a subset of columns to parse.", "Changed in version 1.0.0.", "Passing in an integer for usecols will no longer work. Please pass in a list of ints from 0 to usecols inclusive instead.", "You can specify a comma-delimited set of Excel columns and ranges as a string:", "If usecols is a list of integers, then it is assumed to be the file column indices to be parsed.", "Element order is ignored, so usecols=[0, 1] is the same as [1, 0].", "If usecols is a list of strings, it is assumed that each string corresponds to a column name provided either by the user in names or inferred from the document header row(s). Those strings define which columns will be parsed:", "Element order is ignored, so usecols=['baz', 'joe'] is the same as ['joe', 'baz'].", "If usecols is callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to True.", "Datetime-like values are normally automatically converted to the appropriate dtype when reading the excel file. But if you have a column of strings that look like dates (but are not actually formatted as dates in excel), you can use the parse_dates keyword to parse those strings to datetimes:", "It is possible to transform the contents of Excel cells via the converters option. For instance, to convert a column to boolean:", "This options handles missing values and treats exceptions in the converters as missing data. Transformations are applied cell by cell rather than to the column as a whole, so the array dtype is not guaranteed. For instance, a column of integers with missing values cannot be transformed to an array with integer dtype, because NaN is strictly a float. You can manually mask missing data to recover integer dtype:", "As an alternative to converters, the type for an entire column can be specified using the dtype keyword, which takes a dictionary mapping column names to types. To interpret data with no type inference, use the type str or object.", "To write a DataFrame object to a sheet of an Excel file, you can use the to_excel instance method. The arguments are largely the same as to_csv described above, the first argument being the name of the excel file, and the optional second argument the name of the sheet to which the DataFrame should be written. For example:", "Files with a .xls extension will be written using xlwt and those with a .xlsx extension will be written using xlsxwriter (if available) or openpyxl.", "The DataFrame will be written in a way that tries to mimic the REPL output. The index_label will be placed in the second row instead of the first. You can place it in the first row by setting the merge_cells option in to_excel() to False:", "In order to write separate DataFrames to separate sheets in a single Excel file, one can pass an ExcelWriter.", "pandas supports writing Excel files to buffer-like objects such as StringIO or BytesIO using ExcelWriter.", "Note", "engine is optional but recommended. Setting the engine determines the version of workbook produced. Setting engine='xlrd' will produce an Excel 2003-format workbook (xls). Using either 'openpyxl' or 'xlsxwriter' will produce an Excel 2007-format workbook (xlsx). If omitted, an Excel 2007-formatted workbook is produced.", "Deprecated since version 1.2.0: As the xlwt package is no longer maintained, the xlwt engine will be removed from a future version of pandas. This is the only engine in pandas that supports writing to .xls files.", "pandas chooses an Excel writer via two methods:", "the engine keyword argument", "the filename extension (via the default specified in config options)", "By default, pandas uses the XlsxWriter for .xlsx, openpyxl for .xlsm, and xlwt for .xls files. If you have multiple engines installed, you can set the default engine through setting the config options io.excel.xlsx.writer and io.excel.xls.writer. pandas will fall back on openpyxl for .xlsx files if Xlsxwriter is not available.", "To specify which writer you want to use, you can pass an engine keyword argument to to_excel and to ExcelWriter. The built-in engines are:", "openpyxl: version 2.4 or higher is required", "xlsxwriter", "xlwt", "The look and feel of Excel worksheets created from pandas can be modified using the following parameters on the DataFrame\u2019s to_excel method.", "float_format : Format string for floating point numbers (default None).", "freeze_panes : A tuple of two integers representing the bottommost row and rightmost column to freeze. Each of these parameters is one-based, so (1, 1) will freeze the first row and first column (default None).", "Using the Xlsxwriter engine provides many options for controlling the format of an Excel worksheet created with the to_excel method. Excellent examples can be found in the Xlsxwriter documentation here: https://xlsxwriter.readthedocs.io/working_with_pandas.html", "New in version 0.25.", "The read_excel() method can also read OpenDocument spreadsheets using the odfpy module. The semantics and features for reading OpenDocument spreadsheets match what can be done for Excel files using engine='odf'.", "Note", "Currently pandas only supports reading OpenDocument spreadsheets. Writing is not implemented.", "New in version 1.0.0.", "The read_excel() method can also read binary Excel files using the pyxlsb module. The semantics and features for reading binary Excel files mostly match what can be done for Excel files using engine='pyxlsb'. pyxlsb does not recognize datetime types in files and will return floats instead.", "Note", "Currently pandas only supports reading binary Excel files. Writing is not implemented.", "A handy way to grab data is to use the read_clipboard() method, which takes the contents of the clipboard buffer and passes them to the read_csv method. For instance, you can copy the following text to the clipboard (CTRL-C on many operating systems):", "And then import the data directly to a DataFrame by calling:", "The to_clipboard method can be used to write the contents of a DataFrame to the clipboard. Following which you can paste the clipboard contents into other applications (CTRL-V on many operating systems). Here we illustrate writing a DataFrame into clipboard and reading it back.", "We can see that we got the same content back, which we had earlier written to the clipboard.", "Note", "You may need to install xclip or xsel (with PyQt5, PyQt4 or qtpy) on Linux to use these methods.", "All pandas objects are equipped with to_pickle methods which use Python\u2019s cPickle module to save data structures to disk using the pickle format.", "The read_pickle function in the pandas namespace can be used to load any pickled pandas object (or any other pickled object) from file:", "Warning", "Loading pickled data received from untrusted sources can be unsafe.", "See: https://docs.python.org/3/library/pickle.html", "Warning", "read_pickle() is only guaranteed backwards compatible back to pandas version 0.20.3", "read_pickle(), DataFrame.to_pickle() and Series.to_pickle() can read and write compressed pickle files. The compression types of gzip, bz2, xz, zstd are supported for reading and writing. The zip file format only supports reading and must contain only one data file to be read.", "The compression type can be an explicit parameter or be inferred from the file extension. If \u2018infer\u2019, then use gzip, bz2, zip, xz, zstd if filename ends in '.gz', '.bz2', '.zip', '.xz', or '.zst', respectively.", "The compression parameter can also be a dict in order to pass options to the compression protocol. It must have a 'method' key set to the name of the compression protocol, which must be one of {'zip', 'gzip', 'bz2', 'xz', 'zstd'}. All other key-value pairs are passed to the underlying compression library.", "Using an explicit compression type:", "Inferring compression type from the extension:", "The default is to \u2018infer\u2019:", "Passing options to the compression protocol in order to speed up compression:", "pandas support for msgpack has been removed in version 1.0.0. It is recommended to use pickle instead.", "Alternatively, you can also the Arrow IPC serialization format for on-the-wire transmission of pandas objects. For documentation on pyarrow, see here.", "HDFStore is a dict-like object which reads and writes pandas using the high performance HDF5 format using the excellent PyTables library. See the cookbook for some advanced strategies", "Warning", "pandas uses PyTables for reading and writing HDF5 files, which allows serializing object-dtype data with pickle. Loading pickled data received from untrusted sources can be unsafe.", "See: https://docs.python.org/3/library/pickle.html for more.", "Objects can be written to the file just like adding key-value pairs to a dict:", "In a current or later Python session, you can retrieve stored objects:", "Deletion of the object specified by the key:", "Closing a Store and using a context manager:", "HDFStore supports a top-level API using read_hdf for reading and to_hdf for writing, similar to how read_csv and to_csv work.", "HDFStore will by default not drop rows that are all missing. This behavior can be changed by setting dropna=True.", "The examples above show storing using put, which write the HDF5 to PyTables in a fixed array format, called the fixed format. These types of stores are not appendable once written (though you can simply remove them and rewrite). Nor are they queryable; they must be retrieved in their entirety. They also do not support dataframes with non-unique column names. The fixed format stores offer very fast writing and slightly faster reading than table stores. This format is specified by default when using put or to_hdf or by format='fixed' or format='f'.", "Warning", "A fixed format will raise a TypeError if you try to retrieve using a where:", "HDFStore supports another PyTables format on disk, the table format. Conceptually a table is shaped very much like a DataFrame, with rows and columns. A table may be appended to in the same or other sessions. In addition, delete and query type operations are supported. This format is specified by format='table' or format='t' to append or put or to_hdf.", "This format can be set as an option as well pd.set_option('io.hdf.default_format','table') to enable put/append/to_hdf to by default store in the table format.", "Note", "You can also create a table by passing format='table' or format='t' to a put operation.", "Keys to a store can be specified as a string. These can be in a hierarchical path-name like format (e.g. foo/bar/bah), which will generate a hierarchy of sub-stores (or Groups in PyTables parlance). Keys can be specified without the leading \u2018/\u2019 and are always absolute (e.g. \u2018foo\u2019 refers to \u2018/foo\u2019). Removal operations can remove everything in the sub-store and below, so be careful.", "You can walk through the group hierarchy using the walk method which will yield a tuple for each group key along with the relative keys of its contents.", "Warning", "Hierarchical keys cannot be retrieved as dotted (attribute) access as described above for items stored under the root node.", "Instead, use explicit string based keys:", "Storing mixed-dtype data is supported. Strings are stored as a fixed-width using the maximum size of the appended column. Subsequent attempts at appending longer strings will raise a ValueError.", "Passing min_itemsize={`values`: size} as a parameter to append will set a larger minimum for the string columns. Storing floats,\nstrings, ints, bools, datetime64 are currently supported. For string columns, passing nan_rep = 'nan' to append will change the default nan representation on disk (which converts to/from np.nan), this defaults to nan.", "Storing MultiIndex DataFrames as tables is very similar to storing/selecting from homogeneous index DataFrames.", "Note", "The index keyword is reserved and cannot be use as a level name.", "select and delete operations have an optional criterion that can be specified to select/delete only a subset of the data. This allows one to have a very large on-disk table and retrieve only a portion of the data.", "A query is specified using the Term class under the hood, as a boolean expression.", "index and columns are supported indexers of DataFrames.", "if data_columns are specified, these can be used as additional indexers.", "level name in a MultiIndex, with default name level_0, level_1, \u2026 if not provided.", "Valid comparison operators are:", "=, ==, !=, >, >=, <, <=", "Valid boolean expressions are combined with:", "| : or", "& : and", "( and ) : for grouping", "These rules are similar to how boolean expressions are used in pandas for indexing.", "Note", "= will be automatically expanded to the comparison operator ==", "~ is the not operator, but can only be used in very limited circumstances", "If a list/tuple of expressions is passed they will be combined via &", "The following are valid expressions:", "'index >= date'", "\"columns = ['A', 'D']\"", "\"columns in ['A', 'D']\"", "'columns = A'", "'columns == A'", "\"~(columns = ['A', 'B'])\"", "'index > df.index[3] & string = \"bar\"'", "'(index > df.index[3] & index <= df.index[6]) | string = \"bar\"'", "\"ts >= Timestamp('2012-02-01')\"", "\"major_axis>=20130101\"", "The indexers are on the left-hand side of the sub-expression:", "columns, major_axis, ts", "The right-hand side of the sub-expression (after a comparison operator) can be:", "functions that will be evaluated, e.g. Timestamp('2012-02-01')", "strings, e.g. \"bar\"", "date-like, e.g. 20130101, or \"20130101\"", "lists, e.g. \"['A', 'B']\"", "variables that are defined in the local names space, e.g. date", "Note", "Passing a string to a query by interpolating it into the query expression is not recommended. Simply assign the string of interest to a variable and use that variable in an expression. For example, do this", "instead of this", "The latter will not work and will raise a SyntaxError.Note that there\u2019s a single quote followed by a double quote in the string variable.", "If you must interpolate, use the '%r' format specifier", "which will quote string.", "Here are some examples:", "Use boolean expressions, with in-line function evaluation.", "Use inline column reference.", "The columns keyword can be supplied to select a list of columns to be returned, this is equivalent to passing a 'columns=list_of_columns_to_filter':", "start and stop parameters can be specified to limit the total search space. These are in terms of the total number of rows in a table.", "Note", "select will raise a ValueError if the query expression has an unknown variable reference. Usually this means that you are trying to select on a column that is not a data_column.", "select will raise a SyntaxError if the query expression is not valid.", "You can store and query using the timedelta64[ns] type. Terms can be specified in the format: <float>(<unit>), where float may be signed (and fractional), and unit can be D,s,ms,us,ns for the timedelta. Here\u2019s an example:", "Selecting from a MultiIndex can be achieved by using the name of the level.", "If the MultiIndex levels names are None, the levels are automatically made available via the level_n keyword with n the level of the MultiIndex you want to select from.", "You can create/modify an index for a table with create_table_index after data is already in the table (after and append/put operation). Creating a table index is highly encouraged. This will speed your queries a great deal when you use a select with the indexed dimension as the where.", "Note", "Indexes are automagically created on the indexables and any data columns you specify. This behavior can be turned off by passing index=False to append.", "Oftentimes when appending large amounts of data to a store, it is useful to turn off index creation for each append, then recreate at the end.", "Then create the index when finished appending.", "See here for how to create a completely-sorted-index (CSI) on an existing store.", "You can designate (and index) certain columns that you want to be able to perform queries (other than the indexable columns, which you can always query). For instance say you want to perform this common operation, on-disk, and return just the frame that matches this query. You can specify data_columns = True to force all columns to be data_columns.", "There is some performance degradation by making lots of columns into data columns, so it is up to the user to designate these. In addition, you cannot change data columns (nor indexables) after the first append/put operation (Of course you can simply read in the data and create a new table!).", "You can pass iterator=True or chunksize=number_in_a_chunk to select and select_as_multiple to return an iterator on the results. The default is 50,000 rows returned in a chunk.", "Note", "You can also use the iterator with read_hdf which will open, then automatically close the store when finished iterating.", "Note, that the chunksize keyword applies to the source rows. So if you are doing a query, then the chunksize will subdivide the total rows in the table and the query applied, returning an iterator on potentially unequal sized chunks.", "Here is a recipe for generating a query and using it to create equal sized return chunks.", "To retrieve a single indexable or data column, use the method select_column. This will, for example, enable you to get the index very quickly. These return a Series of the result, indexed by the row number. These do not currently accept the where selector.", "Sometimes you want to get the coordinates (a.k.a the index locations) of your query. This returns an Int64Index of the resulting locations. These coordinates can also be passed to subsequent where operations.", "Sometime your query can involve creating a list of rows to select. Usually this mask would be a resulting index from an indexing operation. This example selects the months of a datetimeindex which are 5.", "If you want to inspect the stored object, retrieve via get_storer. You could use this programmatically to say get the number of rows in an object.", "The methods append_to_multiple and select_as_multiple can perform appending/selecting from multiple tables at once. The idea is to have one table (call it the selector table) that you index most/all of the columns, and perform your queries. The other table(s) are data tables with an index matching the selector table\u2019s index. You can then perform a very fast query on the selector table, yet get lots of data back. This method is similar to having a very wide table, but enables more efficient queries.", "The append_to_multiple method splits a given single DataFrame into multiple tables according to d, a dictionary that maps the table names to a list of \u2018columns\u2019 you want in that table. If None is used in place of a list, that table will have the remaining unspecified columns of the given DataFrame. The argument selector defines which table is the selector table (which you can make queries from). The argument dropna will drop rows from the input DataFrame to ensure tables are synchronized. This means that if a row for one of the tables being written to is entirely np.NaN, that row will be dropped from all tables.", "If dropna is False, THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES. Remember that entirely np.Nan rows are not written to the HDFStore, so if you choose to call dropna=False, some tables may have more rows than others, and therefore select_as_multiple may not work or it may return unexpected results.", "You can delete from a table selectively by specifying a where. In deleting rows, it is important to understand the PyTables deletes rows by erasing the rows, then moving the following data. Thus deleting can potentially be a very expensive operation depending on the orientation of your data. To get optimal performance, it\u2019s worthwhile to have the dimension you are deleting be the first of the indexables.", "Data is ordered (on the disk) in terms of the indexables. Here\u2019s a simple use case. You store panel-type data, with dates in the major_axis and ids in the minor_axis. The data is then interleaved like this:", "id_1", "id_2", ".", "id_n", "id_1", ".", "id_n", "It should be clear that a delete operation on the major_axis will be fairly quick, as one chunk is removed, then the following data moved. On the other hand a delete operation on the minor_axis will be very expensive. In this case it would almost certainly be faster to rewrite the table using a where that selects all but the missing data.", "Warning", "Please note that HDF5 DOES NOT RECLAIM SPACE in the h5 files automatically. Thus, repeatedly deleting (or removing nodes) and adding again, WILL TEND TO INCREASE THE FILE SIZE.", "To repack and clean the file, use ptrepack.", "PyTables allows the stored data to be compressed. This applies to all kinds of stores, not just tables. Two parameters are used to control compression: complevel and complib.", "complevel specifies if and how hard data is to be compressed. complevel=0 and complevel=None disables compression and 0<complevel<10 enables compression.", "complib specifies which compression library to use. If nothing is specified the default library zlib is used. A compression library usually optimizes for either good compression rates or speed and the results will depend on the type of data. Which type of compression to choose depends on your specific needs and data. The list of supported compression libraries:", "zlib: The default compression library. A classic in terms of compression, achieves good compression rates but is somewhat slow.", "lzo: Fast compression and decompression.", "bzip2: Good compression rates.", "blosc: Fast compression and decompression.", "Support for alternative blosc compressors:", "blosc:blosclz This is the default compressor for blosc", "blosc:lz4: A compact, very popular and fast compressor.", "blosc:lz4hc: A tweaked version of LZ4, produces better compression ratios at the expense of speed.", "blosc:snappy: A popular compressor used in many places.", "blosc:zlib: A classic; somewhat slower than the previous ones, but achieving better compression ratios.", "blosc:zstd: An extremely well balanced codec; it provides the best compression ratios among the others above, and at reasonably fast speed.", "If complib is defined as something other than the listed libraries a ValueError exception is issued.", "Note", "If the library specified with the complib option is missing on your platform, compression defaults to zlib without further ado.", "Enable compression for all objects within the file:", "Or on-the-fly compression (this only applies to tables) in stores where compression is not enabled:", "PyTables offers better write performance when tables are compressed after they are written, as opposed to turning on compression at the very beginning. You can use the supplied PyTables utility ptrepack. In addition, ptrepack can change compression levels after the fact.", "Furthermore ptrepack in.h5 out.h5 will repack the file to allow you to reuse previously deleted space. Alternatively, one can simply remove the file and write again, or use the copy method.", "Warning", "HDFStore is not-threadsafe for writing. The underlying PyTables only supports concurrent reads (via threading or processes). If you need reading and writing at the same time, you need to serialize these operations in a single thread in a single process. You will corrupt your data otherwise. See the (GH2397) for more information.", "If you use locks to manage write access between multiple processes, you may want to use fsync() before releasing write locks. For convenience you can use store.flush(fsync=True) to do this for you.", "Once a table is created columns (DataFrame) are fixed; only exactly the same columns can be appended", "Be aware that timezones (e.g., pytz.timezone('US/Eastern')) are not necessarily equal across timezone versions. So if data is localized to a specific timezone in the HDFStore using one version of a timezone library and that data is updated with another version, the data will be converted to UTC since these timezones are not considered equal. Either use the same version of timezone library or use tz_convert with the updated timezone definition.", "Warning", "PyTables will show a NaturalNameWarning if a column name cannot be used as an attribute selector. Natural identifiers contain only letters, numbers, and underscores, and may not begin with a number. Other identifiers cannot be used in a where clause and are generally a bad idea.", "HDFStore will map an object dtype to the PyTables underlying dtype. This means the following types are known to work:", "Type", "Represents missing values", "floating : float64, float32, float16", "np.nan", "integer : int64, int32, int8, uint64,uint32, uint8", "boolean", "datetime64[ns]", "NaT", "timedelta64[ns]", "NaT", "categorical : see the section below", "object : strings", "np.nan", "unicode columns are not supported, and WILL FAIL.", "You can write data that contains category dtypes to a HDFStore. Queries work the same as if it was an object array. However, the category dtyped data is stored in a more efficient manner.", "min_itemsize", "The underlying implementation of HDFStore uses a fixed column width (itemsize) for string columns. A string column itemsize is calculated as the maximum of the length of data (for that column) that is passed to the HDFStore, in the first append. Subsequent appends, may introduce a string for a column larger than the column can hold, an Exception will be raised (otherwise you could have a silent truncation of these columns, leading to loss of information). In the future we may relax this and allow a user-specified truncation to occur.", "Pass min_itemsize on the first table creation to a-priori specify the minimum length of a particular string column. min_itemsize can be an integer, or a dict mapping a column name to an integer. You can pass values as a key to allow all indexables or data_columns to have this min_itemsize.", "Passing a min_itemsize dict will cause all passed columns to be created as data_columns automatically.", "Note", "If you are not passing any data_columns, then the min_itemsize will be the maximum of the length of any string passed", "nan_rep", "String columns will serialize a np.nan (a missing value) with the nan_rep string representation. This defaults to the string value nan. You could inadvertently turn an actual nan value into a missing value.", "HDFStore writes table format objects in specific formats suitable for producing loss-less round trips to pandas objects. For external compatibility, HDFStore can read native PyTables format tables.", "It is possible to write an HDFStore object that can easily be imported into R using the rhdf5 library (Package website). Create a table format store like this:", "In R this file can be read into a data.frame object using the rhdf5 library. The following example function reads the corresponding column names and data values from the values and assembles them into a data.frame:", "Now you can import the DataFrame into R:", "Note", "The R function lists the entire HDF5 file\u2019s contents and assembles the data.frame object from all matching nodes, so use this only as a starting point if you have stored multiple DataFrame objects to a single HDF5 file.", "tables format come with a writing performance penalty as compared to fixed stores. The benefit is the ability to append/delete and query (potentially very large amounts of data). Write times are generally longer as compared with regular stores. Query times can be quite fast, especially on an indexed axis.", "You can pass chunksize=<int> to append, specifying the write chunksize (default is 50000). This will significantly lower your memory usage on writing.", "You can pass expectedrows=<int> to the first append, to set the TOTAL number of rows that PyTables will expect. This will optimize read/write performance.", "Duplicate rows can be written to tables, but are filtered out in selection (with the last items being selected; thus a table is unique on major, minor pairs)", "A PerformanceWarning will be raised if you are attempting to store types that will be pickled by PyTables (rather than stored as endemic types). See Here for more information and some solutions.", "Feather provides binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy.", "Feather is designed to faithfully serialize and de-serialize DataFrames, supporting all of the pandas dtypes, including extension dtypes such as categorical and datetime with tz.", "Several caveats:", "The format will NOT write an Index, or MultiIndex for the DataFrame and will raise an error if a non-default one is provided. You can .reset_index() to store the index or .reset_index(drop=True) to ignore it.", "Duplicate column names and non-string columns names are not supported", "Actual Python objects in object dtype columns are not supported. These will raise a helpful error message on an attempt at serialization.", "See the Full Documentation.", "Write to a feather file.", "Read from a feather file.", "Apache Parquet provides a partitioned binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy. Parquet can use a variety of compression techniques to shrink the file size as much as possible while still maintaining good read performance.", "Parquet is designed to faithfully serialize and de-serialize DataFrame s, supporting all of the pandas dtypes, including extension dtypes such as datetime with tz.", "Several caveats.", "Duplicate column names and non-string columns names are not supported.", "The pyarrow engine always writes the index to the output, but fastparquet only writes non-default indexes. This extra column can cause problems for non-pandas consumers that are not expecting it. You can force including or omitting indexes with the index argument, regardless of the underlying engine.", "Index level names, if specified, must be strings.", "In the pyarrow engine, categorical dtypes for non-string types can be serialized to parquet, but will de-serialize as their primitive dtype.", "The pyarrow engine preserves the ordered flag of categorical dtypes with string types. fastparquet does not preserve the ordered flag.", "Non supported types include Interval and actual Python object types. These will raise a helpful error message on an attempt at serialization. Period type is supported with pyarrow >= 0.16.0.", "The pyarrow engine preserves extension data types such as the nullable integer and string data type (requiring pyarrow >= 0.16.0, and requiring the extension type to implement the needed protocols, see the extension types documentation).", "You can specify an engine to direct the serialization. This can be one of pyarrow, or fastparquet, or auto. If the engine is NOT specified, then the pd.options.io.parquet.engine option is checked; if this is also auto, then pyarrow is tried, and falling back to fastparquet.", "See the documentation for pyarrow and fastparquet.", "Note", "These engines are very similar and should read/write nearly identical parquet format files. Currently pyarrow does not support timedelta data, fastparquet>=0.1.4 supports timezone aware datetimes. These libraries differ by having different underlying dependencies (fastparquet by using numba, while pyarrow uses a c-library).", "Write to a parquet file.", "Read from a parquet file.", "Read only certain columns of a parquet file.", "Serializing a DataFrame to parquet may include the implicit index as one or more columns in the output file. Thus, this code:", "creates a parquet file with three columns if you use pyarrow for serialization: a, b, and __index_level_0__. If you\u2019re using fastparquet, the index may or may not be written to the file.", "This unexpected extra column causes some databases like Amazon Redshift to reject the file, because that column doesn\u2019t exist in the target table.", "If you want to omit a dataframe\u2019s indexes when writing, pass index=False to to_parquet():", "This creates a parquet file with just the two expected columns, a and b. If your DataFrame has a custom index, you won\u2019t get it back when you load this file into a DataFrame.", "Passing index=True will always write the index, even if that\u2019s not the underlying engine\u2019s default behavior.", "Parquet supports partitioning of data based on the values of one or more columns.", "The path specifies the parent directory to which data will be saved. The partition_cols are the column names by which the dataset will be partitioned. Columns are partitioned in the order they are given. The partition splits are determined by the unique values in the partition columns. The above example creates a partitioned dataset that may look like:", "New in version 1.0.0.", "Similar to the parquet format, the ORC Format is a binary columnar serialization for data frames. It is designed to make reading data frames efficient. pandas provides only a reader for the ORC format, read_orc(). This requires the pyarrow library.", "Warning", "It is highly recommended to install pyarrow using conda due to some issues occurred by pyarrow.", "read_orc() is not supported on Windows yet, you can find valid environments on install optional dependencies.", "The pandas.io.sql module provides a collection of query wrappers to both facilitate data retrieval and to reduce dependency on DB-specific API. Database abstraction is provided by SQLAlchemy if installed. In addition you will need a driver library for your database. Examples of such drivers are psycopg2 for PostgreSQL or pymysql for MySQL. For SQLite this is included in Python\u2019s standard library by default. You can find an overview of supported drivers for each SQL dialect in the SQLAlchemy docs.", "If SQLAlchemy is not installed, a fallback is only provided for sqlite (and for mysql for backwards compatibility, but this is deprecated and will be removed in a future version). This mode requires a Python database adapter which respect the Python DB-API.", "See also some cookbook examples for some advanced strategies.", "The key functions are:", "read_sql_table(table_name, con[, schema, ...])", "Read SQL database table into a DataFrame.", "read_sql_query(sql, con[, index_col, ...])", "Read SQL query into a DataFrame.", "read_sql(sql, con[, index_col, ...])", "Read SQL query or database table into a DataFrame.", "DataFrame.to_sql(name, con[, schema, ...])", "Write records stored in a DataFrame to a SQL database.", "Note", "The function read_sql() is a convenience wrapper around read_sql_table() and read_sql_query() (and for backward compatibility) and will delegate to specific function depending on the provided input (database table name or sql query). Table names do not need to be quoted if they have special characters.", "In the following example, we use the SQlite SQL database engine. You can use a temporary SQLite database where data are stored in \u201cmemory\u201d.", "To connect with SQLAlchemy you use the create_engine() function to create an engine object from database URI. You only need to create the engine once per database you are connecting to. For more information on create_engine() and the URI formatting, see the examples below and the SQLAlchemy documentation", "If you want to manage your own connections you can pass one of those instead. The example below opens a connection to the database using a Python context manager that automatically closes the connection after the block has completed. See the SQLAlchemy docs for an explanation of how the database connection is handled.", "Warning", "When you open a connection to a database you are also responsible for closing it. Side effects of leaving a connection open may include locking the database or other breaking behaviour.", "Assuming the following data is in a DataFrame data, we can insert it into the database using to_sql().", "id", "Date", "Col_1", "Col_2", "Col_3", "26", "2012-10-18", "X", "25.7", "True", "42", "2012-10-19", "Y", "-12.4", "False", "63", "2012-10-20", "Z", "5.73", "True", "With some databases, writing large DataFrames can result in errors due to packet size limitations being exceeded. This can be avoided by setting the chunksize parameter when calling to_sql. For example, the following writes data to the database in batches of 1000 rows at a time:", "to_sql() will try to map your data to an appropriate SQL data type based on the dtype of the data. When you have columns of dtype object, pandas will try to infer the data type.", "You can always override the default type by specifying the desired SQL type of any of the columns by using the dtype argument. This argument needs a dictionary mapping column names to SQLAlchemy types (or strings for the sqlite3 fallback mode). For example, specifying to use the sqlalchemy String type instead of the default Text type for string columns:", "Note", "Due to the limited support for timedelta\u2019s in the different database flavors, columns with type timedelta64 will be written as integer values as nanoseconds to the database and a warning will be raised.", "Note", "Columns of category dtype will be converted to the dense representation as you would get with np.asarray(categorical) (e.g. for string categories this gives an array of strings). Because of this, reading the database table back in does not generate a categorical.", "Using SQLAlchemy, to_sql() is capable of writing datetime data that is timezone naive or timezone aware. However, the resulting data stored in the database ultimately depends on the supported data type for datetime data of the database system being used.", "The following table lists supported data types for datetime data for some common databases. Other database dialects may have different data types for datetime data.", "Database", "SQL Datetime Types", "Timezone Support", "SQLite", "TEXT", "No", "MySQL", "TIMESTAMP or DATETIME", "No", "PostgreSQL", "TIMESTAMP or TIMESTAMP WITH TIME ZONE", "Yes", "When writing timezone aware data to databases that do not support timezones, the data will be written as timezone naive timestamps that are in local time with respect to the timezone.", "read_sql_table() is also capable of reading datetime data that is timezone aware or naive. When reading TIMESTAMP WITH TIME ZONE types, pandas will convert the data to UTC.", "The parameter method controls the SQL insertion clause used. Possible values are:", "None: Uses standard SQL INSERT clause (one per row).", "'multi': Pass multiple values in a single INSERT clause. It uses a special SQL syntax not supported by all backends. This usually provides better performance for analytic databases like Presto and Redshift, but has worse performance for traditional SQL backend if the table contains many columns. For more information check the SQLAlchemy documentation.", "callable with signature (pd_table, conn, keys, data_iter): This can be used to implement a more performant insertion method based on specific backend dialect features.", "Example of a callable using PostgreSQL COPY clause:", "read_sql_table() will read a database table given the table name and optionally a subset of columns to read.", "Note", "In order to use read_sql_table(), you must have the SQLAlchemy optional dependency installed.", "Note", "Note that pandas infers column dtypes from query outputs, and not by looking up data types in the physical database schema. For example, assume userid is an integer column in a table. Then, intuitively, select userid ... will return integer-valued series, while select cast(userid as text) ... will return object-valued (str) series. Accordingly, if the query output is empty, then all resulting columns will be returned as object-valued (since they are most general). If you foresee that your query will sometimes generate an empty result, you may want to explicitly typecast afterwards to ensure dtype integrity.", "You can also specify the name of the column as the DataFrame index, and specify a subset of columns to be read.", "And you can explicitly force columns to be parsed as dates:", "If needed you can explicitly specify a format string, or a dict of arguments to pass to pandas.to_datetime():", "You can check if a table exists using has_table()", "Reading from and writing to different schema\u2019s is supported through the schema keyword in the read_sql_table() and to_sql() functions. Note however that this depends on the database flavor (sqlite does not have schema\u2019s). For example:", "You can query using raw SQL in the read_sql_query() function. In this case you must use the SQL variant appropriate for your database. When using SQLAlchemy, you can also pass SQLAlchemy Expression language constructs, which are database-agnostic.", "Of course, you can specify a more \u201ccomplex\u201d query.", "The read_sql_query() function supports a chunksize argument. Specifying this will return an iterator through chunks of the query result:", "You can also run a plain query without creating a DataFrame with execute(). This is useful for queries that don\u2019t return values, such as INSERT. This is functionally equivalent to calling execute on the SQLAlchemy engine or db connection object. Again, you must use the SQL syntax variant appropriate for your database.", "To connect with SQLAlchemy you use the create_engine() function to create an engine object from database URI. You only need to create the engine once per database you are connecting to.", "For more information see the examples the SQLAlchemy documentation", "You can use SQLAlchemy constructs to describe your query.", "Use sqlalchemy.text() to specify query parameters in a backend-neutral way", "If you have an SQLAlchemy description of your database you can express where conditions using SQLAlchemy expressions", "You can combine SQLAlchemy expressions with parameters passed to read_sql() using sqlalchemy.bindparam()", "The use of sqlite is supported without using SQLAlchemy. This mode requires a Python database adapter which respect the Python DB-API.", "You can create connections like so:", "And then issue the following queries:", "Warning", "Starting in 0.20.0, pandas has split off Google BigQuery support into the separate package pandas-gbq. You can pip install pandas-gbq to get it.", "The pandas-gbq package provides functionality to read/write from Google BigQuery.", "pandas integrates with this external package. if pandas-gbq is installed, you can use the pandas methods pd.read_gbq and DataFrame.to_gbq, which will call the respective functions from pandas-gbq.", "Full documentation can be found here.", "The method to_stata() will write a DataFrame into a .dta file. The format version of this file is always 115 (Stata 12).", "Stata data files have limited data type support; only strings with 244 or fewer characters, int8, int16, int32, float32 and float64 can be stored in .dta files. Additionally, Stata reserves certain values to represent missing data. Exporting a non-missing value that is outside of the permitted range in Stata for a particular data type will retype the variable to the next larger size. For example, int8 values are restricted to lie between -127 and 100 in Stata, and so variables with values above 100 will trigger a conversion to int16. nan values in floating points data types are stored as the basic missing data type (. in Stata).", "Note", "It is not possible to export missing data values for integer data types.", "The Stata writer gracefully handles other data types including int64, bool, uint8, uint16, uint32 by casting to the smallest supported type that can represent the data. For example, data with a type of uint8 will be cast to int8 if all values are less than 100 (the upper bound for non-missing int8 data in Stata), or, if values are outside of this range, the variable is cast to int16.", "Warning", "Conversion from int64 to float64 may result in a loss of precision if int64 values are larger than 2**53.", "Warning", "StataWriter and to_stata() only support fixed width strings containing up to 244 characters, a limitation imposed by the version 115 dta file format. Attempting to write Stata dta files with strings longer than 244 characters raises a ValueError.", "The top-level function read_stata will read a dta file and return either a DataFrame or a StataReader that can be used to read the file incrementally.", "Specifying a chunksize yields a StataReader instance that can be used to read chunksize lines from the file at a time. The StataReader object can be used as an iterator.", "For more fine-grained control, use iterator=True and specify chunksize with each call to read().", "Currently the index is retrieved as a column.", "The parameter convert_categoricals indicates whether value labels should be read and used to create a Categorical variable from them. Value labels can also be retrieved by the function value_labels, which requires read() to be called before use.", "The parameter convert_missing indicates whether missing value representations in Stata should be preserved. If False (the default), missing values are represented as np.nan. If True, missing values are represented using StataMissingValue objects, and columns containing missing values will have object data type.", "Note", "read_stata() and StataReader support .dta formats 113-115 (Stata 10-12), 117 (Stata 13), and 118 (Stata 14).", "Note", "Setting preserve_dtypes=False will upcast to the standard pandas data types: int64 for all integer types and float64 for floating point data. By default, the Stata data types are preserved when importing.", "Categorical data can be exported to Stata data files as value labeled data. The exported data consists of the underlying category codes as integer data values and the categories as value labels. Stata does not have an explicit equivalent to a Categorical and information about whether the variable is ordered is lost when exporting.", "Warning", "Stata only supports string value labels, and so str is called on the categories when exporting data. Exporting Categorical variables with non-string categories produces a warning, and can result a loss of information if the str representations of the categories are not unique.", "Labeled data can similarly be imported from Stata data files as Categorical variables using the keyword argument convert_categoricals (True by default). The keyword argument order_categoricals (True by default) determines whether imported Categorical variables are ordered.", "Note", "When importing categorical data, the values of the variables in the Stata data file are not preserved since Categorical variables always use integer data types between -1 and n-1 where n is the number of categories. If the original values in the Stata data file are required, these can be imported by setting convert_categoricals=False, which will import original data (but not the variable labels). The original values can be matched to the imported categorical data since there is a simple mapping between the original Stata data values and the category codes of imported Categorical variables: missing values are assigned code -1, and the smallest original value is assigned 0, the second smallest is assigned 1 and so on until the largest original value is assigned the code n-1.", "Note", "Stata supports partially labeled series. These series have value labels for some but not all data values. Importing a partially labeled series will produce a Categorical with string categories for the values that are labeled and numeric categories for values with no label.", "The top-level function read_sas() can read (but not write) SAS XPORT (.xpt) and (since v0.18.0) SAS7BDAT (.sas7bdat) format files.", "SAS files only contain two value types: ASCII text and floating point values (usually 8 bytes but sometimes truncated). For xport files, there is no automatic type conversion to integers, dates, or categoricals. For SAS7BDAT files, the format codes may allow date variables to be automatically converted to dates. By default the whole file is read and returned as a DataFrame.", "Specify a chunksize or use iterator=True to obtain reader objects (XportReader or SAS7BDATReader) for incrementally reading the file. The reader objects also have attributes that contain additional information about the file and its variables.", "Read a SAS7BDAT file:", "Obtain an iterator and read an XPORT file 100,000 lines at a time:", "The specification for the xport file format is available from the SAS web site.", "No official documentation is available for the SAS7BDAT format.", "New in version 0.25.0.", "The top-level function read_spss() can read (but not write) SPSS SAV (.sav) and ZSAV (.zsav) format files.", "SPSS files contain column names. By default the whole file is read, categorical columns are converted into pd.Categorical, and a DataFrame with all columns is returned.", "Specify the usecols parameter to obtain a subset of columns. Specify convert_categoricals=False to avoid converting categorical columns into pd.Categorical.", "Read an SPSS file:", "Extract a subset of columns contained in usecols from an SPSS file and avoid converting categorical columns into pd.Categorical:", "More information about the SAV and ZSAV file formats is available here.", "pandas itself only supports IO with a limited set of file formats that map cleanly to its tabular data model. For reading and writing other file formats into and from pandas, we recommend these packages from the broader community.", "xarray provides data structures inspired by the pandas DataFrame for working with multi-dimensional datasets, with a focus on the netCDF file format and easy conversion to and from pandas.", "This is an informal comparison of various IO methods, using pandas 0.24.2. Timings are machine dependent and small differences should be ignored.", "The following test functions will be used below to compare the performance of several IO methods:", "When writing, the top three functions in terms of speed are test_feather_write, test_hdf_fixed_write and test_hdf_fixed_write_compress.", "When reading, the top three functions in terms of speed are test_feather_read, test_pickle_read and test_hdf_fixed_read.", "The files test.pkl.compress, test.parquet and test.feather took the least space on disk (in bytes)."]}, {"name": "Merge, join, concatenate and compare", "path": "user_guide/merging", "type": "Manual", "text": ["pandas provides various facilities for easily combining together Series or DataFrame with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.", "In addition, pandas also provides utilities to compare two Series or DataFrame and summarize their differences.", "The concat() function (in the main pandas namespace) does all of the heavy lifting of performing concatenation operations along an axis while performing optional set logic (union or intersection) of the indexes (if any) on the other axes. Note that I say \u201cif any\u201d because there is only a single possible axis of concatenation for Series.", "Before diving into all of the details of concat and what it can do, here is a simple example:", "Like its sibling function on ndarrays, numpy.concatenate, pandas.concat takes a list or dict of homogeneously-typed objects and concatenates them with some configurable handling of \u201cwhat to do with the other axes\u201d:", "objs : a sequence or mapping of Series or DataFrame objects. If a dict is passed, the sorted keys will be used as the keys argument, unless it is passed, in which case the values will be selected (see below). Any None objects will be dropped silently unless they are all None in which case a ValueError will be raised.", "axis : {0, 1, \u2026}, default 0. The axis to concatenate along.", "join : {\u2018inner\u2019, \u2018outer\u2019}, default \u2018outer\u2019. How to handle indexes on other axis(es). Outer for union and inner for intersection.", "ignore_index : boolean, default False. If True, do not use the index values on the concatenation axis. The resulting axis will be labeled 0, \u2026, n - 1. This is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information. Note the index values on the other axes are still respected in the join.", "keys : sequence, default None. Construct hierarchical index using the passed keys as the outermost level. If multiple levels passed, should contain tuples.", "levels : list of sequences, default None. Specific levels (unique values) to use for constructing a MultiIndex. Otherwise they will be inferred from the keys.", "names : list, default None. Names for the levels in the resulting hierarchical index.", "verify_integrity : boolean, default False. Check whether the new concatenated axis contains duplicates. This can be very expensive relative to the actual data concatenation.", "copy : boolean, default True. If False, do not copy data unnecessarily.", "Without a little bit of context many of these arguments don\u2019t make much sense. Let\u2019s revisit the above example. Suppose we wanted to associate specific keys with each of the pieces of the chopped up DataFrame. We can do this using the keys argument:", "As you can see (if you\u2019ve read the rest of the documentation), the resulting object\u2019s index has a hierarchical index. This means that we can now select out each chunk by key:", "It\u2019s not a stretch to see how this can be very useful. More detail on this functionality below.", "Note", "It is worth noting that concat() (and therefore append()) makes a full copy of the data, and that constantly reusing this function can create a significant performance hit. If you need to use the operation over several datasets, use a list comprehension.", "Note", "When concatenating DataFrames with named axes, pandas will attempt to preserve these index/column names whenever possible. In the case where all inputs share a common name, this name will be assigned to the result. When the input names do not all agree, the result will be unnamed. The same is true for MultiIndex, but the logic is applied separately on a level-by-level basis.", "When gluing together multiple DataFrames, you have a choice of how to handle the other axes (other than the one being concatenated). This can be done in the following two ways:", "Take the union of them all, join='outer'. This is the default option as it results in zero information loss.", "Take the intersection, join='inner'.", "Here is an example of each of these methods. First, the default join='outer' behavior:", "Here is the same thing with join='inner':", "Lastly, suppose we just wanted to reuse the exact index from the original DataFrame:", "Similarly, we could index before the concatenation:", "For DataFrame objects which don\u2019t have a meaningful index, you may wish to append them and ignore the fact that they may have overlapping indexes. To do this, use the ignore_index argument:", "You can concatenate a mix of Series and DataFrame objects. The Series will be transformed to DataFrame with the column name as the name of the Series.", "Note", "Since we\u2019re concatenating a Series to a DataFrame, we could have achieved the same result with DataFrame.assign(). To concatenate an arbitrary number of pandas objects (DataFrame or Series), use concat.", "If unnamed Series are passed they will be numbered consecutively.", "Passing ignore_index=True will drop all name references.", "A fairly common use of the keys argument is to override the column names when creating a new DataFrame based on existing Series. Notice how the default behaviour consists on letting the resulting DataFrame inherit the parent Series\u2019 name, when these existed.", "Through the keys argument we can override the existing column names.", "Let\u2019s consider a variation of the very first example presented:", "You can also pass a dict to concat in which case the dict keys will be used for the keys argument (unless other keys are specified):", "The MultiIndex created has levels that are constructed from the passed keys and the index of the DataFrame pieces:", "If you wish to specify other levels (as will occasionally be the case), you can do so using the levels argument:", "This is fairly esoteric, but it is actually necessary for implementing things like GroupBy where the order of a categorical variable is meaningful.", "If you have a series that you want to append as a single row to a DataFrame, you can convert the row into a DataFrame and use concat", "You should use ignore_index with this method to instruct DataFrame to discard its index. If you wish to preserve the index, you should construct an appropriately-indexed DataFrame and append or concatenate those objects.", "pandas has full-featured, high performance in-memory join operations idiomatically very similar to relational databases like SQL. These methods perform significantly better (in some cases well over an order of magnitude better) than other open source implementations (like base::merge.data.frame in R). The reason for this is careful algorithmic design and the internal layout of the data in DataFrame.", "See the cookbook for some advanced strategies.", "Users who are familiar with SQL but new to pandas might be interested in a comparison with SQL.", "pandas provides a single function, merge(), as the entry point for all standard database join operations between DataFrame or named Series objects:", "left: A DataFrame or named Series object.", "right: Another DataFrame or named Series object.", "on: Column or index level names to join on. Must be found in both the left and right DataFrame and/or Series objects. If not passed and left_index and right_index are False, the intersection of the columns in the DataFrames and/or Series will be inferred to be the join keys.", "left_on: Columns or index levels from the left DataFrame or Series to use as keys. Can either be column names, index level names, or arrays with length equal to the length of the DataFrame or Series.", "right_on: Columns or index levels from the right DataFrame or Series to use as keys. Can either be column names, index level names, or arrays with length equal to the length of the DataFrame or Series.", "left_index: If True, use the index (row labels) from the left DataFrame or Series as its join key(s). In the case of a DataFrame or Series with a MultiIndex (hierarchical), the number of levels must match the number of join keys from the right DataFrame or Series.", "right_index: Same usage as left_index for the right DataFrame or Series", "how: One of 'left', 'right', 'outer', 'inner', 'cross'. Defaults to inner. See below for more detailed description of each method.", "sort: Sort the result DataFrame by the join keys in lexicographical order. Defaults to True, setting to False will improve performance substantially in many cases.", "suffixes: A tuple of string suffixes to apply to overlapping columns. Defaults to ('_x', '_y').", "copy: Always copy data (default True) from the passed DataFrame or named Series objects, even when reindexing is not necessary. Cannot be avoided in many cases but may improve performance / memory usage. The cases where copying can be avoided are somewhat pathological but this option is provided nonetheless.", "indicator: Add a column to the output DataFrame called _merge with information on the source of each row. _merge is Categorical-type and takes on a value of left_only for observations whose merge key only appears in 'left' DataFrame or Series, right_only for observations whose merge key only appears in 'right' DataFrame or Series, and both if the observation\u2019s merge key is found in both.", "validate : string, default None. If specified, checks if merge is of specified type.", "\u201cone_to_one\u201d or \u201c1:1\u201d: checks if merge keys are unique in both left and right datasets.", "\u201cone_to_many\u201d or \u201c1:m\u201d: checks if merge keys are unique in left dataset.", "\u201cmany_to_one\u201d or \u201cm:1\u201d: checks if merge keys are unique in right dataset.", "\u201cmany_to_many\u201d or \u201cm:m\u201d: allowed, but does not result in checks.", "Note", "Support for specifying index levels as the on, left_on, and right_on parameters was added in version 0.23.0. Support for merging named Series objects was added in version 0.24.0.", "The return type will be the same as left. If left is a DataFrame or named Series and right is a subclass of DataFrame, the return type will still be DataFrame.", "merge is a function in the pandas namespace, and it is also available as a DataFrame instance method merge(), with the calling DataFrame being implicitly considered the left object in the join.", "The related join() method, uses merge internally for the index-on-index (by default) and column(s)-on-index join. If you are joining on index only, you may wish to use DataFrame.join to save yourself some typing.", "Experienced users of relational databases like SQL will be familiar with the terminology used to describe join operations between two SQL-table like structures (DataFrame objects). There are several cases to consider which are very important to understand:", "one-to-one joins: for example when joining two DataFrame objects on their indexes (which must contain unique values).", "many-to-one joins: for example when joining an index (unique) to one or more columns in a different DataFrame.", "many-to-many joins: joining columns on columns.", "Note", "When joining columns on columns (potentially a many-to-many join), any indexes on the passed DataFrame objects will be discarded.", "It is worth spending some time understanding the result of the many-to-many join case. In SQL / standard relational algebra, if a key combination appears more than once in both tables, the resulting table will have the Cartesian product of the associated data. Here is a very basic example with one unique key combination:", "Here is a more complicated example with multiple join keys. Only the keys appearing in left and right are present (the intersection), since how='inner' by default.", "The how argument to merge specifies how to determine which keys are to be included in the resulting table. If a key combination does not appear in either the left or right tables, the values in the joined table will be NA. Here is a summary of the how options and their SQL equivalent names:", "Merge method", "SQL Join Name", "Description", "left", "LEFT OUTER JOIN", "Use keys from left frame only", "right", "RIGHT OUTER JOIN", "Use keys from right frame only", "outer", "FULL OUTER JOIN", "Use union of keys from both frames", "inner", "INNER JOIN", "Use intersection of keys from both frames", "cross", "CROSS JOIN", "Create the cartesian product of rows of both frames", "You can merge a mult-indexed Series and a DataFrame, if the names of the MultiIndex correspond to the columns from the DataFrame. Transform the Series to a DataFrame using Series.reset_index() before merging, as shown in the following example.", "Here is another example with duplicate join keys in DataFrames:", "Warning", "Joining / merging on duplicate keys can cause a returned frame that is the multiplication of the row dimensions, which may result in memory overflow. It is the user\u2019 s responsibility to manage duplicate values in keys before joining large DataFrames.", "Users can use the validate argument to automatically check whether there are unexpected duplicates in their merge keys. Key uniqueness is checked before merge operations and so should protect against memory overflows. Checking key uniqueness is also a good way to ensure user data structures are as expected.", "In the following example, there are duplicate values of B in the right DataFrame. As this is not a one-to-one merge \u2013 as specified in the validate argument \u2013 an exception will be raised.", "If the user is aware of the duplicates in the right DataFrame but wants to ensure there are no duplicates in the left DataFrame, one can use the validate='one_to_many' argument instead, which will not raise an exception.", "merge() accepts the argument indicator. If True, a Categorical-type column called _merge will be added to the output object that takes on values:", "Observation Origin", "_merge value", "Merge key only in 'left' frame", "left_only", "Merge key only in 'right' frame", "right_only", "Merge key in both frames", "both", "The indicator argument will also accept string arguments, in which case the indicator function will use the value of the passed string as the name for the indicator column.", "Merging will preserve the dtype of the join keys.", "We are able to preserve the join keys:", "Of course if you have missing values that are introduced, then the resulting dtype will be upcast.", "Merging will preserve category dtypes of the mergands. See also the section on categoricals.", "The left frame.", "The right frame.", "The merged result:", "Note", "The category dtypes must be exactly the same, meaning the same categories and the ordered attribute. Otherwise the result will coerce to the categories\u2019 dtype.", "Note", "Merging on category dtypes that are the same can be quite performant compared to object dtype merging.", "DataFrame.join() is a convenient method for combining the columns of two potentially differently-indexed DataFrames into a single result DataFrame. Here is a very basic example:", "The same as above, but with how='inner'.", "The data alignment here is on the indexes (row labels). This same behavior can be achieved using merge plus additional arguments instructing it to use the indexes:", "join() takes an optional on argument which may be a column or multiple column names, which specifies that the passed DataFrame is to be aligned on that column in the DataFrame. These two function calls are completely equivalent:", "Obviously you can choose whichever form you find more convenient. For many-to-one joins (where one of the DataFrame\u2019s is already indexed by the join key), using join may be more convenient. Here is a simple example:", "To join on multiple keys, the passed DataFrame must have a MultiIndex:", "Now this can be joined by passing the two key column names:", "The default for DataFrame.join is to perform a left join (essentially a \u201cVLOOKUP\u201d operation, for Excel users), which uses only the keys found in the calling DataFrame. Other join types, for example inner join, can be just as easily performed:", "As you can see, this drops any rows where there was no match.", "You can join a singly-indexed DataFrame with a level of a MultiIndexed DataFrame. The level will match on the name of the index of the singly-indexed frame against a level name of the MultiIndexed frame.", "This is equivalent but less verbose and more memory efficient / faster than this.", "This is supported in a limited way, provided that the index for the right argument is completely used in the join, and is a subset of the indices in the left argument, as in this example:", "If that condition is not satisfied, a join with two multi-indexes can be done using the following code.", "Strings passed as the on, left_on, and right_on parameters may refer to either column names or index level names. This enables merging DataFrame instances on a combination of index levels and columns without resetting indexes.", "Note", "When DataFrames are merged on a string that matches an index level in both frames, the index level is preserved as an index level in the resulting DataFrame.", "Note", "When DataFrames are merged using only some of the levels of a MultiIndex, the extra levels will be dropped from the resulting merge. In order to preserve those levels, use reset_index on those level names to move those levels to columns prior to doing the merge.", "Note", "If a string matches both a column name and an index level name, then a warning is issued and the column takes precedence. This will result in an ambiguity error in a future version.", "The merge suffixes argument takes a tuple of list of strings to append to overlapping column names in the input DataFrames to disambiguate the result columns:", "DataFrame.join() has lsuffix and rsuffix arguments which behave similarly.", "A list or tuple of DataFrames can also be passed to join() to join them together on their indexes.", "Another fairly common situation is to have two like-indexed (or similarly indexed) Series or DataFrame objects and wanting to \u201cpatch\u201d values in one object from values for matching indices in the other. Here is an example:", "For this, use the combine_first() method:", "Note that this method only takes values from the right DataFrame if they are missing in the left DataFrame. A related method, update(), alters non-NA values in place:", "A merge_ordered() function allows combining time series and other ordered data. In particular it has an optional fill_method keyword to fill/interpolate missing data:", "A merge_asof() is similar to an ordered left-join except that we match on nearest key rather than equal keys. For each row in the left DataFrame, we select the last row in the right DataFrame whose on key is less than the left\u2019s key. Both DataFrames must be sorted by the key.", "Optionally an asof merge can perform a group-wise merge. This matches the by key equally, in addition to the nearest match on the on key.", "For example; we might have trades and quotes and we want to asof merge them.", "By default we are taking the asof of the quotes.", "We only asof within 2ms between the quote time and the trade time.", "We only asof within 10ms between the quote time and the trade time and we exclude exact matches on time. Note that though we exclude the exact matches (of the quotes), prior quotes do propagate to that point in time.", "The compare() and compare() methods allow you to compare two DataFrame or Series, respectively, and summarize their differences.", "This feature was added in V1.1.0.", "For example, you might want to compare two DataFrame and stack their differences side by side.", "By default, if two corresponding values are equal, they will be shown as NaN. Furthermore, if all values in an entire row / column, the row / column will be omitted from the result. The remaining differences will be aligned on columns.", "If you wish, you may choose to stack the differences on rows.", "If you wish to keep all original rows and columns, set keep_shape argument to True.", "You may also keep all the original values even if they are equal."]}, {"name": "MultiIndex / advanced indexing", "path": "user_guide/advanced", "type": "Manual", "text": ["This section covers indexing with a MultiIndex and other advanced indexing features.", "See the Indexing and Selecting Data for general indexing documentation.", "Warning", "Whether a copy or a reference is returned for a setting operation may depend on the context. This is sometimes called chained assignment and should be avoided. See Returning a View versus Copy.", "See the cookbook for some advanced strategies.", "Hierarchical / Multi-level indexing is very exciting as it opens the door to some quite sophisticated data analysis and manipulation, especially for working with higher dimensional data. In essence, it enables you to store and manipulate data with an arbitrary number of dimensions in lower dimensional data structures like Series (1d) and DataFrame (2d).", "In this section, we will show what exactly we mean by \u201chierarchical\u201d indexing and how it integrates with all of the pandas indexing functionality described above and in prior sections. Later, when discussing group by and pivoting and reshaping data, we\u2019ll show non-trivial applications to illustrate how it aids in structuring data for analysis.", "See the cookbook for some advanced strategies.", "The MultiIndex object is the hierarchical analogue of the standard Index object which typically stores the axis labels in pandas objects. You can think of MultiIndex as an array of tuples where each tuple is unique. A MultiIndex can be created from a list of arrays (using MultiIndex.from_arrays()), an array of tuples (using MultiIndex.from_tuples()), a crossed set of iterables (using MultiIndex.from_product()), or a DataFrame (using MultiIndex.from_frame()). The Index constructor will attempt to return a MultiIndex when it is passed a list of tuples. The following examples demonstrate different ways to initialize MultiIndexes.", "When you want every pairing of the elements in two iterables, it can be easier to use the MultiIndex.from_product() method:", "You can also construct a MultiIndex from a DataFrame directly, using the method MultiIndex.from_frame(). This is a complementary method to MultiIndex.to_frame().", "As a convenience, you can pass a list of arrays directly into Series or DataFrame to construct a MultiIndex automatically:", "All of the MultiIndex constructors accept a names argument which stores string names for the levels themselves. If no names are provided, None will be assigned:", "This index can back any axis of a pandas object, and the number of levels of the index is up to you:", "We\u2019ve \u201csparsified\u201d the higher levels of the indexes to make the console output a bit easier on the eyes. Note that how the index is displayed can be controlled using the multi_sparse option in pandas.set_options():", "It\u2019s worth keeping in mind that there\u2019s nothing preventing you from using tuples as atomic labels on an axis:", "The reason that the MultiIndex matters is that it can allow you to do grouping, selection, and reshaping operations as we will describe below and in subsequent areas of the documentation. As you will see in later sections, you can find yourself working with hierarchically-indexed data without creating a MultiIndex explicitly yourself. However, when loading data from a file, you may wish to generate your own MultiIndex when preparing the data set.", "The method get_level_values() will return a vector of the labels for each location at a particular level:", "One of the important features of hierarchical indexing is that you can select data by a \u201cpartial\u201d label identifying a subgroup in the data. Partial selection \u201cdrops\u201d levels of the hierarchical index in the result in a completely analogous way to selecting a column in a regular DataFrame:", "See Cross-section with hierarchical index for how to select on a deeper level.", "The MultiIndex keeps all the defined levels of an index, even if they are not actually used. When slicing an index, you may notice this. For example:", "This is done to avoid a recomputation of the levels in order to make slicing highly performant. If you want to see only the used levels, you can use the get_level_values() method.", "To reconstruct the MultiIndex with only the used levels, the remove_unused_levels() method may be used.", "Operations between differently-indexed objects having MultiIndex on the axes will work as you expect; data alignment will work the same as an Index of tuples:", "The reindex() method of Series/DataFrames can be called with another MultiIndex, or even a list or array of tuples:", "Syntactically integrating MultiIndex in advanced indexing with .loc is a bit challenging, but we\u2019ve made every effort to do so. In general, MultiIndex keys take the form of tuples. For example, the following works as you would expect:", "Note that df.loc['bar', 'two'] would also work in this example, but this shorthand notation can lead to ambiguity in general.", "If you also want to index a specific column with .loc, you must use a tuple like this:", "You don\u2019t have to specify all levels of the MultiIndex by passing only the first elements of the tuple. For example, you can use \u201cpartial\u201d indexing to get all elements with bar in the first level as follows:", "This is a shortcut for the slightly more verbose notation df.loc[('bar',),] (equivalent to df.loc['bar',] in this example).", "\u201cPartial\u201d slicing also works quite nicely.", "You can slice with a \u2018range\u2019 of values, by providing a slice of tuples.", "Passing a list of labels or tuples works similar to reindexing:", "Note", "It is important to note that tuples and lists are not treated identically in pandas when it comes to indexing. Whereas a tuple is interpreted as one multi-level key, a list is used to specify several keys. Or in other words, tuples go horizontally (traversing levels), lists go vertically (scanning levels).", "Importantly, a list of tuples indexes several complete MultiIndex keys, whereas a tuple of lists refer to several values within a level:", "You can slice a MultiIndex by providing multiple indexers.", "You can provide any of the selectors as if you are indexing by label, see Selection by Label, including slices, lists of labels, labels, and boolean indexers.", "You can use slice(None) to select all the contents of that level. You do not need to specify all the deeper levels, they will be implied as slice(None).", "As usual, both sides of the slicers are included as this is label indexing.", "Warning", "You should specify all axes in the .loc specifier, meaning the indexer for the index and for the columns. There are some ambiguous cases where the passed indexer could be mis-interpreted as indexing both axes, rather than into say the MultiIndex for the rows.", "You should do this:", "You should not do this:", "Basic MultiIndex slicing using slices, lists, and labels.", "You can use pandas.IndexSlice to facilitate a more natural syntax using :, rather than using slice(None).", "It is possible to perform quite complicated selections using this method on multiple axes at the same time.", "Using a boolean indexer you can provide selection related to the values.", "You can also specify the axis argument to .loc to interpret the passed slicers on a single axis.", "Furthermore, you can set the values using the following methods.", "You can use a right-hand-side of an alignable object as well.", "The xs() method of DataFrame additionally takes a level argument to make selecting data at a particular level of a MultiIndex easier.", "You can also select on the columns with xs, by providing the axis argument.", "xs also allows selection with multiple keys.", "You can pass drop_level=False to xs to retain the level that was selected.", "Compare the above with the result using drop_level=True (the default value).", "Using the parameter level in the reindex() and align() methods of pandas objects is useful to broadcast values across a level. For instance:", "The swaplevel() method can switch the order of two levels:", "The reorder_levels() method generalizes the swaplevel method, allowing you to permute the hierarchical index levels in one step:", "The rename() method is used to rename the labels of a MultiIndex, and is typically used to rename the columns of a DataFrame. The columns argument of rename allows a dictionary to be specified that includes only the columns you wish to rename.", "This method can also be used to rename specific labels of the main index of the DataFrame.", "The rename_axis() method is used to rename the name of a Index or MultiIndex. In particular, the names of the levels of a MultiIndex can be specified, which is useful if reset_index() is later used to move the values from the MultiIndex to a column.", "Note that the columns of a DataFrame are an index, so that using rename_axis with the columns argument will change the name of that index.", "Both rename and rename_axis support specifying a dictionary, Series or a mapping function to map labels/names to new values.", "When working with an Index object directly, rather than via a DataFrame, Index.set_names() can be used to change the names.", "You cannot set the names of the MultiIndex via a level.", "Use Index.set_names() instead.", "For MultiIndex-ed objects to be indexed and sliced effectively, they need to be sorted. As with any index, you can use sort_index().", "You may also pass a level name to sort_index if the MultiIndex levels are named.", "On higher dimensional objects, you can sort any of the other axes by level if they have a MultiIndex:", "Indexing will work even if the data are not sorted, but will be rather inefficient (and show a PerformanceWarning). It will also return a copy of the data rather than a view:", "Furthermore, if you try to index something that is not fully lexsorted, this can raise:", "The is_monotonic_increasing() method on a MultiIndex shows if the index is sorted:", "And now selection works as expected.", "Similar to NumPy ndarrays, pandas Index, Series, and DataFrame also provides the take() method that retrieves elements along a given axis at the given indices. The given indices must be either a list or an ndarray of integer index positions. take will also accept negative integers as relative positions to the end of the object.", "For DataFrames, the given indices should be a 1d list or ndarray that specifies row or column positions.", "It is important to note that the take method on pandas objects are not intended to work on boolean indices and may return unexpected results.", "Finally, as a small note on performance, because the take method handles a narrower range of inputs, it can offer performance that is a good deal faster than fancy indexing.", "We have discussed MultiIndex in the previous sections pretty extensively. Documentation about DatetimeIndex and PeriodIndex are shown here, and documentation about TimedeltaIndex is found here.", "In the following sub-sections we will highlight some other index types.", "CategoricalIndex is a type of index that is useful for supporting indexing with duplicates. This is a container around a Categorical and allows efficient indexing and storage of an index with a large number of duplicated elements.", "Setting the index will create a CategoricalIndex.", "Indexing with __getitem__/.iloc/.loc works similarly to an Index with duplicates. The indexers must be in the category or the operation will raise a KeyError.", "The CategoricalIndex is preserved after indexing:", "Sorting the index will sort by the order of the categories (recall that we created the index with CategoricalDtype(list('cab')), so the sorted order is cab).", "Groupby operations on the index will preserve the index nature as well.", "Reindexing operations will return a resulting index based on the type of the passed indexer. Passing a list will return a plain-old Index; indexing with a Categorical will return a CategoricalIndex, indexed according to the categories of the passed Categorical dtype. This allows one to arbitrarily index these even with values not in the categories, similarly to how you can reindex any pandas index.", "Warning", "Reshaping and Comparison operations on a CategoricalIndex must have the same categories or a TypeError will be raised.", "Deprecated since version 1.4.0: In pandas 2.0, Index will become the default index type for numeric types instead of Int64Index, Float64Index and UInt64Index and those index types are therefore deprecated and will be removed in a futire version. RangeIndex will not be removed, as it represents an optimized version of an integer index.", "Int64Index is a fundamental basic index in pandas. This is an immutable array implementing an ordered, sliceable set.", "RangeIndex is a sub-class of Int64Index that provides the default index for all NDFrame objects. RangeIndex is an optimized version of Int64Index that can represent a monotonic ordered set. These are analogous to Python range types.", "Deprecated since version 1.4.0: Index will become the default index type for numeric types in the future instead of Int64Index, Float64Index and UInt64Index and those index types are therefore deprecated and will be removed in a future version of Pandas. RangeIndex will not be removed as it represents an optimized version of an integer index.", "By default a Float64Index will be automatically created when passing floating, or mixed-integer-floating values in index creation. This enables a pure label-based slicing paradigm that makes [],ix,loc for scalar indexing and slicing work exactly the same.", "Scalar selection for [],.loc will always be label based. An integer will match an equal float index (e.g. 3 is equivalent to 3.0).", "The only positional indexing is via iloc.", "A scalar index that is not found will raise a KeyError. Slicing is primarily on the values of the index when using [],ix,loc, and always positional when using iloc. The exception is when the slice is boolean, in which case it will always be positional.", "In float indexes, slicing using floats is allowed.", "In non-float indexes, slicing using floats will raise a TypeError.", "Here is a typical use-case for using this type of indexing. Imagine that you have a somewhat irregular timedelta-like indexing scheme, but the data is recorded as floats. This could, for example, be millisecond offsets.", "Selection operations then will always work on a value basis, for all selection operators.", "You could retrieve the first 1 second (1000 ms) of data as such:", "If you need integer based selection, you should use iloc:", "IntervalIndex together with its own dtype, IntervalDtype as well as the Interval scalar type, allow first-class support in pandas for interval notation.", "The IntervalIndex allows some unique indexing and is also used as a return type for the categories in cut() and qcut().", "An IntervalIndex can be used in Series and in DataFrame as the index.", "Label based indexing via .loc along the edges of an interval works as you would expect, selecting that particular interval.", "If you select a label contained within an interval, this will also select the interval.", "Selecting using an Interval will only return exact matches (starting from pandas 0.25.0).", "Trying to select an Interval that is not exactly contained in the IntervalIndex will raise a KeyError.", "Selecting all Intervals that overlap a given Interval can be performed using the overlaps() method to create a boolean indexer.", "cut() and qcut() both return a Categorical object, and the bins they create are stored as an IntervalIndex in its .categories attribute.", "cut() also accepts an IntervalIndex for its bins argument, which enables a useful pandas idiom. First, We call cut() with some data and bins set to a fixed number, to generate the bins. Then, we pass the values of .categories as the bins argument in subsequent calls to cut(), supplying new data which will be binned into the same bins.", "Any value which falls outside all bins will be assigned a NaN value.", "If we need intervals on a regular frequency, we can use the interval_range() function to create an IntervalIndex using various combinations of start, end, and periods. The default frequency for interval_range is a 1 for numeric intervals, and calendar day for datetime-like intervals:", "The freq parameter can used to specify non-default frequencies, and can utilize a variety of frequency aliases with datetime-like intervals:", "Additionally, the closed parameter can be used to specify which side(s) the intervals are closed on. Intervals are closed on the right side by default.", "Specifying start, end, and periods will generate a range of evenly spaced intervals from start to end inclusively, with periods number of elements in the resulting IntervalIndex:", "Label-based indexing with integer axis labels is a thorny topic. It has been discussed heavily on mailing lists and among various members of the scientific Python community. In pandas, our general viewpoint is that labels matter more than integer locations. Therefore, with an integer axis index only label-based indexing is possible with the standard tools like .loc. The following code will generate exceptions:", "This deliberate decision was made to prevent ambiguities and subtle bugs (many users reported finding bugs when the API change was made to stop \u201cfalling back\u201d on position-based indexing).", "If the index of a Series or DataFrame is monotonically increasing or decreasing, then the bounds of a label-based slice can be outside the range of the index, much like slice indexing a normal Python list. Monotonicity of an index can be tested with the is_monotonic_increasing() and is_monotonic_decreasing() attributes.", "On the other hand, if the index is not monotonic, then both slice bounds must be unique members of the index.", "Index.is_monotonic_increasing and Index.is_monotonic_decreasing only check that an index is weakly monotonic. To check for strict monotonicity, you can combine one of those with the is_unique() attribute.", "Compared with standard Python sequence slicing in which the slice endpoint is not inclusive, label-based slicing in pandas is inclusive. The primary reason for this is that it is often not possible to easily determine the \u201csuccessor\u201d or next element after a particular label in an index. For example, consider the following Series:", "Suppose we wished to slice from c to e, using integers this would be accomplished as such:", "However, if you only had c and e, determining the next element in the index can be somewhat complicated. For example, the following does not work:", "A very common use case is to limit a time series to start and end at two specific dates. To enable this, we made the design choice to make label-based slicing include both endpoints:", "This is most definitely a \u201cpracticality beats purity\u201d sort of thing, but it is something to watch out for if you expect label-based slicing to behave exactly in the way that standard Python integer slicing works.", "The different indexing operation can potentially change the dtype of a Series.", "This is because the (re)indexing operations above silently inserts NaNs and the dtype changes accordingly. This can cause some issues when using numpy ufuncs such as numpy.logical_and.", "See the GH2388 for a more detailed discussion."]}, {"name": "Nullable Boolean data type", "path": "user_guide/boolean", "type": "Manual", "text": ["Note", "BooleanArray is currently experimental. Its API or implementation may change without warning.", "New in version 1.0.0.", "pandas allows indexing with NA values in a boolean array, which are treated as False.", "Changed in version 1.0.2.", "If you would prefer to keep the NA values you can manually fill them with fillna(True).", "arrays.BooleanArray implements Kleene Logic (sometimes called three-value logic) for logical operations like & (and), | (or) and ^ (exclusive-or).", "This table demonstrates the results for every combination. These operations are symmetrical, so flipping the left- and right-hand side makes no difference in the result.", "Expression", "Result", "True & True", "True", "True & False", "False", "True & NA", "NA", "False & False", "False", "False & NA", "False", "NA & NA", "NA", "True | True", "True", "True | False", "True", "True | NA", "True", "False | False", "False", "False | NA", "NA", "NA | NA", "NA", "True ^ True", "False", "True ^ False", "True", "True ^ NA", "NA", "False ^ False", "False", "False ^ NA", "NA", "NA ^ NA", "NA", "When an NA is present in an operation, the output value is NA only if the result cannot be determined solely based on the other input. For example, True | NA is True, because both True | True and True | False are True. In that case, we don\u2019t actually need to consider the value of the NA.", "On the other hand, True & NA is NA. The result depends on whether the NA really is True or False, since True & True is True, but True & False is False, so we can\u2019t determine the output.", "This differs from how np.nan behaves in logical operations. pandas treated np.nan is always false in the output.", "In or", "In and"]}, {"name": "Nullable integer data type", "path": "user_guide/integer_na", "type": "Manual", "text": ["Note", "IntegerArray is currently experimental. Its API or implementation may change without warning.", "Changed in version 1.0.0: Now uses pandas.NA as the missing value rather than numpy.nan.", "In Working with missing data, we saw that pandas primarily uses NaN to represent missing data. Because NaN is a float, this forces an array of integers with any missing values to become floating point. In some cases, this may not matter much. But if your integer column is, say, an identifier, casting to float can be problematic. Some integers cannot even be represented as floating point numbers.", "pandas can represent integer data with possibly missing values using arrays.IntegerArray. This is an extension types implemented within pandas.", "Or the string alias \"Int64\" (note the capital \"I\", to differentiate from NumPy\u2019s 'int64' dtype:", "All NA-like values are replaced with pandas.NA.", "This array can be stored in a DataFrame or Series like any NumPy array.", "You can also pass the list-like object to the Series constructor with the dtype.", "Warning", "Currently pandas.array() and pandas.Series() use different rules for dtype inference. pandas.array() will infer a nullable- integer dtype", "For backwards-compatibility, Series infers these as either integer or float dtype", "We recommend explicitly providing the dtype to avoid confusion.", "In the future, we may provide an option for Series to infer a nullable-integer dtype.", "Operations involving an integer array will behave similar to NumPy arrays. Missing values will be propagated, and the data will be coerced to another dtype if needed.", "These dtypes can operate as part of DataFrame.", "These dtypes can be merged & reshaped & casted.", "Reduction and groupby operations such as \u2018sum\u2019 work as well.", "arrays.IntegerArray uses pandas.NA as its scalar missing value. Slicing a single element that\u2019s missing will return pandas.NA"]}, {"name": "Options and settings", "path": "user_guide/options", "type": "Manual", "text": ["pandas has an options system that lets you customize some aspects of its behaviour, display-related options being those the user is most likely to adjust.", "Options have a full \u201cdotted-style\u201d, case-insensitive name (e.g. display.max_rows). You can get/set options directly as attributes of the top-level options attribute:", "The API is composed of 5 relevant functions, available directly from the pandas namespace:", "get_option() / set_option() - get/set the value of a single option.", "reset_option() - reset one or more options to their default value.", "describe_option() - print the descriptions of one or more options.", "option_context() - execute a codeblock with a set of options that revert to prior settings after execution.", "Note: Developers can check out pandas/core/config_init.py for more information.", "All of the functions above accept a regexp pattern (re.search style) as an argument, and so passing in a substring will work - as long as it is unambiguous:", "The following will not work because it matches multiple option names, e.g. display.max_colwidth, display.max_rows, display.max_columns:", "Note: Using this form of shorthand may cause your code to break if new options with similar names are added in future versions.", "You can get a list of available options and their descriptions with describe_option. When called with no argument describe_option will print out the descriptions for all available options.", "As described above, get_option() and set_option() are available from the pandas namespace. To change an option, call set_option('option regex', new_value).", "Note: The option \u2018mode.sim_interactive\u2019 is mostly used for debugging purposes.", "All options also have a default value, and you can use reset_option to do just that:", "It\u2019s also possible to reset multiple options at once (using a regex):", "option_context context manager has been exposed through the top-level API, allowing you to execute code with given option values. Option values are restored automatically when you exit the with block:", "Using startup scripts for the Python/IPython environment to import pandas and set options makes working with pandas more efficient. To do this, create a .py or .ipy script in the startup directory of the desired profile. An example where the startup folder is in a default IPython profile can be found at:", "More information can be found in the IPython documentation. An example startup script for pandas is displayed below:", "The following is a walk-through of the more frequently used display options.", "display.max_rows and display.max_columns sets the maximum number of rows and columns displayed when a frame is pretty-printed. Truncated lines are replaced by an ellipsis.", "Once the display.max_rows is exceeded, the display.min_rows options determines how many rows are shown in the truncated repr.", "display.expand_frame_repr allows for the representation of dataframes to stretch across pages, wrapped over the full column vs row-wise.", "display.large_repr lets you select whether to display dataframes that exceed max_columns or max_rows as a truncated frame, or as a summary.", "display.max_colwidth sets the maximum width of columns. Cells of this length or longer will be truncated with an ellipsis.", "display.max_info_columns sets a threshold for when by-column info will be given.", "display.max_info_rows: df.info() will usually show null-counts for each column. For large frames this can be quite slow. max_info_rows and max_info_cols limit this null check only to frames with smaller dimensions then specified. Note that you can specify the option df.info(null_counts=True) to override on showing a particular frame.", "display.precision sets the output display precision in terms of decimal places. This is only a suggestion.", "display.chop_threshold sets at what level pandas rounds to zero when it displays a Series of DataFrame. This setting does not change the precision at which the number is stored.", "display.colheader_justify controls the justification of the headers. The options are \u2018right\u2019, and \u2018left\u2019.", "Option", "Default", "Function", "display.chop_threshold", "None", "If set to a float value, all float values smaller then the given threshold will be displayed as exactly 0 by repr and friends.", "display.colheader_justify", "right", "Controls the justification of column headers. used by DataFrameFormatter.", "display.column_space", "12", "No description available.", "display.date_dayfirst", "False", "When True, prints and parses dates with the day first, eg 20/01/2005", "display.date_yearfirst", "False", "When True, prints and parses dates with the year first, eg 2005/01/20", "display.encoding", "UTF-8", "Defaults to the detected encoding of the console. Specifies the encoding to be used for strings returned by to_string, these are generally strings meant to be displayed on the console.", "display.expand_frame_repr", "True", "Whether to print out the full DataFrame repr for wide DataFrames across multiple lines, max_columns is still respected, but the output will wrap-around across multiple \u201cpages\u201d if its width exceeds display.width.", "display.float_format", "None", "The callable should accept a floating point number and return a string with the desired format of the number. This is used in some places like SeriesFormatter. See core.format.EngFormatter for an example.", "display.large_repr", "truncate", "For DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show a truncated table (the default), or switch to the view from df.info() (the behaviour in earlier versions of pandas). allowable settings, [\u2018truncate\u2019, \u2018info\u2019]", "display.latex.repr", "False", "Whether to produce a latex DataFrame representation for Jupyter frontends that support it.", "display.latex.escape", "True", "Escapes special characters in DataFrames, when using the to_latex method.", "display.latex.longtable", "False", "Specifies if the to_latex method of a DataFrame uses the longtable format.", "display.latex.multicolumn", "True", "Combines columns when using a MultiIndex", "display.latex.multicolumn_format", "\u2018l\u2019", "Alignment of multicolumn labels", "display.latex.multirow", "False", "Combines rows when using a MultiIndex. Centered instead of top-aligned, separated by clines.", "display.max_columns", "0 or 20", "max_rows and max_columns are used in __repr__() methods to decide if to_string() or info() is used to render an object to a string. In case Python/IPython is running in a terminal this is set to 0 by default and pandas will correctly auto-detect the width of the terminal and switch to a smaller format in case all columns would not fit vertically. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection, in which case the default is set to 20. \u2018None\u2019 value means unlimited.", "display.max_colwidth", "50", "The maximum width in characters of a column in the repr of a pandas data structure. When the column overflows, a \u201c\u2026\u201d placeholder is embedded in the output. \u2018None\u2019 value means unlimited.", "display.max_info_columns", "100", "max_info_columns is used in DataFrame.info method to decide if per column information will be printed.", "display.max_info_rows", "1690785", "df.info() will usually show null-counts for each column. For large frames this can be quite slow. max_info_rows and max_info_cols limit this null check only to frames with smaller dimensions then specified.", "display.max_rows", "60", "This sets the maximum number of rows pandas should output when printing out various output. For example, this value determines whether the repr() for a dataframe prints out fully or just a truncated or summary repr. \u2018None\u2019 value means unlimited.", "display.min_rows", "10", "The numbers of rows to show in a truncated repr (when max_rows is exceeded). Ignored when max_rows is set to None or 0. When set to None, follows the value of max_rows.", "display.max_seq_items", "100", "when pretty-printing a long sequence, no more then max_seq_items will be printed. If items are omitted, they will be denoted by the addition of \u201c\u2026\u201d to the resulting string. If set to None, the number of items to be printed is unlimited.", "display.memory_usage", "True", "This specifies if the memory usage of a DataFrame should be displayed when the df.info() method is invoked.", "display.multi_sparse", "True", "\u201cSparsify\u201d MultiIndex display (don\u2019t display repeated elements in outer levels within groups)", "display.notebook_repr_html", "True", "When True, IPython notebook will use html representation for pandas objects (if it is available).", "display.pprint_nest_depth", "3", "Controls the number of nested levels to process when pretty-printing", "display.precision", "6", "Floating point output precision in terms of number of places after the decimal, for regular formatting as well as scientific notation. Similar to numpy\u2019s precision print option", "display.show_dimensions", "truncate", "Whether to print out dimensions at the end of DataFrame repr. If \u2018truncate\u2019 is specified, only print out the dimensions if the frame is truncated (e.g. not display all rows and/or columns)", "display.width", "80", "Width of the display in characters. In case Python/IPython is running in a terminal this can be set to None and pandas will correctly auto-detect the width. Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to correctly detect the width.", "display.html.table_schema", "False", "Whether to publish a Table Schema representation for frontends that support it.", "display.html.border", "1", "A border=value attribute is inserted in the <table> tag for the DataFrame HTML repr.", "display.html.use_mathjax", "True", "When True, Jupyter notebook will process table contents using MathJax, rendering mathematical expressions enclosed by the dollar symbol.", "display.max_dir_items", "100", "The number of columns from a dataframe that are added to dir. These columns can then be suggested by tab completion. \u2018None\u2019 value means unlimited.", "io.excel.xls.writer", "xlwt", "The default Excel writer engine for \u2018xls\u2019 files.", "Deprecated since version 1.2.0: As xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas. Since this is the only engine in pandas that supports writing to .xls files, this option will also be removed.", "io.excel.xlsm.writer", "openpyxl", "The default Excel writer engine for \u2018xlsm\u2019 files. Available options: \u2018openpyxl\u2019 (the default).", "io.excel.xlsx.writer", "openpyxl", "The default Excel writer engine for \u2018xlsx\u2019 files.", "io.hdf.default_format", "None", "default format writing format, if None, then put will default to \u2018fixed\u2019 and append will default to \u2018table\u2019", "io.hdf.dropna_table", "True", "drop ALL nan rows when appending to a table", "io.parquet.engine", "None", "The engine to use as a default for parquet reading and writing. If None then try \u2018pyarrow\u2019 and \u2018fastparquet\u2019", "io.sql.engine", "None", "The engine to use as a default for sql reading and writing, with SQLAlchemy as a higher level interface. If None then try \u2018sqlalchemy\u2019", "mode.chained_assignment", "warn", "Controls SettingWithCopyWarning: \u2018raise\u2019, \u2018warn\u2019, or None. Raise an exception, warn, or no action if trying to use chained assignment.", "mode.sim_interactive", "False", "Whether to simulate interactive mode for purposes of testing.", "mode.use_inf_as_na", "False", "True means treat None, NaN, -INF, INF as NA (old way), False means None and NaN are null, but INF, -INF are not NA (new way).", "compute.use_bottleneck", "True", "Use the bottleneck library to accelerate computation if it is installed.", "compute.use_numexpr", "True", "Use the numexpr library to accelerate computation if it is installed.", "plotting.backend", "matplotlib", "Change the plotting backend to a different backend than the current matplotlib one. Backends can be implemented as third-party libraries implementing the pandas plotting API. They can use other plotting libraries like Bokeh, Altair, etc.", "plotting.matplotlib.register_converters", "True", "Register custom converters with matplotlib. Set to False to de-register.", "styler.sparse.index", "True", "\u201cSparsify\u201d MultiIndex display for rows in Styler output (don\u2019t display repeated elements in outer levels within groups).", "styler.sparse.columns", "True", "\u201cSparsify\u201d MultiIndex display for columns in Styler output.", "styler.render.repr", "html", "Standard output format for Styler rendered in Jupyter Notebook. Should be one of \u201chtml\u201d or \u201clatex\u201d.", "styler.render.max_elements", "262144", "Maximum number of datapoints that Styler will render trimming either rows, columns or both to fit.", "styler.render.max_rows", "None", "Maximum number of rows that Styler will render. By default this is dynamic based on max_elements.", "styler.render.max_columns", "None", "Maximum number of columns that Styler will render. By default this is dynamic based on max_elements.", "styler.render.encoding", "utf-8", "Default encoding for output HTML or LaTeX files.", "styler.format.formatter", "None", "Object to specify formatting functions to Styler.format.", "styler.format.na_rep", "None", "String representation for missing data.", "styler.format.precision", "6", "Precision to display floating point and complex numbers.", "styler.format.decimal", ".", "String representation for decimal point separator for floating point and complex numbers.", "styler.format.thousands", "None", "String representation for thousands separator for integers, and floating point and complex numbers.", "styler.format.escape", "None", "Whether to escape \u201chtml\u201d or \u201clatex\u201d special characters in the display representation.", "styler.html.mathjax", "True", "If set to False will render specific CSS classes to table attributes that will prevent Mathjax from rendering in Jupyter Notebook.", "styler.latex.multicol_align", "r", "Alignment of headers in a merged column due to sparsification. Can be in {\u201cr\u201d, \u201cc\u201d, \u201cl\u201d}.", "styler.latex.multirow_align", "c", "Alignment of index labels in a merged row due to sparsification. Can be in {\u201cc\u201d, \u201ct\u201d, \u201cb\u201d}.", "styler.latex.environment", "None", "If given will replace the default \\\\begin{table} environment. If \u201clongtable\u201d is specified this will render with a specific \u201clongtable\u201d template with longtable features.", "styler.latex.hrules", "False", "If set to True will render \\\\toprule, \\\\midrule, and \\bottomrule by default.", "pandas also allows you to set how numbers are displayed in the console. This option is not set through the set_options API.", "Use the set_eng_float_format function to alter the floating-point formatting of pandas objects to produce a particular format.", "For instance:", "To round floats on a case-by-case basis, you can also use round() and round().", "Warning", "Enabling this option will affect the performance for printing of DataFrame and Series (about 2 times slower). Use only when it is actually required.", "Some East Asian countries use Unicode characters whose width corresponds to two Latin characters. If a DataFrame or Series contains these characters, the default output mode may not align them properly.", "Note", "Screen captures are attached for each output to show the actual results.", "Enabling display.unicode.east_asian_width allows pandas to check each character\u2019s \u201cEast Asian Width\u201d property. These characters can be aligned properly by setting this option to True. However, this will result in longer render times than the standard len function.", "In addition, Unicode characters whose width is \u201cAmbiguous\u201d can either be 1 or 2 characters wide depending on the terminal setting or encoding. The option display.unicode.ambiguous_as_wide can be used to handle the ambiguity.", "By default, an \u201cAmbiguous\u201d character\u2019s width, such as \u201c\u00a1\u201d (inverted exclamation) in the example below, is taken to be 1.", "Enabling display.unicode.ambiguous_as_wide makes pandas interpret these characters\u2019 widths to be 2. (Note that this option will only be effective when display.unicode.east_asian_width is enabled.)", "However, setting this option incorrectly for your terminal will cause these characters to be aligned incorrectly:", "DataFrame and Series will publish a Table Schema representation by default. False by default, this can be enabled globally with the display.html.table_schema option:", "Only 'display.max_rows' are serialized and published."]}, {"name": "pandas arrays, scalars, and data types", "path": "reference/arrays", "type": "Pandas arrays", "text": ["For most data types, pandas uses NumPy arrays as the concrete objects contained with a Index, Series, or DataFrame.", "For some data types, pandas extends NumPy\u2019s type system. String aliases for these types can be found at dtypes.", "Kind of Data", "pandas Data Type", "Scalar", "Array", "TZ-aware datetime", "DatetimeTZDtype", "Timestamp", "Datetime data", "Timedeltas", "(none)", "Timedelta", "Timedelta data", "Period (time spans)", "PeriodDtype", "Period", "Timespan data", "Intervals", "IntervalDtype", "Interval", "Interval data", "Nullable Integer", "Int64Dtype, \u2026", "(none)", "Nullable integer", "Categorical", "CategoricalDtype", "(none)", "Categorical data", "Sparse", "SparseDtype", "(none)", "Sparse data", "Strings", "StringDtype", "str", "Text data", "Boolean (with NA)", "BooleanDtype", "bool", "Boolean data with missing values", "pandas and third-party libraries can extend NumPy\u2019s type system (see Extension types). The top-level array() method can be used to create a new array, which may be stored in a Series, Index, or as a column in a DataFrame.", "array(data[, dtype, copy])", "Create an array.", "NumPy cannot natively represent timezone-aware datetimes. pandas supports this with the arrays.DatetimeArray extension array, which can hold timezone-naive or timezone-aware values.", "Timestamp, a subclass of datetime.datetime, is pandas\u2019 scalar type for timezone-naive or timezone-aware datetime data.", "Timestamp([ts_input, freq, tz, unit, year, ...])", "Pandas replacement for python datetime.datetime object.", "Timestamp.asm8", "Return numpy datetime64 format in nanoseconds.", "Timestamp.day", "Timestamp.dayofweek", "Return day of the week.", "Timestamp.day_of_week", "Return day of the week.", "Timestamp.dayofyear", "Return the day of the year.", "Timestamp.day_of_year", "Return the day of the year.", "Timestamp.days_in_month", "Return the number of days in the month.", "Timestamp.daysinmonth", "Return the number of days in the month.", "Timestamp.fold", "Timestamp.hour", "Timestamp.is_leap_year", "Return True if year is a leap year.", "Timestamp.is_month_end", "Return True if date is last day of month.", "Timestamp.is_month_start", "Return True if date is first day of month.", "Timestamp.is_quarter_end", "Return True if date is last day of the quarter.", "Timestamp.is_quarter_start", "Return True if date is first day of the quarter.", "Timestamp.is_year_end", "Return True if date is last day of the year.", "Timestamp.is_year_start", "Return True if date is first day of the year.", "Timestamp.max", "Timestamp.microsecond", "Timestamp.min", "Timestamp.minute", "Timestamp.month", "Timestamp.nanosecond", "Timestamp.quarter", "Return the quarter of the year.", "Timestamp.resolution", "Timestamp.second", "Timestamp.tz", "Alias for tzinfo.", "Timestamp.tzinfo", "Timestamp.value", "Timestamp.week", "Return the week number of the year.", "Timestamp.weekofyear", "Return the week number of the year.", "Timestamp.year", "Timestamp.astimezone(tz)", "Convert timezone-aware Timestamp to another time zone.", "Timestamp.ceil(freq[, ambiguous, nonexistent])", "Return a new Timestamp ceiled to this resolution.", "Timestamp.combine(date, time)", "Combine date, time into datetime with same date and time fields.", "Timestamp.ctime", "Return ctime() style string.", "Timestamp.date", "Return date object with same year, month and day.", "Timestamp.day_name", "Return the day name of the Timestamp with specified locale.", "Timestamp.dst", "Return self.tzinfo.dst(self).", "Timestamp.floor(freq[, ambiguous, nonexistent])", "Return a new Timestamp floored to this resolution.", "Timestamp.freq", "Timestamp.freqstr", "Return the total number of days in the month.", "Timestamp.fromordinal(ordinal[, freq, tz])", "Passed an ordinal, translate and convert to a ts.", "Timestamp.fromtimestamp(ts)", "Transform timestamp[, tz] to tz's local time from POSIX timestamp.", "Timestamp.isocalendar", "Return a 3-tuple containing ISO year, week number, and weekday.", "Timestamp.isoformat", "Return the time formatted according to ISO 8610.", "Timestamp.isoweekday()", "Return the day of the week represented by the date.", "Timestamp.month_name", "Return the month name of the Timestamp with specified locale.", "Timestamp.normalize", "Normalize Timestamp to midnight, preserving tz information.", "Timestamp.now([tz])", "Return new Timestamp object representing current time local to tz.", "Timestamp.replace([year, month, day, hour, ...])", "Implements datetime.replace, handles nanoseconds.", "Timestamp.round(freq[, ambiguous, nonexistent])", "Round the Timestamp to the specified resolution.", "Timestamp.strftime(format)", "Return a string representing the given POSIX timestamp controlled by an explicit format string.", "Timestamp.strptime(string, format)", "Function is not implemented.", "Timestamp.time", "Return time object with same time but with tzinfo=None.", "Timestamp.timestamp", "Return POSIX timestamp as float.", "Timestamp.timetuple", "Return time tuple, compatible with time.localtime().", "Timestamp.timetz", "Return time object with same time and tzinfo.", "Timestamp.to_datetime64", "Return a numpy.datetime64 object with 'ns' precision.", "Timestamp.to_numpy", "Convert the Timestamp to a NumPy datetime64.", "Timestamp.to_julian_date()", "Convert TimeStamp to a Julian Date.", "Timestamp.to_period", "Return an period of which this timestamp is an observation.", "Timestamp.to_pydatetime", "Convert a Timestamp object to a native Python datetime object.", "Timestamp.today(cls[, tz])", "Return the current time in the local timezone.", "Timestamp.toordinal", "Return proleptic Gregorian ordinal.", "Timestamp.tz_convert(tz)", "Convert timezone-aware Timestamp to another time zone.", "Timestamp.tz_localize(tz[, ambiguous, ...])", "Convert naive Timestamp to local time zone, or remove timezone from timezone-aware Timestamp.", "Timestamp.tzname", "Return self.tzinfo.tzname(self).", "Timestamp.utcfromtimestamp(ts)", "Construct a naive UTC datetime from a POSIX timestamp.", "Timestamp.utcnow()", "Return a new Timestamp representing UTC day and time.", "Timestamp.utcoffset", "Return self.tzinfo.utcoffset(self).", "Timestamp.utctimetuple", "Return UTC time tuple, compatible with time.localtime().", "Timestamp.weekday()", "Return the day of the week represented by the date.", "A collection of timestamps may be stored in a arrays.DatetimeArray. For timezone-aware data, the .dtype of a arrays.DatetimeArray is a DatetimeTZDtype. For timezone-naive data, np.dtype(\"datetime64[ns]\") is used.", "If the data are timezone-aware, then every value in the array must have the same timezone.", "arrays.DatetimeArray(values[, dtype, freq, copy])", "Pandas ExtensionArray for tz-naive or tz-aware datetime data.", "DatetimeTZDtype([unit, tz])", "An ExtensionDtype for timezone-aware datetime data.", "NumPy can natively represent timedeltas. pandas provides Timedelta for symmetry with Timestamp.", "Timedelta([value, unit])", "Represents a duration, the difference between two dates or times.", "Timedelta.asm8", "Return a numpy timedelta64 array scalar view.", "Timedelta.components", "Return a components namedtuple-like.", "Timedelta.days", "Number of days.", "Timedelta.delta", "Return the timedelta in nanoseconds (ns), for internal compatibility.", "Timedelta.freq", "Timedelta.is_populated", "Timedelta.max", "Timedelta.microseconds", "Number of microseconds (>= 0 and less than 1 second).", "Timedelta.min", "Timedelta.nanoseconds", "Return the number of nanoseconds (n), where 0 <= n < 1 microsecond.", "Timedelta.resolution", "Timedelta.seconds", "Number of seconds (>= 0 and less than 1 day).", "Timedelta.value", "Timedelta.view", "Array view compatibility.", "Timedelta.ceil(freq)", "Return a new Timedelta ceiled to this resolution.", "Timedelta.floor(freq)", "Return a new Timedelta floored to this resolution.", "Timedelta.isoformat", "Format Timedelta as ISO 8601 Duration like P[n]Y[n]M[n]DT[n]H[n]M[n]S, where the [n] s are replaced by the values.", "Timedelta.round(freq)", "Round the Timedelta to the specified resolution.", "Timedelta.to_pytimedelta", "Convert a pandas Timedelta object into a python datetime.timedelta object.", "Timedelta.to_timedelta64", "Return a numpy.timedelta64 object with 'ns' precision.", "Timedelta.to_numpy", "Convert the Timedelta to a NumPy timedelta64.", "Timedelta.total_seconds", "Total seconds in the duration.", "A collection of Timedelta may be stored in a TimedeltaArray.", "arrays.TimedeltaArray(values[, dtype, freq, ...])", "Pandas ExtensionArray for timedelta data.", "pandas represents spans of times as Period objects.", "Period([value, freq, ordinal, year, month, ...])", "Represents a period of time.", "Period.day", "Get day of the month that a Period falls on.", "Period.dayofweek", "Day of the week the period lies in, with Monday=0 and Sunday=6.", "Period.day_of_week", "Day of the week the period lies in, with Monday=0 and Sunday=6.", "Period.dayofyear", "Return the day of the year.", "Period.day_of_year", "Return the day of the year.", "Period.days_in_month", "Get the total number of days in the month that this period falls on.", "Period.daysinmonth", "Get the total number of days of the month that the Period falls in.", "Period.end_time", "Get the Timestamp for the end of the period.", "Period.freq", "Period.freqstr", "Return a string representation of the frequency.", "Period.hour", "Get the hour of the day component of the Period.", "Period.is_leap_year", "Return True if the period's year is in a leap year.", "Period.minute", "Get minute of the hour component of the Period.", "Period.month", "Return the month this Period falls on.", "Period.ordinal", "Period.quarter", "Return the quarter this Period falls on.", "Period.qyear", "Fiscal year the Period lies in according to its starting-quarter.", "Period.second", "Get the second component of the Period.", "Period.start_time", "Get the Timestamp for the start of the period.", "Period.week", "Get the week of the year on the given Period.", "Period.weekday", "Day of the week the period lies in, with Monday=0 and Sunday=6.", "Period.weekofyear", "Get the week of the year on the given Period.", "Period.year", "Return the year this Period falls on.", "Period.asfreq", "Convert Period to desired frequency, at the start or end of the interval.", "Period.now", "Return the period of now's date.", "Period.strftime", "Returns the string representation of the Period, depending on the selected fmt.", "Period.to_timestamp", "Return the Timestamp representation of the Period.", "A collection of Period may be stored in a arrays.PeriodArray. Every period in a arrays.PeriodArray must have the same freq.", "arrays.PeriodArray(values[, dtype, freq, copy])", "Pandas ExtensionArray for storing Period data.", "PeriodDtype([freq])", "An ExtensionDtype for Period data.", "Arbitrary intervals can be represented as Interval objects.", "Interval", "Immutable object implementing an Interval, a bounded slice-like interval.", "Interval.closed", "Whether the interval is closed on the left-side, right-side, both or neither.", "Interval.closed_left", "Check if the interval is closed on the left side.", "Interval.closed_right", "Check if the interval is closed on the right side.", "Interval.is_empty", "Indicates if an interval is empty, meaning it contains no points.", "Interval.left", "Left bound for the interval.", "Interval.length", "Return the length of the Interval.", "Interval.mid", "Return the midpoint of the Interval.", "Interval.open_left", "Check if the interval is open on the left side.", "Interval.open_right", "Check if the interval is open on the right side.", "Interval.overlaps", "Check whether two Interval objects overlap.", "Interval.right", "Right bound for the interval.", "A collection of intervals may be stored in an arrays.IntervalArray.", "arrays.IntervalArray(data[, closed, dtype, ...])", "Pandas array for interval data that are closed on the same side.", "IntervalDtype([subtype, closed])", "An ExtensionDtype for Interval data.", "numpy.ndarray cannot natively represent integer-data with missing values. pandas provides this through arrays.IntegerArray.", "arrays.IntegerArray(values, mask[, copy])", "Array of integer (optional missing) values.", "Int8Dtype()", "An ExtensionDtype for int8 integer data.", "Int16Dtype()", "An ExtensionDtype for int16 integer data.", "Int32Dtype()", "An ExtensionDtype for int32 integer data.", "Int64Dtype()", "An ExtensionDtype for int64 integer data.", "UInt8Dtype()", "An ExtensionDtype for uint8 integer data.", "UInt16Dtype()", "An ExtensionDtype for uint16 integer data.", "UInt32Dtype()", "An ExtensionDtype for uint32 integer data.", "UInt64Dtype()", "An ExtensionDtype for uint64 integer data.", "pandas defines a custom data type for representing data that can take only a limited, fixed set of values. The dtype of a Categorical can be described by a CategoricalDtype.", "CategoricalDtype([categories, ordered])", "Type for categorical data with the categories and orderedness.", "CategoricalDtype.categories", "An Index containing the unique categories allowed.", "CategoricalDtype.ordered", "Whether the categories have an ordered relationship.", "Categorical data can be stored in a pandas.Categorical", "Categorical(values[, categories, ordered, ...])", "Represent a categorical variable in classic R / S-plus fashion.", "The alternative Categorical.from_codes() constructor can be used when you have the categories and integer codes already:", "Categorical.from_codes(codes[, categories, ...])", "Make a Categorical type from codes and categories or dtype.", "The dtype information is available on the Categorical", "Categorical.dtype", "The CategoricalDtype for this instance.", "Categorical.categories", "The categories of this categorical.", "Categorical.ordered", "Whether the categories have an ordered relationship.", "Categorical.codes", "The category codes of this categorical.", "np.asarray(categorical) works by implementing the array interface. Be aware, that this converts the Categorical back to a NumPy array, so categories and order information is not preserved!", "Categorical.__array__([dtype])", "The numpy array interface.", "A Categorical can be stored in a Series or DataFrame. To create a Series of dtype category, use cat = s.astype(dtype) or Series(..., dtype=dtype) where dtype is either", "the string 'category'", "an instance of CategoricalDtype.", "If the Series is of dtype CategoricalDtype, Series.cat can be used to change the categorical data. See Categorical accessor for more.", "Data where a single value is repeated many times (e.g. 0 or NaN) may be stored efficiently as a arrays.SparseArray.", "arrays.SparseArray(data[, sparse_index, ...])", "An ExtensionArray for storing sparse data.", "SparseDtype([dtype, fill_value])", "Dtype for data stored in SparseArray.", "The Series.sparse accessor may be used to access sparse-specific attributes and methods if the Series contains sparse values. See Sparse accessor for more.", "When working with text data, where each valid element is a string or missing, we recommend using StringDtype (with the alias \"string\").", "arrays.StringArray(values[, copy])", "Extension array for string data.", "arrays.ArrowStringArray(values)", "Extension array for string data in a pyarrow.ChunkedArray.", "StringDtype([storage])", "Extension dtype for string data.", "The Series.str accessor is available for Series backed by a arrays.StringArray. See String handling for more.", "The boolean dtype (with the alias \"boolean\") provides support for storing boolean data (True, False) with missing values, which is not possible with a bool numpy.ndarray.", "arrays.BooleanArray(values, mask[, copy])", "Array of boolean (True/False) data with missing values.", "BooleanDtype()", "Extension dtype for boolean data."]}, {"name": "pandas.api.extensions.ExtensionArray", "path": "reference/api/pandas.api.extensions.extensionarray", "type": "Extensions", "text": ["Abstract base class for custom 1-D array types.", "pandas will recognize instances of this class as proper arrays with a custom type and will not attempt to coerce them to objects. They may be stored directly inside a DataFrame or Series.", "Notes", "The interface includes the following abstract methods that must be implemented by subclasses:", "_from_sequence", "_from_factorized", "__getitem__", "__len__", "__eq__", "dtype", "nbytes", "isna", "take", "copy", "_concat_same_type", "A default repr displaying the type, (truncated) data, length, and dtype is provided. It can be customized or replaced by by overriding:", "__repr__ : A default repr for the ExtensionArray.", "_formatter : Print scalars inside a Series or DataFrame.", "Some methods require casting the ExtensionArray to an ndarray of Python objects with self.astype(object), which may be expensive. When performance is a concern, we highly recommend overriding the following methods:", "fillna", "dropna", "unique", "factorize / _values_for_factorize", "argsort / _values_for_argsort", "searchsorted", "The remaining methods implemented on this class should be performant, as they only compose abstract methods. Still, a more efficient implementation may be available, and these methods can be overridden.", "One can implement methods to handle array reductions.", "_reduce", "One can implement methods to handle parsing from strings that will be used in methods such as pandas.io.parsers.read_csv.", "_from_sequence_of_strings", "This class does not inherit from \u2018abc.ABCMeta\u2019 for performance reasons. Methods and properties required by the interface raise pandas.errors.AbstractMethodError and no register method is provided for registering virtual subclasses.", "ExtensionArrays are limited to 1 dimension.", "They may be backed by none, one, or many NumPy arrays. For example, pandas.Categorical is an extension array backed by two arrays, one for codes and one for categories. An array of IPv6 address may be backed by a NumPy structured array with two fields, one for the lower 64 bits and one for the upper 64 bits. Or they may be backed by some other storage type, like Python lists. Pandas makes no assumptions on how the data are stored, just that it can be converted to a NumPy array. The ExtensionArray interface does not impose any rules on how this data is stored. However, currently, the backing data cannot be stored in attributes called .values or ._values to ensure full compatibility with pandas internals. But other names as .data, ._data, ._items, \u2026 can be freely used.", "If implementing NumPy\u2019s __array_ufunc__ interface, pandas expects that", "You defer by returning NotImplemented when any Series are present in inputs. Pandas will extract the arrays and call the ufunc again.", "You define a _HANDLED_TYPES tuple as an attribute on the class. Pandas inspect this to determine whether the ufunc is valid for the types present.", "See NumPy universal functions for more.", "By default, ExtensionArrays are not hashable. Immutable subclasses may override this behavior.", "Attributes", "dtype", "An instance of 'ExtensionDtype'.", "nbytes", "The number of bytes needed to store this object in memory.", "ndim", "Extension Arrays are only allowed to be 1-dimensional.", "shape", "Return a tuple of the array dimensions.", "Methods", "argsort([ascending, kind, na_position])", "Return the indices that would sort this array.", "astype(dtype[, copy])", "Cast to a NumPy array or ExtensionArray with 'dtype'.", "copy()", "Return a copy of the array.", "dropna()", "Return ExtensionArray without NA values.", "factorize([na_sentinel])", "Encode the extension array as an enumerated type.", "fillna([value, method, limit])", "Fill NA/NaN values using the specified method.", "equals(other)", "Return if another array is equivalent to this array.", "insert(loc, item)", "Insert an item at the given position.", "isin(values)", "Pointwise comparison for set containment in the given values.", "isna()", "A 1-D array indicating if each value is missing.", "ravel([order])", "Return a flattened view on this array.", "repeat(repeats[, axis])", "Repeat elements of a ExtensionArray.", "searchsorted(value[, side, sorter])", "Find indices where elements should be inserted to maintain order.", "shift([periods, fill_value])", "Shift values by desired number.", "take(indices, *[, allow_fill, fill_value])", "Take elements from an array.", "tolist()", "Return a list of the values.", "unique()", "Compute the ExtensionArray of unique values.", "view([dtype])", "Return a view on the array.", "_concat_same_type(to_concat)", "Concatenate multiple array of this dtype.", "_formatter([boxed])", "Formatting function for scalar values.", "_from_factorized(values, original)", "Reconstruct an ExtensionArray after factorization.", "_from_sequence(scalars, *[, dtype, copy])", "Construct a new ExtensionArray from a sequence of scalars.", "_from_sequence_of_strings(strings, *[, ...])", "Construct a new ExtensionArray from a sequence of strings.", "_reduce(name, *[, skipna])", "Return a scalar result of performing the reduction operation.", "_values_for_argsort()", "Return values for sorting.", "_values_for_factorize()", "Return an array and missing value suitable for factorization."]}, {"name": "pandas.api.extensions.ExtensionArray._concat_same_type", "path": "reference/api/pandas.api.extensions.extensionarray._concat_same_type", "type": "Extensions", "text": ["Concatenate multiple array of this dtype."]}, {"name": "pandas.api.extensions.ExtensionArray._formatter", "path": "reference/api/pandas.api.extensions.extensionarray._formatter", "type": "Extensions", "text": ["Formatting function for scalar values.", "This is used in the default \u2018__repr__\u2019. The returned formatting function receives instances of your scalar type.", "An indicated for whether or not your array is being printed within a Series, DataFrame, or Index (True), or just by itself (False). This may be useful if you want scalar values to appear differently within a Series versus on its own (e.g. quoted or not).", "A callable that gets instances of the scalar type and returns a string. By default, repr() is used when boxed=False and str() is used when boxed=True."]}, {"name": "pandas.api.extensions.ExtensionArray._from_factorized", "path": "reference/api/pandas.api.extensions.extensionarray._from_factorized", "type": "Extensions", "text": ["Reconstruct an ExtensionArray after factorization.", "An integer ndarray with the factorized values.", "The original ExtensionArray that factorize was called on.", "See also", "Top-level factorize method that dispatches here.", "Encode the extension array as an enumerated type."]}, {"name": "pandas.api.extensions.ExtensionArray._from_sequence", "path": "reference/api/pandas.api.extensions.extensionarray._from_sequence", "type": "Extensions", "text": ["Construct a new ExtensionArray from a sequence of scalars.", "Each element will be an instance of the scalar type for this array, cls.dtype.type or be converted into this type in this method.", "Construct for this particular dtype. This should be a Dtype compatible with the ExtensionArray.", "If True, copy the underlying data."]}, {"name": "pandas.api.extensions.ExtensionArray._from_sequence_of_strings", "path": "reference/api/pandas.api.extensions.extensionarray._from_sequence_of_strings", "type": "Extensions", "text": ["Construct a new ExtensionArray from a sequence of strings.", "Each element will be an instance of the scalar type for this array, cls.dtype.type.", "Construct for this particular dtype. This should be a Dtype compatible with the ExtensionArray.", "If True, copy the underlying data."]}, {"name": "pandas.api.extensions.ExtensionArray._reduce", "path": "reference/api/pandas.api.extensions.extensionarray._reduce", "type": "Extensions", "text": ["Return a scalar result of performing the reduction operation.", "Name of the function, supported values are: { any, all, min, max, sum, mean, median, prod, std, var, sem, kurt, skew }.", "If True, skip NaN values.", "Additional keyword arguments passed to the reduction function. Currently, ddof is the only supported kwarg."]}, {"name": "pandas.api.extensions.ExtensionArray._values_for_argsort", "path": "reference/api/pandas.api.extensions.extensionarray._values_for_argsort", "type": "Extensions", "text": ["Return values for sorting.", "The transformed values should maintain the ordering between values within the array.", "See also", "Return the indices that would sort this array."]}, {"name": "pandas.api.extensions.ExtensionArray._values_for_factorize", "path": "reference/api/pandas.api.extensions.extensionarray._values_for_factorize", "type": "Extensions", "text": ["Return an array and missing value suitable for factorization.", "An array suitable for factorization. This should maintain order and be a supported dtype (Float64, Int64, UInt64, String, Object). By default, the extension array is cast to object dtype.", "The value in values to consider missing. This will be treated as NA in the factorization routines, so it will be coded as na_sentinel and not included in uniques. By default, np.nan is used.", "Notes", "The values returned by this method are also used in pandas.util.hash_pandas_object()."]}, {"name": "pandas.api.extensions.ExtensionArray.argsort", "path": "reference/api/pandas.api.extensions.extensionarray.argsort", "type": "Extensions", "text": ["Return the indices that would sort this array.", "Whether the indices should result in an ascending or descending sort.", "Sorting algorithm.", "Passed through to numpy.argsort().", "Array of indices that sort self. If NaN values are contained, NaN values are placed at the end.", "See also", "Sorting implementation used internally."]}, {"name": "pandas.api.extensions.ExtensionArray.astype", "path": "reference/api/pandas.api.extensions.extensionarray.astype", "type": "Extensions", "text": ["Cast to a NumPy array or ExtensionArray with \u2018dtype\u2019.", "Typecode or data-type to which the array is cast.", "Whether to copy the data, even if not necessary. If False, a copy is made only if the old dtype does not match the new dtype.", "An ExtensionArray if dtype is ExtensionDtype, Otherwise a NumPy ndarray with \u2018dtype\u2019 for its dtype."]}, {"name": "pandas.api.extensions.ExtensionArray.copy", "path": "reference/api/pandas.api.extensions.extensionarray.copy", "type": "Extensions", "text": ["Return a copy of the array."]}, {"name": "pandas.api.extensions.ExtensionArray.dropna", "path": "reference/api/pandas.api.extensions.extensionarray.dropna", "type": "Extensions", "text": ["Return ExtensionArray without NA values."]}, {"name": "pandas.api.extensions.ExtensionArray.dtype", "path": "reference/api/pandas.api.extensions.extensionarray.dtype", "type": "Extensions", "text": ["An instance of \u2018ExtensionDtype\u2019."]}, {"name": "pandas.api.extensions.ExtensionArray.equals", "path": "reference/api/pandas.api.extensions.extensionarray.equals", "type": "Extensions", "text": ["Return if another array is equivalent to this array.", "Equivalent means that both arrays have the same shape and dtype, and all values compare equal. Missing values in the same location are considered equal (in contrast with normal equality).", "Array to compare to this Array.", "Whether the arrays are equivalent."]}, {"name": "pandas.api.extensions.ExtensionArray.factorize", "path": "reference/api/pandas.api.extensions.extensionarray.factorize", "type": "Extensions", "text": ["Encode the extension array as an enumerated type.", "Value to use in the codes array to indicate missing values.", "An integer NumPy array that\u2019s an indexer into the original ExtensionArray.", "An ExtensionArray containing the unique values of self.", "Note", "uniques will not contain an entry for the NA value of the ExtensionArray if there are any missing values present in self.", "See also", "Top-level factorize method that dispatches here.", "Notes", "pandas.factorize() offers a sort keyword as well."]}, {"name": "pandas.api.extensions.ExtensionArray.fillna", "path": "reference/api/pandas.api.extensions.extensionarray.fillna", "type": "Extensions", "text": ["Fill NA/NaN values using the specified method.", "If a scalar value is passed it is used to fill all missing values. Alternatively, an array-like \u2018value\u2019 can be given. It\u2019s expected that the array-like have the same length as \u2018self\u2019.", "Method to use for filling holes in reindexed Series pad / ffill: propagate last valid observation forward to next valid backfill / bfill: use NEXT valid observation to fill gap.", "If method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled.", "With NA/NaN filled."]}, {"name": "pandas.api.extensions.ExtensionArray.insert", "path": "reference/api/pandas.api.extensions.extensionarray.insert", "type": "Extensions", "text": ["Insert an item at the given position.", "Notes", "This method should be both type and dtype-preserving. If the item cannot be held in an array of this type/dtype, either ValueError or TypeError should be raised.", "The default implementation relies on _from_sequence to raise on invalid items."]}, {"name": "pandas.api.extensions.ExtensionArray.isin", "path": "reference/api/pandas.api.extensions.extensionarray.isin", "type": "Extensions", "text": ["Pointwise comparison for set containment in the given values.", "Roughly equivalent to np.array([x in values for x in self])"]}, {"name": "pandas.api.extensions.ExtensionArray.isna", "path": "reference/api/pandas.api.extensions.extensionarray.isna", "type": "Extensions", "text": ["A 1-D array indicating if each value is missing.", "In most cases, this should return a NumPy ndarray. For exceptional cases like SparseArray, where returning an ndarray would be expensive, an ExtensionArray may be returned.", "Notes", "If returning an ExtensionArray, then", "na_values._is_boolean should be True", "na_values should implement ExtensionArray._reduce()", "na_values.any and na_values.all should be implemented"]}, {"name": "pandas.api.extensions.ExtensionArray.nbytes", "path": "reference/api/pandas.api.extensions.extensionarray.nbytes", "type": "Extensions", "text": ["The number of bytes needed to store this object in memory."]}, {"name": "pandas.api.extensions.ExtensionArray.ndim", "path": "reference/api/pandas.api.extensions.extensionarray.ndim", "type": "Extensions", "text": ["Extension Arrays are only allowed to be 1-dimensional."]}, {"name": "pandas.api.extensions.ExtensionArray.ravel", "path": "reference/api/pandas.api.extensions.extensionarray.ravel", "type": "Extensions", "text": ["Return a flattened view on this array.", "Notes", "Because ExtensionArrays are 1D-only, this is a no-op.", "The \u201corder\u201d argument is ignored, is for compatibility with NumPy."]}, {"name": "pandas.api.extensions.ExtensionArray.repeat", "path": "reference/api/pandas.api.extensions.extensionarray.repeat", "type": "Extensions", "text": ["Repeat elements of a ExtensionArray.", "Returns a new ExtensionArray where each element of the current ExtensionArray is repeated consecutively a given number of times.", "The number of repetitions for each element. This should be a non-negative integer. Repeating 0 times will return an empty ExtensionArray.", "Must be None. Has no effect but is accepted for compatibility with numpy.", "Newly created ExtensionArray with repeated elements.", "See also", "Equivalent function for Series.", "Equivalent function for Index.", "Similar method for numpy.ndarray.", "Take arbitrary positions.", "Examples"]}, {"name": "pandas.api.extensions.ExtensionArray.searchsorted", "path": "reference/api/pandas.api.extensions.extensionarray.searchsorted", "type": "Extensions", "text": ["Find indices where elements should be inserted to maintain order.", "Find the indices into a sorted array self (a) such that, if the corresponding elements in value were inserted before the indices, the order of self would be preserved.", "Assuming that self is sorted:", "side", "returned index i satisfies", "left", "self[i-1] < value <= self[i]", "right", "self[i-1] <= value < self[i]", "Value(s) to insert into self.", "If \u2018left\u2019, the index of the first suitable location found is given. If \u2018right\u2019, return the last such index. If there is no suitable index, return either 0 or N (where N is the length of self).", "Optional array of integer indices that sort array a into ascending order. They are typically the result of argsort.", "If value is array-like, array of insertion points. If value is scalar, a single integer.", "See also", "Similar method from NumPy."]}, {"name": "pandas.api.extensions.ExtensionArray.shape", "path": "reference/api/pandas.api.extensions.extensionarray.shape", "type": "Extensions", "text": ["Return a tuple of the array dimensions."]}, {"name": "pandas.api.extensions.ExtensionArray.shift", "path": "reference/api/pandas.api.extensions.extensionarray.shift", "type": "Extensions", "text": ["Shift values by desired number.", "Newly introduced missing values are filled with self.dtype.na_value.", "The number of periods to shift. Negative values are allowed for shifting backwards.", "The scalar value to use for newly introduced missing values. The default is self.dtype.na_value.", "Shifted.", "Notes", "If self is empty or periods is 0, a copy of self is returned.", "If periods > len(self), then an array of size len(self) is returned, with all values filled with self.dtype.na_value."]}, {"name": "pandas.api.extensions.ExtensionArray.take", "path": "reference/api/pandas.api.extensions.extensionarray.take", "type": "Extensions", "text": ["Take elements from an array.", "Indices to be taken.", "How to handle negative values in indices.", "False: negative values in indices indicate positional indices from the right (the default). This is similar to numpy.take().", "True: negative values in indices indicate missing values. These values are set to fill_value. Any other other negative values raise a ValueError.", "Fill value to use for NA-indices when allow_fill is True. This may be None, in which case the default NA value for the type, self.dtype.na_value, is used.", "For many ExtensionArrays, there will be two representations of fill_value: a user-facing \u201cboxed\u201d scalar, and a low-level physical NA value. fill_value should be the user-facing version, and the implementation should handle translating that to the physical version for processing the take if necessary.", "When the indices are out of bounds for the array.", "When indices contains negative values other than -1 and allow_fill is True.", "See also", "Take elements from an array along an axis.", "Take elements from an array.", "Notes", "ExtensionArray.take is called by Series.__getitem__, .loc, iloc, when indices is a sequence of values. Additionally, it\u2019s called by Series.reindex(), or any other method that causes realignment, with a fill_value.", "Examples", "Here\u2019s an example implementation, which relies on casting the extension array to object dtype. This uses the helper method pandas.api.extensions.take()."]}, {"name": "pandas.api.extensions.ExtensionArray.tolist", "path": "reference/api/pandas.api.extensions.extensionarray.tolist", "type": "Extensions", "text": ["Return a list of the values.", "These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)"]}, {"name": "pandas.api.extensions.ExtensionArray.unique", "path": "reference/api/pandas.api.extensions.extensionarray.unique", "type": "Extensions", "text": ["Compute the ExtensionArray of unique values."]}, {"name": "pandas.api.extensions.ExtensionArray.view", "path": "reference/api/pandas.api.extensions.extensionarray.view", "type": "Extensions", "text": ["Return a view on the array.", "Default None.", "A view on the ExtensionArray\u2019s data."]}, {"name": "pandas.api.extensions.ExtensionDtype", "path": "reference/api/pandas.api.extensions.extensiondtype", "type": "Extensions", "text": ["A custom data type, to be paired with an ExtensionArray.", "See also", "Register an ExtensionType with pandas as class decorator.", "Abstract base class for custom 1-D array types.", "Notes", "The interface includes the following abstract methods that must be implemented by subclasses:", "type", "name", "construct_array_type", "The following attributes and methods influence the behavior of the dtype in pandas operations", "_is_numeric", "_is_boolean", "_get_common_dtype", "The na_value class attribute can be used to set the default NA value for this type. numpy.nan is used by default.", "ExtensionDtypes are required to be hashable. The base class provides a default implementation, which relies on the _metadata class attribute. _metadata should be a tuple containing the strings that define your data type. For example, with PeriodDtype that\u2019s the freq attribute.", "If you have a parametrized dtype you should set the ``_metadata`` class property.", "Ideally, the attributes in _metadata will match the parameters to your ExtensionDtype.__init__ (if any). If any of the attributes in _metadata don\u2019t implement the standard __eq__ or __hash__, the default implementations here will not work.", "For interaction with Apache Arrow (pyarrow), a __from_arrow__ method can be implemented: this method receives a pyarrow Array or ChunkedArray as only argument and is expected to return the appropriate pandas ExtensionArray for this dtype and the passed values:", "This class does not inherit from \u2018abc.ABCMeta\u2019 for performance reasons. Methods and properties required by the interface raise pandas.errors.AbstractMethodError and no register method is provided for registering virtual subclasses.", "Attributes", "kind", "A character code (one of 'biufcmMOSUV'), default 'O'", "na_value", "Default NA value to use for this type.", "name", "A string identifying the data type.", "names", "Ordered list of field names, or None if there are no fields.", "type", "The scalar type for the array, e.g.", "Methods", "construct_array_type()", "Return the array type associated with this dtype.", "construct_from_string(string)", "Construct this type from a string.", "empty(shape)", "Construct an ExtensionArray of this dtype with the given shape.", "is_dtype(dtype)", "Check if we match 'dtype'."]}, {"name": "pandas.api.extensions.ExtensionDtype.construct_array_type", "path": "reference/api/pandas.api.extensions.extensiondtype.construct_array_type", "type": "Extensions", "text": ["Return the array type associated with this dtype."]}, {"name": "pandas.api.extensions.ExtensionDtype.construct_from_string", "path": "reference/api/pandas.api.extensions.extensiondtype.construct_from_string", "type": "Extensions", "text": ["Construct this type from a string.", "This is useful mainly for data types that accept parameters. For example, a period dtype accepts a frequency parameter that can be set as period[H] (where H means hourly frequency).", "By default, in the abstract class, just the name of the type is expected. But subclasses can overwrite this method to accept parameters.", "The name of the type, for example category.", "Instance of the dtype.", "If a class cannot be constructed from this \u2018string\u2019.", "Examples", "For extension dtypes with arguments the following may be an adequate implementation."]}, {"name": "pandas.api.extensions.ExtensionDtype.empty", "path": "reference/api/pandas.api.extensions.extensiondtype.empty", "type": "Extensions", "text": ["Construct an ExtensionArray of this dtype with the given shape.", "Analogous to numpy.empty."]}, {"name": "pandas.api.extensions.ExtensionDtype.is_dtype", "path": "reference/api/pandas.api.extensions.extensiondtype.is_dtype", "type": "Extensions", "text": ["Check if we match \u2018dtype\u2019.", "The object to check.", "Notes", "The default implementation is True if", "cls.construct_from_string(dtype) is an instance of cls.", "dtype is an object and is an instance of cls", "dtype has a dtype attribute, and any of the above conditions is true for dtype.dtype."]}, {"name": "pandas.api.extensions.ExtensionDtype.kind", "path": "reference/api/pandas.api.extensions.extensiondtype.kind", "type": "Extensions", "text": ["A character code (one of \u2018biufcmMOSUV\u2019), default \u2018O\u2019", "This should match the NumPy dtype used when the array is converted to an ndarray, which is probably \u2018O\u2019 for object if the extension type cannot be represented as a built-in NumPy type.", "See also"]}, {"name": "pandas.api.extensions.ExtensionDtype.na_value", "path": "reference/api/pandas.api.extensions.extensiondtype.na_value", "type": "Extensions", "text": ["Default NA value to use for this type.", "This is used in e.g. ExtensionArray.take. This should be the user-facing \u201cboxed\u201d version of the NA value, not the physical NA value for storage. e.g. for JSONArray, this is an empty dictionary."]}, {"name": "pandas.api.extensions.ExtensionDtype.name", "path": "reference/api/pandas.api.extensions.extensiondtype.name", "type": "Extensions", "text": ["A string identifying the data type.", "Will be used for display in, e.g. Series.dtype"]}, {"name": "pandas.api.extensions.ExtensionDtype.names", "path": "reference/api/pandas.api.extensions.extensiondtype.names", "type": "Extensions", "text": ["Ordered list of field names, or None if there are no fields.", "This is for compatibility with NumPy arrays, and may be removed in the future."]}, {"name": "pandas.api.extensions.ExtensionDtype.type", "path": "reference/api/pandas.api.extensions.extensiondtype.type", "type": "Extensions", "text": ["The scalar type for the array, e.g. int", "It\u2019s expected ExtensionArray[item] returns an instance of ExtensionDtype.type for scalar item, assuming that value is valid (not NA). NA values do not need to be instances of type."]}, {"name": "pandas.api.extensions.register_dataframe_accessor", "path": "reference/api/pandas.api.extensions.register_dataframe_accessor", "type": "Extensions", "text": ["Register a custom accessor on DataFrame objects.", "Name under which the accessor should be registered. A warning is issued if this name conflicts with a preexisting attribute.", "A class decorator.", "See also", "Register a custom accessor on DataFrame objects.", "Register a custom accessor on Series objects.", "Register a custom accessor on Index objects.", "Notes", "When accessed, your accessor will be initialized with the pandas object the user is interacting with. So the signature must be", "For consistency with pandas methods, you should raise an AttributeError if the data passed to your accessor has an incorrect dtype.", "Examples", "In your library code:", "Back in an interactive IPython session:"]}, {"name": "pandas.api.extensions.register_extension_dtype", "path": "reference/api/pandas.api.extensions.register_extension_dtype", "type": "Extensions", "text": ["Register an ExtensionType with pandas as class decorator.", "This enables operations like .astype(name) for the name of the ExtensionDtype.", "A class decorator.", "Examples"]}, {"name": "pandas.api.extensions.register_index_accessor", "path": "reference/api/pandas.api.extensions.register_index_accessor", "type": "Extensions", "text": ["Register a custom accessor on Index objects.", "Name under which the accessor should be registered. A warning is issued if this name conflicts with a preexisting attribute.", "A class decorator.", "See also", "Register a custom accessor on DataFrame objects.", "Register a custom accessor on Series objects.", "Register a custom accessor on Index objects.", "Notes", "When accessed, your accessor will be initialized with the pandas object the user is interacting with. So the signature must be", "For consistency with pandas methods, you should raise an AttributeError if the data passed to your accessor has an incorrect dtype.", "Examples", "In your library code:", "Back in an interactive IPython session:"]}, {"name": "pandas.api.extensions.register_series_accessor", "path": "reference/api/pandas.api.extensions.register_series_accessor", "type": "Extensions", "text": ["Register a custom accessor on Series objects.", "Name under which the accessor should be registered. A warning is issued if this name conflicts with a preexisting attribute.", "A class decorator.", "See also", "Register a custom accessor on DataFrame objects.", "Register a custom accessor on Series objects.", "Register a custom accessor on Index objects.", "Notes", "When accessed, your accessor will be initialized with the pandas object the user is interacting with. So the signature must be", "For consistency with pandas methods, you should raise an AttributeError if the data passed to your accessor has an incorrect dtype.", "Examples", "In your library code:", "Back in an interactive IPython session:"]}, {"name": "pandas.api.indexers.BaseIndexer", "path": "reference/api/pandas.api.indexers.baseindexer", "type": "Window", "text": ["Base class for window bounds calculations.", "Methods", "get_window_bounds([num_values, min_periods, ...])", "Computes the bounds of a window."]}, {"name": "pandas.api.indexers.BaseIndexer.get_window_bounds", "path": "reference/api/pandas.api.indexers.baseindexer.get_window_bounds", "type": "Window", "text": ["Computes the bounds of a window.", "number of values that will be aggregated over", "the number of rows in a window", "min_periods passed from the top level rolling API", "center passed from the top level rolling API", "closed passed from the top level rolling API", "win_type passed from the top level rolling API"]}, {"name": "pandas.api.indexers.check_array_indexer", "path": "reference/api/pandas.api.indexers.check_array_indexer", "type": "Extensions", "text": ["Check if indexer is a valid array indexer for array.", "For a boolean mask, array and indexer are checked to have the same length. The dtype is validated, and if it is an integer or boolean ExtensionArray, it is checked if there are missing values present, and it is converted to the appropriate numpy array. Other dtypes will raise an error.", "Non-array indexers (integer, slice, Ellipsis, tuples, ..) are passed through as is.", "New in version 1.0.0.", "The array that is being indexed (only used for the length).", "The array-like that\u2019s used to index. List-like input that is not yet a numpy array or an ExtensionArray is converted to one. Other input types are passed through as is.", "The validated indexer as a numpy array that can be used to index.", "When the lengths don\u2019t match.", "When indexer cannot be converted to a numpy ndarray to index (e.g. presence of missing values).", "See also", "Check if key is of boolean dtype.", "Examples", "When checking a boolean mask, a boolean ndarray is returned when the arguments are all valid.", "An IndexError is raised when the lengths don\u2019t match.", "NA values in a boolean array are treated as False.", "A numpy boolean mask will get passed through (if the length is correct):", "Similarly for integer indexers, an integer ndarray is returned when it is a valid indexer, otherwise an error is (for integer indexers, a matching length is not required):", "For non-integer/boolean dtypes, an appropriate error is raised:"]}, {"name": "pandas.api.indexers.FixedForwardWindowIndexer", "path": "reference/api/pandas.api.indexers.fixedforwardwindowindexer", "type": "Window", "text": ["Creates window boundaries for fixed-length windows that include the current row.", "Examples", "Methods", "get_window_bounds([num_values, min_periods, ...])", "Computes the bounds of a window."]}, {"name": "pandas.api.indexers.FixedForwardWindowIndexer.get_window_bounds", "path": "reference/api/pandas.api.indexers.fixedforwardwindowindexer.get_window_bounds", "type": "Window", "text": ["Computes the bounds of a window.", "number of values that will be aggregated over", "the number of rows in a window", "min_periods passed from the top level rolling API", "center passed from the top level rolling API", "closed passed from the top level rolling API", "win_type passed from the top level rolling API"]}, {"name": "pandas.api.indexers.VariableOffsetWindowIndexer", "path": "reference/api/pandas.api.indexers.variableoffsetwindowindexer", "type": "Window", "text": ["Calculate window boundaries based on a non-fixed offset such as a BusinessDay.", "Methods", "get_window_bounds([num_values, min_periods, ...])", "Computes the bounds of a window."]}, {"name": "pandas.api.indexers.VariableOffsetWindowIndexer.get_window_bounds", "path": "reference/api/pandas.api.indexers.variableoffsetwindowindexer.get_window_bounds", "type": "Window", "text": ["Computes the bounds of a window.", "number of values that will be aggregated over", "the number of rows in a window", "min_periods passed from the top level rolling API", "center passed from the top level rolling API", "closed passed from the top level rolling API", "win_type passed from the top level rolling API"]}, {"name": "pandas.api.types.infer_dtype", "path": "reference/api/pandas.api.types.infer_dtype", "type": "General utility functions", "text": ["Efficiently infer the type of a passed val, or list-like array of values. Return a string describing the type.", "Ignore NaN values when inferring the type.", "Describing the common type of the input data.", "If ndarray-like but cannot infer the dtype", "Notes", "\u2018mixed\u2019 is the catchall for anything that is not otherwise specialized", "\u2018mixed-integer-float\u2019 are floats and integers", "\u2018mixed-integer\u2019 are integers mixed with non-integers", "\u2018unknown-array\u2019 is the catchall for something that is an array (has a dtype attribute), but has a dtype unknown to pandas (e.g. external extension array)", "Examples"]}, {"name": "pandas.api.types.is_bool", "path": "reference/api/pandas.api.types.is_bool", "type": "General utility functions", "text": ["Return True if given object is boolean."]}, {"name": "pandas.api.types.is_bool_dtype", "path": "reference/api/pandas.api.types.is_bool_dtype", "type": "General utility functions", "text": ["Check whether the provided array or dtype is of a boolean dtype.", "The array or dtype to check.", "Whether or not the array or dtype is of a boolean dtype.", "Notes", "An ExtensionArray is considered boolean when the _is_boolean attribute is set to True.", "Examples"]}, {"name": "pandas.api.types.is_categorical", "path": "reference/api/pandas.api.types.is_categorical", "type": "General utility functions", "text": ["Check whether an array-like is a Categorical instance.", "The array-like to check.", "Whether or not the array-like is of a Categorical instance.", "Examples", "Categoricals, Series Categoricals, and CategoricalIndex will return True."]}, {"name": "pandas.api.types.is_categorical_dtype", "path": "reference/api/pandas.api.types.is_categorical_dtype", "type": "General utility functions", "text": ["Check whether an array-like or dtype is of the Categorical dtype.", "The array-like or dtype to check.", "Whether or not the array-like or dtype is of the Categorical dtype.", "Examples"]}, {"name": "pandas.api.types.is_complex", "path": "reference/api/pandas.api.types.is_complex", "type": "General utility functions", "text": ["Return True if given object is complex."]}, {"name": "pandas.api.types.is_complex_dtype", "path": "reference/api/pandas.api.types.is_complex_dtype", "type": "General utility functions", "text": ["Check whether the provided array or dtype is of a complex dtype.", "The array or dtype to check.", "Whether or not the array or dtype is of a complex dtype.", "Examples"]}, {"name": "pandas.api.types.is_datetime64_any_dtype", "path": "reference/api/pandas.api.types.is_datetime64_any_dtype", "type": "General utility functions", "text": ["Check whether the provided array or dtype is of the datetime64 dtype.", "The array or dtype to check.", "Whether or not the array or dtype is of the datetime64 dtype.", "Examples"]}, {"name": "pandas.api.types.is_datetime64_dtype", "path": "reference/api/pandas.api.types.is_datetime64_dtype", "type": "General utility functions", "text": ["Check whether an array-like or dtype is of the datetime64 dtype.", "The array-like or dtype to check.", "Whether or not the array-like or dtype is of the datetime64 dtype.", "Examples"]}, {"name": "pandas.api.types.is_datetime64_ns_dtype", "path": "reference/api/pandas.api.types.is_datetime64_ns_dtype", "type": "General utility functions", "text": ["Check whether the provided array or dtype is of the datetime64[ns] dtype.", "The array or dtype to check.", "Whether or not the array or dtype is of the datetime64[ns] dtype.", "Examples"]}, {"name": "pandas.api.types.is_datetime64tz_dtype", "path": "reference/api/pandas.api.types.is_datetime64tz_dtype", "type": "General utility functions", "text": ["Check whether an array-like or dtype is of a DatetimeTZDtype dtype.", "The array-like or dtype to check.", "Whether or not the array-like or dtype is of a DatetimeTZDtype dtype.", "Examples"]}, {"name": "pandas.api.types.is_dict_like", "path": "reference/api/pandas.api.types.is_dict_like", "type": "General utility functions", "text": ["Check if the object is dict-like.", "Whether obj has dict-like properties.", "Examples"]}, {"name": "pandas.api.types.is_extension_array_dtype", "path": "reference/api/pandas.api.types.is_extension_array_dtype", "type": "General utility functions", "text": ["Check if an object is a pandas extension array type.", "See the Use Guide for more.", "For array-like input, the .dtype attribute will be extracted.", "Whether the arr_or_dtype is an extension array type.", "Notes", "This checks whether an object implements the pandas extension array interface. In pandas, this includes:", "Categorical", "Sparse", "Interval", "Period", "DatetimeArray", "TimedeltaArray", "Third-party libraries may implement arrays or types satisfying this interface as well.", "Examples"]}, {"name": "pandas.api.types.is_extension_type", "path": "reference/api/pandas.api.types.is_extension_type", "type": "General utility functions", "text": ["Check whether an array-like is of a pandas extension class instance.", "Deprecated since version 1.0.0: Use is_extension_array_dtype instead.", "Extension classes include categoricals, pandas sparse objects (i.e. classes represented within the pandas library and not ones external to it like scipy sparse matrices), and datetime-like arrays.", "The array-like to check.", "Whether or not the array-like is of a pandas extension class instance.", "Examples"]}, {"name": "pandas.api.types.is_file_like", "path": "reference/api/pandas.api.types.is_file_like", "type": "General utility functions", "text": ["Check if the object is a file-like object.", "For objects to be considered file-like, they must be an iterator AND have either a read and/or write method as an attribute.", "Note: file-like objects must be iterable, but iterable objects need not be file-like.", "Whether obj has file-like properties.", "Examples"]}, {"name": "pandas.api.types.is_float", "path": "reference/api/pandas.api.types.is_float", "type": "General utility functions", "text": ["Return True if given object is float."]}, {"name": "pandas.api.types.is_float_dtype", "path": "reference/api/pandas.api.types.is_float_dtype", "type": "General utility functions", "text": ["Check whether the provided array or dtype is of a float dtype.", "This function is internal and should not be exposed in the public API.", "The array or dtype to check.", "Whether or not the array or dtype is of a float dtype.", "Examples"]}, {"name": "pandas.api.types.is_hashable", "path": "reference/api/pandas.api.types.is_hashable", "type": "General utility functions", "text": ["Return True if hash(obj) will succeed, False otherwise.", "Some types will pass a test against collections.abc.Hashable but fail when they are actually hashed with hash().", "Distinguish between these and other types by trying the call to hash() and seeing if they raise TypeError.", "Examples"]}, {"name": "pandas.api.types.is_int64_dtype", "path": "reference/api/pandas.api.types.is_int64_dtype", "type": "General utility functions", "text": ["Check whether the provided array or dtype is of the int64 dtype.", "The array or dtype to check.", "Whether or not the array or dtype is of the int64 dtype.", "Notes", "Depending on system architecture, the return value of is_int64_dtype( int) will be True if the OS uses 64-bit integers and False if the OS uses 32-bit integers.", "Examples"]}, {"name": "pandas.api.types.is_integer", "path": "reference/api/pandas.api.types.is_integer", "type": "General utility functions", "text": ["Return True if given object is integer."]}, {"name": "pandas.api.types.is_integer_dtype", "path": "reference/api/pandas.api.types.is_integer_dtype", "type": "General utility functions", "text": ["Check whether the provided array or dtype is of an integer dtype.", "Unlike in is_any_int_dtype, timedelta64 instances will return False.", "The nullable Integer dtypes (e.g. pandas.Int64Dtype) are also considered as integer by this function.", "The array or dtype to check.", "Whether or not the array or dtype is of an integer dtype and not an instance of timedelta64.", "Examples"]}, {"name": "pandas.api.types.is_interval", "path": "reference/api/pandas.api.types.is_interval", "type": "General utility functions", "text": []}, {"name": "pandas.api.types.is_interval_dtype", "path": "reference/api/pandas.api.types.is_interval_dtype", "type": "General utility functions", "text": ["Check whether an array-like or dtype is of the Interval dtype.", "The array-like or dtype to check.", "Whether or not the array-like or dtype is of the Interval dtype.", "Examples"]}, {"name": "pandas.api.types.is_iterator", "path": "reference/api/pandas.api.types.is_iterator", "type": "General utility functions", "text": ["Check if the object is an iterator.", "This is intended for generators, not list-like objects.", "Whether obj is an iterator.", "Examples"]}, {"name": "pandas.api.types.is_list_like", "path": "reference/api/pandas.api.types.is_list_like", "type": "General utility functions", "text": ["Check if the object is list-like.", "Objects that are considered list-like are for example Python lists, tuples, sets, NumPy arrays, and Pandas Series.", "Strings and datetime objects, however, are not considered list-like.", "Object to check.", "If this parameter is False, sets will not be considered list-like.", "Whether obj has list-like properties.", "Examples"]}, {"name": "pandas.api.types.is_named_tuple", "path": "reference/api/pandas.api.types.is_named_tuple", "type": "General utility functions", "text": ["Check if the object is a named tuple.", "Whether obj is a named tuple.", "Examples"]}, {"name": "pandas.api.types.is_number", "path": "reference/api/pandas.api.types.is_number", "type": "General utility functions", "text": ["Check if the object is a number.", "Returns True when the object is a number, and False if is not.", "The object to check if is a number.", "Whether obj is a number or not.", "See also", "Checks a subgroup of numbers.", "Examples", "Booleans are valid because they are int subclass."]}, {"name": "pandas.api.types.is_numeric_dtype", "path": "reference/api/pandas.api.types.is_numeric_dtype", "type": "General utility functions", "text": ["Check whether the provided array or dtype is of a numeric dtype.", "The array or dtype to check.", "Whether or not the array or dtype is of a numeric dtype.", "Examples"]}, {"name": "pandas.api.types.is_object_dtype", "path": "reference/api/pandas.api.types.is_object_dtype", "type": "General utility functions", "text": ["Check whether an array-like or dtype is of the object dtype.", "The array-like or dtype to check.", "Whether or not the array-like or dtype is of the object dtype.", "Examples"]}, {"name": "pandas.api.types.is_period_dtype", "path": "reference/api/pandas.api.types.is_period_dtype", "type": "General utility functions", "text": ["Check whether an array-like or dtype is of the Period dtype.", "The array-like or dtype to check.", "Whether or not the array-like or dtype is of the Period dtype.", "Examples"]}, {"name": "pandas.api.types.is_re", "path": "reference/api/pandas.api.types.is_re", "type": "General utility functions", "text": ["Check if the object is a regex pattern instance.", "Whether obj is a regex pattern.", "Examples"]}, {"name": "pandas.api.types.is_re_compilable", "path": "reference/api/pandas.api.types.is_re_compilable", "type": "General utility functions", "text": ["Check if the object can be compiled into a regex pattern instance.", "Whether obj can be compiled as a regex pattern.", "Examples"]}, {"name": "pandas.api.types.is_scalar", "path": "reference/api/pandas.api.types.is_scalar", "type": "General utility functions", "text": ["Return True if given object is scalar.", "This includes:", "numpy array scalar (e.g. np.int64)", "Python builtin numerics", "Python builtin byte arrays and strings", "None", "datetime.datetime", "datetime.timedelta", "Period", "decimal.Decimal", "Interval", "DateOffset", "Fraction", "Number.", "Return True if given object is scalar.", "Examples", "pandas supports PEP 3141 numbers:"]}, {"name": "pandas.api.types.is_signed_integer_dtype", "path": "reference/api/pandas.api.types.is_signed_integer_dtype", "type": "General utility functions", "text": ["Check whether the provided array or dtype is of a signed integer dtype.", "Unlike in is_any_int_dtype, timedelta64 instances will return False.", "The nullable Integer dtypes (e.g. pandas.Int64Dtype) are also considered as integer by this function.", "The array or dtype to check.", "Whether or not the array or dtype is of a signed integer dtype and not an instance of timedelta64.", "Examples"]}, {"name": "pandas.api.types.is_sparse", "path": "reference/api/pandas.api.types.is_sparse", "type": "General utility functions", "text": ["Check whether an array-like is a 1-D pandas sparse array.", "Check that the one-dimensional array-like is a pandas sparse array. Returns True if it is a pandas sparse array, not another type of sparse array.", "Array-like to check.", "Whether or not the array-like is a pandas sparse array.", "Examples", "Returns True if the parameter is a 1-D pandas sparse array.", "Returns False if the parameter is not sparse.", "Returns False if the parameter is not a pandas sparse array.", "Returns False if the parameter has more than one dimension."]}, {"name": "pandas.api.types.is_string_dtype", "path": "reference/api/pandas.api.types.is_string_dtype", "type": "General utility functions", "text": ["Check whether the provided array or dtype is of the string dtype.", "The array or dtype to check.", "Whether or not the array or dtype is of the string dtype.", "Examples"]}, {"name": "pandas.api.types.is_timedelta64_dtype", "path": "reference/api/pandas.api.types.is_timedelta64_dtype", "type": "General utility functions", "text": ["Check whether an array-like or dtype is of the timedelta64 dtype.", "The array-like or dtype to check.", "Whether or not the array-like or dtype is of the timedelta64 dtype.", "Examples"]}, {"name": "pandas.api.types.is_timedelta64_ns_dtype", "path": "reference/api/pandas.api.types.is_timedelta64_ns_dtype", "type": "General utility functions", "text": ["Check whether the provided array or dtype is of the timedelta64[ns] dtype.", "This is a very specific dtype, so generic ones like np.timedelta64 will return False if passed into this function.", "The array or dtype to check.", "Whether or not the array or dtype is of the timedelta64[ns] dtype.", "Examples"]}, {"name": "pandas.api.types.is_unsigned_integer_dtype", "path": "reference/api/pandas.api.types.is_unsigned_integer_dtype", "type": "General utility functions", "text": ["Check whether the provided array or dtype is of an unsigned integer dtype.", "The nullable Integer dtypes (e.g. pandas.UInt64Dtype) are also considered as integer by this function.", "The array or dtype to check.", "Whether or not the array or dtype is of an unsigned integer dtype.", "Examples"]}, {"name": "pandas.api.types.pandas_dtype", "path": "reference/api/pandas.api.types.pandas_dtype", "type": "General utility functions", "text": ["Convert input into a pandas only dtype object or a numpy dtype object."]}, {"name": "pandas.api.types.union_categoricals", "path": "reference/api/pandas.api.types.union_categoricals", "type": "General utility functions", "text": ["Combine list-like of Categorical-like, unioning categories.", "All categories must have the same dtype.", "Categorical, CategoricalIndex, or Series with dtype=\u2019category\u2019.", "If true, resulting categories will be lexsorted, otherwise they will be ordered as they appear in the data.", "If true, the ordered attribute of the Categoricals will be ignored. Results in an unordered categorical.", "all inputs do not have the same dtype", "all inputs do not have the same ordered property", "all inputs are ordered and their categories are not identical", "sort_categories=True and Categoricals are ordered", "Empty list of categoricals passed", "Notes", "To learn more about categories, see link", "Examples", "If you want to combine categoricals that do not necessarily have the same categories, union_categoricals will combine a list-like of categoricals. The new categories will be the union of the categories being combined.", "By default, the resulting categories will be ordered as they appear in the categories of the data. If you want the categories to be lexsorted, use sort_categories=True argument.", "union_categoricals also works with the case of combining two categoricals of the same categories and order information (e.g. what you could also append for).", "Raises TypeError because the categories are ordered and not identical.", "New in version 0.20.0", "Ordered categoricals with different categories or orderings can be combined by using the ignore_ordered=True argument.", "union_categoricals also works with a CategoricalIndex, or Series containing categorical data, but note that the resulting array will always be a plain Categorical"]}, {"name": "pandas.array", "path": "reference/api/pandas.array", "type": "Pandas arrays", "text": ["Create an array.", "The scalars inside data should be instances of the scalar type for dtype. It\u2019s expected that data represents a 1-dimensional array of data.", "When data is an Index or Series, the underlying array will be extracted from data.", "The dtype to use for the array. This may be a NumPy dtype or an extension type registered with pandas using pandas.api.extensions.register_extension_dtype().", "If not specified, there are two possibilities:", "When data is a Series, Index, or ExtensionArray, the dtype will be taken from the data.", "Otherwise, pandas will attempt to infer the dtype from the data.", "Note that when data is a NumPy array, data.dtype is not used for inferring the array type. This is because NumPy cannot represent all the types of data that can be held in extension arrays.", "Currently, pandas will infer an extension dtype for sequences of", "Scalar Type", "Array Type", "pandas.Interval", "pandas.arrays.IntervalArray", "pandas.Period", "pandas.arrays.PeriodArray", "datetime.datetime", "pandas.arrays.DatetimeArray", "datetime.timedelta", "pandas.arrays.TimedeltaArray", "int", "pandas.arrays.IntegerArray", "float", "pandas.arrays.FloatingArray", "str", "pandas.arrays.StringArray or pandas.arrays.ArrowStringArray", "bool", "pandas.arrays.BooleanArray", "The ExtensionArray created when the scalar type is str is determined by pd.options.mode.string_storage if the dtype is not explicitly given.", "For all other cases, NumPy\u2019s usual inference rules will be used.", "Changed in version 1.0.0: Pandas infers nullable-integer dtype for integer data, string dtype for string data, and nullable-boolean dtype for boolean data.", "Changed in version 1.2.0: Pandas now also infers nullable-floating dtype for float-like input data", "Whether to copy the data, even if not necessary. Depending on the type of data, creating the new array may require copying data, even if copy=False.", "The newly created array.", "When data is not 1-dimensional.", "See also", "Construct a NumPy array.", "Construct a pandas Series.", "Construct a pandas Index.", "ExtensionArray wrapping a NumPy array.", "Extract the array stored within a Series.", "Notes", "Omitting the dtype argument means pandas will attempt to infer the best array type from the values in the data. As new array types are added by pandas and 3rd party libraries, the \u201cbest\u201d array type may change. We recommend specifying dtype to ensure that", "the correct array type for the data is returned", "the returned array type doesn\u2019t change as new extension types are added by pandas and third-party libraries", "Additionally, if the underlying memory representation of the returned array matters, we recommend specifying the dtype as a concrete object rather than a string alias or allowing it to be inferred. For example, a future version of pandas or a 3rd-party library may include a dedicated ExtensionArray for string data. In this event, the following would no longer return a arrays.PandasArray backed by a NumPy array.", "This would instead return the new ExtensionArray dedicated for string data. If you really need the new array to be backed by a NumPy array, specify that in the dtype.", "Finally, Pandas has arrays that mostly overlap with NumPy", "arrays.DatetimeArray", "arrays.TimedeltaArray", "When data with a datetime64[ns] or timedelta64[ns] dtype is passed, pandas will always return a DatetimeArray or TimedeltaArray rather than a PandasArray. This is for symmetry with the case of timezone-aware data, which NumPy does not natively support.", "Examples", "If a dtype is not specified, pandas will infer the best dtype from the values. See the description of dtype for the types pandas infers for.", "You can use the string alias for dtype", "Or specify the actual dtype", "If pandas does not infer a dedicated extension type a arrays.PandasArray is returned.", "As mentioned in the \u201cNotes\u201d section, new extension types may be added in the future (by pandas or 3rd party libraries), causing the return value to no longer be a arrays.PandasArray. Specify the dtype as a NumPy dtype if you need to ensure there\u2019s no future change in behavior.", "data must be 1-dimensional. A ValueError is raised when the input has the wrong dimensionality."]}, {"name": "pandas.arrays.ArrowStringArray", "path": "reference/api/pandas.arrays.arrowstringarray", "type": "Pandas arrays", "text": ["Extension array for string data in a pyarrow.ChunkedArray.", "New in version 1.2.0.", "Warning", "ArrowStringArray is considered experimental. The implementation and parts of the API may change without warning.", "The array of data.", "See also", "The recommended function for creating a ArrowStringArray.", "The string methods are available on Series backed by a ArrowStringArray.", "Notes", "ArrowStringArray returns a BooleanArray for comparison methods.", "Examples", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.arrays.BooleanArray", "path": "reference/api/pandas.arrays.booleanarray", "type": "Pandas arrays", "text": ["Array of boolean (True/False) data with missing values.", "This is a pandas Extension array for boolean data, under the hood represented by 2 numpy arrays: a boolean array with the data and a boolean array with the mask (True indicating missing).", "BooleanArray implements Kleene logic (sometimes called three-value logic) for logical operations. See Kleene logical operations for more.", "To construct an BooleanArray from generic array-like input, use pandas.array() specifying dtype=\"boolean\" (see examples below).", "New in version 1.0.0.", "Warning", "BooleanArray is considered experimental. The implementation and parts of the API may change without warning.", "A 1-d boolean-dtype array with the data.", "A 1-d boolean-dtype array indicating missing values (True indicates missing).", "Whether to copy the values and mask arrays.", "Examples", "Create an BooleanArray with pandas.array():", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.arrays.DatetimeArray", "path": "reference/api/pandas.arrays.datetimearray", "type": "Pandas arrays", "text": ["Pandas ExtensionArray for tz-naive or tz-aware datetime data.", "Warning", "DatetimeArray is currently experimental, and its API may change without warning. In particular, DatetimeArray.dtype is expected to change to always be an instance of an ExtensionDtype subclass.", "The datetime data.", "For DatetimeArray values (or a Series or Index boxing one), dtype and freq will be extracted from values.", "Note that the only NumPy dtype allowed is \u2018datetime64[ns]\u2019.", "The frequency.", "Whether to copy the underlying array of values.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.arrays.IntegerArray", "path": "reference/api/pandas.arrays.integerarray", "type": "Pandas arrays", "text": ["Array of integer (optional missing) values.", "Changed in version 1.0.0: Now uses pandas.NA as the missing value rather than numpy.nan.", "Warning", "IntegerArray is currently experimental, and its API or internal implementation may change without warning.", "We represent an IntegerArray with 2 numpy arrays:", "data: contains a numpy integer array of the appropriate dtype", "mask: a boolean array holding a mask on the data, True is missing", "To construct an IntegerArray from generic array-like input, use pandas.array() with one of the integer dtypes (see examples).", "See Nullable integer data type for more.", "A 1-d integer-dtype array.", "A 1-d boolean-dtype array indicating missing values.", "Whether to copy the values and mask.", "Examples", "Create an IntegerArray with pandas.array().", "String aliases for the dtypes are also available. They are capitalized.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.arrays.IntervalArray", "path": "reference/api/pandas.arrays.intervalarray", "type": "Pandas arrays", "text": ["Pandas array for interval data that are closed on the same side.", "New in version 0.24.0.", "Array-like containing Interval objects from which to build the IntervalArray.", "Whether the intervals are closed on the left-side, right-side, both or neither.", "If None, dtype will be inferred.", "Copy the input data.", "Verify that the IntervalArray is valid.", "See also", "The base pandas Index type.", "A bounded slice-like interval; the elements of an IntervalArray.", "Function to create a fixed frequency IntervalIndex.", "Bin values into discrete Intervals.", "Bin values into equal-sized Intervals based on rank or sample quantiles.", "Notes", "See the user guide for more.", "Examples", "A new IntervalArray can be constructed directly from an array-like of Interval objects:", "It may also be constructed using one of the constructor methods: IntervalArray.from_arrays(), IntervalArray.from_breaks(), and IntervalArray.from_tuples().", "Attributes", "left", "Return the left endpoints of each Interval in the IntervalArray as an Index.", "right", "Return the right endpoints of each Interval in the IntervalArray as an Index.", "closed", "Whether the intervals are closed on the left-side, right-side, both or neither.", "mid", "Return the midpoint of each Interval in the IntervalArray as an Index.", "length", "Return an Index with entries denoting the length of each Interval in the IntervalArray.", "is_empty", "Indicates if an interval is empty, meaning it contains no points.", "is_non_overlapping_monotonic", "Return True if the IntervalArray is non-overlapping (no Intervals share points) and is either monotonic increasing or monotonic decreasing, else False.", "Methods", "from_arrays(left, right[, closed, copy, dtype])", "Construct from two arrays defining the left and right bounds.", "from_tuples(data[, closed, copy, dtype])", "Construct an IntervalArray from an array-like of tuples.", "from_breaks(breaks[, closed, copy, dtype])", "Construct an IntervalArray from an array of splits.", "contains(other)", "Check elementwise if the Intervals contain the value.", "overlaps(other)", "Check elementwise if an Interval overlaps the values in the IntervalArray.", "set_closed(closed)", "Return an IntervalArray identical to the current one, but closed on the specified side.", "to_tuples([na_tuple])", "Return an ndarray of tuples of the form (left, right)."]}, {"name": "pandas.arrays.IntervalArray.closed", "path": "reference/api/pandas.arrays.intervalarray.closed", "type": "Pandas arrays", "text": ["Whether the intervals are closed on the left-side, right-side, both or neither."]}, {"name": "pandas.arrays.IntervalArray.contains", "path": "reference/api/pandas.arrays.intervalarray.contains", "type": "Pandas arrays", "text": ["Check elementwise if the Intervals contain the value.", "Return a boolean mask whether the value is contained in the Intervals of the IntervalArray.", "New in version 0.25.0.", "The value to check whether it is contained in the Intervals.", "See also", "Check whether Interval object contains value.", "Check if an Interval overlaps the values in the IntervalArray.", "Examples"]}, {"name": "pandas.arrays.IntervalArray.from_arrays", "path": "reference/api/pandas.arrays.intervalarray.from_arrays", "type": "Pandas arrays", "text": ["Construct from two arrays defining the left and right bounds.", "Left bounds for each interval.", "Right bounds for each interval.", "Whether the intervals are closed on the left-side, right-side, both or neither.", "Copy the data.", "If None, dtype will be inferred.", "When a value is missing in only one of left or right. When a value in left is greater than the corresponding value in right.", "See also", "Function to create a fixed frequency IntervalIndex.", "Construct an IntervalArray from an array of splits.", "Construct an IntervalArray from an array-like of tuples.", "Notes", "Each element of left must be less than or equal to the right element at the same position. If an element is missing, it must be missing in both left and right. A TypeError is raised when using an unsupported type for left or right. At the moment, \u2018category\u2019, \u2018object\u2019, and \u2018string\u2019 subtypes are not supported."]}, {"name": "pandas.arrays.IntervalArray.from_breaks", "path": "reference/api/pandas.arrays.intervalarray.from_breaks", "type": "Pandas arrays", "text": ["Construct an IntervalArray from an array of splits.", "Left and right bounds for each interval.", "Whether the intervals are closed on the left-side, right-side, both or neither.", "Copy the data.", "If None, dtype will be inferred.", "See also", "Function to create a fixed frequency IntervalIndex.", "Construct from a left and right array.", "Construct from a sequence of tuples.", "Examples"]}, {"name": "pandas.arrays.IntervalArray.from_tuples", "path": "reference/api/pandas.arrays.intervalarray.from_tuples", "type": "Pandas arrays", "text": ["Construct an IntervalArray from an array-like of tuples.", "Array of tuples.", "Whether the intervals are closed on the left-side, right-side, both or neither.", "By-default copy the data, this is compat only and ignored.", "If None, dtype will be inferred.", "See also", "Function to create a fixed frequency IntervalIndex.", "Construct an IntervalArray from a left and right array.", "Construct an IntervalArray from an array of splits.", "Examples"]}, {"name": "pandas.arrays.IntervalArray.is_empty", "path": "reference/api/pandas.arrays.intervalarray.is_empty", "type": "Pandas arrays", "text": ["Indicates if an interval is empty, meaning it contains no points.", "New in version 0.25.0.", "A boolean indicating if a scalar Interval is empty, or a boolean ndarray positionally indicating if an Interval in an IntervalArray or IntervalIndex is empty.", "Examples", "An Interval that contains points is not empty:", "An Interval that does not contain any points is empty:", "An Interval that contains a single point is not empty:", "An IntervalArray or IntervalIndex returns a boolean ndarray positionally indicating if an Interval is empty:", "Missing values are not considered empty:"]}, {"name": "pandas.arrays.IntervalArray.is_non_overlapping_monotonic", "path": "reference/api/pandas.arrays.intervalarray.is_non_overlapping_monotonic", "type": "Pandas arrays", "text": ["Return True if the IntervalArray is non-overlapping (no Intervals share points) and is either monotonic increasing or monotonic decreasing, else False."]}, {"name": "pandas.arrays.IntervalArray.left", "path": "reference/api/pandas.arrays.intervalarray.left", "type": "Pandas arrays", "text": ["Return the left endpoints of each Interval in the IntervalArray as an Index."]}, {"name": "pandas.arrays.IntervalArray.length", "path": "reference/api/pandas.arrays.intervalarray.length", "type": "Pandas arrays", "text": ["Return an Index with entries denoting the length of each Interval in the IntervalArray."]}, {"name": "pandas.arrays.IntervalArray.mid", "path": "reference/api/pandas.arrays.intervalarray.mid", "type": "Pandas arrays", "text": ["Return the midpoint of each Interval in the IntervalArray as an Index."]}, {"name": "pandas.arrays.IntervalArray.overlaps", "path": "reference/api/pandas.arrays.intervalarray.overlaps", "type": "Pandas arrays", "text": ["Check elementwise if an Interval overlaps the values in the IntervalArray.", "Two intervals overlap if they share a common point, including closed endpoints. Intervals that only have an open endpoint in common do not overlap.", "Interval to check against for an overlap.", "Boolean array positionally indicating where an overlap occurs.", "See also", "Check whether two Interval objects overlap.", "Examples", "Intervals that share closed endpoints overlap:", "Intervals that only have an open endpoint in common do not overlap:"]}, {"name": "pandas.arrays.IntervalArray.right", "path": "reference/api/pandas.arrays.intervalarray.right", "type": "Pandas arrays", "text": ["Return the right endpoints of each Interval in the IntervalArray as an Index."]}, {"name": "pandas.arrays.IntervalArray.set_closed", "path": "reference/api/pandas.arrays.intervalarray.set_closed", "type": "Pandas arrays", "text": ["Return an IntervalArray identical to the current one, but closed on the specified side.", "Whether the intervals are closed on the left-side, right-side, both or neither.", "Examples"]}, {"name": "pandas.arrays.IntervalArray.to_tuples", "path": "reference/api/pandas.arrays.intervalarray.to_tuples", "type": "Pandas arrays", "text": ["Return an ndarray of tuples of the form (left, right).", "Returns NA as a tuple if True, (nan, nan), or just as the NA value itself if False, nan."]}, {"name": "pandas.arrays.PandasArray", "path": "reference/api/pandas.arrays.pandasarray", "type": "Pandas arrays", "text": ["A pandas ExtensionArray for NumPy data.", "This is mostly for internal compatibility, and is not especially useful on its own.", "The NumPy ndarray to wrap. Must be 1-dimensional.", "Whether to copy values.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.arrays.PeriodArray", "path": "reference/api/pandas.arrays.periodarray", "type": "Input/output", "text": ["Pandas ExtensionArray for storing Period data.", "Users should use period_array() to create new instances. Alternatively, array() can be used to create new instances from a sequence of Period scalars.", "The data to store. These should be arrays that can be directly converted to ordinals without inference or copy (PeriodArray, ndarray[int64]), or a box around such an array (Series[period], PeriodIndex).", "A PeriodDtype instance from which to extract a freq. If both freq and dtype are specified, then the frequencies must match.", "The freq to use for the array. Mostly applicable when values is an ndarray of integers, when freq is required. When values is a PeriodArray (or box around), it\u2019s checked that values.freq matches freq.", "Whether to copy the ordinals before storing.", "See also", "Represents a period of time.", "Immutable Index for period data.", "Create a fixed-frequency PeriodArray.", "Construct a pandas array.", "Notes", "There are two components to a PeriodArray", "ordinals : integer ndarray", "freq : pd.tseries.offsets.Offset", "The values are physically stored as a 1-D ndarray of integers. These are called \u201cordinals\u201d and represent some kind of offset from a base.", "The freq indicates the span covered by each element of the array. All elements in the PeriodArray have the same freq.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.arrays.SparseArray", "path": "reference/api/pandas.arrays.sparsearray", "type": "Pandas arrays", "text": ["An ExtensionArray for storing sparse data.", "A dense array of values to store in the SparseArray. This may contain fill_value.", "Deprecated since version 1.4.0: Use a function like np.full to construct an array with the desired repeats of the scalar value instead.", "Elements in data that are fill_value are not stored in the SparseArray. For memory savings, this should be the most common value in data. By default, fill_value depends on the dtype of data:", "data.dtype", "na_value", "float", "np.nan", "int", "0", "bool", "False", "datetime64", "pd.NaT", "timedelta64", "pd.NaT", "The fill value is potentially specified in three ways. In order of precedence, these are", "The fill_value argument", "dtype.fill_value if fill_value is None and dtype is a SparseDtype", "data.dtype.fill_value if fill_value is None and dtype is not a SparseDtype and data is a SparseArray.", "Can be \u2018integer\u2019 or \u2018block\u2019, default is \u2018integer\u2019. The type of storage for sparse locations.", "\u2018block\u2019: Stores a block and block_length for each contiguous span of sparse values. This is best when sparse data tends to be clumped together, with large regions of fill-value values between sparse values.", "\u2018integer\u2019: uses an integer to store the location of each sparse value.", "The dtype to use for the SparseArray. For numpy dtypes, this determines the dtype of self.sp_values. For SparseDtype, this determines self.sp_values and self.fill_value.", "Whether to explicitly copy the incoming data array.", "Examples", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.arrays.StringArray", "path": "reference/api/pandas.arrays.stringarray", "type": "Pandas arrays", "text": ["Extension array for string data.", "New in version 1.0.0.", "Warning", "StringArray is considered experimental. The implementation and parts of the API may change without warning.", "The array of data.", "Warning", "Currently, this expects an object-dtype ndarray where the elements are Python strings or pandas.NA. This may change without warning in the future. Use pandas.array() with dtype=\"string\" for a stable way of creating a StringArray from any sequence.", "Whether to copy the array of data.", "See also", "The recommended function for creating a StringArray.", "The string methods are available on Series backed by a StringArray.", "Notes", "StringArray returns a BooleanArray for comparison methods.", "Examples", "Unlike arrays instantiated with dtype=\"object\", StringArray will convert the values to strings.", "However, instantiating StringArrays directly with non-strings will raise an error.", "For comparison methods, StringArray returns a pandas.BooleanArray:", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.arrays.TimedeltaArray", "path": "reference/api/pandas.arrays.timedeltaarray", "type": "Pandas arrays", "text": ["Pandas ExtensionArray for timedelta data.", "Warning", "TimedeltaArray is currently experimental, and its API may change without warning. In particular, TimedeltaArray.dtype is expected to change to be an instance of an ExtensionDtype subclass.", "The timedelta data.", "Currently, only numpy.dtype(\"timedelta64[ns]\") is accepted.", "Whether to copy the underlying array of data.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.bdate_range", "path": "reference/api/pandas.bdate_range", "type": "General functions", "text": ["Return a fixed frequency DatetimeIndex, with business day as the default frequency.", "Left bound for generating dates.", "Right bound for generating dates.", "Number of periods to generate.", "Frequency strings can have multiples, e.g. \u20185H\u2019.", "Time zone name for returning localized DatetimeIndex, for example Asia/Beijing.", "Normalize start/end dates to midnight before generating date range.", "Name of the resulting DatetimeIndex.", "Weekmask of valid business days, passed to numpy.busdaycalendar, only used when custom frequency strings are passed. The default value None is equivalent to \u2018Mon Tue Wed Thu Fri\u2019.", "Dates to exclude from the set of valid business days, passed to numpy.busdaycalendar, only used when custom frequency strings are passed.", "Make the interval closed with respect to the given frequency to the \u2018left\u2019, \u2018right\u2019, or both sides (None).", "Deprecated since version 1.4.0: Argument closed has been deprecated to standardize boundary inputs. Use inclusive instead, to set each bound as closed or open.", "Include boundaries; Whether to set each bound as closed or open.", "New in version 1.4.0.", "For compatibility. Has no effect on the result.", "Notes", "Of the four parameters: start, end, periods, and freq, exactly three must be specified. Specifying freq is a requirement for bdate_range. Use date_range if specifying freq is not desired.", "To learn more about the frequency strings, please see this link.", "Examples", "Note how the two weekend days are skipped in the result."]}, {"name": "pandas.BooleanDtype", "path": "reference/api/pandas.booleandtype", "type": "Pandas arrays", "text": ["Extension dtype for boolean data.", "New in version 1.0.0.", "Warning", "BooleanDtype is considered experimental. The implementation and parts of the API may change without warning.", "Examples", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.Categorical", "path": "reference/api/pandas.categorical", "type": "Pandas arrays", "text": ["Represent a categorical variable in classic R / S-plus fashion.", "Categoricals can only take on only a limited, and usually fixed, number of possible values (categories). In contrast to statistical categorical variables, a Categorical might have an order, but numerical operations (additions, divisions, \u2026) are not possible.", "All values of the Categorical are either in categories or np.nan. Assigning values outside of categories will raise a ValueError. Order is defined by the order of the categories, not lexical order of the values.", "The values of the categorical. If categories are given, values not in categories will be replaced with NaN.", "The unique categories for this categorical. If not given, the categories are assumed to be the unique values of values (sorted, if possible, otherwise in the order in which they appear).", "Whether or not this categorical is treated as a ordered categorical. If True, the resulting categorical will be ordered. An ordered categorical respects, when sorted, the order of its categories attribute (which in turn is the categories argument, if provided).", "An instance of CategoricalDtype to use for this categorical.", "If the categories do not validate.", "If an explicit ordered=True is given but no categories and the values are not sortable.", "See also", "Type for categorical data.", "An Index with an underlying Categorical.", "Notes", "See the user guide for more.", "Examples", "Missing values are not included as a category.", "However, their presence is indicated in the codes attribute by code -1.", "Ordered Categoricals can be sorted according to the custom order of the categories and can have a min and max value.", "Attributes", "categories", "The categories of this categorical.", "codes", "The category codes of this categorical.", "ordered", "Whether the categories have an ordered relationship.", "dtype", "The CategoricalDtype for this instance.", "Methods", "from_codes(codes[, categories, ordered, dtype])", "Make a Categorical type from codes and categories or dtype.", "__array__([dtype])", "The numpy array interface."]}, {"name": "pandas.Categorical.__array__", "path": "reference/api/pandas.categorical.__array__", "type": "Pandas arrays", "text": ["The numpy array interface.", "A numpy array of either the specified dtype or, if dtype==None (default), the same dtype as categorical.categories.dtype."]}, {"name": "pandas.Categorical.categories", "path": "reference/api/pandas.categorical.categories", "type": "Pandas arrays", "text": ["The categories of this categorical.", "Setting assigns new values to each category (effectively a rename of each individual category).", "The assigned value has to be a list-like object. All items must be unique and the number of items in the new categories must be the same as the number of items in the old categories.", "Assigning to categories is a inplace operation!", "If the new categories do not validate as categories or if the number of new categories is unequal the number of old categories", "See also", "Rename categories.", "Reorder categories.", "Add new categories.", "Remove the specified categories.", "Remove categories which are not used.", "Set the categories to the specified ones."]}, {"name": "pandas.Categorical.codes", "path": "reference/api/pandas.categorical.codes", "type": "Pandas arrays", "text": ["The category codes of this categorical.", "Codes are an array of integers which are the positions of the actual values in the categories array.", "There is no setter, use the other categorical methods and the normal item setter to change values in the categorical.", "A non-writable view of the codes array."]}, {"name": "pandas.Categorical.dtype", "path": "reference/api/pandas.categorical.dtype", "type": "Pandas arrays", "text": ["The CategoricalDtype for this instance."]}, {"name": "pandas.Categorical.from_codes", "path": "reference/api/pandas.categorical.from_codes", "type": "Pandas arrays", "text": ["Make a Categorical type from codes and categories or dtype.", "This constructor is useful if you already have codes and categories/dtype and so do not need the (computation intensive) factorization step, which is usually done on the constructor.", "If your data does not follow this convention, please use the normal constructor.", "An integer array, where each integer points to a category in categories or dtype.categories, or else is -1 for NaN.", "The categories for the categorical. Items need to be unique. If the categories are not given here, then they must be provided in dtype.", "Whether or not this categorical is treated as an ordered categorical. If not given here or in dtype, the resulting categorical will be unordered.", "If CategoricalDtype, cannot be used together with categories or ordered.", "Examples"]}, {"name": "pandas.Categorical.ordered", "path": "reference/api/pandas.categorical.ordered", "type": "Pandas arrays", "text": ["Whether the categories have an ordered relationship."]}, {"name": "pandas.CategoricalDtype", "path": "reference/api/pandas.categoricaldtype", "type": "Pandas arrays", "text": ["Type for categorical data with the categories and orderedness.", "Must be unique, and must not contain any nulls. The categories are stored in an Index, and if an index is provided the dtype of that index will be used.", "Whether or not this categorical is treated as a ordered categorical. None can be used to maintain the ordered value of existing categoricals when used in operations that combine categoricals, e.g. astype, and will resolve to False if there is no existing ordered to maintain.", "See also", "Represent a categorical variable in classic R / S-plus fashion.", "Notes", "This class is useful for specifying the type of a Categorical independent of the values. See CategoricalDtype for more.", "Examples", "An empty CategoricalDtype with a specific dtype can be created by providing an empty index. As follows,", "Attributes", "categories", "An Index containing the unique categories allowed.", "ordered", "Whether the categories have an ordered relationship.", "Methods", "None"]}, {"name": "pandas.CategoricalDtype.categories", "path": "reference/api/pandas.categoricaldtype.categories", "type": "Pandas arrays", "text": ["An Index containing the unique categories allowed."]}, {"name": "pandas.CategoricalDtype.ordered", "path": "reference/api/pandas.categoricaldtype.ordered", "type": "Pandas arrays", "text": ["Whether the categories have an ordered relationship."]}, {"name": "pandas.CategoricalIndex", "path": "reference/api/pandas.categoricalindex", "type": "Index Objects", "text": ["Index based on an underlying Categorical.", "CategoricalIndex, like Categorical, can only take on a limited, and usually fixed, number of possible values (categories). Also, like Categorical, it might have an order, but numerical operations (additions, divisions, \u2026) are not possible.", "The values of the categorical. If categories are given, values not in categories will be replaced with NaN.", "The categories for the categorical. Items need to be unique. If the categories are not given here (and also not in dtype), they will be inferred from the data.", "Whether or not this categorical is treated as an ordered categorical. If not given here or in dtype, the resulting categorical will be unordered.", "If CategoricalDtype, cannot be used together with categories or ordered.", "Make a copy of input ndarray.", "Name to be stored in the index.", "If the categories do not validate.", "If an explicit ordered=True is given but no categories and the values are not sortable.", "See also", "The base pandas Index type.", "A categorical array.", "Type for categorical data.", "Notes", "See the user guide for more.", "Examples", "CategoricalIndex can also be instantiated from a Categorical:", "Ordered CategoricalIndex can have a min and max value.", "Attributes", "codes", "The category codes of this categorical.", "categories", "The categories of this categorical.", "ordered", "Whether the categories have an ordered relationship.", "Methods", "rename_categories(*args, **kwargs)", "Rename categories.", "reorder_categories(*args, **kwargs)", "Reorder categories as specified in new_categories.", "add_categories(*args, **kwargs)", "Add new categories.", "remove_categories(*args, **kwargs)", "Remove the specified categories.", "remove_unused_categories(*args, **kwargs)", "Remove categories which are not used.", "set_categories(*args, **kwargs)", "Set the categories to the specified new_categories.", "as_ordered(*args, **kwargs)", "Set the Categorical to be ordered.", "as_unordered(*args, **kwargs)", "Set the Categorical to be unordered.", "map(mapper)", "Map values using input an input mapping or function."]}, {"name": "pandas.CategoricalIndex.add_categories", "path": "reference/api/pandas.categoricalindex.add_categories", "type": "Index Objects", "text": ["Add new categories.", "new_categories will be included at the last/highest place in the categories and will be unused directly after this call.", "The new categories to be included.", "Whether or not to add the categories inplace or return a copy of this categorical with added categories.", "Deprecated since version 1.3.0.", "Categorical with new categories added or None if inplace=True.", "If the new categories include old categories or do not validate as categories", "See also", "Rename categories.", "Reorder categories.", "Remove the specified categories.", "Remove categories which are not used.", "Set the categories to the specified ones.", "Examples"]}, {"name": "pandas.CategoricalIndex.as_ordered", "path": "reference/api/pandas.categoricalindex.as_ordered", "type": "Index Objects", "text": ["Set the Categorical to be ordered.", "Whether or not to set the ordered attribute in-place or return a copy of this categorical with ordered set to True.", "Ordered Categorical or None if inplace=True."]}, {"name": "pandas.CategoricalIndex.as_unordered", "path": "reference/api/pandas.categoricalindex.as_unordered", "type": "Index Objects", "text": ["Set the Categorical to be unordered.", "Whether or not to set the ordered attribute in-place or return a copy of this categorical with ordered set to False.", "Unordered Categorical or None if inplace=True."]}, {"name": "pandas.CategoricalIndex.categories", "path": "reference/api/pandas.categoricalindex.categories", "type": "Index Objects", "text": ["The categories of this categorical.", "Setting assigns new values to each category (effectively a rename of each individual category).", "The assigned value has to be a list-like object. All items must be unique and the number of items in the new categories must be the same as the number of items in the old categories.", "Assigning to categories is a inplace operation!", "If the new categories do not validate as categories or if the number of new categories is unequal the number of old categories", "See also", "Rename categories.", "Reorder categories.", "Add new categories.", "Remove the specified categories.", "Remove categories which are not used.", "Set the categories to the specified ones."]}, {"name": "pandas.CategoricalIndex.codes", "path": "reference/api/pandas.categoricalindex.codes", "type": "Index Objects", "text": ["The category codes of this categorical.", "Codes are an array of integers which are the positions of the actual values in the categories array.", "There is no setter, use the other categorical methods and the normal item setter to change values in the categorical.", "A non-writable view of the codes array."]}, {"name": "pandas.CategoricalIndex.equals", "path": "reference/api/pandas.categoricalindex.equals", "type": "Index Objects", "text": ["Determine if two CategoricalIndex objects contain the same elements.", "If two CategoricalIndex objects have equal elements True, otherwise False."]}, {"name": "pandas.CategoricalIndex.map", "path": "reference/api/pandas.categoricalindex.map", "type": "Index Objects", "text": ["Map values using input an input mapping or function.", "Maps the values (their categories, not the codes) of the index to new categories. If the mapping correspondence is one-to-one the result is a CategoricalIndex which has the same order property as the original, otherwise an Index is returned.", "If a dict or Series is used any unmapped category is mapped to NaN. Note that if this happens an Index will be returned.", "Mapping correspondence.", "Mapped index.", "See also", "Apply a mapping correspondence on an Index.", "Apply a mapping correspondence on a Series.", "Apply more complex functions on a Series.", "Examples", "If the mapping is one-to-one the ordering of the categories is preserved:", "If the mapping is not one-to-one an Index is returned:", "If a dict is used, all unmapped categories are mapped to NaN and the result is an Index:"]}, {"name": "pandas.CategoricalIndex.ordered", "path": "reference/api/pandas.categoricalindex.ordered", "type": "Index Objects", "text": ["Whether the categories have an ordered relationship."]}, {"name": "pandas.CategoricalIndex.remove_categories", "path": "reference/api/pandas.categoricalindex.remove_categories", "type": "Index Objects", "text": ["Remove the specified categories.", "removals must be included in the old categories. Values which were in the removed categories will be set to NaN", "The categories which should be removed.", "Whether or not to remove the categories inplace or return a copy of this categorical with removed categories.", "Deprecated since version 1.3.0.", "Categorical with removed categories or None if inplace=True.", "If the removals are not contained in the categories", "See also", "Rename categories.", "Reorder categories.", "Add new categories.", "Remove categories which are not used.", "Set the categories to the specified ones.", "Examples"]}, {"name": "pandas.CategoricalIndex.remove_unused_categories", "path": "reference/api/pandas.categoricalindex.remove_unused_categories", "type": "Index Objects", "text": ["Remove categories which are not used.", "Whether or not to drop unused categories inplace or return a copy of this categorical with unused categories dropped.", "Deprecated since version 1.2.0.", "Categorical with unused categories dropped or None if inplace=True.", "See also", "Rename categories.", "Reorder categories.", "Add new categories.", "Remove the specified categories.", "Set the categories to the specified ones.", "Examples"]}, {"name": "pandas.CategoricalIndex.rename_categories", "path": "reference/api/pandas.categoricalindex.rename_categories", "type": "Index Objects", "text": ["Rename categories.", "New categories which will replace old categories.", "list-like: all items must be unique and the number of items in the new categories must match the existing number of categories.", "dict-like: specifies a mapping from old categories to new. Categories not contained in the mapping are passed through and extra categories in the mapping are ignored.", "callable : a callable that is called on all items in the old categories and whose return values comprise the new categories.", "Whether or not to rename the categories inplace or return a copy of this categorical with renamed categories.", "Deprecated since version 1.3.0.", "Categorical with removed categories or None if inplace=True.", "If new categories are list-like and do not have the same number of items than the current categories or do not validate as categories", "See also", "Reorder categories.", "Add new categories.", "Remove the specified categories.", "Remove categories which are not used.", "Set the categories to the specified ones.", "Examples", "For dict-like new_categories, extra keys are ignored and categories not in the dictionary are passed through", "You may also provide a callable to create the new categories"]}, {"name": "pandas.CategoricalIndex.reorder_categories", "path": "reference/api/pandas.categoricalindex.reorder_categories", "type": "Index Objects", "text": ["Reorder categories as specified in new_categories.", "new_categories need to include all old categories and no new category items.", "The categories in new order.", "Whether or not the categorical is treated as a ordered categorical. If not given, do not change the ordered information.", "Whether or not to reorder the categories inplace or return a copy of this categorical with reordered categories.", "Deprecated since version 1.3.0.", "Categorical with removed categories or None if inplace=True.", "If the new categories do not contain all old category items or any new ones", "See also", "Rename categories.", "Add new categories.", "Remove the specified categories.", "Remove categories which are not used.", "Set the categories to the specified ones."]}, {"name": "pandas.CategoricalIndex.set_categories", "path": "reference/api/pandas.categoricalindex.set_categories", "type": "Index Objects", "text": ["Set the categories to the specified new_categories.", "new_categories can include new categories (which will result in unused categories) or remove old categories (which results in values set to NaN). If rename==True, the categories will simple be renamed (less or more items than in old categories will result in values set to NaN or in unused categories respectively).", "This method can be used to perform more than one action of adding, removing, and reordering simultaneously and is therefore faster than performing the individual steps via the more specialised methods.", "On the other hand this methods does not do checks (e.g., whether the old categories are included in the new categories on a reorder), which can result in surprising changes, for example when using special string dtypes, which does not considers a S1 string equal to a single char python string.", "The categories in new order.", "Whether or not the categorical is treated as a ordered categorical. If not given, do not change the ordered information.", "Whether or not the new_categories should be considered as a rename of the old categories or as reordered categories.", "Whether or not to reorder the categories in-place or return a copy of this categorical with reordered categories.", "Deprecated since version 1.3.0.", "If new_categories does not validate as categories", "See also", "Rename categories.", "Reorder categories.", "Add new categories.", "Remove the specified categories.", "Remove categories which are not used."]}, {"name": "pandas.concat", "path": "reference/api/pandas.concat", "type": "General functions", "text": ["Concatenate pandas objects along a particular axis with optional set logic along the other axes.", "Can also add a layer of hierarchical indexing on the concatenation axis, which may be useful if the labels are the same (or overlapping) on the passed axis number.", "If a mapping is passed, the sorted keys will be used as the keys argument, unless it is passed, in which case the values will be selected (see below). Any None objects will be dropped silently unless they are all None in which case a ValueError will be raised.", "The axis to concatenate along.", "How to handle indexes on other axis (or axes).", "If True, do not use the index values along the concatenation axis. The resulting axis will be labeled 0, \u2026, n - 1. This is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information. Note the index values on the other axes are still respected in the join.", "If multiple levels passed, should contain tuples. Construct hierarchical index using the passed keys as the outermost level.", "Specific levels (unique values) to use for constructing a MultiIndex. Otherwise they will be inferred from the keys.", "Names for the levels in the resulting hierarchical index.", "Check whether the new concatenated axis contains duplicates. This can be very expensive relative to the actual data concatenation.", "Sort non-concatenation axis if it is not already aligned when join is \u2018outer\u2019. This has no effect when join='inner', which already preserves the order of the non-concatenation axis.", "Changed in version 1.0.0: Changed to not sort by default.", "If False, do not copy data unnecessarily.", "When concatenating all Series along the index (axis=0), a Series is returned. When objs contains at least one DataFrame, a DataFrame is returned. When concatenating along the columns (axis=1), a DataFrame is returned.", "See also", "Concatenate Series.", "Concatenate DataFrames.", "Join DataFrames using indexes.", "Merge DataFrames by indexes or columns.", "Notes", "The keys, levels, and names arguments are all optional.", "A walkthrough of how this method fits in with other tools for combining pandas objects can be found here.", "Examples", "Combine two Series.", "Clear the existing index and reset it in the result by setting the ignore_index option to True.", "Add a hierarchical index at the outermost level of the data with the keys option.", "Label the index keys you create with the names option.", "Combine two DataFrame objects with identical columns.", "Combine DataFrame objects with overlapping columns and return everything. Columns outside the intersection will be filled with NaN values.", "Combine DataFrame objects with overlapping columns and return only those that are shared by passing inner to the join keyword argument.", "Combine DataFrame objects horizontally along the x axis by passing in axis=1.", "Prevent the result from including duplicate index values with the verify_integrity option."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.aggregate", "path": "reference/api/pandas.core.groupby.dataframegroupby.aggregate", "type": "GroupBy", "text": ["Aggregate using one or more operations over the specified axis.", "Function to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply.", "Accepted combinations are:", "function", "string function name", "list of functions and/or function names, e.g. [np.sum, 'mean']", "dict of axis labels -> functions, function names or list of such.", "Can also accept a Numba JIT function with engine='numba' specified. Only passing a single function is supported with this engine.", "If the 'numba' engine is chosen, the function must be a user defined function with values and index as the first and second arguments respectively in the function signature. Each group\u2019s index will be passed to the user defined function and optionally available for use.", "Changed in version 1.1.0.", "Positional arguments to pass to func.", "'cython' : Runs the function through C-extensions from cython.", "'numba' : Runs the function through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.1.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to the function", "New in version 1.1.0.", "Keyword arguments to be passed into func.", "See also", "Apply function func group-wise and combine the results together.", "Aggregate using one or more operations over the specified axis.", "Transforms the Series on each group based on the given function.", "Notes", "When using engine='numba', there will be no \u201cfall back\u201d behavior internally. The group data and group index will be passed as numpy arrays to the JITed user defined function, and no alternative execution attempts will be tried.", "Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.", "Changed in version 1.3.0: The resulting dtype will reflect the return value of the passed func, see the examples below.", "Examples", "The aggregation is for each column.", "Multiple aggregations", "Select a column for aggregation", "Different aggregations per column", "To control the output names with different aggregations per column, pandas supports \u201cnamed aggregation\u201d", "The keywords are the output column names", "The values are tuples whose first element is the column to select and the second element is the aggregation to apply to that column. Pandas provides the pandas.NamedAgg namedtuple with the fields ['column', 'aggfunc'] to make it clearer what the arguments are. As usual, the aggregation can be a callable or a string alias.", "See Named aggregation for more.", "Changed in version 1.3.0: The resulting dtype will reflect the return value of the aggregating function."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.all", "path": "reference/api/pandas.core.groupby.dataframegroupby.all", "type": "GroupBy", "text": ["Return True if all values in the group are truthful, else False.", "Flag to ignore nan values during truth testing.", "DataFrame or Series of boolean values, where a value is True if all elements are True within its respective group, False otherwise.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.any", "path": "reference/api/pandas.core.groupby.dataframegroupby.any", "type": "GroupBy", "text": ["Return True if any value in the group is truthful, else False.", "Flag to ignore nan values during truth testing.", "DataFrame or Series of boolean values, where a value is True if any element is True within its respective group, False otherwise.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.backfill", "path": "reference/api/pandas.core.groupby.dataframegroupby.backfill", "type": "GroupBy", "text": ["Backward fill the values.", "Limit of how many values to fill.", "Object with missing values filled.", "See also", "Backward fill the missing values in the dataset.", "Backward fill the missing values in the dataset.", "Fill NaN values of a Series.", "Fill NaN values of a DataFrame."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.bfill", "path": "reference/api/pandas.core.groupby.dataframegroupby.bfill", "type": "GroupBy", "text": ["Backward fill the values.", "Limit of how many values to fill.", "Object with missing values filled.", "See also", "Backward fill the missing values in the dataset.", "Backward fill the missing values in the dataset.", "Fill NaN values of a Series.", "Fill NaN values of a DataFrame."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.boxplot", "path": "reference/api/pandas.core.groupby.dataframegroupby.boxplot", "type": "GroupBy", "text": ["Make box plots from DataFrameGroupBy data.", "False - no subplots will be used", "True - create a subplot for each group.", "Can be any valid input to groupby.", "The layout of the plot: (rows, columns).", "Whether x-axes will be shared among subplots.", "Whether y-axes will be shared among subplots.", "Backend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.", "New in version 1.0.0.", "All other plotting keyword arguments to be passed to matplotlib\u2019s boxplot function.", "Examples", "You can create boxplots for grouped data and show them as separate subplots:", "The subplots=False option shows the boxplots in a single figure."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.corr", "path": "reference/api/pandas.core.groupby.dataframegroupby.corr", "type": "GroupBy", "text": ["Compute pairwise correlation of columns, excluding NA/null values.", "Method of correlation:", "pearson : standard correlation coefficient", "kendall : Kendall Tau correlation coefficient", "spearman : Spearman rank correlation", "and returning a float. Note that the returned matrix from corr will have 1 along the diagonals and will be symmetric regardless of the callable\u2019s behavior.", "Minimum number of observations required per pair of columns to have a valid result. Currently only available for Pearson and Spearman correlation.", "Correlation matrix.", "See also", "Compute pairwise correlation with another DataFrame or Series.", "Compute the correlation between two Series.", "Examples"]}, {"name": "pandas.core.groupby.DataFrameGroupBy.corrwith", "path": "reference/api/pandas.core.groupby.dataframegroupby.corrwith", "type": "GroupBy", "text": ["Compute pairwise correlation.", "Pairwise correlation is computed between rows or columns of DataFrame with rows or columns of Series or DataFrame. DataFrames are first aligned along both axes before computing the correlations.", "Object with which to compute correlations.", "The axis to use. 0 or \u2018index\u2019 to compute column-wise, 1 or \u2018columns\u2019 for row-wise.", "Drop missing indices from result.", "Method of correlation:", "pearson : standard correlation coefficient", "kendall : Kendall Tau correlation coefficient", "spearman : Spearman rank correlation", "and returning a float.", "Pairwise correlations.", "See also", "Compute pairwise correlation of columns."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.count", "path": "reference/api/pandas.core.groupby.dataframegroupby.count", "type": "GroupBy", "text": ["Compute count of group, excluding missing values.", "Count of values within each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.cov", "path": "reference/api/pandas.core.groupby.dataframegroupby.cov", "type": "GroupBy", "text": ["Compute pairwise covariance of columns, excluding NA/null values.", "Compute the pairwise covariance among the series of a DataFrame. The returned data frame is the covariance matrix of the columns of the DataFrame.", "Both NA and null values are automatically excluded from the calculation. (See the note below about bias from missing values.) A threshold can be set for the minimum number of observations for each value created. Comparisons with observations below this threshold will be returned as NaN.", "This method is generally used for the analysis of time series data to understand the relationship between different measures across time.", "Minimum number of observations required per pair of columns to have a valid result.", "Delta degrees of freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "New in version 1.1.0.", "The covariance matrix of the series of the DataFrame.", "See also", "Compute covariance with another Series.", "Exponential weighted sample covariance.", "Expanding sample covariance.", "Rolling sample covariance.", "Notes", "Returns the covariance matrix of the DataFrame\u2019s time series. The covariance is normalized by N-ddof.", "For DataFrames that have Series that are missing data (assuming that data is missing at random) the returned covariance matrix will be an unbiased estimate of the variance and covariance between the member Series.", "However, for many applications this estimate may not be acceptable because the estimate covariance matrix is not guaranteed to be positive semi-definite. This could lead to estimate correlations having absolute values which are greater than one, and/or a non-invertible covariance matrix. See Estimation of covariance matrices for more details.", "Examples", "Minimum number of periods", "This method also supports an optional min_periods keyword that specifies the required minimum number of non-NA observations for each column pair in order to have a valid result:"]}, {"name": "pandas.core.groupby.DataFrameGroupBy.cumcount", "path": "reference/api/pandas.core.groupby.dataframegroupby.cumcount", "type": "GroupBy", "text": ["Number each item in each group from 0 to the length of that group - 1.", "Essentially this is equivalent to", "If False, number in reverse, from length of group - 1 to 0.", "Sequence number of each element within each group.", "See also", "Number the groups themselves.", "Examples"]}, {"name": "pandas.core.groupby.DataFrameGroupBy.cummax", "path": "reference/api/pandas.core.groupby.dataframegroupby.cummax", "type": "GroupBy", "text": ["Cumulative max for each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.cummin", "path": "reference/api/pandas.core.groupby.dataframegroupby.cummin", "type": "GroupBy", "text": ["Cumulative min for each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.cumprod", "path": "reference/api/pandas.core.groupby.dataframegroupby.cumprod", "type": "GroupBy", "text": ["Cumulative product for each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.cumsum", "path": "reference/api/pandas.core.groupby.dataframegroupby.cumsum", "type": "GroupBy", "text": ["Cumulative sum for each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.describe", "path": "reference/api/pandas.core.groupby.dataframegroupby.describe", "type": "GroupBy", "text": ["Generate descriptive statistics.", "Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values.", "Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. Refer to the notes below for more detail.", "The percentiles to include in the output. All should fall between 0 and 1. The default is [.25, .5, .75], which returns the 25th, 50th, and 75th percentiles.", "A white list of data types to include in the result. Ignored for Series. Here are the options:", "\u2018all\u2019 : All columns of the input will be included in the output.", "A list-like of dtypes : Limits the results to the provided data types. To limit the result to numeric types submit numpy.number. To limit it instead to object columns submit the numpy.object data type. Strings can also be used in the style of select_dtypes (e.g. df.describe(include=['O'])). To select pandas categorical columns, use 'category'", "None (default) : The result will include all numeric columns.", "A black list of data types to omit from the result. Ignored for Series. Here are the options:", "A list-like of dtypes : Excludes the provided data types from the result. To exclude numeric types submit numpy.number. To exclude object columns submit the data type numpy.object. Strings can also be used in the style of select_dtypes (e.g. df.describe(exclude=['O'])). To exclude pandas categorical columns, use 'category'", "None (default) : The result will exclude nothing.", "Whether to treat datetime dtypes as numeric. This affects statistics calculated for the column. For DataFrame input, this also controls whether datetime columns are included by default.", "New in version 1.1.0.", "Summary statistics of the Series or Dataframe provided.", "See also", "Count number of non-NA/null observations.", "Maximum of the values in the object.", "Minimum of the values in the object.", "Mean of the values.", "Standard deviation of the observations.", "Subset of a DataFrame including/excluding columns based on their dtype.", "Notes", "For numeric data, the result\u2019s index will include count, mean, std, min, max as well as lower, 50 and upper percentiles. By default the lower percentile is 25 and the upper percentile is 75. The 50 percentile is the same as the median.", "For object data (e.g. strings or timestamps), the result\u2019s index will include count, unique, top, and freq. The top is the most common value. The freq is the most common value\u2019s frequency. Timestamps also include the first and last items.", "If multiple object values have the highest count, then the count and top results will be arbitrarily chosen from among those with the highest count.", "For mixed data types provided via a DataFrame, the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If include='all' is provided as an option, the result will include a union of attributes of each type.", "The include and exclude parameters can be used to limit which columns in a DataFrame are analyzed for the output. The parameters are ignored when analyzing a Series.", "Examples", "Describing a numeric Series.", "Describing a categorical Series.", "Describing a timestamp Series.", "Describing a DataFrame. By default only numeric fields are returned.", "Describing all columns of a DataFrame regardless of data type.", "Describing a column from a DataFrame by accessing it as an attribute.", "Including only numeric columns in a DataFrame description.", "Including only string columns in a DataFrame description.", "Including only categorical columns from a DataFrame description.", "Excluding numeric columns from a DataFrame description.", "Excluding object columns from a DataFrame description."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.diff", "path": "reference/api/pandas.core.groupby.dataframegroupby.diff", "type": "GroupBy", "text": ["First discrete difference of element.", "Calculates the difference of a Dataframe element compared with another element in the Dataframe (default is element in previous row).", "Periods to shift for calculating difference, accepts negative values.", "Take difference over rows (0) or columns (1).", "First differences of the Series.", "See also", "Percent change over given number of periods.", "Shift index by desired number of periods with an optional time freq.", "First discrete difference of object.", "Notes", "For boolean dtypes, this uses operator.xor() rather than operator.sub(). The result is calculated according to current dtype in Dataframe, however dtype of the result is always float64.", "Examples", "Difference with previous row", "Difference with previous column", "Difference with 3rd previous row", "Difference with following row", "Overflow in input dtype"]}, {"name": "pandas.core.groupby.DataFrameGroupBy.ffill", "path": "reference/api/pandas.core.groupby.dataframegroupby.ffill", "type": "GroupBy", "text": ["Forward fill the values.", "Limit of how many values to fill.", "Object with missing values filled.", "See also", "Returns Series with minimum number of char in object.", "Object with missing values filled or None if inplace=True.", "Fill NaN values of a Series.", "Fill NaN values of a DataFrame."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.fillna", "path": "reference/api/pandas.core.groupby.dataframegroupby.fillna", "type": "GroupBy", "text": ["Fill NA/NaN values using the specified method.", "Value to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of values specifying which value to use for each index (for a Series) or column (for a DataFrame). Values not in the dict/Series/DataFrame will not be filled. This value cannot be a list.", "Method to use for filling holes in reindexed Series pad / ffill: propagate last valid observation forward to next valid backfill / bfill: use next valid observation to fill gap.", "Axis along which to fill missing values.", "If True, fill in-place. Note: this will modify any other views on this object (e.g., a no-copy slice for a column in a DataFrame).", "If method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None.", "A dict of item->dtype of what to downcast if possible, or the string \u2018infer\u2019 which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible).", "Object with missing values filled or None if inplace=True.", "See also", "Fill NaN values using interpolation.", "Conform object to new index.", "Convert TimeSeries to specified frequency.", "Examples", "Replace all NaN elements with 0s.", "We can also propagate non-null values forward or backward.", "Replace all NaN elements in column \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019, with 0, 1, 2, and 3 respectively.", "Only replace the first NaN element.", "When filling using a DataFrame, replacement happens along the same column names and same indices", "Note that column D is not affected since it is not present in df2."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.filter", "path": "reference/api/pandas.core.groupby.dataframegroupby.filter", "type": "GroupBy", "text": ["Return a copy of a DataFrame excluding filtered elements.", "Elements from groups are filtered if they do not satisfy the boolean criterion specified by func.", "Function to apply to each subframe. Should return True or False.", "If False, groups that evaluate False are filled with NaNs.", "Notes", "Each subframe is endowed the attribute \u2018name\u2019 in case you need to know which group you are working on.", "Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.", "Examples"]}, {"name": "pandas.core.groupby.DataFrameGroupBy.hist", "path": "reference/api/pandas.core.groupby.dataframegroupby.hist", "type": "GroupBy", "text": ["Make a histogram of the DataFrame\u2019s columns.", "A histogram is a representation of the distribution of data. This function calls matplotlib.pyplot.hist(), on each series in the DataFrame, resulting in one histogram per column.", "The pandas object holding the data.", "If passed, will be used to limit data to a subset of columns.", "If passed, then used to form histograms for separate groups.", "Whether to show axis grid lines.", "If specified changes the x-axis label size.", "Rotation of x axis labels. For example, a value of 90 displays the x labels rotated 90 degrees clockwise.", "If specified changes the y-axis label size.", "Rotation of y axis labels. For example, a value of 90 displays the y labels rotated 90 degrees clockwise.", "The axes to plot the histogram on.", "In case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in. Note that passing in both an ax and sharex=True will alter all x axis labels for all subplots in a figure.", "In case subplots=True, share y axis and set some y axis labels to invisible.", "The size in inches of the figure to create. Uses the value in matplotlib.rcParams by default.", "Tuple of (rows, columns) for the layout of the histograms.", "Number of histogram bins to be used. If an integer is given, bins + 1 bin edges are calculated and returned. If bins is a sequence, gives bin edges, including left edge of first bin and right edge of last bin. In this case, bins is returned unmodified.", "Backend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.", "New in version 1.0.0.", "Whether to show the legend.", "New in version 1.1.0.", "All other plotting keyword arguments to be passed to matplotlib.pyplot.hist().", "See also", "Plot a histogram using matplotlib.", "Examples", "This example draws a histogram based on the length and width of some animals, displayed in three bins"]}, {"name": "pandas.core.groupby.DataFrameGroupBy.idxmax", "path": "reference/api/pandas.core.groupby.dataframegroupby.idxmax", "type": "GroupBy", "text": ["Return index of first occurrence of maximum over requested axis.", "NA/null values are excluded.", "The axis to use. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "Indexes of maxima along the specified axis.", "If the row/column is empty", "See also", "Return index of the maximum element.", "Notes", "This method is the DataFrame version of ndarray.argmax.", "Examples", "Consider a dataset containing food consumption in Argentina.", "By default, it returns the index for the maximum value in each column.", "To return the index for the maximum value in each row, use axis=\"columns\"."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.idxmin", "path": "reference/api/pandas.core.groupby.dataframegroupby.idxmin", "type": "GroupBy", "text": ["Return index of first occurrence of minimum over requested axis.", "NA/null values are excluded.", "The axis to use. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "Indexes of minima along the specified axis.", "If the row/column is empty", "See also", "Return index of the minimum element.", "Notes", "This method is the DataFrame version of ndarray.argmin.", "Examples", "Consider a dataset containing food consumption in Argentina.", "By default, it returns the index for the minimum value in each column.", "To return the index for the minimum value in each row, use axis=\"columns\"."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.mad", "path": "reference/api/pandas.core.groupby.dataframegroupby.mad", "type": "GroupBy", "text": ["Return the mean absolute deviation of the values over the requested axis.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.nunique", "path": "reference/api/pandas.core.groupby.dataframegroupby.nunique", "type": "GroupBy", "text": ["Return DataFrame with counts of unique elements in each position.", "Don\u2019t include NaN in the counts.", "Examples", "Check for rows with the same id but conflicting values:"]}, {"name": "pandas.core.groupby.DataFrameGroupBy.pad", "path": "reference/api/pandas.core.groupby.dataframegroupby.pad", "type": "GroupBy", "text": ["Forward fill the values.", "Limit of how many values to fill.", "Object with missing values filled.", "See also", "Returns Series with minimum number of char in object.", "Object with missing values filled or None if inplace=True.", "Fill NaN values of a Series.", "Fill NaN values of a DataFrame."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.pct_change", "path": "reference/api/pandas.core.groupby.dataframegroupby.pct_change", "type": "GroupBy", "text": ["Calculate pct_change of each value to previous entry in group.", "Percentage changes within each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.plot", "path": "reference/api/pandas.core.groupby.dataframegroupby.plot", "type": "GroupBy", "text": ["Class implementing the .plot attribute for groupby objects."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.quantile", "path": "reference/api/pandas.core.groupby.dataframegroupby.quantile", "type": "GroupBy", "text": ["Return group values at the given quantile, a la numpy.percentile.", "Value(s) between 0 and 1 providing the quantile(s) to compute.", "Method to use when the desired quantile falls between two points.", "Return type determined by caller of GroupBy object.", "See also", "Similar method for Series.", "Similar method for DataFrame.", "NumPy method to compute qth percentile.", "Examples"]}, {"name": "pandas.core.groupby.DataFrameGroupBy.rank", "path": "reference/api/pandas.core.groupby.dataframegroupby.rank", "type": "GroupBy", "text": ["Provide the rank of values within each group.", "average: average rank of group.", "min: lowest rank in group.", "max: highest rank in group.", "first: ranks assigned in order they appear in the array.", "dense: like \u2018min\u2019, but rank always increases by 1 between groups.", "False for ranks by high (1) to low (N).", "keep: leave NA values where they are.", "top: smallest rank if ascending.", "bottom: smallest rank if descending.", "Compute percentage rank of data within each group.", "The axis of the object over which to compute the rank.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame.", "Examples"]}, {"name": "pandas.core.groupby.DataFrameGroupBy.resample", "path": "reference/api/pandas.core.groupby.dataframegroupby.resample", "type": "GroupBy", "text": ["Provide resampling when using a TimeGrouper.", "Given a grouper, the function resamples it according to a string \u201cstring\u201d -> \u201cfrequency\u201d.", "See the frequency aliases documentation for more details.", "The offset string or object representing target grouper conversion.", "Possible arguments are how, fill_method, limit, kind and on, and other arguments of TimeGrouper.", "Return a new grouper with our resampler appended.", "See also", "Specify a frequency to resample with when grouping by a key.", "Frequency conversion and resampling of time series.", "Examples", "Downsample the DataFrame into 3 minute bins and sum the values of the timestamps falling into a bin.", "Upsample the series into 30 second bins.", "Resample by month. Values are assigned to the month of the period.", "Downsample the series into 3 minute bins as above, but close the right side of the bin interval.", "Downsample the series into 3 minute bins and close the right side of the bin interval, but label each bin using the right edge instead of the left."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.sample", "path": "reference/api/pandas.core.groupby.dataframegroupby.sample", "type": "GroupBy", "text": ["Return a random sample of items from each group.", "You can use random_state for reproducibility.", "New in version 1.1.0.", "Number of items to return for each group. Cannot be used with frac and must be no larger than the smallest group unless replace is True. Default is one if frac is None.", "Fraction of items to return. Cannot be used with n.", "Allow or disallow sampling of the same row more than once.", "Default None results in equal probability weighting. If passed a list-like then values must have the same length as the underlying DataFrame or Series object and will be used as sampling probabilities after normalization within each group. Values must be non-negative with at least one positive element within each group.", "If int, array-like, or BitGenerator, seed for random number generator. If np.random.RandomState or np.random.Generator, use as given.", "Changed in version 1.4.0: np.random.Generator objects now accepted", "A new object of same type as caller containing items randomly sampled within each group from the caller object.", "See also", "Generate random samples from a DataFrame object.", "Generate a random sample from a given 1-D numpy array.", "Examples", "Select one row at random for each distinct value in column a. The random_state argument can be used to guarantee reproducibility:", "Set frac to sample fixed proportions rather than counts:", "Control sample probabilities within groups by setting weights:"]}, {"name": "pandas.core.groupby.DataFrameGroupBy.shift", "path": "reference/api/pandas.core.groupby.dataframegroupby.shift", "type": "GroupBy", "text": ["Shift each group by periods observations.", "If freq is passed, the index will be increased using the periods and the freq.", "Number of periods to shift.", "Frequency string.", "Shift direction.", "The scalar value to use for newly introduced missing values.", "Object shifted within each group.", "See also", "Shift values of Index.", "Shift the time index, using the index\u2019s frequency if available."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.size", "path": "reference/api/pandas.core.groupby.dataframegroupby.size", "type": "GroupBy", "text": ["Compute group sizes.", "Number of rows in each group as a Series if as_index is True or a DataFrame if as_index is False.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.skew", "path": "reference/api/pandas.core.groupby.dataframegroupby.skew", "type": "GroupBy", "text": ["Return unbiased skew over requested axis.", "Normalized by N-1.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Additional keyword arguments to be passed to the function."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.take", "path": "reference/api/pandas.core.groupby.dataframegroupby.take", "type": "GroupBy", "text": ["Return the elements in the given positional indices along an axis.", "This means that we are not indexing according to actual values in the index attribute of the object. We are indexing according to the actual position of the element in the object.", "An array of ints indicating which positions to take.", "The axis on which to select elements. 0 means that we are selecting rows, 1 means that we are selecting columns.", "Before pandas 1.0, is_copy=False can be specified to ensure that the return value is an actual copy. Starting with pandas 1.0, take always returns a copy, and the keyword is therefore deprecated.", "Deprecated since version 1.0.0.", "For compatibility with numpy.take(). Has no effect on the output.", "An array-like containing the elements taken from the object.", "See also", "Select a subset of a DataFrame by labels.", "Select a subset of a DataFrame by positions.", "Take elements from an array along an axis.", "Examples", "Take elements at positions 0 and 3 along the axis 0 (default).", "Note how the actual indices selected (0 and 1) do not correspond to our selected indices 0 and 3. That\u2019s because we are selecting the 0th and 3rd rows, not rows whose indices equal 0 and 3.", "Take elements at indices 1 and 2 along the axis 1 (column selection).", "We may take elements using negative integers for positive indices, starting from the end of the object, just like with Python lists."]}, {"name": "pandas.core.groupby.DataFrameGroupBy.transform", "path": "reference/api/pandas.core.groupby.dataframegroupby.transform", "type": "GroupBy", "text": ["Call function producing a like-indexed DataFrame on each group and return a DataFrame having the same indexes as the original object filled with the transformed values.", "Function to apply to each group.", "Can also accept a Numba JIT function with engine='numba' specified.", "If the 'numba' engine is chosen, the function must be a user defined function with values and index as the first and second arguments respectively in the function signature. Each group\u2019s index will be passed to the user defined function and optionally available for use.", "Changed in version 1.1.0.", "Positional arguments to pass to func.", "'cython' : Runs the function through C-extensions from cython.", "'numba' : Runs the function through JIT compiled code from numba.", "None : Defaults to 'cython' or the global setting compute.use_numba", "New in version 1.1.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to the function", "New in version 1.1.0.", "Keyword arguments to be passed into func.", "See also", "Apply function func group-wise and combine the results together.", "Aggregate using one or more operations over the specified axis.", "Call func on self producing a DataFrame with the same axis shape as self.", "Notes", "Each group is endowed the attribute \u2018name\u2019 in case you need to know which group you are working on.", "The current implementation imposes three requirements on f:", "f must return a value that either has the same shape as the input subframe or can be broadcast to the shape of the input subframe. For example, if f returns a scalar it will be broadcast to have the same shape as the input subframe.", "if this is a DataFrame, f must support application column-by-column in the subframe. If f also supports application to the entire subframe, then a fast path is used starting from the second chunk.", "f must not mutate groups. Mutation is not supported and may produce unexpected results. See Mutating with User Defined Function (UDF) methods for more details.", "When using engine='numba', there will be no \u201cfall back\u201d behavior internally. The group data and group index will be passed as numpy arrays to the JITed user defined function, and no alternative execution attempts will be tried.", "Changed in version 1.3.0: The resulting dtype will reflect the return value of the passed func, see the examples below.", "Examples", "Broadcast result of the transformation", "Changed in version 1.3.0: The resulting dtype will reflect the return value of the passed func, for example:"]}, {"name": "pandas.core.groupby.DataFrameGroupBy.tshift", "path": "reference/api/pandas.core.groupby.dataframegroupby.tshift", "type": "GroupBy", "text": ["Shift the time index, using the index\u2019s frequency if available.", "Deprecated since version 1.1.0: Use shift instead.", "Number of periods to move, can be positive or negative.", "Increment to use from the tseries module or time rule expressed as a string (e.g. \u2018EOM\u2019).", "Corresponds to the axis that contains the Index.", "Notes", "If freq is not specified then tries to use the freq or inferred_freq attributes of the index. If neither of those attributes exist, a ValueError is thrown"]}, {"name": "pandas.core.groupby.DataFrameGroupBy.value_counts", "path": "reference/api/pandas.core.groupby.dataframegroupby.value_counts", "type": "GroupBy", "text": ["Return a Series or DataFrame containing counts of unique rows.", "New in version 1.4.0.", "Columns to use when counting unique combinations.", "Return proportions rather than frequencies.", "Sort by frequencies.", "Sort in ascending order.", "Don\u2019t include counts of rows that contain NA values.", "Series if the groupby as_index is True, otherwise DataFrame.", "See also", "Equivalent method on Series.", "Equivalent method on DataFrame.", "Equivalent method on SeriesGroupBy.", "Notes", "If the groupby as_index is True then the returned Series will have a MultiIndex with one level per input column.", "If the groupby as_index is False then the returned DataFrame will have an additional column with the value_counts. The column is labelled \u2018count\u2019 or \u2018proportion\u2019, depending on the normalize parameter.", "By default, rows that contain any NA values are omitted from the result.", "By default, the result will be in descending order so that the first element of each group is the most frequently-occurring row.", "Examples"]}, {"name": "pandas.core.groupby.GroupBy.__iter__", "path": "reference/api/pandas.core.groupby.groupby.__iter__", "type": "GroupBy", "text": ["Groupby iterator."]}, {"name": "pandas.core.groupby.GroupBy.agg", "path": "reference/api/pandas.core.groupby.groupby.agg", "type": "GroupBy", "text": []}, {"name": "pandas.core.groupby.GroupBy.all", "path": "reference/api/pandas.core.groupby.groupby.all", "type": "GroupBy", "text": ["Return True if all values in the group are truthful, else False.", "Flag to ignore nan values during truth testing.", "DataFrame or Series of boolean values, where a value is True if all elements are True within its respective group, False otherwise.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.any", "path": "reference/api/pandas.core.groupby.groupby.any", "type": "GroupBy", "text": ["Return True if any value in the group is truthful, else False.", "Flag to ignore nan values during truth testing.", "DataFrame or Series of boolean values, where a value is True if any element is True within its respective group, False otherwise.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.apply", "path": "reference/api/pandas.core.groupby.groupby.apply", "type": "GroupBy", "text": ["Apply function func group-wise and combine the results together.", "The function passed to apply must take a dataframe as its first argument and return a DataFrame, Series or scalar. apply will then take care of combining the results back together into a single dataframe or series. apply is therefore a highly flexible grouping method.", "While apply is a very flexible method, its downside is that using it can be quite a bit slower than using more specific methods like agg or transform. Pandas offers a wide range of method that will be much faster than using apply for their specific purposes, so try to use them before reaching for apply.", "A callable that takes a dataframe as its first argument, and returns a dataframe, a series or a scalar. In addition the callable may take positional and keyword arguments.", "Optional positional and keyword arguments to pass to func.", "See also", "Apply function to the full GroupBy object instead of to each group.", "Apply aggregate function to the GroupBy object.", "Apply function column-by-column to the GroupBy object.", "Apply a function to a Series.", "Apply a function to each row or column of a DataFrame.", "Notes", "Changed in version 1.3.0: The resulting dtype will reflect the return value of the passed func, see the examples below.", "Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.", "Examples", "Notice that g has two groups, a and b. Calling apply in various ways, we can get different grouping results:", "Example 1: below the function passed to apply takes a DataFrame as its argument and returns a DataFrame. apply combines the result for each group together into a new DataFrame:", "Example 2: The function passed to apply takes a DataFrame as its argument and returns a Series. apply combines the result for each group together into a new DataFrame.", "Changed in version 1.3.0: The resulting dtype will reflect the return value of the passed func.", "Example 3: The function passed to apply takes a DataFrame as its argument and returns a scalar. apply combines the result for each group together into a Series, including setting the index as appropriate:"]}, {"name": "pandas.core.groupby.GroupBy.backfill", "path": "reference/api/pandas.core.groupby.groupby.backfill", "type": "GroupBy", "text": ["Backward fill the values.", "Limit of how many values to fill.", "Object with missing values filled.", "See also", "Backward fill the missing values in the dataset.", "Backward fill the missing values in the dataset.", "Fill NaN values of a Series.", "Fill NaN values of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.bfill", "path": "reference/api/pandas.core.groupby.groupby.bfill", "type": "GroupBy", "text": ["Backward fill the values.", "Limit of how many values to fill.", "Object with missing values filled.", "See also", "Backward fill the missing values in the dataset.", "Backward fill the missing values in the dataset.", "Fill NaN values of a Series.", "Fill NaN values of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.count", "path": "reference/api/pandas.core.groupby.groupby.count", "type": "GroupBy", "text": ["Compute count of group, excluding missing values.", "Count of values within each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.cumcount", "path": "reference/api/pandas.core.groupby.groupby.cumcount", "type": "GroupBy", "text": ["Number each item in each group from 0 to the length of that group - 1.", "Essentially this is equivalent to", "If False, number in reverse, from length of group - 1 to 0.", "Sequence number of each element within each group.", "See also", "Number the groups themselves.", "Examples"]}, {"name": "pandas.core.groupby.GroupBy.cummax", "path": "reference/api/pandas.core.groupby.groupby.cummax", "type": "GroupBy", "text": ["Cumulative max for each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.cummin", "path": "reference/api/pandas.core.groupby.groupby.cummin", "type": "GroupBy", "text": ["Cumulative min for each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.cumprod", "path": "reference/api/pandas.core.groupby.groupby.cumprod", "type": "GroupBy", "text": ["Cumulative product for each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.cumsum", "path": "reference/api/pandas.core.groupby.groupby.cumsum", "type": "GroupBy", "text": ["Cumulative sum for each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.ffill", "path": "reference/api/pandas.core.groupby.groupby.ffill", "type": "GroupBy", "text": ["Forward fill the values.", "Limit of how many values to fill.", "Object with missing values filled.", "See also", "Returns Series with minimum number of char in object.", "Object with missing values filled or None if inplace=True.", "Fill NaN values of a Series.", "Fill NaN values of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.first", "path": "reference/api/pandas.core.groupby.groupby.first", "type": "GroupBy", "text": ["Compute first of group values.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Computed first of values within each group."]}, {"name": "pandas.core.groupby.GroupBy.get_group", "path": "reference/api/pandas.core.groupby.groupby.get_group", "type": "GroupBy", "text": ["Construct DataFrame from group with provided name.", "The name of the group to get as a DataFrame.", "The DataFrame to take the DataFrame out of. If it is None, the object groupby was called on will be used."]}, {"name": "pandas.core.groupby.GroupBy.groups", "path": "reference/api/pandas.core.groupby.groupby.groups", "type": "GroupBy", "text": ["Dict {group name -> group labels}."]}, {"name": "pandas.core.groupby.GroupBy.head", "path": "reference/api/pandas.core.groupby.groupby.head", "type": "GroupBy", "text": ["Return first n rows of each group.", "Similar to .apply(lambda x: x.head(n)), but it returns a subset of rows from the original DataFrame with original index and order preserved (as_index flag is ignored).", "If positive: number of entries to include from start of each group. If negative: number of entries to exclude from end of each group.", "Subset of original Series or DataFrame as determined by n.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame.", "Examples"]}, {"name": "pandas.core.groupby.GroupBy.indices", "path": "reference/api/pandas.core.groupby.groupby.indices", "type": "GroupBy", "text": ["Dict {group name -> group indices}."]}, {"name": "pandas.core.groupby.GroupBy.last", "path": "reference/api/pandas.core.groupby.groupby.last", "type": "GroupBy", "text": ["Compute last of group values.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Computed last of values within each group."]}, {"name": "pandas.core.groupby.GroupBy.max", "path": "reference/api/pandas.core.groupby.groupby.max", "type": "GroupBy", "text": ["Compute max of group values.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Computed max of values within each group."]}, {"name": "pandas.core.groupby.GroupBy.mean", "path": "reference/api/pandas.core.groupby.groupby.mean", "type": "GroupBy", "text": ["Compute mean of groups, excluding missing values.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.4.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {{'nopython': True, 'nogil': False, 'parallel': False}}", "New in version 1.4.0.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame.", "Examples", "Groupby one column and return the mean of the remaining columns in each group.", "Groupby two columns and return the mean of the remaining column.", "Groupby one column and return the mean of only particular column in the group."]}, {"name": "pandas.core.groupby.GroupBy.median", "path": "reference/api/pandas.core.groupby.groupby.median", "type": "GroupBy", "text": ["Compute median of groups, excluding missing values.", "For multiple groupings, the result index will be a MultiIndex", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.", "Median of values within each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.min", "path": "reference/api/pandas.core.groupby.groupby.min", "type": "GroupBy", "text": ["Compute min of group values.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Computed min of values within each group."]}, {"name": "pandas.core.groupby.GroupBy.ngroup", "path": "reference/api/pandas.core.groupby.groupby.ngroup", "type": "GroupBy", "text": ["Number each group from 0 to the number of groups - 1.", "This is the enumerative complement of cumcount. Note that the numbers given to the groups match the order in which the groups would be seen when iterating over the groupby object, not the order they are first observed.", "If False, number in reverse, from number of group - 1 to 0.", "Unique numbers for each group.", "See also", "Number the rows in each group.", "Examples"]}, {"name": "pandas.core.groupby.GroupBy.nth", "path": "reference/api/pandas.core.groupby.groupby.nth", "type": "GroupBy", "text": ["Take the nth row from each group if n is an int, otherwise a subset of rows.", "Can be either a call or an index. dropna is not available with index notation. Index notation accepts a comma separated list of integers and slices.", "If dropna, will take the nth non-null row, dropna is either \u2018all\u2019 or \u2018any\u2019; this is equivalent to calling dropna(how=dropna) before the groupby.", "A single nth value for the row or a list of nth values or slices.", "Changed in version 1.4.0: Added slice and lists containiing slices. Added index notation.", "Apply the specified dropna operation before counting which row is the nth row. Only supported if n is an int.", "N-th value within each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame.", "Examples", "Index notation may also be used", "Specifying dropna allows count ignoring NaN", "NaNs denote group exhausted when using dropna", "Specifying as_index=False in groupby keeps the original index."]}, {"name": "pandas.core.groupby.GroupBy.ohlc", "path": "reference/api/pandas.core.groupby.groupby.ohlc", "type": "GroupBy", "text": ["Compute open, high, low and close values of a group, excluding missing values.", "For multiple groupings, the result index will be a MultiIndex", "Open, high, low and close values within each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.pad", "path": "reference/api/pandas.core.groupby.groupby.pad", "type": "GroupBy", "text": ["Forward fill the values.", "Limit of how many values to fill.", "Object with missing values filled.", "See also", "Returns Series with minimum number of char in object.", "Object with missing values filled or None if inplace=True.", "Fill NaN values of a Series.", "Fill NaN values of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.pct_change", "path": "reference/api/pandas.core.groupby.groupby.pct_change", "type": "GroupBy", "text": ["Calculate pct_change of each value to previous entry in group.", "Percentage changes within each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.pipe", "path": "reference/api/pandas.core.groupby.groupby.pipe", "type": "GroupBy", "text": ["Apply a function func with arguments to this GroupBy object and return the function\u2019s result.", "Use .pipe when you want to improve readability by chaining together functions that expect Series, DataFrames, GroupBy or Resampler objects. Instead of writing", "You can write", "which is much more readable.", "Function to apply to this GroupBy object or, alternatively, a (callable, data_keyword) tuple where data_keyword is a string indicating the keyword of callable that expects the GroupBy object.", "Positional arguments passed into func.", "A dictionary of keyword arguments passed into func.", "See also", "Apply a function with arguments to a series.", "Apply a function with arguments to a dataframe.", "Apply function to each group instead of to the full GroupBy object.", "Notes", "See more here", "Examples", "To get the difference between each groups maximum and minimum value in one pass, you can do"]}, {"name": "pandas.core.groupby.GroupBy.prod", "path": "reference/api/pandas.core.groupby.groupby.prod", "type": "GroupBy", "text": ["Compute prod of group values.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Computed prod of values within each group."]}, {"name": "pandas.core.groupby.GroupBy.rank", "path": "reference/api/pandas.core.groupby.groupby.rank", "type": "GroupBy", "text": ["Provide the rank of values within each group.", "average: average rank of group.", "min: lowest rank in group.", "max: highest rank in group.", "first: ranks assigned in order they appear in the array.", "dense: like \u2018min\u2019, but rank always increases by 1 between groups.", "False for ranks by high (1) to low (N).", "keep: leave NA values where they are.", "top: smallest rank if ascending.", "bottom: smallest rank if descending.", "Compute percentage rank of data within each group.", "The axis of the object over which to compute the rank.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame.", "Examples"]}, {"name": "pandas.core.groupby.GroupBy.sem", "path": "reference/api/pandas.core.groupby.groupby.sem", "type": "GroupBy", "text": ["Compute standard error of the mean of groups, excluding missing values.", "For multiple groupings, the result index will be a MultiIndex.", "Degrees of freedom.", "Standard error of the mean of values within each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.size", "path": "reference/api/pandas.core.groupby.groupby.size", "type": "GroupBy", "text": ["Compute group sizes.", "Number of rows in each group as a Series if as_index is True or a DataFrame if as_index is False.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.std", "path": "reference/api/pandas.core.groupby.groupby.std", "type": "GroupBy", "text": ["Compute standard deviation of groups, excluding missing values.", "For multiple groupings, the result index will be a MultiIndex.", "Degrees of freedom.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.4.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {{'nopython': True, 'nogil': False, 'parallel': False}}", "New in version 1.4.0.", "Standard deviation of values within each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.GroupBy.sum", "path": "reference/api/pandas.core.groupby.groupby.sum", "type": "GroupBy", "text": ["Compute sum of group values.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Computed sum of values within each group."]}, {"name": "pandas.core.groupby.GroupBy.tail", "path": "reference/api/pandas.core.groupby.groupby.tail", "type": "GroupBy", "text": ["Return last n rows of each group.", "Similar to .apply(lambda x: x.tail(n)), but it returns a subset of rows from the original DataFrame with original index and order preserved (as_index flag is ignored).", "If positive: number of entries to include from end of each group. If negative: number of entries to exclude from start of each group.", "Subset of original Series or DataFrame as determined by n.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame.", "Examples"]}, {"name": "pandas.core.groupby.GroupBy.var", "path": "reference/api/pandas.core.groupby.groupby.var", "type": "GroupBy", "text": ["Compute variance of groups, excluding missing values.", "For multiple groupings, the result index will be a MultiIndex.", "Degrees of freedom.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.4.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {{'nopython': True, 'nogil': False, 'parallel': False}}", "New in version 1.4.0.", "Variance of values within each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.groupby.SeriesGroupBy.aggregate", "path": "reference/api/pandas.core.groupby.seriesgroupby.aggregate", "type": "Series", "text": ["Aggregate using one or more operations over the specified axis.", "Function to use for aggregating the data. If a function, must either work when passed a Series or when passed to Series.apply.", "Accepted combinations are:", "function", "string function name", "list of functions and/or function names, e.g. [np.sum, 'mean']", "dict of axis labels -> functions, function names or list of such.", "Can also accept a Numba JIT function with engine='numba' specified. Only passing a single function is supported with this engine.", "If the 'numba' engine is chosen, the function must be a user defined function with values and index as the first and second arguments respectively in the function signature. Each group\u2019s index will be passed to the user defined function and optionally available for use.", "Changed in version 1.1.0.", "Positional arguments to pass to func.", "'cython' : Runs the function through C-extensions from cython.", "'numba' : Runs the function through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.1.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to the function", "New in version 1.1.0.", "Keyword arguments to be passed into func.", "See also", "Apply function func group-wise and combine the results together.", "Aggregate using one or more operations over the specified axis.", "Transforms the Series on each group based on the given function.", "Notes", "When using engine='numba', there will be no \u201cfall back\u201d behavior internally. The group data and group index will be passed as numpy arrays to the JITed user defined function, and no alternative execution attempts will be tried.", "Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.", "Changed in version 1.3.0: The resulting dtype will reflect the return value of the passed func, see the examples below.", "Examples", "The output column names can be controlled by passing the desired column names and aggregations as keyword arguments.", "Changed in version 1.3.0: The resulting dtype will reflect the return value of the aggregating function."]}, {"name": "pandas.core.groupby.SeriesGroupBy.hist", "path": "reference/api/pandas.core.groupby.seriesgroupby.hist", "type": "Series", "text": ["Draw histogram of the input series using matplotlib.", "If passed, then used to form histograms for separate groups.", "If not passed, uses gca().", "Whether to show axis grid lines.", "If specified changes the x-axis label size.", "Rotation of x axis labels.", "If specified changes the y-axis label size.", "Rotation of y axis labels.", "Figure size in inches by default.", "Number of histogram bins to be used. If an integer is given, bins + 1 bin edges are calculated and returned. If bins is a sequence, gives bin edges, including left edge of first bin and right edge of last bin. In this case, bins is returned unmodified.", "Backend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.", "New in version 1.0.0.", "Whether to show the legend.", "New in version 1.1.0.", "To be passed to the actual plotting function.", "A histogram plot.", "See also", "Plot a histogram using matplotlib."]}, {"name": "pandas.core.groupby.SeriesGroupBy.is_monotonic_decreasing", "path": "reference/api/pandas.core.groupby.seriesgroupby.is_monotonic_decreasing", "type": "Series", "text": ["Return boolean if values in the object are monotonic_decreasing."]}, {"name": "pandas.core.groupby.SeriesGroupBy.is_monotonic_increasing", "path": "reference/api/pandas.core.groupby.seriesgroupby.is_monotonic_increasing", "type": "Series", "text": ["Alias for is_monotonic."]}, {"name": "pandas.core.groupby.SeriesGroupBy.nlargest", "path": "reference/api/pandas.core.groupby.seriesgroupby.nlargest", "type": "Series", "text": ["Return the largest n elements.", "Return this many descending sorted values.", "When there are duplicate values that cannot all fit in a Series of n elements:", "first : return the first n occurrences in order of appearance.", "last : return the last n occurrences in reverse order of appearance.", "all : keep all occurrences. This can result in a Series of size larger than n.", "The n largest values in the Series, sorted in decreasing order.", "See also", "Get the n smallest elements.", "Sort Series by values.", "Return the first n rows.", "Notes", "Faster than .sort_values(ascending=False).head(n) for small n relative to the size of the Series object.", "Examples", "The n largest elements where n=5 by default.", "The n largest elements where n=3. Default keep value is \u2018first\u2019 so Malta will be kept.", "The n largest elements where n=3 and keeping the last duplicates. Brunei will be kept since it is the last with value 434000 based on the index order.", "The n largest elements where n=3 with all duplicates kept. Note that the returned Series has five elements due to the three duplicates."]}, {"name": "pandas.core.groupby.SeriesGroupBy.nsmallest", "path": "reference/api/pandas.core.groupby.seriesgroupby.nsmallest", "type": "Series", "text": ["Return the smallest n elements.", "Return this many ascending sorted values.", "When there are duplicate values that cannot all fit in a Series of n elements:", "first : return the first n occurrences in order of appearance.", "last : return the last n occurrences in reverse order of appearance.", "all : keep all occurrences. This can result in a Series of size larger than n.", "The n smallest values in the Series, sorted in increasing order.", "See also", "Get the n largest elements.", "Sort Series by values.", "Return the first n rows.", "Notes", "Faster than .sort_values().head(n) for small n relative to the size of the Series object.", "Examples", "The n smallest elements where n=5 by default.", "The n smallest elements where n=3. Default keep value is \u2018first\u2019 so Nauru and Tuvalu will be kept.", "The n smallest elements where n=3 and keeping the last duplicates. Anguilla and Tuvalu will be kept since they are the last with value 11300 based on the index order.", "The n smallest elements where n=3 with all duplicates kept. Note that the returned Series has four elements due to the three duplicates."]}, {"name": "pandas.core.groupby.SeriesGroupBy.nunique", "path": "reference/api/pandas.core.groupby.seriesgroupby.nunique", "type": "Series", "text": ["Return number of unique elements in the group.", "Number of unique values within each group."]}, {"name": "pandas.core.groupby.SeriesGroupBy.transform", "path": "reference/api/pandas.core.groupby.seriesgroupby.transform", "type": "Series", "text": ["Call function producing a like-indexed Series on each group and return a Series having the same indexes as the original object filled with the transformed values.", "Function to apply to each group.", "Can also accept a Numba JIT function with engine='numba' specified.", "If the 'numba' engine is chosen, the function must be a user defined function with values and index as the first and second arguments respectively in the function signature. Each group\u2019s index will be passed to the user defined function and optionally available for use.", "Changed in version 1.1.0.", "Positional arguments to pass to func.", "'cython' : Runs the function through C-extensions from cython.", "'numba' : Runs the function through JIT compiled code from numba.", "None : Defaults to 'cython' or the global setting compute.use_numba", "New in version 1.1.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to the function", "New in version 1.1.0.", "Keyword arguments to be passed into func.", "See also", "Apply function func group-wise and combine the results together.", "Aggregate using one or more operations over the specified axis.", "Call func on self producing a Series with the same axis shape as self.", "Notes", "Each group is endowed the attribute \u2018name\u2019 in case you need to know which group you are working on.", "The current implementation imposes three requirements on f:", "f must return a value that either has the same shape as the input subframe or can be broadcast to the shape of the input subframe. For example, if f returns a scalar it will be broadcast to have the same shape as the input subframe.", "if this is a DataFrame, f must support application column-by-column in the subframe. If f also supports application to the entire subframe, then a fast path is used starting from the second chunk.", "f must not mutate groups. Mutation is not supported and may produce unexpected results. See Mutating with User Defined Function (UDF) methods for more details.", "When using engine='numba', there will be no \u201cfall back\u201d behavior internally. The group data and group index will be passed as numpy arrays to the JITed user defined function, and no alternative execution attempts will be tried.", "Changed in version 1.3.0: The resulting dtype will reflect the return value of the passed func, see the examples below.", "Examples", "Broadcast result of the transformation", "Changed in version 1.3.0: The resulting dtype will reflect the return value of the passed func, for example:"]}, {"name": "pandas.core.groupby.SeriesGroupBy.unique", "path": "reference/api/pandas.core.groupby.seriesgroupby.unique", "type": "Series", "text": ["Return unique values of Series object.", "Uniques are returned in order of appearance. Hash table-based unique, therefore does NOT sort.", "The unique values returned as a NumPy array. See Notes.", "See also", "Top-level unique method for any 1-d array-like object.", "Return Index with unique values from an Index object.", "Notes", "Returns the unique values as a NumPy array. In case of an extension-array backed Series, a new ExtensionArray of that type with just the unique values is returned. This includes", "Categorical", "Period", "Datetime with Timezone", "Interval", "Sparse", "IntegerNA", "See Examples section.", "Examples", "An Categorical will return categories in the order of appearance and with the same dtype."]}, {"name": "pandas.core.groupby.SeriesGroupBy.value_counts", "path": "reference/api/pandas.core.groupby.seriesgroupby.value_counts", "type": "Series", "text": []}, {"name": "pandas.core.resample.Resampler.__iter__", "path": "reference/api/pandas.core.resample.resampler.__iter__", "type": "Resampling", "text": ["Groupby iterator."]}, {"name": "pandas.core.resample.Resampler.aggregate", "path": "reference/api/pandas.core.resample.resampler.aggregate", "type": "Resampling", "text": ["Aggregate using one or more operations over the specified axis.", "Function to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply.", "Accepted combinations are:", "function", "string function name", "list of functions and/or function names, e.g. [np.sum, 'mean']", "dict of axis labels -> functions, function names or list of such.", "Positional arguments to pass to func.", "Keyword arguments to pass to func.", "The return can be:", "scalar : when Series.agg is called with single function", "Series : when DataFrame.agg is called with a single function", "DataFrame : when DataFrame.agg is called with several functions", "Return scalar, Series or DataFrame.", "See also", "Aggregate using callable, string, dict, or list of string/callables.", "Transforms the Series on each group based on the given function.", "Aggregate using one or more operations over the specified axis.", "Notes", "agg is an alias for aggregate. Use the alias.", "Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.", "A passed user-defined-function will be passed a Series for evaluation.", "Examples"]}, {"name": "pandas.core.resample.Resampler.apply", "path": "reference/api/pandas.core.resample.resampler.apply", "type": "Resampling", "text": ["Aggregate using one or more operations over the specified axis.", "Function to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply.", "Accepted combinations are:", "function", "string function name", "list of functions and/or function names, e.g. [np.sum, 'mean']", "dict of axis labels -> functions, function names or list of such.", "Positional arguments to pass to func.", "Keyword arguments to pass to func.", "The return can be:", "scalar : when Series.agg is called with single function", "Series : when DataFrame.agg is called with a single function", "DataFrame : when DataFrame.agg is called with several functions", "Return scalar, Series or DataFrame.", "See also", "Aggregate using callable, string, dict, or list of string/callables.", "Transforms the Series on each group based on the given function.", "Aggregate using one or more operations over the specified axis.", "Notes", "agg is an alias for aggregate. Use the alias.", "Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.", "A passed user-defined-function will be passed a Series for evaluation.", "Examples"]}, {"name": "pandas.core.resample.Resampler.asfreq", "path": "reference/api/pandas.core.resample.resampler.asfreq", "type": "Resampling", "text": ["Return the values at the new freq, essentially a reindex.", "Value to use for missing values, applied during upsampling (note this does not fill NaNs that already were present).", "Values at the specified freq.", "See also", "Convert TimeSeries to specified frequency.", "Convert TimeSeries to specified frequency."]}, {"name": "pandas.core.resample.Resampler.backfill", "path": "reference/api/pandas.core.resample.resampler.backfill", "type": "Resampling", "text": ["Backward fill the new missing values in the resampled data.", "In statistics, imputation is the process of replacing missing data with substituted values [1]. When resampling data, missing values may appear (e.g., when the resampling frequency is higher than the original frequency). The backward fill will replace NaN values that appeared in the resampled data with the next value in the original sequence. Missing values that existed in the original data will not be modified.", "Limit of how many values to fill.", "An upsampled Series or DataFrame with backward filled NaN values.", "See also", "Alias of backfill.", "Fill NaN values using the specified method, which can be \u2018backfill\u2019.", "Fill NaN values with nearest neighbor starting from center.", "Forward fill NaN values.", "Fill NaN values in the Series using the specified method, which can be \u2018backfill\u2019.", "Fill NaN values in the DataFrame using the specified method, which can be \u2018backfill\u2019.", "References", "https://en.wikipedia.org/wiki/Imputation_(statistics)", "Examples", "Resampling a Series:", "Resampling a DataFrame that has missing values:"]}, {"name": "pandas.core.resample.Resampler.bfill", "path": "reference/api/pandas.core.resample.resampler.bfill", "type": "Resampling", "text": ["Backward fill the new missing values in the resampled data.", "In statistics, imputation is the process of replacing missing data with substituted values [1]. When resampling data, missing values may appear (e.g., when the resampling frequency is higher than the original frequency). The backward fill will replace NaN values that appeared in the resampled data with the next value in the original sequence. Missing values that existed in the original data will not be modified.", "Limit of how many values to fill.", "An upsampled Series or DataFrame with backward filled NaN values.", "See also", "Alias of backfill.", "Fill NaN values using the specified method, which can be \u2018backfill\u2019.", "Fill NaN values with nearest neighbor starting from center.", "Forward fill NaN values.", "Fill NaN values in the Series using the specified method, which can be \u2018backfill\u2019.", "Fill NaN values in the DataFrame using the specified method, which can be \u2018backfill\u2019.", "References", "https://en.wikipedia.org/wiki/Imputation_(statistics)", "Examples", "Resampling a Series:", "Resampling a DataFrame that has missing values:"]}, {"name": "pandas.core.resample.Resampler.count", "path": "reference/api/pandas.core.resample.resampler.count", "type": "Resampling", "text": ["Compute count of group, excluding missing values.", "Count of values within each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.resample.Resampler.ffill", "path": "reference/api/pandas.core.resample.resampler.ffill", "type": "Resampling", "text": ["Forward fill the values.", "Limit of how many values to fill.", "See also", "Fill NA/NaN values using the specified method.", "Fill NA/NaN values using the specified method."]}, {"name": "pandas.core.resample.Resampler.fillna", "path": "reference/api/pandas.core.resample.resampler.fillna", "type": "Resampling", "text": ["Fill missing values introduced by upsampling.", "In statistics, imputation is the process of replacing missing data with substituted values [1]. When resampling data, missing values may appear (e.g., when the resampling frequency is higher than the original frequency).", "Missing values that existed in the original data will not be modified.", "Method to use for filling holes in resampled data", "\u2018pad\u2019 or \u2018ffill\u2019: use previous valid observation to fill gap (forward fill).", "\u2018backfill\u2019 or \u2018bfill\u2019: use next valid observation to fill gap.", "\u2018nearest\u2019: use nearest valid observation to fill gap.", "Limit of how many consecutive missing values to fill.", "An upsampled Series or DataFrame with missing values filled.", "See also", "Backward fill NaN values in the resampled data.", "Forward fill NaN values in the resampled data.", "Fill NaN values in the resampled data with nearest neighbor starting from center.", "Fill NaN values using interpolation.", "Fill NaN values in the Series using the specified method, which can be \u2018bfill\u2019 and \u2018ffill\u2019.", "Fill NaN values in the DataFrame using the specified method, which can be \u2018bfill\u2019 and \u2018ffill\u2019.", "References", "https://en.wikipedia.org/wiki/Imputation_(statistics)", "Examples", "Resampling a Series:", "Without filling the missing values you get:", "Missing values present before the upsampling are not affected.", "DataFrame resampling is done column-wise. All the same options are available."]}, {"name": "pandas.core.resample.Resampler.first", "path": "reference/api/pandas.core.resample.resampler.first", "type": "Resampling", "text": ["Compute first of group values.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Computed first of values within each group."]}, {"name": "pandas.core.resample.Resampler.get_group", "path": "reference/api/pandas.core.resample.resampler.get_group", "type": "Resampling", "text": ["Construct DataFrame from group with provided name.", "The name of the group to get as a DataFrame.", "The DataFrame to take the DataFrame out of. If it is None, the object groupby was called on will be used."]}, {"name": "pandas.core.resample.Resampler.groups", "path": "reference/api/pandas.core.resample.resampler.groups", "type": "Resampling", "text": ["Dict {group name -> group labels}."]}, {"name": "pandas.core.resample.Resampler.indices", "path": "reference/api/pandas.core.resample.resampler.indices", "type": "Resampling", "text": ["Dict {group name -> group indices}."]}, {"name": "pandas.core.resample.Resampler.interpolate", "path": "reference/api/pandas.core.resample.resampler.interpolate", "type": "Resampling", "text": ["Interpolate values according to different methods.", "Fill NaN values using an interpolation method.", "Please note that only method='linear' is supported for DataFrame/Series with a MultiIndex.", "Interpolation technique to use. One of:", "\u2018linear\u2019: Ignore the index and treat the values as equally spaced. This is the only method supported on MultiIndexes.", "\u2018time\u2019: Works on daily and higher resolution data to interpolate given length of interval.", "\u2018index\u2019, \u2018values\u2019: use the actual numerical values of the index.", "\u2018pad\u2019: Fill in NaNs using existing values.", "\u2018nearest\u2019, \u2018zero\u2019, \u2018slinear\u2019, \u2018quadratic\u2019, \u2018cubic\u2019, \u2018spline\u2019, \u2018barycentric\u2019, \u2018polynomial\u2019: Passed to scipy.interpolate.interp1d. These methods use the numerical values of the index. Both \u2018polynomial\u2019 and \u2018spline\u2019 require that you also specify an order (int), e.g. df.interpolate(method='polynomial', order=5).", "\u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019, \u2018akima\u2019, \u2018cubicspline\u2019: Wrappers around the SciPy interpolation methods of similar names. See Notes.", "\u2018from_derivatives\u2019: Refers to scipy.interpolate.BPoly.from_derivatives which replaces \u2018piecewise_polynomial\u2019 interpolation method in scipy 0.18.", "Axis to interpolate along.", "Maximum number of consecutive NaNs to fill. Must be greater than 0.", "Update the data in place if possible.", "Consecutive NaNs will be filled in this direction.", "If \u2018method\u2019 is \u2018pad\u2019 or \u2018ffill\u2019, \u2018limit_direction\u2019 must be \u2018forward\u2019.", "If \u2018method\u2019 is \u2018backfill\u2019 or \u2018bfill\u2019, \u2018limit_direction\u2019 must be \u2018backwards\u2019.", "If \u2018method\u2019 is \u2018backfill\u2019 or \u2018bfill\u2019, the default is \u2018backward\u2019", "else the default is \u2018forward\u2019", "Changed in version 1.1.0: raises ValueError if limit_direction is \u2018forward\u2019 or \u2018both\u2019 and method is \u2018backfill\u2019 or \u2018bfill\u2019. raises ValueError if limit_direction is \u2018backward\u2019 or \u2018both\u2019 and method is \u2018pad\u2019 or \u2018ffill\u2019.", "If limit is specified, consecutive NaNs will be filled with this restriction.", "None: No fill restriction.", "\u2018inside\u2019: Only fill NaNs surrounded by valid values (interpolate).", "\u2018outside\u2019: Only fill NaNs outside valid values (extrapolate).", "Downcast dtypes if possible.", "Keyword arguments to pass on to the interpolating function.", "Returns the same object type as the caller, interpolated at some or all NaN values or None if inplace=True.", "See also", "Fill missing values using different methods.", "Piecewise cubic polynomials (Akima interpolator).", "Piecewise polynomial in the Bernstein basis.", "Interpolate a 1-D function.", "Interpolate polynomial (Krogh interpolator).", "PCHIP 1-d monotonic cubic interpolation.", "Cubic spline data interpolator.", "Notes", "The \u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019 and \u2018akima\u2019 methods are wrappers around the respective SciPy implementations of similar names. These use the actual numerical values of the index. For more information on their behavior, see the SciPy documentation and SciPy tutorial.", "Examples", "Filling in NaN in a Series via linear interpolation.", "Filling in NaN in a Series by padding, but filling at most two consecutive NaN at a time.", "Filling in NaN in a Series via polynomial interpolation or splines: Both \u2018polynomial\u2019 and \u2018spline\u2019 methods require that you also specify an order (int).", "Fill the DataFrame forward (that is, going down) along each column using linear interpolation.", "Note how the last entry in column \u2018a\u2019 is interpolated differently, because there is no entry after it to use for interpolation. Note how the first entry in column \u2018b\u2019 remains NaN, because there is no entry before it to use for interpolation.", "Using polynomial interpolation."]}, {"name": "pandas.core.resample.Resampler.last", "path": "reference/api/pandas.core.resample.resampler.last", "type": "Resampling", "text": ["Compute last of group values.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Computed last of values within each group."]}, {"name": "pandas.core.resample.Resampler.max", "path": "reference/api/pandas.core.resample.resampler.max", "type": "Resampling", "text": ["Compute max of group values.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Computed max of values within each group."]}, {"name": "pandas.core.resample.Resampler.mean", "path": "reference/api/pandas.core.resample.resampler.mean", "type": "Resampling", "text": ["Compute mean of groups, excluding missing values.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.4.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {{'nopython': True, 'nogil': False, 'parallel': False}}", "New in version 1.4.0.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame.", "Examples", "Groupby one column and return the mean of the remaining columns in each group.", "Groupby two columns and return the mean of the remaining column.", "Groupby one column and return the mean of only particular column in the group."]}, {"name": "pandas.core.resample.Resampler.median", "path": "reference/api/pandas.core.resample.resampler.median", "type": "Resampling", "text": ["Compute median of groups, excluding missing values.", "For multiple groupings, the result index will be a MultiIndex", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.", "Median of values within each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.resample.Resampler.min", "path": "reference/api/pandas.core.resample.resampler.min", "type": "Resampling", "text": ["Compute min of group values.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Computed min of values within each group."]}, {"name": "pandas.core.resample.Resampler.nearest", "path": "reference/api/pandas.core.resample.resampler.nearest", "type": "Resampling", "text": ["Resample by using the nearest value.", "When resampling data, missing values may appear (e.g., when the resampling frequency is higher than the original frequency). The nearest method will replace NaN values that appeared in the resampled data with the value from the nearest member of the sequence, based on the index value. Missing values that existed in the original data will not be modified. If limit is given, fill only this many values in each direction for each of the original values.", "Limit of how many values to fill.", "An upsampled Series or DataFrame with NaN values filled with their nearest value.", "See also", "Backward fill the new missing values in the resampled data.", "Forward fill NaN values.", "Examples", "Limit the number of upsampled values imputed by the nearest:"]}, {"name": "pandas.core.resample.Resampler.nunique", "path": "reference/api/pandas.core.resample.resampler.nunique", "type": "Resampling", "text": ["Return number of unique elements in the group.", "Number of unique values within each group."]}, {"name": "pandas.core.resample.Resampler.ohlc", "path": "reference/api/pandas.core.resample.resampler.ohlc", "type": "Resampling", "text": ["Compute open, high, low and close values of a group, excluding missing values.", "For multiple groupings, the result index will be a MultiIndex", "Open, high, low and close values within each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.resample.Resampler.pad", "path": "reference/api/pandas.core.resample.resampler.pad", "type": "Resampling", "text": ["Forward fill the values.", "Limit of how many values to fill.", "See also", "Fill NA/NaN values using the specified method.", "Fill NA/NaN values using the specified method."]}, {"name": "pandas.core.resample.Resampler.pipe", "path": "reference/api/pandas.core.resample.resampler.pipe", "type": "Resampling", "text": ["Apply a function func with arguments to this Resampler object and return the function\u2019s result.", "Use .pipe when you want to improve readability by chaining together functions that expect Series, DataFrames, GroupBy or Resampler objects. Instead of writing", "You can write", "which is much more readable.", "Function to apply to this Resampler object or, alternatively, a (callable, data_keyword) tuple where data_keyword is a string indicating the keyword of callable that expects the Resampler object.", "Positional arguments passed into func.", "A dictionary of keyword arguments passed into func.", "See also", "Apply a function with arguments to a series.", "Apply a function with arguments to a dataframe.", "Apply function to each group instead of to the full Resampler object.", "Notes", "See more here", "Examples", "To get the difference between each 2-day period\u2019s maximum and minimum value in one pass, you can do"]}, {"name": "pandas.core.resample.Resampler.prod", "path": "reference/api/pandas.core.resample.resampler.prod", "type": "Resampling", "text": ["Compute prod of group values.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Computed prod of values within each group."]}, {"name": "pandas.core.resample.Resampler.quantile", "path": "reference/api/pandas.core.resample.resampler.quantile", "type": "Resampling", "text": ["Return value at the given quantile.", "Quantile of values within each group.", "See also", "Return a series, where the index is q and the values are the quantiles.", "Return a DataFrame, where the columns are the columns of self, and the values are the quantiles.", "Return a DataFrame, where the coulmns are groupby columns, and the values are its quantiles."]}, {"name": "pandas.core.resample.Resampler.sem", "path": "reference/api/pandas.core.resample.resampler.sem", "type": "Resampling", "text": ["Compute standard error of the mean of groups, excluding missing values.", "For multiple groupings, the result index will be a MultiIndex.", "Degrees of freedom.", "Standard error of the mean of values within each group.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.resample.Resampler.size", "path": "reference/api/pandas.core.resample.resampler.size", "type": "Resampling", "text": ["Compute group sizes.", "Number of rows in each group as a Series if as_index is True or a DataFrame if as_index is False.", "See also", "Apply a function groupby to a Series.", "Apply a function groupby to each row or column of a DataFrame."]}, {"name": "pandas.core.resample.Resampler.std", "path": "reference/api/pandas.core.resample.resampler.std", "type": "Resampling", "text": ["Compute standard deviation of groups, excluding missing values.", "Degrees of freedom.", "Standard deviation of values within each group."]}, {"name": "pandas.core.resample.Resampler.sum", "path": "reference/api/pandas.core.resample.resampler.sum", "type": "Resampling", "text": ["Compute sum of group values.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Computed sum of values within each group."]}, {"name": "pandas.core.resample.Resampler.transform", "path": "reference/api/pandas.core.resample.resampler.transform", "type": "Resampling", "text": ["Call function producing a like-indexed Series on each group and return a Series with the transformed values.", "To apply to each group. Should return a Series with the same index.", "Examples"]}, {"name": "pandas.core.resample.Resampler.var", "path": "reference/api/pandas.core.resample.resampler.var", "type": "Resampling", "text": ["Compute variance of groups, excluding missing values.", "Degrees of freedom.", "Variance of values within each group."]}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.corr", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.corr", "type": "Window", "text": ["Calculate the ewm (exponential weighted moment) sample correlation.", "If not supplied then will default to self and produce pairwise output.", "If False then only matching columns between self and other will be used and the output will be a DataFrame. If True then all pairwise combinations will be calculated and the output will be a MultiIndex DataFrame in the case of DataFrame inputs. In the case of missing elements, only complete pairwise observations will be used.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling ewm with Series data.", "Calling ewm with DataFrames.", "Aggregating corr for Series.", "Aggregating corr for DataFrame."]}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.cov", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.cov", "type": "Window", "text": ["Calculate the ewm (exponential weighted moment) sample covariance.", "If not supplied then will default to self and produce pairwise output.", "If False then only matching columns between self and other will be used and the output will be a DataFrame. If True then all pairwise combinations will be calculated and the output will be a MultiIndex DataFrame in the case of DataFrame inputs. In the case of missing elements, only complete pairwise observations will be used.", "Use a standard estimation bias correction.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling ewm with Series data.", "Calling ewm with DataFrames.", "Aggregating cov for Series.", "Aggregating cov for DataFrame."]}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.mean", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.mean", "type": "Window", "text": ["Calculate the ewm (exponential weighted moment) mean.", "For NumPy compatibility and will not have an effect on the result.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.3.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}", "New in version 1.3.0.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling ewm with Series data.", "Calling ewm with DataFrames.", "Aggregating mean for Series.", "Aggregating mean for DataFrame.", "Notes", "See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine."]}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.std", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.std", "type": "Window", "text": ["Calculate the ewm (exponential weighted moment) standard deviation.", "Use a standard estimation bias correction.", "For NumPy compatibility and will not have an effect on the result.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling ewm with Series data.", "Calling ewm with DataFrames.", "Aggregating std for Series.", "Aggregating std for DataFrame."]}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.sum", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.sum", "type": "Window", "text": ["Calculate the ewm (exponential weighted moment) sum.", "For NumPy compatibility and will not have an effect on the result.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.3.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}", "New in version 1.3.0.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling ewm with Series data.", "Calling ewm with DataFrames.", "Aggregating sum for Series.", "Aggregating sum for DataFrame.", "Notes", "See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine."]}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.var", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.var", "type": "Window", "text": ["Calculate the ewm (exponential weighted moment) variance.", "Use a standard estimation bias correction.", "For NumPy compatibility and will not have an effect on the result.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling ewm with Series data.", "Calling ewm with DataFrames.", "Aggregating var for Series.", "Aggregating var for DataFrame."]}, {"name": "pandas.core.window.expanding.Expanding.aggregate", "path": "reference/api/pandas.core.window.expanding.expanding.aggregate", "type": "Window", "text": ["Aggregate using one or more operations over the specified axis.", "Function to use for aggregating the data. If a function, must either work when passed a Series/Dataframe or when passed to Series/Dataframe.apply.", "Accepted combinations are:", "function", "string function name", "list of functions and/or function names, e.g. [np.sum, 'mean']", "dict of axis labels -> functions, function names or list of such.", "Positional arguments to pass to func.", "Keyword arguments to pass to func.", "The return can be:", "scalar : when Series.agg is called with single function", "Series : when DataFrame.agg is called with a single function", "DataFrame : when DataFrame.agg is called with several functions", "Return scalar, Series or DataFrame.", "See also", "Similar DataFrame method.", "Similar Series method.", "Notes", "agg is an alias for aggregate. Use the alias.", "Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.", "A passed user-defined-function will be passed a Series for evaluation.", "Examples"]}, {"name": "pandas.core.window.expanding.Expanding.apply", "path": "reference/api/pandas.core.window.expanding.expanding.apply", "type": "Window", "text": ["Calculate the expanding custom aggregation function.", "Must produce a single value from an ndarray input if raw=True or a single value from a Series if raw=False. Can also accept a Numba JIT function with engine='numba' specified.", "Changed in version 1.0.0.", "False : passes each row or column as a Series to the function.", "True : the passed function will receive ndarray objects instead. If you are just applying a NumPy reduction function this will achieve much better performance.", "'cython' : Runs rolling apply through C-extensions from cython.", "'numba' : Runs rolling apply through JIT compiled code from numba. Only available when raw is set to True.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.0.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to both the func and the apply rolling aggregation.", "New in version 1.0.0.", "Positional arguments to be passed into func.", "Keyword arguments to be passed into func.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling expanding with Series data.", "Calling expanding with DataFrames.", "Aggregating apply for Series.", "Aggregating apply for DataFrame."]}, {"name": "pandas.core.window.expanding.Expanding.corr", "path": "reference/api/pandas.core.window.expanding.expanding.corr", "type": "Window", "text": ["Calculate the expanding correlation.", "If not supplied then will default to self and produce pairwise output.", "If False then only matching columns between self and other will be used and the output will be a DataFrame. If True then all pairwise combinations will be calculated and the output will be a MultiIndexed DataFrame in the case of DataFrame inputs. In the case of missing elements, only complete pairwise observations will be used.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Similar method to calculate covariance.", "NumPy Pearson\u2019s correlation calculation.", "Calling expanding with Series data.", "Calling expanding with DataFrames.", "Aggregating corr for Series.", "Aggregating corr for DataFrame.", "Notes", "This function uses Pearson\u2019s definition of correlation (https://en.wikipedia.org/wiki/Pearson_correlation_coefficient).", "When other is not specified, the output will be self correlation (e.g. all 1\u2019s), except for DataFrame inputs with pairwise set to True.", "Function will return NaN for correlations of equal valued sequences; this is the result of a 0/0 division error.", "When pairwise is set to False, only matching columns between self and other will be used.", "When pairwise is set to True, the output will be a MultiIndex DataFrame with the original index on the first level, and the other DataFrame columns on the second level.", "In the case of missing elements, only complete pairwise observations will be used."]}, {"name": "pandas.core.window.expanding.Expanding.count", "path": "reference/api/pandas.core.window.expanding.expanding.count", "type": "Window", "text": ["Calculate the expanding count of non NaN observations.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling expanding with Series data.", "Calling expanding with DataFrames.", "Aggregating count for Series.", "Aggregating count for DataFrame."]}, {"name": "pandas.core.window.expanding.Expanding.cov", "path": "reference/api/pandas.core.window.expanding.expanding.cov", "type": "Window", "text": ["Calculate the expanding sample covariance.", "If not supplied then will default to self and produce pairwise output.", "If False then only matching columns between self and other will be used and the output will be a DataFrame. If True then all pairwise combinations will be calculated and the output will be a MultiIndexed DataFrame in the case of DataFrame inputs. In the case of missing elements, only complete pairwise observations will be used.", "Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling expanding with Series data.", "Calling expanding with DataFrames.", "Aggregating cov for Series.", "Aggregating cov for DataFrame."]}, {"name": "pandas.core.window.expanding.Expanding.kurt", "path": "reference/api/pandas.core.window.expanding.expanding.kurt", "type": "Window", "text": ["Calculate the expanding Fisher\u2019s definition of kurtosis without bias.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Reference SciPy method.", "Calling expanding with Series data.", "Calling expanding with DataFrames.", "Aggregating kurt for Series.", "Aggregating kurt for DataFrame.", "Notes", "A minimum of four periods is required for the calculation.", "Examples", "The example below will show a rolling calculation with a window size of four matching the equivalent function call using scipy.stats."]}, {"name": "pandas.core.window.expanding.Expanding.max", "path": "reference/api/pandas.core.window.expanding.expanding.max", "type": "Window", "text": ["Calculate the expanding maximum.", "For NumPy compatibility and will not have an effect on the result.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.3.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}", "New in version 1.3.0.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling expanding with Series data.", "Calling expanding with DataFrames.", "Aggregating max for Series.", "Aggregating max for DataFrame.", "Notes", "See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine."]}, {"name": "pandas.core.window.expanding.Expanding.mean", "path": "reference/api/pandas.core.window.expanding.expanding.mean", "type": "Window", "text": ["Calculate the expanding mean.", "For NumPy compatibility and will not have an effect on the result.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.3.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}", "New in version 1.3.0.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling expanding with Series data.", "Calling expanding with DataFrames.", "Aggregating mean for Series.", "Aggregating mean for DataFrame.", "Notes", "See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine."]}, {"name": "pandas.core.window.expanding.Expanding.median", "path": "reference/api/pandas.core.window.expanding.expanding.median", "type": "Window", "text": ["Calculate the expanding median.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.3.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}", "New in version 1.3.0.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling expanding with Series data.", "Calling expanding with DataFrames.", "Aggregating median for Series.", "Aggregating median for DataFrame.", "Notes", "See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine."]}, {"name": "pandas.core.window.expanding.Expanding.min", "path": "reference/api/pandas.core.window.expanding.expanding.min", "type": "Window", "text": ["Calculate the expanding minimum.", "For NumPy compatibility and will not have an effect on the result.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.3.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}", "New in version 1.3.0.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling expanding with Series data.", "Calling expanding with DataFrames.", "Aggregating min for Series.", "Aggregating min for DataFrame.", "Notes", "See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine."]}, {"name": "pandas.core.window.expanding.Expanding.quantile", "path": "reference/api/pandas.core.window.expanding.expanding.quantile", "type": "Window", "text": ["Calculate the expanding quantile.", "Quantile to compute. 0 <= quantile <= 1.", "This optional parameter specifies the interpolation method to use, when the desired quantile lies between two data points i and j:", "linear: i + (j - i) * fraction, where fraction is the fractional part of the index surrounded by i and j.", "lower: i.", "higher: j.", "nearest: i or j whichever is nearest.", "midpoint: (i + j) / 2.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling expanding with Series data.", "Calling expanding with DataFrames.", "Aggregating quantile for Series.", "Aggregating quantile for DataFrame."]}, {"name": "pandas.core.window.expanding.Expanding.rank", "path": "reference/api/pandas.core.window.expanding.expanding.rank", "type": "Window", "text": ["Calculate the expanding rank.", "New in version 1.4.0.", "How to rank the group of records that have the same value (i.e. ties):", "average: average rank of the group", "min: lowest rank in the group", "max: highest rank in the group", "Whether or not the elements should be ranked in ascending order.", "Whether or not to display the returned rankings in percentile form.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling expanding with Series data.", "Calling expanding with DataFrames.", "Aggregating rank for Series.", "Aggregating rank for DataFrame.", "Examples"]}, {"name": "pandas.core.window.expanding.Expanding.sem", "path": "reference/api/pandas.core.window.expanding.expanding.sem", "type": "Window", "text": ["Calculate the expanding standard error of mean.", "Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "For NumPy compatibility and will not have an effect on the result.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling expanding with Series data.", "Calling expanding with DataFrames.", "Aggregating sem for Series.", "Aggregating sem for DataFrame.", "Notes", "A minimum of one period is required for the calculation.", "Examples"]}, {"name": "pandas.core.window.expanding.Expanding.skew", "path": "reference/api/pandas.core.window.expanding.expanding.skew", "type": "Window", "text": ["Calculate the expanding unbiased skewness.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Third moment of a probability density.", "Calling expanding with Series data.", "Calling expanding with DataFrames.", "Aggregating skew for Series.", "Aggregating skew for DataFrame.", "Notes", "A minimum of three periods is required for the rolling calculation."]}, {"name": "pandas.core.window.expanding.Expanding.std", "path": "reference/api/pandas.core.window.expanding.expanding.std", "type": "Window", "text": ["Calculate the expanding standard deviation.", "Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "For NumPy compatibility and will not have an effect on the result.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.4.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}", "New in version 1.4.0.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Equivalent method for NumPy array.", "Calling expanding with Series data.", "Calling expanding with DataFrames.", "Aggregating std for Series.", "Aggregating std for DataFrame.", "Notes", "The default ddof of 1 used in Series.std() is different than the default ddof of 0 in numpy.std().", "A minimum of one period is required for the rolling calculation.", "Examples"]}, {"name": "pandas.core.window.expanding.Expanding.sum", "path": "reference/api/pandas.core.window.expanding.expanding.sum", "type": "Window", "text": ["Calculate the expanding sum.", "For NumPy compatibility and will not have an effect on the result.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.3.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}", "New in version 1.3.0.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling expanding with Series data.", "Calling expanding with DataFrames.", "Aggregating sum for Series.", "Aggregating sum for DataFrame.", "Notes", "See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine."]}, {"name": "pandas.core.window.expanding.Expanding.var", "path": "reference/api/pandas.core.window.expanding.expanding.var", "type": "Window", "text": ["Calculate the expanding variance.", "Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "For NumPy compatibility and will not have an effect on the result.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.4.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}", "New in version 1.4.0.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Equivalent method for NumPy array.", "Calling expanding with Series data.", "Calling expanding with DataFrames.", "Aggregating var for Series.", "Aggregating var for DataFrame.", "Notes", "The default ddof of 1 used in Series.var() is different than the default ddof of 0 in numpy.var().", "A minimum of one period is required for the rolling calculation.", "Examples"]}, {"name": "pandas.core.window.rolling.Rolling.aggregate", "path": "reference/api/pandas.core.window.rolling.rolling.aggregate", "type": "Window", "text": ["Aggregate using one or more operations over the specified axis.", "Function to use for aggregating the data. If a function, must either work when passed a Series/Dataframe or when passed to Series/Dataframe.apply.", "Accepted combinations are:", "function", "string function name", "list of functions and/or function names, e.g. [np.sum, 'mean']", "dict of axis labels -> functions, function names or list of such.", "Positional arguments to pass to func.", "Keyword arguments to pass to func.", "The return can be:", "scalar : when Series.agg is called with single function", "Series : when DataFrame.agg is called with a single function", "DataFrame : when DataFrame.agg is called with several functions", "Return scalar, Series or DataFrame.", "See also", "Calling object with Series data.", "Calling object with DataFrame data.", "Notes", "agg is an alias for aggregate. Use the alias.", "Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.", "A passed user-defined-function will be passed a Series for evaluation.", "Examples"]}, {"name": "pandas.core.window.rolling.Rolling.apply", "path": "reference/api/pandas.core.window.rolling.rolling.apply", "type": "Window", "text": ["Calculate the rolling custom aggregation function.", "Must produce a single value from an ndarray input if raw=True or a single value from a Series if raw=False. Can also accept a Numba JIT function with engine='numba' specified.", "Changed in version 1.0.0.", "False : passes each row or column as a Series to the function.", "True : the passed function will receive ndarray objects instead. If you are just applying a NumPy reduction function this will achieve much better performance.", "'cython' : Runs rolling apply through C-extensions from cython.", "'numba' : Runs rolling apply through JIT compiled code from numba. Only available when raw is set to True.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.0.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to both the func and the apply rolling aggregation.", "New in version 1.0.0.", "Positional arguments to be passed into func.", "Keyword arguments to be passed into func.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating apply for Series.", "Aggregating apply for DataFrame."]}, {"name": "pandas.core.window.rolling.Rolling.corr", "path": "reference/api/pandas.core.window.rolling.rolling.corr", "type": "Window", "text": ["Calculate the rolling correlation.", "If not supplied then will default to self and produce pairwise output.", "If False then only matching columns between self and other will be used and the output will be a DataFrame. If True then all pairwise combinations will be calculated and the output will be a MultiIndexed DataFrame in the case of DataFrame inputs. In the case of missing elements, only complete pairwise observations will be used.", "Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Similar method to calculate covariance.", "NumPy Pearson\u2019s correlation calculation.", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating corr for Series.", "Aggregating corr for DataFrame.", "Notes", "This function uses Pearson\u2019s definition of correlation (https://en.wikipedia.org/wiki/Pearson_correlation_coefficient).", "When other is not specified, the output will be self correlation (e.g. all 1\u2019s), except for DataFrame inputs with pairwise set to True.", "Function will return NaN for correlations of equal valued sequences; this is the result of a 0/0 division error.", "When pairwise is set to False, only matching columns between self and other will be used.", "When pairwise is set to True, the output will be a MultiIndex DataFrame with the original index on the first level, and the other DataFrame columns on the second level.", "In the case of missing elements, only complete pairwise observations will be used.", "Examples", "The below example shows a rolling calculation with a window size of four matching the equivalent function call using numpy.corrcoef().", "The below example shows a similar rolling calculation on a DataFrame using the pairwise option."]}, {"name": "pandas.core.window.rolling.Rolling.count", "path": "reference/api/pandas.core.window.rolling.rolling.count", "type": "Window", "text": ["Calculate the rolling count of non NaN observations.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating count for Series.", "Aggregating count for DataFrame.", "Examples"]}, {"name": "pandas.core.window.rolling.Rolling.cov", "path": "reference/api/pandas.core.window.rolling.rolling.cov", "type": "Window", "text": ["Calculate the rolling sample covariance.", "If not supplied then will default to self and produce pairwise output.", "If False then only matching columns between self and other will be used and the output will be a DataFrame. If True then all pairwise combinations will be calculated and the output will be a MultiIndexed DataFrame in the case of DataFrame inputs. In the case of missing elements, only complete pairwise observations will be used.", "Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating cov for Series.", "Aggregating cov for DataFrame."]}, {"name": "pandas.core.window.rolling.Rolling.kurt", "path": "reference/api/pandas.core.window.rolling.rolling.kurt", "type": "Window", "text": ["Calculate the rolling Fisher\u2019s definition of kurtosis without bias.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Reference SciPy method.", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating kurt for Series.", "Aggregating kurt for DataFrame.", "Notes", "A minimum of four periods is required for the calculation.", "Examples", "The example below will show a rolling calculation with a window size of four matching the equivalent function call using scipy.stats."]}, {"name": "pandas.core.window.rolling.Rolling.max", "path": "reference/api/pandas.core.window.rolling.rolling.max", "type": "Window", "text": ["Calculate the rolling maximum.", "For NumPy compatibility and will not have an effect on the result.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.3.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}", "New in version 1.3.0.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating max for Series.", "Aggregating max for DataFrame.", "Notes", "See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine."]}, {"name": "pandas.core.window.rolling.Rolling.mean", "path": "reference/api/pandas.core.window.rolling.rolling.mean", "type": "Window", "text": ["Calculate the rolling mean.", "For NumPy compatibility and will not have an effect on the result.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.3.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}", "New in version 1.3.0.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating mean for Series.", "Aggregating mean for DataFrame.", "Notes", "See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine.", "Examples", "The below examples will show rolling mean calculations with window sizes of two and three, respectively."]}, {"name": "pandas.core.window.rolling.Rolling.median", "path": "reference/api/pandas.core.window.rolling.rolling.median", "type": "Window", "text": ["Calculate the rolling median.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.3.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}", "New in version 1.3.0.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating median for Series.", "Aggregating median for DataFrame.", "Notes", "See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine.", "Examples", "Compute the rolling median of a series with a window size of 3."]}, {"name": "pandas.core.window.rolling.Rolling.min", "path": "reference/api/pandas.core.window.rolling.rolling.min", "type": "Window", "text": ["Calculate the rolling minimum.", "For NumPy compatibility and will not have an effect on the result.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.3.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}", "New in version 1.3.0.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating min for Series.", "Aggregating min for DataFrame.", "Notes", "See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine.", "Examples", "Performing a rolling minimum with a window size of 3."]}, {"name": "pandas.core.window.rolling.Rolling.quantile", "path": "reference/api/pandas.core.window.rolling.rolling.quantile", "type": "Window", "text": ["Calculate the rolling quantile.", "Quantile to compute. 0 <= quantile <= 1.", "This optional parameter specifies the interpolation method to use, when the desired quantile lies between two data points i and j:", "linear: i + (j - i) * fraction, where fraction is the fractional part of the index surrounded by i and j.", "lower: i.", "higher: j.", "nearest: i or j whichever is nearest.", "midpoint: (i + j) / 2.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating quantile for Series.", "Aggregating quantile for DataFrame.", "Examples"]}, {"name": "pandas.core.window.rolling.Rolling.rank", "path": "reference/api/pandas.core.window.rolling.rolling.rank", "type": "Window", "text": ["Calculate the rolling rank.", "New in version 1.4.0.", "How to rank the group of records that have the same value (i.e. ties):", "average: average rank of the group", "min: lowest rank in the group", "max: highest rank in the group", "Whether or not the elements should be ranked in ascending order.", "Whether or not to display the returned rankings in percentile form.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating rank for Series.", "Aggregating rank for DataFrame.", "Examples"]}, {"name": "pandas.core.window.rolling.Rolling.sem", "path": "reference/api/pandas.core.window.rolling.rolling.sem", "type": "Window", "text": ["Calculate the rolling standard error of mean.", "Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "For NumPy compatibility and will not have an effect on the result.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating sem for Series.", "Aggregating sem for DataFrame.", "Notes", "A minimum of one period is required for the calculation.", "Examples"]}, {"name": "pandas.core.window.rolling.Rolling.skew", "path": "reference/api/pandas.core.window.rolling.rolling.skew", "type": "Window", "text": ["Calculate the rolling unbiased skewness.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Third moment of a probability density.", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating skew for Series.", "Aggregating skew for DataFrame.", "Notes", "A minimum of three periods is required for the rolling calculation."]}, {"name": "pandas.core.window.rolling.Rolling.std", "path": "reference/api/pandas.core.window.rolling.rolling.std", "type": "Window", "text": ["Calculate the rolling standard deviation.", "Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "For NumPy compatibility and will not have an effect on the result.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.4.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}", "New in version 1.4.0.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Equivalent method for NumPy array.", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating std for Series.", "Aggregating std for DataFrame.", "Notes", "The default ddof of 1 used in Series.std() is different than the default ddof of 0 in numpy.std().", "A minimum of one period is required for the rolling calculation.", "The implementation is susceptible to floating point imprecision as shown in the example below.", "Examples"]}, {"name": "pandas.core.window.rolling.Rolling.sum", "path": "reference/api/pandas.core.window.rolling.rolling.sum", "type": "Window", "text": ["Calculate the rolling sum.", "For NumPy compatibility and will not have an effect on the result.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.3.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}", "New in version 1.3.0.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating sum for Series.", "Aggregating sum for DataFrame.", "Notes", "See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine.", "Examples", "For DataFrame, each sum is computed column-wise."]}, {"name": "pandas.core.window.rolling.Rolling.var", "path": "reference/api/pandas.core.window.rolling.rolling.var", "type": "Window", "text": ["Calculate the rolling variance.", "Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "For NumPy compatibility and will not have an effect on the result.", "'cython' : Runs the operation through C-extensions from cython.", "'numba' : Runs the operation through JIT compiled code from numba.", "None : Defaults to 'cython' or globally setting compute.use_numba", "New in version 1.4.0.", "For 'cython' engine, there are no accepted engine_kwargs", "For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}", "New in version 1.4.0.", "For NumPy compatibility and will not have an effect on the result.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Equivalent method for NumPy array.", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating var for Series.", "Aggregating var for DataFrame.", "Notes", "The default ddof of 1 used in Series.var() is different than the default ddof of 0 in numpy.var().", "A minimum of one period is required for the rolling calculation.", "The implementation is susceptible to floating point imprecision as shown in the example below.", "Examples"]}, {"name": "pandas.core.window.rolling.Window.mean", "path": "reference/api/pandas.core.window.rolling.window.mean", "type": "Window", "text": ["Calculate the rolling weighted window mean.", "Keyword arguments to configure the SciPy weighted window type.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating mean for Series.", "Aggregating mean for DataFrame."]}, {"name": "pandas.core.window.rolling.Window.std", "path": "reference/api/pandas.core.window.rolling.window.std", "type": "Window", "text": ["Calculate the rolling weighted window standard deviation.", "New in version 1.0.0.", "Keyword arguments to configure the SciPy weighted window type.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating std for Series.", "Aggregating std for DataFrame."]}, {"name": "pandas.core.window.rolling.Window.sum", "path": "reference/api/pandas.core.window.rolling.window.sum", "type": "Window", "text": ["Calculate the rolling weighted window sum.", "Keyword arguments to configure the SciPy weighted window type.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating sum for Series.", "Aggregating sum for DataFrame."]}, {"name": "pandas.core.window.rolling.Window.var", "path": "reference/api/pandas.core.window.rolling.window.var", "type": "Window", "text": ["Calculate the rolling weighted window variance.", "New in version 1.0.0.", "Keyword arguments to configure the SciPy weighted window type.", "Return type is the same as the original object with np.float64 dtype.", "See also", "Calling rolling with Series data.", "Calling rolling with DataFrames.", "Aggregating var for Series.", "Aggregating var for DataFrame."]}, {"name": "pandas.crosstab", "path": "reference/api/pandas.crosstab", "type": "General functions", "text": ["Compute a simple cross tabulation of two (or more) factors. By default computes a frequency table of the factors unless an array of values and an aggregation function are passed.", "Values to group by in the rows.", "Values to group by in the columns.", "Array of values to aggregate according to the factors. Requires aggfunc be specified.", "If passed, must match number of row arrays passed.", "If passed, must match number of column arrays passed.", "If specified, requires values be specified as well.", "Add row/column margins (subtotals).", "Name of the row/column that will contain the totals when margins is True.", "Do not include columns whose entries are all NaN.", "Normalize by dividing all values by the sum of values.", "If passed \u2018all\u2019 or True, will normalize over all values.", "If passed \u2018index\u2019 will normalize over each row.", "If passed \u2018columns\u2019 will normalize over each column.", "If margins is True, will also normalize margin values.", "Cross tabulation of the data.", "See also", "Reshape data based on column values.", "Create a pivot table as a DataFrame.", "Notes", "Any Series passed will have their name attributes used unless row or column names for the cross-tabulation are specified.", "Any input passed containing Categorical data will have all of its categories included in the cross-tabulation, even if the actual data does not contain any instances of a particular category.", "In the event that there aren\u2019t overlapping indexes an empty DataFrame will be returned.", "Examples", "Here \u2018c\u2019 and \u2018f\u2019 are not represented in the data and will not be shown in the output because dropna is True by default. Set dropna=False to preserve categories with no data."]}, {"name": "pandas.cut", "path": "reference/api/pandas.cut", "type": "General functions", "text": ["Bin values into discrete intervals.", "Use cut when you need to segment and sort data values into bins. This function is also useful for going from a continuous variable to a categorical variable. For example, cut could convert ages to groups of age ranges. Supports binning into an equal number of bins, or a pre-specified array of bins.", "The input array to be binned. Must be 1-dimensional.", "The criteria to bin by.", "int : Defines the number of equal-width bins in the range of x. The range of x is extended by .1% on each side to include the minimum and maximum values of x.", "sequence of scalars : Defines the bin edges allowing for non-uniform width. No extension of the range of x is done.", "IntervalIndex : Defines the exact bins to be used. Note that IntervalIndex for bins must be non-overlapping.", "Indicates whether bins includes the rightmost edge or not. If right == True (the default), then the bins [1, 2, 3, 4] indicate (1,2], (2,3], (3,4]. This argument is ignored when bins is an IntervalIndex.", "Specifies the labels for the returned bins. Must be the same length as the resulting bins. If False, returns only integer indicators of the bins. This affects the type of the output container (see below). This argument is ignored when bins is an IntervalIndex. If True, raises an error. When ordered=False, labels must be provided.", "Whether to return the bins or not. Useful when bins is provided as a scalar.", "The precision at which to store and display the bins labels.", "Whether the first interval should be left-inclusive or not.", "If bin edges are not unique, raise ValueError or drop non-uniques.", "Whether the labels are ordered or not. Applies to returned types Categorical and Series (with Categorical dtype). If True, the resulting categorical will be ordered. If False, the resulting categorical will be unordered (labels must be provided).", "New in version 1.1.0.", "An array-like object representing the respective bin for each value of x. The type depends on the value of labels.", "None (default) : returns a Series for Series x or a Categorical for all other inputs. The values stored within are Interval dtype.", "sequence of scalars : returns a Series for Series x or a Categorical for all other inputs. The values stored within are whatever the type in the sequence is.", "False : returns an ndarray of integers.", "The computed or specified bins. Only returned when retbins=True. For scalar or sequence bins, this is an ndarray with the computed bins. If set duplicates=drop, bins will drop non-unique bin. For an IntervalIndex bins, this is equal to bins.", "See also", "Discretize variable into equal-sized buckets based on rank or based on sample quantiles.", "Array type for storing data that come from a fixed set of values.", "One-dimensional array with axis labels (including time series).", "Immutable Index implementing an ordered, sliceable set.", "Notes", "Any NA values will be NA in the result. Out of bounds values will be NA in the resulting Series or Categorical object.", "Examples", "Discretize into three equal-sized bins.", "Discovers the same bins, but assign them specific labels. Notice that the returned Categorical\u2019s categories are labels and is ordered.", "ordered=False will result in unordered categories when labels are passed. This parameter can be used to allow non-unique labels:", "labels=False implies you just want the bins back.", "Passing a Series as an input returns a Series with categorical dtype:", "Passing a Series as an input returns a Series with mapping value. It is used to map numerically to intervals based on bins.", "Use drop optional when bins is not unique", "Passing an IntervalIndex for bins results in those categories exactly. Notice that values not covered by the IntervalIndex are set to NaN. 0 is to the left of the first bin (which is closed on the right), and 1.5 falls between two bins."]}, {"name": "pandas.DataFrame", "path": "reference/api/pandas.dataframe", "type": "DataFrame", "text": ["Two-dimensional, size-mutable, potentially heterogeneous tabular data.", "Data structure also contains labeled axes (rows and columns). Arithmetic operations align on both row and column labels. Can be thought of as a dict-like container for Series objects. The primary pandas data structure.", "Dict can contain Series, arrays, constants, dataclass or list-like objects. If data is a dict, column order follows insertion-order. If a dict contains Series which have an index defined, it is aligned by its index.", "Changed in version 0.25.0: If data is a list of dicts, column order follows insertion-order.", "Index to use for resulting frame. Will default to RangeIndex if no indexing information part of input data and no index provided.", "Column labels to use for resulting frame when data does not have them, defaulting to RangeIndex(0, 1, 2, \u2026, n). If data contains column labels, will perform column selection instead.", "Data type to force. Only a single dtype is allowed. If None, infer.", "Copy data from inputs. For dict data, the default of None behaves like copy=True. For DataFrame or 2d ndarray input, the default of None behaves like copy=False.", "Changed in version 1.3.0.", "See also", "Constructor from tuples, also record arrays.", "From dicts of Series, arrays, or dicts.", "Read a comma-separated values (csv) file into DataFrame.", "Read general delimited file into DataFrame.", "Read text from clipboard into DataFrame.", "Examples", "Constructing DataFrame from a dictionary.", "Notice that the inferred dtype is int64.", "To enforce a single dtype:", "Constructing DataFrame from a dictionary including Series:", "Constructing DataFrame from numpy ndarray:", "Constructing DataFrame from a numpy ndarray that has labeled columns:", "Constructing DataFrame from dataclass:", "Attributes", "at", "Access a single value for a row/column label pair.", "attrs", "Dictionary of global attributes of this dataset.", "axes", "Return a list representing the axes of the DataFrame.", "columns", "The column labels of the DataFrame.", "dtypes", "Return the dtypes in the DataFrame.", "empty", "Indicator whether Series/DataFrame is empty.", "flags", "Get the properties associated with this pandas object.", "iat", "Access a single value for a row/column pair by integer position.", "iloc", "Purely integer-location based indexing for selection by position.", "index", "The index (row labels) of the DataFrame.", "loc", "Access a group of rows and columns by label(s) or a boolean array.", "ndim", "Return an int representing the number of axes / array dimensions.", "shape", "Return a tuple representing the dimensionality of the DataFrame.", "size", "Return an int representing the number of elements in this object.", "style", "Returns a Styler object.", "values", "Return a Numpy representation of the DataFrame.", "T", "Methods", "abs()", "Return a Series/DataFrame with absolute numeric value of each element.", "add(other[, axis, level, fill_value])", "Get Addition of dataframe and other, element-wise (binary operator add).", "add_prefix(prefix)", "Prefix labels with string prefix.", "add_suffix(suffix)", "Suffix labels with string suffix.", "agg([func, axis])", "Aggregate using one or more operations over the specified axis.", "aggregate([func, axis])", "Aggregate using one or more operations over the specified axis.", "align(other[, join, axis, level, copy, ...])", "Align two objects on their axes with the specified join method.", "all([axis, bool_only, skipna, level])", "Return whether all elements are True, potentially over an axis.", "any([axis, bool_only, skipna, level])", "Return whether any element is True, potentially over an axis.", "append(other[, ignore_index, ...])", "Append rows of other to the end of caller, returning a new object.", "apply(func[, axis, raw, result_type, args])", "Apply a function along an axis of the DataFrame.", "applymap(func[, na_action])", "Apply a function to a Dataframe elementwise.", "asfreq(freq[, method, how, normalize, ...])", "Convert time series to specified frequency.", "asof(where[, subset])", "Return the last row(s) without any NaNs before where.", "assign(**kwargs)", "Assign new columns to a DataFrame.", "astype(dtype[, copy, errors])", "Cast a pandas object to a specified dtype dtype.", "at_time(time[, asof, axis])", "Select values at particular time of day (e.g., 9:30AM).", "backfill([axis, inplace, limit, downcast])", "Synonym for DataFrame.fillna() with method='bfill'.", "between_time(start_time, end_time[, ...])", "Select values between particular times of the day (e.g., 9:00-9:30 AM).", "bfill([axis, inplace, limit, downcast])", "Synonym for DataFrame.fillna() with method='bfill'.", "bool()", "Return the bool of a single element Series or DataFrame.", "boxplot([column, by, ax, fontsize, rot, ...])", "Make a box plot from DataFrame columns.", "clip([lower, upper, axis, inplace])", "Trim values at input threshold(s).", "combine(other, func[, fill_value, overwrite])", "Perform column-wise combine with another DataFrame.", "combine_first(other)", "Update null elements with value in the same location in other.", "compare(other[, align_axis, keep_shape, ...])", "Compare to another DataFrame and show the differences.", "convert_dtypes([infer_objects, ...])", "Convert columns to best possible dtypes using dtypes supporting pd.NA.", "copy([deep])", "Make a copy of this object's indices and data.", "corr([method, min_periods])", "Compute pairwise correlation of columns, excluding NA/null values.", "corrwith(other[, axis, drop, method])", "Compute pairwise correlation.", "count([axis, level, numeric_only])", "Count non-NA cells for each column or row.", "cov([min_periods, ddof])", "Compute pairwise covariance of columns, excluding NA/null values.", "cummax([axis, skipna])", "Return cumulative maximum over a DataFrame or Series axis.", "cummin([axis, skipna])", "Return cumulative minimum over a DataFrame or Series axis.", "cumprod([axis, skipna])", "Return cumulative product over a DataFrame or Series axis.", "cumsum([axis, skipna])", "Return cumulative sum over a DataFrame or Series axis.", "describe([percentiles, include, exclude, ...])", "Generate descriptive statistics.", "diff([periods, axis])", "First discrete difference of element.", "div(other[, axis, level, fill_value])", "Get Floating division of dataframe and other, element-wise (binary operator truediv).", "divide(other[, axis, level, fill_value])", "Get Floating division of dataframe and other, element-wise (binary operator truediv).", "dot(other)", "Compute the matrix multiplication between the DataFrame and other.", "drop([labels, axis, index, columns, level, ...])", "Drop specified labels from rows or columns.", "drop_duplicates([subset, keep, inplace, ...])", "Return DataFrame with duplicate rows removed.", "droplevel(level[, axis])", "Return Series/DataFrame with requested index / column level(s) removed.", "dropna([axis, how, thresh, subset, inplace])", "Remove missing values.", "duplicated([subset, keep])", "Return boolean Series denoting duplicate rows.", "eq(other[, axis, level])", "Get Equal to of dataframe and other, element-wise (binary operator eq).", "equals(other)", "Test whether two objects contain the same elements.", "eval(expr[, inplace])", "Evaluate a string describing operations on DataFrame columns.", "ewm([com, span, halflife, alpha, ...])", "Provide exponentially weighted (EW) calculations.", "expanding([min_periods, center, axis, method])", "Provide expanding window calculations.", "explode(column[, ignore_index])", "Transform each element of a list-like to a row, replicating index values.", "ffill([axis, inplace, limit, downcast])", "Synonym for DataFrame.fillna() with method='ffill'.", "fillna([value, method, axis, inplace, ...])", "Fill NA/NaN values using the specified method.", "filter([items, like, regex, axis])", "Subset the dataframe rows or columns according to the specified index labels.", "first(offset)", "Select initial periods of time series data based on a date offset.", "first_valid_index()", "Return index for first non-NA value or None, if no NA value is found.", "floordiv(other[, axis, level, fill_value])", "Get Integer division of dataframe and other, element-wise (binary operator floordiv).", "from_dict(data[, orient, dtype, columns])", "Construct DataFrame from dict of array-like or dicts.", "from_records(data[, index, exclude, ...])", "Convert structured or record ndarray to DataFrame.", "ge(other[, axis, level])", "Get Greater than or equal to of dataframe and other, element-wise (binary operator ge).", "get(key[, default])", "Get item from object for given key (ex: DataFrame column).", "groupby([by, axis, level, as_index, sort, ...])", "Group DataFrame using a mapper or by a Series of columns.", "gt(other[, axis, level])", "Get Greater than of dataframe and other, element-wise (binary operator gt).", "head([n])", "Return the first n rows.", "hist([column, by, grid, xlabelsize, xrot, ...])", "Make a histogram of the DataFrame's columns.", "idxmax([axis, skipna])", "Return index of first occurrence of maximum over requested axis.", "idxmin([axis, skipna])", "Return index of first occurrence of minimum over requested axis.", "infer_objects()", "Attempt to infer better dtypes for object columns.", "info([verbose, buf, max_cols, memory_usage, ...])", "Print a concise summary of a DataFrame.", "insert(loc, column, value[, allow_duplicates])", "Insert column into DataFrame at specified location.", "interpolate([method, axis, limit, inplace, ...])", "Fill NaN values using an interpolation method.", "isin(values)", "Whether each element in the DataFrame is contained in values.", "isna()", "Detect missing values.", "isnull()", "DataFrame.isnull is an alias for DataFrame.isna.", "items()", "Iterate over (column name, Series) pairs.", "iteritems()", "Iterate over (column name, Series) pairs.", "iterrows()", "Iterate over DataFrame rows as (index, Series) pairs.", "itertuples([index, name])", "Iterate over DataFrame rows as namedtuples.", "join(other[, on, how, lsuffix, rsuffix, sort])", "Join columns of another DataFrame.", "keys()", "Get the 'info axis' (see Indexing for more).", "kurt([axis, skipna, level, numeric_only])", "Return unbiased kurtosis over requested axis.", "kurtosis([axis, skipna, level, numeric_only])", "Return unbiased kurtosis over requested axis.", "last(offset)", "Select final periods of time series data based on a date offset.", "last_valid_index()", "Return index for last non-NA value or None, if no NA value is found.", "le(other[, axis, level])", "Get Less than or equal to of dataframe and other, element-wise (binary operator le).", "lookup(row_labels, col_labels)", "(DEPRECATED) Label-based \"fancy indexing\" function for DataFrame.", "lt(other[, axis, level])", "Get Less than of dataframe and other, element-wise (binary operator lt).", "mad([axis, skipna, level])", "Return the mean absolute deviation of the values over the requested axis.", "mask(cond[, other, inplace, axis, level, ...])", "Replace values where the condition is True.", "max([axis, skipna, level, numeric_only])", "Return the maximum of the values over the requested axis.", "mean([axis, skipna, level, numeric_only])", "Return the mean of the values over the requested axis.", "median([axis, skipna, level, numeric_only])", "Return the median of the values over the requested axis.", "melt([id_vars, value_vars, var_name, ...])", "Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.", "memory_usage([index, deep])", "Return the memory usage of each column in bytes.", "merge(right[, how, on, left_on, right_on, ...])", "Merge DataFrame or named Series objects with a database-style join.", "min([axis, skipna, level, numeric_only])", "Return the minimum of the values over the requested axis.", "mod(other[, axis, level, fill_value])", "Get Modulo of dataframe and other, element-wise (binary operator mod).", "mode([axis, numeric_only, dropna])", "Get the mode(s) of each element along the selected axis.", "mul(other[, axis, level, fill_value])", "Get Multiplication of dataframe and other, element-wise (binary operator mul).", "multiply(other[, axis, level, fill_value])", "Get Multiplication of dataframe and other, element-wise (binary operator mul).", "ne(other[, axis, level])", "Get Not equal to of dataframe and other, element-wise (binary operator ne).", "nlargest(n, columns[, keep])", "Return the first n rows ordered by columns in descending order.", "notna()", "Detect existing (non-missing) values.", "notnull()", "DataFrame.notnull is an alias for DataFrame.notna.", "nsmallest(n, columns[, keep])", "Return the first n rows ordered by columns in ascending order.", "nunique([axis, dropna])", "Count number of distinct elements in specified axis.", "pad([axis, inplace, limit, downcast])", "Synonym for DataFrame.fillna() with method='ffill'.", "pct_change([periods, fill_method, limit, freq])", "Percentage change between the current and a prior element.", "pipe(func, *args, **kwargs)", "Apply chainable functions that expect Series or DataFrames.", "pivot([index, columns, values])", "Return reshaped DataFrame organized by given index / column values.", "pivot_table([values, index, columns, ...])", "Create a spreadsheet-style pivot table as a DataFrame.", "plot", "alias of pandas.plotting._core.PlotAccessor", "pop(item)", "Return item and drop from frame.", "pow(other[, axis, level, fill_value])", "Get Exponential power of dataframe and other, element-wise (binary operator pow).", "prod([axis, skipna, level, numeric_only, ...])", "Return the product of the values over the requested axis.", "product([axis, skipna, level, numeric_only, ...])", "Return the product of the values over the requested axis.", "quantile([q, axis, numeric_only, interpolation])", "Return values at the given quantile over requested axis.", "query(expr[, inplace])", "Query the columns of a DataFrame with a boolean expression.", "radd(other[, axis, level, fill_value])", "Get Addition of dataframe and other, element-wise (binary operator radd).", "rank([axis, method, numeric_only, ...])", "Compute numerical data ranks (1 through n) along axis.", "rdiv(other[, axis, level, fill_value])", "Get Floating division of dataframe and other, element-wise (binary operator rtruediv).", "reindex([labels, index, columns, axis, ...])", "Conform Series/DataFrame to new index with optional filling logic.", "reindex_like(other[, method, copy, limit, ...])", "Return an object with matching indices as other object.", "rename([mapper, index, columns, axis, copy, ...])", "Alter axes labels.", "rename_axis([mapper, index, columns, axis, ...])", "Set the name of the axis for the index or columns.", "reorder_levels(order[, axis])", "Rearrange index levels using input order.", "replace([to_replace, value, inplace, limit, ...])", "Replace values given in to_replace with value.", "resample(rule[, axis, closed, label, ...])", "Resample time-series data.", "reset_index([level, drop, inplace, ...])", "Reset the index, or a level of it.", "rfloordiv(other[, axis, level, fill_value])", "Get Integer division of dataframe and other, element-wise (binary operator rfloordiv).", "rmod(other[, axis, level, fill_value])", "Get Modulo of dataframe and other, element-wise (binary operator rmod).", "rmul(other[, axis, level, fill_value])", "Get Multiplication of dataframe and other, element-wise (binary operator rmul).", "rolling(window[, min_periods, center, ...])", "Provide rolling window calculations.", "round([decimals])", "Round a DataFrame to a variable number of decimal places.", "rpow(other[, axis, level, fill_value])", "Get Exponential power of dataframe and other, element-wise (binary operator rpow).", "rsub(other[, axis, level, fill_value])", "Get Subtraction of dataframe and other, element-wise (binary operator rsub).", "rtruediv(other[, axis, level, fill_value])", "Get Floating division of dataframe and other, element-wise (binary operator rtruediv).", "sample([n, frac, replace, weights, ...])", "Return a random sample of items from an axis of object.", "select_dtypes([include, exclude])", "Return a subset of the DataFrame's columns based on the column dtypes.", "sem([axis, skipna, level, ddof, numeric_only])", "Return unbiased standard error of the mean over requested axis.", "set_axis(labels[, axis, inplace])", "Assign desired index to given axis.", "set_flags(*[, copy, allows_duplicate_labels])", "Return a new object with updated flags.", "set_index(keys[, drop, append, inplace, ...])", "Set the DataFrame index using existing columns.", "shift([periods, freq, axis, fill_value])", "Shift index by desired number of periods with an optional time freq.", "skew([axis, skipna, level, numeric_only])", "Return unbiased skew over requested axis.", "slice_shift([periods, axis])", "(DEPRECATED) Equivalent to shift without copying data.", "sort_index([axis, level, ascending, ...])", "Sort object by labels (along an axis).", "sort_values(by[, axis, ascending, inplace, ...])", "Sort by the values along either axis.", "sparse", "alias of pandas.core.arrays.sparse.accessor.SparseFrameAccessor", "squeeze([axis])", "Squeeze 1 dimensional axis objects into scalars.", "stack([level, dropna])", "Stack the prescribed level(s) from columns to index.", "std([axis, skipna, level, ddof, numeric_only])", "Return sample standard deviation over requested axis.", "sub(other[, axis, level, fill_value])", "Get Subtraction of dataframe and other, element-wise (binary operator sub).", "subtract(other[, axis, level, fill_value])", "Get Subtraction of dataframe and other, element-wise (binary operator sub).", "sum([axis, skipna, level, numeric_only, ...])", "Return the sum of the values over the requested axis.", "swapaxes(axis1, axis2[, copy])", "Interchange axes and swap values axes appropriately.", "swaplevel([i, j, axis])", "Swap levels i and j in a MultiIndex.", "tail([n])", "Return the last n rows.", "take(indices[, axis, is_copy])", "Return the elements in the given positional indices along an axis.", "to_clipboard([excel, sep])", "Copy object to the system clipboard.", "to_csv([path_or_buf, sep, na_rep, ...])", "Write object to a comma-separated values (csv) file.", "to_dict([orient, into])", "Convert the DataFrame to a dictionary.", "to_excel(excel_writer[, sheet_name, na_rep, ...])", "Write object to an Excel sheet.", "to_feather(path, **kwargs)", "Write a DataFrame to the binary Feather format.", "to_gbq(destination_table[, project_id, ...])", "Write a DataFrame to a Google BigQuery table.", "to_hdf(path_or_buf, key[, mode, complevel, ...])", "Write the contained data to an HDF5 file using HDFStore.", "to_html([buf, columns, col_space, header, ...])", "Render a DataFrame as an HTML table.", "to_json([path_or_buf, orient, date_format, ...])", "Convert the object to a JSON string.", "to_latex([buf, columns, col_space, header, ...])", "Render object to a LaTeX tabular, longtable, or nested table.", "to_markdown([buf, mode, index, storage_options])", "Print DataFrame in Markdown-friendly format.", "to_numpy([dtype, copy, na_value])", "Convert the DataFrame to a NumPy array.", "to_parquet([path, engine, compression, ...])", "Write a DataFrame to the binary parquet format.", "to_period([freq, axis, copy])", "Convert DataFrame from DatetimeIndex to PeriodIndex.", "to_pickle(path[, compression, protocol, ...])", "Pickle (serialize) object to file.", "to_records([index, column_dtypes, index_dtypes])", "Convert DataFrame to a NumPy record array.", "to_sql(name, con[, schema, if_exists, ...])", "Write records stored in a DataFrame to a SQL database.", "to_stata(path[, convert_dates, write_index, ...])", "Export DataFrame object to Stata dta format.", "to_string([buf, columns, col_space, header, ...])", "Render a DataFrame to a console-friendly tabular output.", "to_timestamp([freq, how, axis, copy])", "Cast to DatetimeIndex of timestamps, at beginning of period.", "to_xarray()", "Return an xarray object from the pandas object.", "to_xml([path_or_buffer, index, root_name, ...])", "Render a DataFrame to an XML document.", "transform(func[, axis])", "Call func on self producing a DataFrame with the same axis shape as self.", "transpose(*args[, copy])", "Transpose index and columns.", "truediv(other[, axis, level, fill_value])", "Get Floating division of dataframe and other, element-wise (binary operator truediv).", "truncate([before, after, axis, copy])", "Truncate a Series or DataFrame before and after some index value.", "tshift([periods, freq, axis])", "(DEPRECATED) Shift the time index, using the index's frequency if available.", "tz_convert(tz[, axis, level, copy])", "Convert tz-aware axis to target time zone.", "tz_localize(tz[, axis, level, copy, ...])", "Localize tz-naive index of a Series or DataFrame to target time zone.", "unstack([level, fill_value])", "Pivot a level of the (necessarily hierarchical) index labels.", "update(other[, join, overwrite, ...])", "Modify in place using non-NA values from another DataFrame.", "value_counts([subset, normalize, sort, ...])", "Return a Series containing counts of unique rows in the DataFrame.", "var([axis, skipna, level, ddof, numeric_only])", "Return unbiased variance over requested axis.", "where(cond[, other, inplace, axis, level, ...])", "Replace values where the condition is False.", "xs(key[, axis, level, drop_level])", "Return cross-section from the Series/DataFrame."]}, {"name": "pandas.DataFrame.__iter__", "path": "reference/api/pandas.dataframe.__iter__", "type": "DataFrame", "text": ["Iterate over info axis.", "Info axis as iterator."]}, {"name": "pandas.DataFrame.abs", "path": "reference/api/pandas.dataframe.abs", "type": "DataFrame", "text": ["Return a Series/DataFrame with absolute numeric value of each element.", "This function only applies to elements that are all numeric.", "Series/DataFrame containing the absolute value of each element.", "See also", "Calculate the absolute value element-wise.", "Notes", "For complex inputs, 1.2 + 1j, the absolute value is \\(\\sqrt{ a^2 + b^2 }\\).", "Examples", "Absolute numeric values in a Series.", "Absolute numeric values in a Series with complex numbers.", "Absolute numeric values in a Series with a Timedelta element.", "Select rows with data closest to certain value using argsort (from StackOverflow)."]}, {"name": "pandas.DataFrame.add", "path": "reference/api/pandas.dataframe.add", "type": "DataFrame", "text": ["Get Addition of dataframe and other, element-wise (binary operator add).", "Equivalent to dataframe + other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, radd.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.add_prefix", "path": "reference/api/pandas.dataframe.add_prefix", "type": "DataFrame", "text": ["Prefix labels with string prefix.", "For Series, the row labels are prefixed. For DataFrame, the column labels are prefixed.", "The string to add before each label.", "New Series or DataFrame with updated labels.", "See also", "Suffix row labels with string suffix.", "Suffix column labels with string suffix.", "Examples"]}, {"name": "pandas.DataFrame.add_suffix", "path": "reference/api/pandas.dataframe.add_suffix", "type": "DataFrame", "text": ["Suffix labels with string suffix.", "For Series, the row labels are suffixed. For DataFrame, the column labels are suffixed.", "The string to add after each label.", "New Series or DataFrame with updated labels.", "See also", "Prefix row labels with string prefix.", "Prefix column labels with string prefix.", "Examples"]}, {"name": "pandas.DataFrame.agg", "path": "reference/api/pandas.dataframe.agg", "type": "DataFrame", "text": ["Aggregate using one or more operations over the specified axis.", "Function to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply.", "Accepted combinations are:", "function", "string function name", "list of functions and/or function names, e.g. [np.sum, 'mean']", "dict of axis labels -> functions, function names or list of such.", "If 0 or \u2018index\u2019: apply function to each column. If 1 or \u2018columns\u2019: apply function to each row.", "Positional arguments to pass to func.", "Keyword arguments to pass to func.", "The return can be:", "scalar : when Series.agg is called with single function", "Series : when DataFrame.agg is called with a single function", "DataFrame : when DataFrame.agg is called with several functions", "Return scalar, Series or DataFrame.", "See also", "Perform any type of operations.", "Perform transformation type operations.", "Perform operations over groups.", "Perform operations over resampled bins.", "Perform operations over rolling window.", "Perform operations over expanding window.", "Perform operation over exponential weighted window.", "Notes", "agg is an alias for aggregate. Use the alias.", "Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.", "A passed user-defined-function will be passed a Series for evaluation.", "Examples", "Aggregate these functions over the rows.", "Different aggregations per column.", "Aggregate different functions over the columns and rename the index of the resulting DataFrame.", "Aggregate over the columns."]}, {"name": "pandas.DataFrame.aggregate", "path": "reference/api/pandas.dataframe.aggregate", "type": "DataFrame", "text": ["Aggregate using one or more operations over the specified axis.", "Function to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply.", "Accepted combinations are:", "function", "string function name", "list of functions and/or function names, e.g. [np.sum, 'mean']", "dict of axis labels -> functions, function names or list of such.", "If 0 or \u2018index\u2019: apply function to each column. If 1 or \u2018columns\u2019: apply function to each row.", "Positional arguments to pass to func.", "Keyword arguments to pass to func.", "The return can be:", "scalar : when Series.agg is called with single function", "Series : when DataFrame.agg is called with a single function", "DataFrame : when DataFrame.agg is called with several functions", "Return scalar, Series or DataFrame.", "See also", "Perform any type of operations.", "Perform transformation type operations.", "Perform operations over groups.", "Perform operations over resampled bins.", "Perform operations over rolling window.", "Perform operations over expanding window.", "Perform operation over exponential weighted window.", "Notes", "agg is an alias for aggregate. Use the alias.", "Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.", "A passed user-defined-function will be passed a Series for evaluation.", "Examples", "Aggregate these functions over the rows.", "Different aggregations per column.", "Aggregate different functions over the columns and rename the index of the resulting DataFrame.", "Aggregate over the columns."]}, {"name": "pandas.DataFrame.align", "path": "reference/api/pandas.dataframe.align", "type": "DataFrame", "text": ["Align two objects on their axes with the specified join method.", "Join method is specified for each axis Index.", "Align on index (0), columns (1), or both (None).", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Always returns new objects. If copy=False and no reindexing is required then original objects are returned.", "Value to use for missing values. Defaults to NaN, but can be any \u201ccompatible\u201d value.", "Method to use for filling holes in reindexed Series:", "pad / ffill: propagate last valid observation forward to next valid.", "backfill / bfill: use NEXT valid observation to fill gap.", "If method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None.", "Filling axis, method and limit.", "Broadcast values along this axis, if aligning two objects of different dimensions.", "Aligned objects.", "Examples", "Align on columns:", "We can also align on the index:", "Finally, the default axis=None will align on both index and columns:"]}, {"name": "pandas.DataFrame.all", "path": "reference/api/pandas.dataframe.all", "type": "DataFrame", "text": ["Return whether all elements are True, potentially over an axis.", "Returns True unless there at least one element within a series or along a Dataframe axis that is False or equivalent (e.g. zero or empty).", "Indicate which axis or axes should be reduced.", "0 / \u2018index\u2019 : reduce the index, return a Series whose index is the original column labels.", "1 / \u2018columns\u2019 : reduce the columns, return a Series whose index is the original index.", "None : reduce all axes, return a scalar.", "Include only boolean columns. If None, will attempt to use everything, then use only boolean data. Not implemented for Series.", "Exclude NA/null values. If the entire row/column is NA and skipna is True, then the result will be True, as for an empty row/column. If skipna is False, then NA are treated as True, because these are not equal to zero.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.", "Additional keywords have no effect but might be accepted for compatibility with NumPy.", "If level is specified, then, DataFrame is returned; otherwise, Series is returned.", "See also", "Return True if all elements are True.", "Return True if one (or more) elements are True.", "Examples", "Series", "DataFrames", "Create a dataframe from a dictionary.", "Default behaviour checks if column-wise values all return True.", "Specify axis='columns' to check if row-wise values all return True.", "Or axis=None for whether every value is True."]}, {"name": "pandas.DataFrame.any", "path": "reference/api/pandas.dataframe.any", "type": "DataFrame", "text": ["Return whether any element is True, potentially over an axis.", "Returns False unless there is at least one element within a series or along a Dataframe axis that is True or equivalent (e.g. non-zero or non-empty).", "Indicate which axis or axes should be reduced.", "0 / \u2018index\u2019 : reduce the index, return a Series whose index is the original column labels.", "1 / \u2018columns\u2019 : reduce the columns, return a Series whose index is the original index.", "None : reduce all axes, return a scalar.", "Include only boolean columns. If None, will attempt to use everything, then use only boolean data. Not implemented for Series.", "Exclude NA/null values. If the entire row/column is NA and skipna is True, then the result will be False, as for an empty row/column. If skipna is False, then NA are treated as True, because these are not equal to zero.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.", "Additional keywords have no effect but might be accepted for compatibility with NumPy.", "If level is specified, then, DataFrame is returned; otherwise, Series is returned.", "See also", "Numpy version of this method.", "Return whether any element is True.", "Return whether all elements are True.", "Return whether any element is True over requested axis.", "Return whether all elements are True over requested axis.", "Examples", "Series", "For Series input, the output is a scalar indicating whether any element is True.", "DataFrame", "Whether each column contains at least one True element (the default).", "Aggregating over the columns.", "Aggregating over the entire DataFrame with axis=None.", "any for an empty DataFrame is an empty Series."]}, {"name": "pandas.DataFrame.append", "path": "reference/api/pandas.dataframe.append", "type": "DataFrame", "text": ["Append rows of other to the end of caller, returning a new object.", "Columns in other that are not in the caller are added as new columns.", "The data to append.", "If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.", "If True, raise ValueError on creating index with duplicates.", "Sort columns if the columns of self and other are not aligned.", "Changed in version 1.0.0: Changed to not sort by default.", "A new DataFrame consisting of the rows of caller and the rows of other.", "See also", "General function to concatenate DataFrame or Series objects.", "Notes", "If a list of dict/series is passed and the keys are all contained in the DataFrame\u2019s index, the order of the columns in the resulting DataFrame will be unchanged.", "Iteratively appending rows to a DataFrame can be more computationally intensive than a single concatenate. A better solution is to append those rows to a list and then concatenate the list with the original DataFrame all at once.", "Examples", "With ignore_index set to True:", "The following, while not recommended methods for generating DataFrames, show two ways to generate a DataFrame from multiple data sources.", "Less efficient:", "More efficient:"]}, {"name": "pandas.DataFrame.apply", "path": "reference/api/pandas.dataframe.apply", "type": "DataFrame", "text": ["Apply a function along an axis of the DataFrame.", "Objects passed to the function are Series objects whose index is either the DataFrame\u2019s index (axis=0) or the DataFrame\u2019s columns (axis=1). By default (result_type=None), the final return type is inferred from the return type of the applied function. Otherwise, it depends on the result_type argument.", "Function to apply to each column or row.", "Axis along which the function is applied:", "0 or \u2018index\u2019: apply function to each column.", "1 or \u2018columns\u2019: apply function to each row.", "Determines if row or column is passed as a Series or ndarray object:", "False : passes each row or column as a Series to the function.", "True : the passed function will receive ndarray objects instead. If you are just applying a NumPy reduction function this will achieve much better performance.", "These only act when axis=1 (columns):", "\u2018expand\u2019 : list-like results will be turned into columns.", "\u2018reduce\u2019 : returns a Series if possible rather than expanding list-like results. This is the opposite of \u2018expand\u2019.", "\u2018broadcast\u2019 : results will be broadcast to the original shape of the DataFrame, the original index and columns will be retained.", "The default behaviour (None) depends on the return value of the applied function: list-like results will be returned as a Series of those. However if the apply function returns a Series these are expanded to columns.", "Positional arguments to pass to func in addition to the array/series.", "Additional keyword arguments to pass as keywords arguments to func.", "Result of applying func along the given axis of the DataFrame.", "See also", "For elementwise operations.", "Only perform aggregating type operations.", "Only perform transforming type operations.", "Notes", "Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.", "Examples", "Using a numpy universal function (in this case the same as np.sqrt(df)):", "Using a reducing function on either axis", "Returning a list-like will result in a Series", "Passing result_type='expand' will expand list-like results to columns of a Dataframe", "Returning a Series inside the function is similar to passing result_type='expand'. The resulting column names will be the Series index.", "Passing result_type='broadcast' will ensure the same shape result, whether list-like or scalar is returned by the function, and broadcast it along the axis. The resulting column names will be the originals."]}, {"name": "pandas.DataFrame.applymap", "path": "reference/api/pandas.dataframe.applymap", "type": "DataFrame", "text": ["Apply a function to a Dataframe elementwise.", "This method applies a function that accepts and returns a scalar to every element of a DataFrame.", "Python function, returns a single value from a single value.", "If \u2018ignore\u2019, propagate NaN values, without passing them to func.", "New in version 1.2.", "Additional keyword arguments to pass as keywords arguments to func.", "New in version 1.3.0.", "Transformed DataFrame.", "See also", "Apply a function along input axis of DataFrame.", "Examples", "Like Series.map, NA values can be ignored:", "Note that a vectorized version of func often exists, which will be much faster. You could square each number elementwise.", "But it\u2019s better to avoid applymap in that case."]}, {"name": "pandas.DataFrame.asfreq", "path": "reference/api/pandas.dataframe.asfreq", "type": "DataFrame", "text": ["Convert time series to specified frequency.", "Returns the original data conformed to a new index with the specified frequency.", "If the index of this DataFrame is a PeriodIndex, the new index is the result of transforming the original index with PeriodIndex.asfreq (so the original index will map one-to-one to the new index).", "Otherwise, the new index will be equivalent to pd.date_range(start, end,\nfreq=freq) where start and end are, respectively, the first and last entries in the original index (see pandas.date_range()). The values corresponding to any timesteps in the new index which were not present in the original index will be null (NaN), unless a method for filling such unknowns is provided (see the method parameter below).", "The resample() method is more appropriate if an operation on each group of timesteps (such as an aggregate) is necessary to represent the data at the new frequency.", "Frequency DateOffset or string.", "Method to use for filling holes in reindexed Series (note this does not fill NaNs that already were present):", "\u2018pad\u2019 / \u2018ffill\u2019: propagate last valid observation forward to next valid", "\u2018backfill\u2019 / \u2018bfill\u2019: use NEXT valid observation to fill.", "For PeriodIndex only (see PeriodIndex.asfreq).", "Whether to reset output index to midnight.", "Value to use for missing values, applied during upsampling (note this does not fill NaNs that already were present).", "DataFrame object reindexed to the specified frequency.", "See also", "Conform DataFrame to new index with optional filling logic.", "Notes", "To learn more about the frequency strings, please see this link.", "Examples", "Start by creating a series with 4 one minute timestamps.", "Upsample the series into 30 second bins.", "Upsample again, providing a fill value.", "Upsample again, providing a method."]}, {"name": "pandas.DataFrame.asof", "path": "reference/api/pandas.dataframe.asof", "type": "DataFrame", "text": ["Return the last row(s) without any NaNs before where.", "The last row (for each element in where, if list) without any NaN is taken. In case of a DataFrame, the last row without NaN considering only the subset of columns (if not None)", "If there is no good value, NaN is returned for a Series or a Series of NaN values for a DataFrame", "Date(s) before which the last row(s) are returned.", "For DataFrame, if not None, only use these columns to check for NaNs.", "The return can be:", "scalar : when self is a Series and where is a scalar", "Series: when self is a Series and where is an array-like, or when self is a DataFrame and where is a scalar", "DataFrame : when self is a DataFrame and where is an array-like", "Return scalar, Series, or DataFrame.", "See also", "Perform an asof merge. Similar to left join.", "Notes", "Dates are assumed to be sorted. Raises if this is not the case.", "Examples", "A Series and a scalar where.", "For a sequence where, a Series is returned. The first value is NaN, because the first element of where is before the first index value.", "Missing values are not considered. The following is 2.0, not NaN, even though NaN is at the index location for 30.", "Take all columns into consideration", "Take a single column into consideration"]}, {"name": "pandas.DataFrame.assign", "path": "reference/api/pandas.dataframe.assign", "type": "DataFrame", "text": ["Assign new columns to a DataFrame.", "Returns a new object with all original columns in addition to new ones. Existing columns that are re-assigned will be overwritten.", "The column names are keywords. If the values are callable, they are computed on the DataFrame and assigned to the new columns. The callable must not change input DataFrame (though pandas doesn\u2019t check it). If the values are not callable, (e.g. a Series, scalar, or array), they are simply assigned.", "A new DataFrame with the new columns in addition to all the existing columns.", "Notes", "Assigning multiple columns within the same assign is possible. Later items in \u2018**kwargs\u2019 may refer to newly created or modified columns in \u2018df\u2019; items are computed and assigned into \u2018df\u2019 in order.", "Examples", "Where the value is a callable, evaluated on df:", "Alternatively, the same behavior can be achieved by directly referencing an existing Series or sequence:", "You can create multiple columns within the same assign where one of the columns depends on another one defined within the same assign:"]}, {"name": "pandas.DataFrame.astype", "path": "reference/api/pandas.dataframe.astype", "type": "DataFrame", "text": ["Cast a pandas object to a specified dtype dtype.", "Use a numpy.dtype or Python type to cast entire pandas object to the same type. Alternatively, use {col: dtype, \u2026}, where col is a column label and dtype is a numpy.dtype or Python type to cast one or more of the DataFrame\u2019s columns to column-specific types.", "Return a copy when copy=True (be very careful setting copy=False as changes to values then may propagate to other pandas objects).", "Control raising of exceptions on invalid data for provided dtype.", "raise : allow exceptions to be raised", "ignore : suppress exceptions. On error return original object.", "See also", "Convert argument to datetime.", "Convert argument to timedelta.", "Convert argument to a numeric type.", "Cast a numpy array to a specified type.", "Notes", "Deprecated since version 1.3.0: Using astype to convert from timezone-naive dtype to timezone-aware dtype is deprecated and will raise in a future version. Use Series.dt.tz_localize() instead.", "Examples", "Create a DataFrame:", "Cast all columns to int32:", "Cast col1 to int32 using a dictionary:", "Create a series:", "Convert to categorical type:", "Convert to ordered categorical type with custom ordering:", "Note that using copy=False and changing data on a new pandas object may propagate changes:", "Create a series of dates:"]}, {"name": "pandas.DataFrame.at", "path": "reference/api/pandas.dataframe.at", "type": "DataFrame", "text": ["Access a single value for a row/column label pair.", "Similar to loc, in that both provide label-based lookups. Use at if you only need to get or set a single value in a DataFrame or Series.", "If \u2018label\u2019 does not exist in DataFrame.", "See also", "Access a single value for a row/column pair by integer position.", "Access a group of rows and columns by label(s).", "Access a single value using a label.", "Examples", "Get value at specified row/column pair", "Set value at specified row/column pair", "Get value within a Series"]}, {"name": "pandas.DataFrame.at_time", "path": "reference/api/pandas.dataframe.at_time", "type": "DataFrame", "text": ["Select values at particular time of day (e.g., 9:30AM).", "If the index is not a DatetimeIndex", "See also", "Select values between particular times of the day.", "Select initial periods of time series based on a date offset.", "Select final periods of time series based on a date offset.", "Get just the index locations for values at particular time of the day.", "Examples"]}, {"name": "pandas.DataFrame.attrs", "path": "reference/api/pandas.dataframe.attrs", "type": "DataFrame", "text": ["Dictionary of global attributes of this dataset.", "Warning", "attrs is experimental and may change without warning.", "See also", "Global flags applying to this object."]}, {"name": "pandas.DataFrame.axes", "path": "reference/api/pandas.dataframe.axes", "type": "DataFrame", "text": ["Return a list representing the axes of the DataFrame.", "It has the row axis labels and column axis labels as the only members. They are returned in that order.", "Examples"]}, {"name": "pandas.DataFrame.backfill", "path": "reference/api/pandas.dataframe.backfill", "type": "DataFrame", "text": ["Synonym for DataFrame.fillna() with method='bfill'.", "Object with missing values filled or None if inplace=True."]}, {"name": "pandas.DataFrame.between_time", "path": "reference/api/pandas.dataframe.between_time", "type": "DataFrame", "text": ["Select values between particular times of the day (e.g., 9:00-9:30 AM).", "By setting start_time to be later than end_time, you can get the times that are not between the two times.", "Initial time as a time filter limit.", "End time as a time filter limit.", "Whether the start time needs to be included in the result.", "Deprecated since version 1.4.0: Arguments include_start and include_end have been deprecated to standardize boundary inputs. Use inclusive instead, to set each bound as closed or open.", "Whether the end time needs to be included in the result.", "Deprecated since version 1.4.0: Arguments include_start and include_end have been deprecated to standardize boundary inputs. Use inclusive instead, to set each bound as closed or open.", "Include boundaries; whether to set each bound as closed or open.", "Determine range time on index or columns value.", "Data from the original object filtered to the specified dates range.", "If the index is not a DatetimeIndex", "See also", "Select values at a particular time of the day.", "Select initial periods of time series based on a date offset.", "Select final periods of time series based on a date offset.", "Get just the index locations for values between particular times of the day.", "Examples", "You get the times that are not between two times by setting start_time later than end_time:"]}, {"name": "pandas.DataFrame.bfill", "path": "reference/api/pandas.dataframe.bfill", "type": "DataFrame", "text": ["Synonym for DataFrame.fillna() with method='bfill'.", "Object with missing values filled or None if inplace=True."]}, {"name": "pandas.DataFrame.bool", "path": "reference/api/pandas.dataframe.bool", "type": "DataFrame", "text": ["Return the bool of a single element Series or DataFrame.", "This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).", "The value in the Series or DataFrame.", "See also", "Change the data type of a Series, including to boolean.", "Change the data type of a DataFrame, including to boolean.", "NumPy boolean data type, used by pandas for boolean values.", "Examples", "The method will only work for single element objects with a boolean value:"]}, {"name": "pandas.DataFrame.boxplot", "path": "reference/api/pandas.dataframe.boxplot", "type": "DataFrame", "text": ["Make a box plot from DataFrame columns.", "Make a box-and-whisker plot from DataFrame columns, optionally grouped by some other columns. A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of box to show the range of the data. By default, they extend no more than 1.5 * IQR (IQR = Q3 - Q1) from the edges of the box, ending at the farthest data point within that interval. Outliers are plotted as separate dots.", "For further details see Wikipedia\u2019s entry for boxplot.", "Column name or list of names, or vector. Can be any valid input to pandas.DataFrame.groupby().", "Column in the DataFrame to pandas.DataFrame.groupby(). One box-plot will be done per value of columns in by.", "The matplotlib axes to be used by boxplot.", "Tick label font size in points or as a string (e.g., large).", "The rotation angle of labels (in degrees) with respect to the screen coordinate system.", "Setting this to True will show the grid.", "The size of the figure to create in matplotlib.", "For example, (3, 5) will display the subplots using 3 columns and 5 rows, starting from the top-left.", "The kind of object to return. The default is axes.", "\u2018axes\u2019 returns the matplotlib axes the boxplot is drawn on.", "\u2018dict\u2019 returns a dictionary whose values are the matplotlib Lines of the boxplot.", "\u2018both\u2019 returns a namedtuple with the axes and dict.", "when grouping with by, a Series mapping columns to return_type is returned.", "If return_type is None, a NumPy array of axes with the same shape as layout is returned.", "Backend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.", "New in version 1.0.0.", "All other plotting keyword arguments to be passed to matplotlib.pyplot.boxplot().", "See Notes.", "See also", "Make a histogram.", "Matplotlib equivalent plot.", "Notes", "The return type depends on the return_type parameter:", "\u2018axes\u2019 : object of class matplotlib.axes.Axes", "\u2018dict\u2019 : dict of matplotlib.lines.Line2D objects", "\u2018both\u2019 : a namedtuple with structure (ax, lines)", "For data grouped with by, return a Series of the above or a numpy array:", "Series", "array (for return_type = None)", "Use return_type='dict' when you want to tweak the appearance of the lines after plotting. In this case a dict containing the Lines making up the boxes, caps, fliers, medians, and whiskers is returned.", "Examples", "Boxplots can be created for every column in the dataframe by df.boxplot() or indicating the columns to be used:", "Boxplots of variables distributions grouped by the values of a third variable can be created using the option by. For instance:", "A list of strings (i.e. ['X', 'Y']) can be passed to boxplot in order to group the data by combination of the variables in the x-axis:", "The layout of boxplot can be adjusted giving a tuple to layout:", "Additional formatting can be done to the boxplot, like suppressing the grid (grid=False), rotating the labels in the x-axis (i.e. rot=45) or changing the fontsize (i.e. fontsize=15):", "The parameter return_type can be used to select the type of element returned by boxplot. When return_type='axes' is selected, the matplotlib axes on which the boxplot is drawn are returned:", "When grouping with by, a Series mapping columns to return_type is returned:", "If return_type is None, a NumPy array of axes with the same shape as layout is returned:"]}, {"name": "pandas.DataFrame.clip", "path": "reference/api/pandas.dataframe.clip", "type": "DataFrame", "text": ["Trim values at input threshold(s).", "Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis.", "Minimum threshold value. All values below this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.", "Maximum threshold value. All values above this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.", "Align object with lower and upper along the given axis.", "Whether to perform the operation in place on the data.", "Additional keywords have no effect but might be accepted for compatibility with numpy.", "Same type as calling object with the values outside the clip boundaries replaced or None if inplace=True.", "See also", "Trim values at input threshold in series.", "Trim values at input threshold in dataframe.", "Clip (limit) the values in an array.", "Examples", "Clips per column using lower and upper thresholds:", "Clips using specific lower and upper thresholds per column element:", "Clips using specific lower threshold per column element, with missing values:"]}, {"name": "pandas.DataFrame.columns", "path": "reference/api/pandas.dataframe.columns", "type": "DataFrame", "text": ["The column labels of the DataFrame."]}, {"name": "pandas.DataFrame.combine", "path": "reference/api/pandas.dataframe.combine", "type": "DataFrame", "text": ["Perform column-wise combine with another DataFrame.", "Combines a DataFrame with other DataFrame using func to element-wise combine columns. The row and column indexes of the resulting DataFrame will be the union of the two.", "The DataFrame to merge column-wise.", "Function that takes two series as inputs and return a Series or a scalar. Used to merge the two dataframes column by columns.", "The value to fill NaNs with prior to passing any column to the merge func.", "If True, columns in self that do not exist in other will be overwritten with NaNs.", "Combination of the provided DataFrames.", "See also", "Combine two DataFrame objects and default to non-null values in frame calling the method.", "Examples", "Combine using a simple function that chooses the smaller column.", "Example using a true element-wise combine function.", "Using fill_value fills Nones prior to passing the column to the merge function.", "However, if the same element in both dataframes is None, that None is preserved", "Example that demonstrates the use of overwrite and behavior when the axis differ between the dataframes.", "Demonstrating the preference of the passed in dataframe."]}, {"name": "pandas.DataFrame.combine_first", "path": "reference/api/pandas.dataframe.combine_first", "type": "DataFrame", "text": ["Update null elements with value in the same location in other.", "Combine two DataFrame objects by filling null values in one DataFrame with non-null values from other DataFrame. The row and column indexes of the resulting DataFrame will be the union of the two.", "Provided DataFrame to use to fill null values.", "The result of combining the provided DataFrame with the other object.", "See also", "Perform series-wise operation on two DataFrames using a given function.", "Examples", "Null values still persist if the location of that null value does not exist in other"]}, {"name": "pandas.DataFrame.compare", "path": "reference/api/pandas.dataframe.compare", "type": "DataFrame", "text": ["Compare to another DataFrame and show the differences.", "New in version 1.1.0.", "Object to compare with.", "Determine which axis to align the comparison on.", "with rows drawn alternately from self and other.", "with columns drawn alternately from self and other.", "If true, all rows and columns are kept. Otherwise, only the ones with different values are kept.", "If true, the result keeps values that are equal. Otherwise, equal values are shown as NaNs.", "DataFrame that shows the differences stacked side by side.", "The resulting index will be a MultiIndex with \u2018self\u2019 and \u2018other\u2019 stacked alternately at the inner level.", "When the two DataFrames don\u2019t have identical labels or shape.", "See also", "Compare with another Series and show differences.", "Test whether two objects contain the same elements.", "Notes", "Matching NaNs will not appear as a difference.", "Can only compare identically-labeled (i.e. same shape, identical row and column labels) DataFrames", "Examples", "Align the differences on columns", "Stack the differences on rows", "Keep the equal values", "Keep all original rows and columns", "Keep all original rows and columns and also all original values"]}, {"name": "pandas.DataFrame.convert_dtypes", "path": "reference/api/pandas.dataframe.convert_dtypes", "type": "General utility functions", "text": ["Convert columns to best possible dtypes using dtypes supporting pd.NA.", "New in version 1.0.0.", "Whether object dtypes should be converted to the best possible types.", "Whether object dtypes should be converted to StringDtype().", "Whether, if possible, conversion can be done to integer extension types.", "Whether object dtypes should be converted to BooleanDtypes().", "Whether, if possible, conversion can be done to floating extension types. If convert_integer is also True, preference will be give to integer dtypes if the floats can be faithfully casted to integers.", "New in version 1.2.0.", "Copy of input object with new dtype.", "See also", "Infer dtypes of objects.", "Convert argument to datetime.", "Convert argument to timedelta.", "Convert argument to a numeric type.", "Notes", "By default, convert_dtypes will attempt to convert a Series (or each Series in a DataFrame) to dtypes that support pd.NA. By using the options convert_string, convert_integer, convert_boolean and convert_boolean, it is possible to turn off individual conversions to StringDtype, the integer extension types, BooleanDtype or floating extension types, respectively.", "For object-dtyped columns, if infer_objects is True, use the inference rules as during normal Series/DataFrame construction. Then, if possible, convert to StringDtype, BooleanDtype or an appropriate integer or floating extension type, otherwise leave as object.", "If the dtype is integer, convert to an appropriate integer extension type.", "If the dtype is numeric, and consists of all integers, convert to an appropriate integer extension type. Otherwise, convert to an appropriate floating extension type.", "Changed in version 1.2: Starting with pandas 1.2, this method also converts float columns to the nullable floating extension type.", "In the future, as new dtypes are added that support pd.NA, the results of this method will change to support those new dtypes.", "Examples", "Start with a DataFrame with default dtypes.", "Convert the DataFrame to use best possible dtypes.", "Start with a Series of strings and missing data represented by np.nan.", "Obtain a Series with dtype StringDtype."]}, {"name": "pandas.DataFrame.copy", "path": "reference/api/pandas.dataframe.copy", "type": "DataFrame", "text": ["Make a copy of this object\u2019s indices and data.", "When deep=True (default), a new object will be created with a copy of the calling object\u2019s data and indices. Modifications to the data or indices of the copy will not be reflected in the original object (see notes below).", "When deep=False, a new object will be created without copying the calling object\u2019s data or index (only references to the data and index are copied). Any changes to the data of the original will be reflected in the shallow copy (and vice versa).", "Make a deep copy, including a copy of the data and the indices. With deep=False neither the indices nor the data are copied.", "Object type matches caller.", "Notes", "When deep=True, data is copied but actual Python objects will not be copied recursively, only the reference to the object. This is in contrast to copy.deepcopy in the Standard Library, which recursively copies object data (see examples below).", "While Index objects are copied when deep=True, the underlying numpy array is not copied for performance reasons. Since Index is immutable, the underlying data can be safely shared and a copy is not needed.", "Examples", "Shallow copy versus default (deep) copy:", "Shallow copy shares data and index with original.", "Deep copy has own copy of data and index.", "Updates to the data shared by shallow copy and original is reflected in both; deep copy remains unchanged.", "Note that when copying an object containing Python objects, a deep copy will copy the data, but will not do so recursively. Updating a nested data object will be reflected in the deep copy."]}, {"name": "pandas.DataFrame.corr", "path": "reference/api/pandas.dataframe.corr", "type": "DataFrame", "text": ["Compute pairwise correlation of columns, excluding NA/null values.", "Method of correlation:", "pearson : standard correlation coefficient", "kendall : Kendall Tau correlation coefficient", "spearman : Spearman rank correlation", "and returning a float. Note that the returned matrix from corr will have 1 along the diagonals and will be symmetric regardless of the callable\u2019s behavior.", "Minimum number of observations required per pair of columns to have a valid result. Currently only available for Pearson and Spearman correlation.", "Correlation matrix.", "See also", "Compute pairwise correlation with another DataFrame or Series.", "Compute the correlation between two Series.", "Examples"]}, {"name": "pandas.DataFrame.corrwith", "path": "reference/api/pandas.dataframe.corrwith", "type": "DataFrame", "text": ["Compute pairwise correlation.", "Pairwise correlation is computed between rows or columns of DataFrame with rows or columns of Series or DataFrame. DataFrames are first aligned along both axes before computing the correlations.", "Object with which to compute correlations.", "The axis to use. 0 or \u2018index\u2019 to compute column-wise, 1 or \u2018columns\u2019 for row-wise.", "Drop missing indices from result.", "Method of correlation:", "pearson : standard correlation coefficient", "kendall : Kendall Tau correlation coefficient", "spearman : Spearman rank correlation", "and returning a float.", "Pairwise correlations.", "See also", "Compute pairwise correlation of columns."]}, {"name": "pandas.DataFrame.count", "path": "reference/api/pandas.dataframe.count", "type": "DataFrame", "text": ["Count non-NA cells for each column or row.", "The values None, NaN, NaT, and optionally numpy.inf (depending on pandas.options.mode.use_inf_as_na) are considered NA.", "If 0 or \u2018index\u2019 counts are generated for each column. If 1 or \u2018columns\u2019 counts are generated for each row.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a DataFrame. A str specifies the level name.", "Include only float, int or boolean data.", "For each column/row the number of non-NA/null entries. If level is specified returns a DataFrame.", "See also", "Number of non-NA elements in a Series.", "Count unique combinations of columns.", "Number of DataFrame rows and columns (including NA elements).", "Boolean same-sized DataFrame showing places of NA elements.", "Examples", "Constructing DataFrame from a dictionary:", "Notice the uncounted NA values:", "Counts for each row:"]}, {"name": "pandas.DataFrame.cov", "path": "reference/api/pandas.dataframe.cov", "type": "DataFrame", "text": ["Compute pairwise covariance of columns, excluding NA/null values.", "Compute the pairwise covariance among the series of a DataFrame. The returned data frame is the covariance matrix of the columns of the DataFrame.", "Both NA and null values are automatically excluded from the calculation. (See the note below about bias from missing values.) A threshold can be set for the minimum number of observations for each value created. Comparisons with observations below this threshold will be returned as NaN.", "This method is generally used for the analysis of time series data to understand the relationship between different measures across time.", "Minimum number of observations required per pair of columns to have a valid result.", "Delta degrees of freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "New in version 1.1.0.", "The covariance matrix of the series of the DataFrame.", "See also", "Compute covariance with another Series.", "Exponential weighted sample covariance.", "Expanding sample covariance.", "Rolling sample covariance.", "Notes", "Returns the covariance matrix of the DataFrame\u2019s time series. The covariance is normalized by N-ddof.", "For DataFrames that have Series that are missing data (assuming that data is missing at random) the returned covariance matrix will be an unbiased estimate of the variance and covariance between the member Series.", "However, for many applications this estimate may not be acceptable because the estimate covariance matrix is not guaranteed to be positive semi-definite. This could lead to estimate correlations having absolute values which are greater than one, and/or a non-invertible covariance matrix. See Estimation of covariance matrices for more details.", "Examples", "Minimum number of periods", "This method also supports an optional min_periods keyword that specifies the required minimum number of non-NA observations for each column pair in order to have a valid result:"]}, {"name": "pandas.DataFrame.cummax", "path": "reference/api/pandas.dataframe.cummax", "type": "DataFrame", "text": ["Return cumulative maximum over a DataFrame or Series axis.", "Returns a DataFrame or Series of the same size containing the cumulative maximum.", "The index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "Additional keywords have no effect but might be accepted for compatibility with NumPy.", "Return cumulative maximum of Series or DataFrame.", "See also", "Similar functionality but ignores NaN values.", "Return the maximum over DataFrame axis.", "Return cumulative maximum over DataFrame axis.", "Return cumulative minimum over DataFrame axis.", "Return cumulative sum over DataFrame axis.", "Return cumulative product over DataFrame axis.", "Examples", "Series", "By default, NA values are ignored.", "To include NA values in the operation, use skipna=False", "DataFrame", "By default, iterates over rows and finds the maximum in each column. This is equivalent to axis=None or axis='index'.", "To iterate over columns and find the maximum in each row, use axis=1"]}, {"name": "pandas.DataFrame.cummin", "path": "reference/api/pandas.dataframe.cummin", "type": "DataFrame", "text": ["Return cumulative minimum over a DataFrame or Series axis.", "Returns a DataFrame or Series of the same size containing the cumulative minimum.", "The index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "Additional keywords have no effect but might be accepted for compatibility with NumPy.", "Return cumulative minimum of Series or DataFrame.", "See also", "Similar functionality but ignores NaN values.", "Return the minimum over DataFrame axis.", "Return cumulative maximum over DataFrame axis.", "Return cumulative minimum over DataFrame axis.", "Return cumulative sum over DataFrame axis.", "Return cumulative product over DataFrame axis.", "Examples", "Series", "By default, NA values are ignored.", "To include NA values in the operation, use skipna=False", "DataFrame", "By default, iterates over rows and finds the minimum in each column. This is equivalent to axis=None or axis='index'.", "To iterate over columns and find the minimum in each row, use axis=1"]}, {"name": "pandas.DataFrame.cumprod", "path": "reference/api/pandas.dataframe.cumprod", "type": "DataFrame", "text": ["Return cumulative product over a DataFrame or Series axis.", "Returns a DataFrame or Series of the same size containing the cumulative product.", "The index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "Additional keywords have no effect but might be accepted for compatibility with NumPy.", "Return cumulative product of Series or DataFrame.", "See also", "Similar functionality but ignores NaN values.", "Return the product over DataFrame axis.", "Return cumulative maximum over DataFrame axis.", "Return cumulative minimum over DataFrame axis.", "Return cumulative sum over DataFrame axis.", "Return cumulative product over DataFrame axis.", "Examples", "Series", "By default, NA values are ignored.", "To include NA values in the operation, use skipna=False", "DataFrame", "By default, iterates over rows and finds the product in each column. This is equivalent to axis=None or axis='index'.", "To iterate over columns and find the product in each row, use axis=1"]}, {"name": "pandas.DataFrame.cumsum", "path": "reference/api/pandas.dataframe.cumsum", "type": "DataFrame", "text": ["Return cumulative sum over a DataFrame or Series axis.", "Returns a DataFrame or Series of the same size containing the cumulative sum.", "The index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "Additional keywords have no effect but might be accepted for compatibility with NumPy.", "Return cumulative sum of Series or DataFrame.", "See also", "Similar functionality but ignores NaN values.", "Return the sum over DataFrame axis.", "Return cumulative maximum over DataFrame axis.", "Return cumulative minimum over DataFrame axis.", "Return cumulative sum over DataFrame axis.", "Return cumulative product over DataFrame axis.", "Examples", "Series", "By default, NA values are ignored.", "To include NA values in the operation, use skipna=False", "DataFrame", "By default, iterates over rows and finds the sum in each column. This is equivalent to axis=None or axis='index'.", "To iterate over columns and find the sum in each row, use axis=1"]}, {"name": "pandas.DataFrame.describe", "path": "reference/api/pandas.dataframe.describe", "type": "DataFrame", "text": ["Generate descriptive statistics.", "Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values.", "Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. Refer to the notes below for more detail.", "The percentiles to include in the output. All should fall between 0 and 1. The default is [.25, .5, .75], which returns the 25th, 50th, and 75th percentiles.", "A white list of data types to include in the result. Ignored for Series. Here are the options:", "\u2018all\u2019 : All columns of the input will be included in the output.", "A list-like of dtypes : Limits the results to the provided data types. To limit the result to numeric types submit numpy.number. To limit it instead to object columns submit the numpy.object data type. Strings can also be used in the style of select_dtypes (e.g. df.describe(include=['O'])). To select pandas categorical columns, use 'category'", "None (default) : The result will include all numeric columns.", "A black list of data types to omit from the result. Ignored for Series. Here are the options:", "A list-like of dtypes : Excludes the provided data types from the result. To exclude numeric types submit numpy.number. To exclude object columns submit the data type numpy.object. Strings can also be used in the style of select_dtypes (e.g. df.describe(exclude=['O'])). To exclude pandas categorical columns, use 'category'", "None (default) : The result will exclude nothing.", "Whether to treat datetime dtypes as numeric. This affects statistics calculated for the column. For DataFrame input, this also controls whether datetime columns are included by default.", "New in version 1.1.0.", "Summary statistics of the Series or Dataframe provided.", "See also", "Count number of non-NA/null observations.", "Maximum of the values in the object.", "Minimum of the values in the object.", "Mean of the values.", "Standard deviation of the observations.", "Subset of a DataFrame including/excluding columns based on their dtype.", "Notes", "For numeric data, the result\u2019s index will include count, mean, std, min, max as well as lower, 50 and upper percentiles. By default the lower percentile is 25 and the upper percentile is 75. The 50 percentile is the same as the median.", "For object data (e.g. strings or timestamps), the result\u2019s index will include count, unique, top, and freq. The top is the most common value. The freq is the most common value\u2019s frequency. Timestamps also include the first and last items.", "If multiple object values have the highest count, then the count and top results will be arbitrarily chosen from among those with the highest count.", "For mixed data types provided via a DataFrame, the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If include='all' is provided as an option, the result will include a union of attributes of each type.", "The include and exclude parameters can be used to limit which columns in a DataFrame are analyzed for the output. The parameters are ignored when analyzing a Series.", "Examples", "Describing a numeric Series.", "Describing a categorical Series.", "Describing a timestamp Series.", "Describing a DataFrame. By default only numeric fields are returned.", "Describing all columns of a DataFrame regardless of data type.", "Describing a column from a DataFrame by accessing it as an attribute.", "Including only numeric columns in a DataFrame description.", "Including only string columns in a DataFrame description.", "Including only categorical columns from a DataFrame description.", "Excluding numeric columns from a DataFrame description.", "Excluding object columns from a DataFrame description."]}, {"name": "pandas.DataFrame.diff", "path": "reference/api/pandas.dataframe.diff", "type": "DataFrame", "text": ["First discrete difference of element.", "Calculates the difference of a Dataframe element compared with another element in the Dataframe (default is element in previous row).", "Periods to shift for calculating difference, accepts negative values.", "Take difference over rows (0) or columns (1).", "First differences of the Series.", "See also", "Percent change over given number of periods.", "Shift index by desired number of periods with an optional time freq.", "First discrete difference of object.", "Notes", "For boolean dtypes, this uses operator.xor() rather than operator.sub(). The result is calculated according to current dtype in Dataframe, however dtype of the result is always float64.", "Examples", "Difference with previous row", "Difference with previous column", "Difference with 3rd previous row", "Difference with following row", "Overflow in input dtype"]}, {"name": "pandas.DataFrame.div", "path": "reference/api/pandas.dataframe.div", "type": "DataFrame", "text": ["Get Floating division of dataframe and other, element-wise (binary operator truediv).", "Equivalent to dataframe / other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rtruediv.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.divide", "path": "reference/api/pandas.dataframe.divide", "type": "DataFrame", "text": ["Get Floating division of dataframe and other, element-wise (binary operator truediv).", "Equivalent to dataframe / other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rtruediv.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.dot", "path": "reference/api/pandas.dataframe.dot", "type": "DataFrame", "text": ["Compute the matrix multiplication between the DataFrame and other.", "This method computes the matrix product between the DataFrame and the values of an other Series, DataFrame or a numpy array.", "It can also be called using self @ other in Python >= 3.5.", "The other object to compute the matrix product with.", "If other is a Series, return the matrix product between self and other as a Series. If other is a DataFrame or a numpy.array, return the matrix product of self and other in a DataFrame of a np.array.", "See also", "Similar method for Series.", "Notes", "The dimensions of DataFrame and other must be compatible in order to compute the matrix multiplication. In addition, the column names of DataFrame and the index of other must contain the same values, as they will be aligned prior to the multiplication.", "The dot method for Series computes the inner product, instead of the matrix product here.", "Examples", "Here we multiply a DataFrame with a Series.", "Here we multiply a DataFrame with another DataFrame.", "Note that the dot method give the same result as @", "The dot method works also if other is an np.array.", "Note how shuffling of the objects does not change the result."]}, {"name": "pandas.DataFrame.drop", "path": "reference/api/pandas.dataframe.drop", "type": "DataFrame", "text": ["Drop specified labels from rows or columns.", "Remove rows or columns by specifying label names and corresponding axis, or by specifying directly index or column names. When using a multi-index, labels on different levels can be removed by specifying the level. See the user guide <advanced.shown_levels> for more information about the now unused levels.", "Index or column labels to drop. A tuple will be used as a single label and not treated as a list-like.", "Whether to drop labels from the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).", "Alternative to specifying axis (labels, axis=0 is equivalent to index=labels).", "Alternative to specifying axis (labels, axis=1 is equivalent to columns=labels).", "For MultiIndex, level from which the labels will be removed.", "If False, return a copy. Otherwise, do operation inplace and return None.", "If \u2018ignore\u2019, suppress error and only existing labels are dropped.", "DataFrame without the removed index or column labels or None if inplace=True.", "If any of the labels is not found in the selected axis.", "See also", "Label-location based indexer for selection by label.", "Return DataFrame with labels on given axis omitted where (all or any) data are missing.", "Return DataFrame with duplicate rows removed, optionally only considering certain columns.", "Return Series with specified index labels removed.", "Examples", "Drop columns", "Drop a row by index", "Drop columns and/or rows of MultiIndex DataFrame", "Drop a specific index combination from the MultiIndex DataFrame, i.e., drop the combination 'falcon' and 'weight', which deletes only the corresponding row"]}, {"name": "pandas.DataFrame.drop_duplicates", "path": "reference/api/pandas.dataframe.drop_duplicates", "type": "DataFrame", "text": ["Return DataFrame with duplicate rows removed.", "Considering certain columns is optional. Indexes, including time indexes are ignored.", "Only consider certain columns for identifying duplicates, by default use all of the columns.", "Determines which duplicates (if any) to keep. - first : Drop duplicates except for the first occurrence. - last : Drop duplicates except for the last occurrence. - False : Drop all duplicates.", "Whether to drop duplicates in place or to return a copy.", "If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.", "New in version 1.0.0.", "DataFrame with duplicates removed or None if inplace=True.", "See also", "Count unique combinations of columns.", "Examples", "Consider dataset containing ramen rating.", "By default, it removes duplicate rows based on all columns.", "To remove duplicates on specific column(s), use subset.", "To remove duplicates and keep last occurrences, use keep."]}, {"name": "pandas.DataFrame.droplevel", "path": "reference/api/pandas.dataframe.droplevel", "type": "DataFrame", "text": ["Return Series/DataFrame with requested index / column level(s) removed.", "If a string is given, must be the name of a level If list-like, elements must be names or positional indexes of levels.", "Axis along which the level(s) is removed:", "0 or \u2018index\u2019: remove level(s) in column.", "1 or \u2018columns\u2019: remove level(s) in row.", "Series/DataFrame with requested index / column level(s) removed.", "Examples"]}, {"name": "pandas.DataFrame.dropna", "path": "reference/api/pandas.dataframe.dropna", "type": "DataFrame", "text": ["Remove missing values.", "See the User Guide for more on which values are considered missing, and how to work with missing data.", "Determine if rows or columns which contain missing values are removed.", "0, or \u2018index\u2019 : Drop rows which contain missing values.", "1, or \u2018columns\u2019 : Drop columns which contain missing value.", "Changed in version 1.0.0: Pass tuple or list to drop on multiple axes. Only a single axis is allowed.", "Determine if row or column is removed from DataFrame, when we have at least one NA or all NA.", "\u2018any\u2019 : If any NA values are present, drop that row or column.", "\u2018all\u2019 : If all values are NA, drop that row or column.", "Require that many non-NA values.", "Labels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include.", "If True, do operation inplace and return None.", "DataFrame with NA entries dropped from it or None if inplace=True.", "See also", "Indicate missing values.", "Indicate existing (non-missing) values.", "Replace missing values.", "Drop missing values.", "Drop missing indices.", "Examples", "Drop the rows where at least one element is missing.", "Drop the columns where at least one element is missing.", "Drop the rows where all elements are missing.", "Keep only the rows with at least 2 non-NA values.", "Define in which columns to look for missing values.", "Keep the DataFrame with valid entries in the same variable."]}, {"name": "pandas.DataFrame.dtypes", "path": "reference/api/pandas.dataframe.dtypes", "type": "General utility functions", "text": ["Return the dtypes in the DataFrame.", "This returns a Series with the data type of each column. The result\u2019s index is the original DataFrame\u2019s columns. Columns with mixed types are stored with the object dtype. See the User Guide for more.", "The data type of each column.", "Examples"]}, {"name": "pandas.DataFrame.duplicated", "path": "reference/api/pandas.dataframe.duplicated", "type": "DataFrame", "text": ["Return boolean Series denoting duplicate rows.", "Considering certain columns is optional.", "Only consider certain columns for identifying duplicates, by default use all of the columns.", "Determines which duplicates (if any) to mark.", "first : Mark duplicates as True except for the first occurrence.", "last : Mark duplicates as True except for the last occurrence.", "False : Mark all duplicates as True.", "Boolean series for each duplicated rows.", "See also", "Equivalent method on index.", "Equivalent method on Series.", "Remove duplicate values from Series.", "Remove duplicate values from DataFrame.", "Examples", "Consider dataset containing ramen rating.", "By default, for each set of duplicated values, the first occurrence is set on False and all others on True.", "By using \u2018last\u2019, the last occurrence of each set of duplicated values is set on False and all others on True.", "By setting keep on False, all duplicates are True.", "To find duplicates on specific column(s), use subset."]}, {"name": "pandas.DataFrame.empty", "path": "reference/api/pandas.dataframe.empty", "type": "DataFrame", "text": ["Indicator whether Series/DataFrame is empty.", "True if Series/DataFrame is entirely empty (no items), meaning any of the axes are of length 0.", "If Series/DataFrame is empty, return True, if not return False.", "See also", "Return series without null values.", "Return DataFrame with labels on given axis omitted where (all or any) data are missing.", "Notes", "If Series/DataFrame contains only NaNs, it is still not considered empty. See the example below.", "Examples", "An example of an actual empty DataFrame. Notice the index is empty:", "If we only have NaNs in our DataFrame, it is not considered empty! We will need to drop the NaNs to make the DataFrame empty:"]}, {"name": "pandas.DataFrame.eq", "path": "reference/api/pandas.dataframe.eq", "type": "DataFrame", "text": ["Get Equal to of dataframe and other, element-wise (binary operator eq).", "Among flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.", "Equivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or columns) and level for comparison.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Result of the comparison.", "See also", "Compare DataFrames for equality elementwise.", "Compare DataFrames for inequality elementwise.", "Compare DataFrames for less than inequality or equality elementwise.", "Compare DataFrames for strictly less than inequality elementwise.", "Compare DataFrames for greater than inequality or equality elementwise.", "Compare DataFrames for strictly greater than inequality elementwise.", "Notes", "Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN).", "Examples", "Comparison with a scalar, using either the operator or method:", "When other is a Series, the columns of a DataFrame are aligned with the index of other and broadcast:", "Use the method to control the broadcast axis:", "When comparing to an arbitrary sequence, the number of columns must match the number elements in other:", "Use the method to control the axis:", "Compare to a DataFrame of different shape.", "Compare to a MultiIndex by level."]}, {"name": "pandas.DataFrame.equals", "path": "reference/api/pandas.dataframe.equals", "type": "DataFrame", "text": ["Test whether two objects contain the same elements.", "This function allows two Series or DataFrames to be compared against each other to see if they have the same shape and elements. NaNs in the same location are considered equal.", "The row/column index do not need to have the same type, as long as the values are considered equal. Corresponding columns must be of the same dtype.", "The other Series or DataFrame to be compared with the first.", "True if all elements are the same in both objects, False otherwise.", "See also", "Compare two Series objects of the same length and return a Series where each element is True if the element in each Series is equal, False otherwise.", "Compare two DataFrame objects of the same shape and return a DataFrame where each element is True if the respective element in each DataFrame is equal, False otherwise.", "Raises an AssertionError if left and right are not equal. Provides an easy interface to ignore inequality in dtypes, indexes and precision among others.", "Like assert_series_equal, but targets DataFrames.", "Return True if two arrays have the same shape and elements, False otherwise.", "Examples", "DataFrames df and exactly_equal have the same types and values for their elements and column labels, which will return True.", "DataFrames df and different_column_type have the same element types and values, but have different types for the column labels, which will still return True.", "DataFrames df and different_data_type have different types for the same values for their elements, and will return False even though their column labels are the same values and types."]}, {"name": "pandas.DataFrame.eval", "path": "reference/api/pandas.dataframe.eval", "type": "DataFrame", "text": ["Evaluate a string describing operations on DataFrame columns.", "Operates on columns only, not specific rows or elements. This allows eval to run arbitrary code, which can make you vulnerable to code injection if you pass user input to this function.", "The expression string to evaluate.", "If the expression contains an assignment, whether to perform the operation inplace and mutate the existing DataFrame. Otherwise, a new DataFrame is returned.", "See the documentation for eval() for complete details on the keyword arguments accepted by query().", "The result of the evaluation or None if inplace=True.", "See also", "Evaluates a boolean expression to query the columns of a frame.", "Can evaluate an expression or function to create new values for a column.", "Evaluate a Python expression as a string using various backends.", "Notes", "For more details see the API documentation for eval(). For detailed examples see enhancing performance with eval.", "Examples", "Assignment is allowed though by default the original DataFrame is not modified.", "Use inplace=True to modify the original DataFrame.", "Multiple columns can be assigned to using multi-line expressions:"]}, {"name": "pandas.DataFrame.ewm", "path": "reference/api/pandas.dataframe.ewm", "type": "DataFrame", "text": ["Provide exponentially weighted (EW) calculations.", "Exactly one parameter: com, span, halflife, or alpha must be provided.", "Specify decay in terms of center of mass", "\\(\\alpha = 1 / (1 + com)\\), for \\(com \\geq 0\\).", "Specify decay in terms of span", "\\(\\alpha = 2 / (span + 1)\\), for \\(span \\geq 1\\).", "Specify decay in terms of half-life", "\\(\\alpha = 1 - \\exp\\left(-\\ln(2) / halflife\\right)\\), for \\(halflife > 0\\).", "If times is specified, the time unit (str or timedelta) over which an observation decays to half its value. Only applicable to mean(), and halflife value will not apply to the other functions.", "New in version 1.1.0.", "Specify smoothing factor \\(\\alpha\\) directly", "\\(0 < \\alpha \\leq 1\\).", "Minimum number of observations in window required to have a value; otherwise, result is np.nan.", "Divide by decaying adjustment factor in beginning periods to account for imbalance in relative weightings (viewing EWMA as a moving average).", "When adjust=True (default), the EW function is calculated using weights \\(w_i = (1 - \\alpha)^i\\). For example, the EW moving average of the series [\\(x_0, x_1, ..., x_t\\)] would be:", "When adjust=False, the exponentially weighted function is calculated recursively:", "Ignore missing values when calculating weights.", "When ignore_na=False (default), weights are based on absolute positions. For example, the weights of \\(x_0\\) and \\(x_2\\) used in calculating the final weighted average of [\\(x_0\\), None, \\(x_2\\)] are \\((1-\\alpha)^2\\) and \\(1\\) if adjust=True, and \\((1-\\alpha)^2\\) and \\(\\alpha\\) if adjust=False.", "When ignore_na=True, weights are based on relative positions. For example, the weights of \\(x_0\\) and \\(x_2\\) used in calculating the final weighted average of [\\(x_0\\), None, \\(x_2\\)] are \\(1-\\alpha\\) and \\(1\\) if adjust=True, and \\(1-\\alpha\\) and \\(\\alpha\\) if adjust=False.", "If 0 or 'index', calculate across the rows.", "If 1 or 'columns', calculate across the columns.", "New in version 1.1.0.", "Only applicable to mean().", "Times corresponding to the observations. Must be monotonically increasing and datetime64[ns] dtype.", "If 1-D array like, a sequence with the same shape as the observations.", "Deprecated since version 1.4.0: If str, the name of the column in the DataFrame representing the times.", "New in version 1.4.0.", "Execute the rolling operation per single column or row ('single') or over the entire object ('table').", "This argument is only implemented when specifying engine='numba' in the method call.", "Only applicable to mean()", "See also", "Provides rolling window calculations.", "Provides expanding transformations.", "Notes", "See Windowing Operations for further usage details and examples.", "Examples", "adjust", "ignore_na", "times", "Exponentially weighted mean with weights calculated with a timedelta halflife relative to times."]}, {"name": "pandas.DataFrame.expanding", "path": "reference/api/pandas.dataframe.expanding", "type": "DataFrame", "text": ["Provide expanding window calculations.", "Minimum number of observations in window required to have a value; otherwise, result is np.nan.", "If False, set the window labels as the right edge of the window index.", "If True, set the window labels as the center of the window index.", "Deprecated since version 1.1.0.", "If 0 or 'index', roll across the rows.", "If 1 or 'columns', roll across the columns.", "Execute the rolling operation per single column or row ('single') or over the entire object ('table').", "This argument is only implemented when specifying engine='numba' in the method call.", "New in version 1.3.0.", "See also", "Provides rolling window calculations.", "Provides exponential weighted functions.", "Notes", "See Windowing Operations for further usage details and examples.", "Examples", "min_periods", "Expanding sum with 1 vs 3 observations needed to calculate a value."]}, {"name": "pandas.DataFrame.explode", "path": "reference/api/pandas.dataframe.explode", "type": "DataFrame", "text": ["Transform each element of a list-like to a row, replicating index values.", "New in version 0.25.0.", "Column(s) to explode. For multiple columns, specify a non-empty list with each element be str or tuple, and all specified columns their list-like data on same row of the frame must have matching length.", "New in version 1.3.0: Multi-column explode", "If True, the resulting index will be labeled 0, 1, \u2026, n - 1.", "New in version 1.1.0.", "Exploded lists to rows of the subset columns; index will be duplicated for these rows.", "If columns of the frame are not unique.", "If specified columns to explode is empty list.", "If specified columns to explode have not matching count of elements rowwise in the frame.", "See also", "Pivot a level of the (necessarily hierarchical) index labels.", "Unpivot a DataFrame from wide format to long format.", "Explode a DataFrame from list-like columns to long format.", "Notes", "This routine will explode list-likes including lists, tuples, sets, Series, and np.ndarray. The result dtype of the subset rows will be object. Scalars will be returned unchanged, and empty list-likes will result in a np.nan for that row. In addition, the ordering of rows in the output will be non-deterministic when exploding sets.", "Examples", "Single-column explode.", "Multi-column explode."]}, {"name": "pandas.DataFrame.ffill", "path": "reference/api/pandas.dataframe.ffill", "type": "DataFrame", "text": ["Synonym for DataFrame.fillna() with method='ffill'.", "Object with missing values filled or None if inplace=True."]}, {"name": "pandas.DataFrame.fillna", "path": "reference/api/pandas.dataframe.fillna", "type": "DataFrame", "text": ["Fill NA/NaN values using the specified method.", "Value to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of values specifying which value to use for each index (for a Series) or column (for a DataFrame). Values not in the dict/Series/DataFrame will not be filled. This value cannot be a list.", "Method to use for filling holes in reindexed Series pad / ffill: propagate last valid observation forward to next valid backfill / bfill: use next valid observation to fill gap.", "Axis along which to fill missing values.", "If True, fill in-place. Note: this will modify any other views on this object (e.g., a no-copy slice for a column in a DataFrame).", "If method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None.", "A dict of item->dtype of what to downcast if possible, or the string \u2018infer\u2019 which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible).", "Object with missing values filled or None if inplace=True.", "See also", "Fill NaN values using interpolation.", "Conform object to new index.", "Convert TimeSeries to specified frequency.", "Examples", "Replace all NaN elements with 0s.", "We can also propagate non-null values forward or backward.", "Replace all NaN elements in column \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019, with 0, 1, 2, and 3 respectively.", "Only replace the first NaN element.", "When filling using a DataFrame, replacement happens along the same column names and same indices", "Note that column D is not affected since it is not present in df2."]}, {"name": "pandas.DataFrame.filter", "path": "reference/api/pandas.dataframe.filter", "type": "DataFrame", "text": ["Subset the dataframe rows or columns according to the specified index labels.", "Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.", "Keep labels from axis which are in items.", "Keep labels from axis for which \u201clike in label == True\u201d.", "Keep labels from axis for which re.search(regex, label) == True.", "The axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, \u2018index\u2019 for Series, \u2018columns\u2019 for DataFrame.", "See also", "Access a group of rows and columns by label(s) or a boolean array.", "Notes", "The items, like, and regex parameters are enforced to be mutually exclusive.", "axis defaults to the info axis that is used when indexing with [].", "Examples"]}, {"name": "pandas.DataFrame.first", "path": "reference/api/pandas.dataframe.first", "type": "DataFrame", "text": ["Select initial periods of time series data based on a date offset.", "When having a DataFrame with dates as index, this function can select the first few rows based on a date offset.", "The offset length of the data that will be selected. For instance, \u20181M\u2019 will display all the rows having their index within the first month.", "A subset of the caller.", "If the index is not a DatetimeIndex", "See also", "Select final periods of time series based on a date offset.", "Select values at a particular time of the day.", "Select values between particular times of the day.", "Examples", "Get the rows for the first 3 days:", "Notice the data for 3 first calendar days were returned, not the first 3 days observed in the dataset, and therefore data for 2018-04-13 was not returned."]}, {"name": "pandas.DataFrame.first_valid_index", "path": "reference/api/pandas.dataframe.first_valid_index", "type": "DataFrame", "text": ["Return index for first non-NA value or None, if no NA value is found.", "Notes", "If all elements are non-NA/null, returns None. Also returns None for empty Series/DataFrame."]}, {"name": "pandas.DataFrame.flags", "path": "reference/api/pandas.dataframe.flags", "type": "DataFrame", "text": ["Get the properties associated with this pandas object.", "The available flags are", "Flags.allows_duplicate_labels", "See also", "Flags that apply to pandas objects.", "Global metadata applying to this dataset.", "Notes", "\u201cFlags\u201d differ from \u201cmetadata\u201d. Flags reflect properties of the pandas object (the Series or DataFrame). Metadata refer to properties of the dataset, and should be stored in DataFrame.attrs.", "Examples", "Flags can be get or set using .", "Or by slicing with a key"]}, {"name": "pandas.DataFrame.floordiv", "path": "reference/api/pandas.dataframe.floordiv", "type": "DataFrame", "text": ["Get Integer division of dataframe and other, element-wise (binary operator floordiv).", "Equivalent to dataframe // other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rfloordiv.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.from_dict", "path": "reference/api/pandas.dataframe.from_dict", "type": "DataFrame", "text": ["Construct DataFrame from dict of array-like or dicts.", "Creates DataFrame object from dictionary by columns or by index allowing dtype specification.", "Of the form {field : array-like} or {field : dict}.", "The \u201corientation\u201d of the data. If the keys of the passed dict should be the columns of the resulting DataFrame, pass \u2018columns\u2019 (default). Otherwise if the keys should be rows, pass \u2018index\u2019. If \u2018tight\u2019, assume a dict with keys [\u2018index\u2019, \u2018columns\u2019, \u2018data\u2019, \u2018index_names\u2019, \u2018column_names\u2019].", "New in version 1.4.0: \u2018tight\u2019 as an allowed value for the orient argument", "Data type to force, otherwise infer.", "Column labels to use when orient='index'. Raises a ValueError if used with orient='columns' or orient='tight'.", "See also", "DataFrame from structured ndarray, sequence of tuples or dicts, or DataFrame.", "DataFrame object creation using constructor.", "Convert the DataFrame to a dictionary.", "Examples", "By default the keys of the dict become the DataFrame columns:", "Specify orient='index' to create the DataFrame using dictionary keys as rows:", "When using the \u2018index\u2019 orientation, the column names can be specified manually:", "Specify orient='tight' to create the DataFrame using a \u2018tight\u2019 format:"]}, {"name": "pandas.DataFrame.from_records", "path": "reference/api/pandas.dataframe.from_records", "type": "DataFrame", "text": ["Convert structured or record ndarray to DataFrame.", "Creates a DataFrame object from a structured ndarray, sequence of tuples or dicts, or DataFrame.", "Structured input data.", "Field of array to use as the index, alternately a specific set of input labels to use.", "Columns or fields to exclude.", "Column names to use. If the passed data do not have names associated with them, this argument provides names for the columns. Otherwise this argument indicates the order of the columns in the result (any names not found in the data will become all-NA columns).", "Attempt to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point, useful for SQL result sets.", "Number of rows to read if data is an iterator.", "See also", "DataFrame from dict of array-like or dicts.", "DataFrame object creation using constructor.", "Examples", "Data can be provided as a structured ndarray:", "Data can be provided as a list of dicts:", "Data can be provided as a list of tuples with corresponding columns:"]}, {"name": "pandas.DataFrame.ge", "path": "reference/api/pandas.dataframe.ge", "type": "DataFrame", "text": ["Get Greater than or equal to of dataframe and other, element-wise (binary operator ge).", "Among flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.", "Equivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or columns) and level for comparison.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Result of the comparison.", "See also", "Compare DataFrames for equality elementwise.", "Compare DataFrames for inequality elementwise.", "Compare DataFrames for less than inequality or equality elementwise.", "Compare DataFrames for strictly less than inequality elementwise.", "Compare DataFrames for greater than inequality or equality elementwise.", "Compare DataFrames for strictly greater than inequality elementwise.", "Notes", "Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN).", "Examples", "Comparison with a scalar, using either the operator or method:", "When other is a Series, the columns of a DataFrame are aligned with the index of other and broadcast:", "Use the method to control the broadcast axis:", "When comparing to an arbitrary sequence, the number of columns must match the number elements in other:", "Use the method to control the axis:", "Compare to a DataFrame of different shape.", "Compare to a MultiIndex by level."]}, {"name": "pandas.DataFrame.get", "path": "reference/api/pandas.dataframe.get", "type": "DataFrame", "text": ["Get item from object for given key (ex: DataFrame column).", "Returns default value if not found.", "Examples", "If the key isn\u2019t found, the default value will be used."]}, {"name": "pandas.DataFrame.groupby", "path": "reference/api/pandas.dataframe.groupby", "type": "GroupBy", "text": ["Group DataFrame using a mapper or by a Series of columns.", "A groupby operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups.", "Used to determine the groups for the groupby. If by is a function, it\u2019s called on each value of the object\u2019s index. If a dict or Series is passed, the Series or dict VALUES will be used to determine the groups (the Series\u2019 values are first aligned; see .align() method). If a list or ndarray of length equal to the selected axis is passed (see the groupby user guide), the values are used as-is to determine the groups. A label or list of labels may be passed to group by the columns in self. Notice that a tuple is interpreted as a (single) key.", "Split along rows (0) or columns (1).", "If the axis is a MultiIndex (hierarchical), group by a particular level or levels.", "For aggregated output, return object with group labels as the index. Only relevant for DataFrame input. as_index=False is effectively \u201cSQL-style\u201d grouped output.", "Sort group keys. Get better performance by turning this off. Note this does not influence the order of observations within each group. Groupby preserves the order of rows within each group.", "When calling apply, add group keys to index to identify pieces.", "Reduce the dimensionality of the return type if possible, otherwise return a consistent type.", "Deprecated since version 1.1.0.", "This only applies if any of the groupers are Categoricals. If True: only show observed values for categorical groupers. If False: show all values for categorical groupers.", "If True, and if group keys contain NA values, NA values together with row/column will be dropped. If False, NA values will also be treated as the key in groups.", "New in version 1.1.0.", "Returns a groupby object that contains information about the groups.", "See also", "Convenience method for frequency conversion and resampling of time series.", "Notes", "See the user guide for more detailed usage and examples, including splitting an object into groups, iterating through groups, selecting a group, aggregation, and more.", "Examples", "Hierarchical Indexes", "We can groupby different levels of a hierarchical index using the level parameter:", "We can also choose to include NA in group keys or not by setting dropna parameter, the default setting is True."]}, {"name": "pandas.DataFrame.gt", "path": "reference/api/pandas.dataframe.gt", "type": "DataFrame", "text": ["Get Greater than of dataframe and other, element-wise (binary operator gt).", "Among flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.", "Equivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or columns) and level for comparison.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Result of the comparison.", "See also", "Compare DataFrames for equality elementwise.", "Compare DataFrames for inequality elementwise.", "Compare DataFrames for less than inequality or equality elementwise.", "Compare DataFrames for strictly less than inequality elementwise.", "Compare DataFrames for greater than inequality or equality elementwise.", "Compare DataFrames for strictly greater than inequality elementwise.", "Notes", "Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN).", "Examples", "Comparison with a scalar, using either the operator or method:", "When other is a Series, the columns of a DataFrame are aligned with the index of other and broadcast:", "Use the method to control the broadcast axis:", "When comparing to an arbitrary sequence, the number of columns must match the number elements in other:", "Use the method to control the axis:", "Compare to a DataFrame of different shape.", "Compare to a MultiIndex by level."]}, {"name": "pandas.DataFrame.head", "path": "reference/api/pandas.dataframe.head", "type": "DataFrame", "text": ["Return the first n rows.", "This function returns the first n rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it.", "For negative values of n, this function returns all rows except the last n rows, equivalent to df[:-n].", "Number of rows to select.", "The first n rows of the caller object.", "See also", "Returns the last n rows.", "Examples", "Viewing the first 5 lines", "Viewing the first n lines (three in this case)", "For negative values of n"]}, {"name": "pandas.DataFrame.hist", "path": "reference/api/pandas.dataframe.hist", "type": "DataFrame", "text": ["Make a histogram of the DataFrame\u2019s columns.", "A histogram is a representation of the distribution of data. This function calls matplotlib.pyplot.hist(), on each series in the DataFrame, resulting in one histogram per column.", "The pandas object holding the data.", "If passed, will be used to limit data to a subset of columns.", "If passed, then used to form histograms for separate groups.", "Whether to show axis grid lines.", "If specified changes the x-axis label size.", "Rotation of x axis labels. For example, a value of 90 displays the x labels rotated 90 degrees clockwise.", "If specified changes the y-axis label size.", "Rotation of y axis labels. For example, a value of 90 displays the y labels rotated 90 degrees clockwise.", "The axes to plot the histogram on.", "In case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in. Note that passing in both an ax and sharex=True will alter all x axis labels for all subplots in a figure.", "In case subplots=True, share y axis and set some y axis labels to invisible.", "The size in inches of the figure to create. Uses the value in matplotlib.rcParams by default.", "Tuple of (rows, columns) for the layout of the histograms.", "Number of histogram bins to be used. If an integer is given, bins + 1 bin edges are calculated and returned. If bins is a sequence, gives bin edges, including left edge of first bin and right edge of last bin. In this case, bins is returned unmodified.", "Backend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.", "New in version 1.0.0.", "Whether to show the legend.", "New in version 1.1.0.", "All other plotting keyword arguments to be passed to matplotlib.pyplot.hist().", "See also", "Plot a histogram using matplotlib.", "Examples", "This example draws a histogram based on the length and width of some animals, displayed in three bins"]}, {"name": "pandas.DataFrame.iat", "path": "reference/api/pandas.dataframe.iat", "type": "DataFrame", "text": ["Access a single value for a row/column pair by integer position.", "Similar to iloc, in that both provide integer-based lookups. Use iat if you only need to get or set a single value in a DataFrame or Series.", "When integer position is out of bounds.", "See also", "Access a single value for a row/column label pair.", "Access a group of rows and columns by label(s).", "Access a group of rows and columns by integer position(s).", "Examples", "Get value at specified row/column pair", "Set value at specified row/column pair", "Get value within a series"]}, {"name": "pandas.DataFrame.idxmax", "path": "reference/api/pandas.dataframe.idxmax", "type": "DataFrame", "text": ["Return index of first occurrence of maximum over requested axis.", "NA/null values are excluded.", "The axis to use. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "Indexes of maxima along the specified axis.", "If the row/column is empty", "See also", "Return index of the maximum element.", "Notes", "This method is the DataFrame version of ndarray.argmax.", "Examples", "Consider a dataset containing food consumption in Argentina.", "By default, it returns the index for the maximum value in each column.", "To return the index for the maximum value in each row, use axis=\"columns\"."]}, {"name": "pandas.DataFrame.idxmin", "path": "reference/api/pandas.dataframe.idxmin", "type": "DataFrame", "text": ["Return index of first occurrence of minimum over requested axis.", "NA/null values are excluded.", "The axis to use. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "Indexes of minima along the specified axis.", "If the row/column is empty", "See also", "Return index of the minimum element.", "Notes", "This method is the DataFrame version of ndarray.argmin.", "Examples", "Consider a dataset containing food consumption in Argentina.", "By default, it returns the index for the minimum value in each column.", "To return the index for the minimum value in each row, use axis=\"columns\"."]}, {"name": "pandas.DataFrame.iloc", "path": "reference/api/pandas.dataframe.iloc", "type": "DataFrame", "text": ["Purely integer-location based indexing for selection by position.", ".iloc[] is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array.", "Allowed inputs are:", "An integer, e.g. 5.", "A list or array of integers, e.g. [4, 3, 0].", "A slice object with ints, e.g. 1:7.", "A boolean array.", "A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above). This is useful in method chains, when you don\u2019t have a reference to the calling object, but would like to base your selection on some value.", ".iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing (this conforms with python/numpy slice semantics).", "See more at Selection by Position.", "See also", "Fast integer location scalar accessor.", "Purely label-location based indexer for selection by label.", "Purely integer-location based indexing for selection by position.", "Examples", "Indexing just the rows", "With a scalar integer.", "With a list of integers.", "With a slice object.", "With a boolean mask the same length as the index.", "With a callable, useful in method chains. The x passed to the lambda is the DataFrame being sliced. This selects the rows whose index label even.", "Indexing both axes", "You can mix the indexer types for the index and columns. Use : to select the entire axis.", "With scalar integers.", "With lists of integers.", "With slice objects.", "With a boolean array whose length matches the columns.", "With a callable function that expects the Series or DataFrame."]}, {"name": "pandas.DataFrame.index", "path": "reference/api/pandas.dataframe.index", "type": "DataFrame", "text": ["The index (row labels) of the DataFrame."]}, {"name": "pandas.DataFrame.infer_objects", "path": "reference/api/pandas.dataframe.infer_objects", "type": "DataFrame", "text": ["Attempt to infer better dtypes for object columns.", "Attempts soft conversion of object-dtyped columns, leaving non-object and unconvertible columns unchanged. The inference rules are the same as during normal Series/DataFrame construction.", "See also", "Convert argument to datetime.", "Convert argument to timedelta.", "Convert argument to numeric type.", "Convert argument to best possible dtype.", "Examples"]}, {"name": "pandas.DataFrame.info", "path": "reference/api/pandas.dataframe.info", "type": "DataFrame", "text": ["Print a concise summary of a DataFrame.", "This method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage.", "DataFrame to print information about.", "Whether to print the full summary. By default, the setting in pandas.options.display.max_info_columns is followed.", "Where to send the output. By default, the output is printed to sys.stdout. Pass a writable buffer if you need to further process the output. max_cols : int, optional When to switch from the verbose to the truncated output. If the DataFrame has more than max_cols columns, the truncated output is used. By default, the setting in pandas.options.display.max_info_columns is used.", "Specifies whether total memory usage of the DataFrame elements (including the index) should be displayed. By default, this follows the pandas.options.display.memory_usage setting.", "True always show memory usage. False never shows memory usage. A value of \u2018deep\u2019 is equivalent to \u201cTrue with deep introspection\u201d. Memory usage is shown in human-readable units (base-2 representation). Without deep introspection a memory estimation is made based in column dtype and number of rows assuming values consume the same memory amount for corresponding dtypes. With deep memory introspection, a real memory usage calculation is performed at the cost of computational resources.", "Whether to show the non-null counts. By default, this is shown only if the DataFrame is smaller than pandas.options.display.max_info_rows and pandas.options.display.max_info_columns. A value of True always shows the counts, and False never shows the counts.", "Deprecated since version 1.2.0: Use show_counts instead.", "This method prints a summary of a DataFrame and returns None.", "See also", "Generate descriptive statistics of DataFrame columns.", "Memory usage of DataFrame columns.", "Examples", "Prints information of all columns:", "Prints a summary of columns count and its dtypes but not per column information:", "Pipe output of DataFrame.info to buffer instead of sys.stdout, get buffer content and writes to a text file:", "The memory_usage parameter allows deep introspection mode, specially useful for big DataFrames and fine-tune memory optimization:"]}, {"name": "pandas.DataFrame.insert", "path": "reference/api/pandas.dataframe.insert", "type": "DataFrame", "text": ["Insert column into DataFrame at specified location.", "Raises a ValueError if column is already contained in the DataFrame, unless allow_duplicates is set to True.", "Insertion index. Must verify 0 <= loc <= len(columns).", "Label of the inserted column.", "See also", "Insert new item by index.", "Examples", "Notice that pandas uses index alignment in case of value from type Series:"]}, {"name": "pandas.DataFrame.interpolate", "path": "reference/api/pandas.dataframe.interpolate", "type": "DataFrame", "text": ["Fill NaN values using an interpolation method.", "Please note that only method='linear' is supported for DataFrame/Series with a MultiIndex.", "Interpolation technique to use. One of:", "\u2018linear\u2019: Ignore the index and treat the values as equally spaced. This is the only method supported on MultiIndexes.", "\u2018time\u2019: Works on daily and higher resolution data to interpolate given length of interval.", "\u2018index\u2019, \u2018values\u2019: use the actual numerical values of the index.", "\u2018pad\u2019: Fill in NaNs using existing values.", "\u2018nearest\u2019, \u2018zero\u2019, \u2018slinear\u2019, \u2018quadratic\u2019, \u2018cubic\u2019, \u2018spline\u2019, \u2018barycentric\u2019, \u2018polynomial\u2019: Passed to scipy.interpolate.interp1d. These methods use the numerical values of the index. Both \u2018polynomial\u2019 and \u2018spline\u2019 require that you also specify an order (int), e.g. df.interpolate(method='polynomial', order=5).", "\u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019, \u2018akima\u2019, \u2018cubicspline\u2019: Wrappers around the SciPy interpolation methods of similar names. See Notes.", "\u2018from_derivatives\u2019: Refers to scipy.interpolate.BPoly.from_derivatives which replaces \u2018piecewise_polynomial\u2019 interpolation method in scipy 0.18.", "Axis to interpolate along.", "Maximum number of consecutive NaNs to fill. Must be greater than 0.", "Update the data in place if possible.", "Consecutive NaNs will be filled in this direction.", "If \u2018method\u2019 is \u2018pad\u2019 or \u2018ffill\u2019, \u2018limit_direction\u2019 must be \u2018forward\u2019.", "If \u2018method\u2019 is \u2018backfill\u2019 or \u2018bfill\u2019, \u2018limit_direction\u2019 must be \u2018backwards\u2019.", "If \u2018method\u2019 is \u2018backfill\u2019 or \u2018bfill\u2019, the default is \u2018backward\u2019", "else the default is \u2018forward\u2019", "Changed in version 1.1.0: raises ValueError if limit_direction is \u2018forward\u2019 or \u2018both\u2019 and method is \u2018backfill\u2019 or \u2018bfill\u2019. raises ValueError if limit_direction is \u2018backward\u2019 or \u2018both\u2019 and method is \u2018pad\u2019 or \u2018ffill\u2019.", "If limit is specified, consecutive NaNs will be filled with this restriction.", "None: No fill restriction.", "\u2018inside\u2019: Only fill NaNs surrounded by valid values (interpolate).", "\u2018outside\u2019: Only fill NaNs outside valid values (extrapolate).", "Downcast dtypes if possible.", "Keyword arguments to pass on to the interpolating function.", "Returns the same object type as the caller, interpolated at some or all NaN values or None if inplace=True.", "See also", "Fill missing values using different methods.", "Piecewise cubic polynomials (Akima interpolator).", "Piecewise polynomial in the Bernstein basis.", "Interpolate a 1-D function.", "Interpolate polynomial (Krogh interpolator).", "PCHIP 1-d monotonic cubic interpolation.", "Cubic spline data interpolator.", "Notes", "The \u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019 and \u2018akima\u2019 methods are wrappers around the respective SciPy implementations of similar names. These use the actual numerical values of the index. For more information on their behavior, see the SciPy documentation and SciPy tutorial.", "Examples", "Filling in NaN in a Series via linear interpolation.", "Filling in NaN in a Series by padding, but filling at most two consecutive NaN at a time.", "Filling in NaN in a Series via polynomial interpolation or splines: Both \u2018polynomial\u2019 and \u2018spline\u2019 methods require that you also specify an order (int).", "Fill the DataFrame forward (that is, going down) along each column using linear interpolation.", "Note how the last entry in column \u2018a\u2019 is interpolated differently, because there is no entry after it to use for interpolation. Note how the first entry in column \u2018b\u2019 remains NaN, because there is no entry before it to use for interpolation.", "Using polynomial interpolation."]}, {"name": "pandas.DataFrame.isin", "path": "reference/api/pandas.dataframe.isin", "type": "DataFrame", "text": ["Whether each element in the DataFrame is contained in values.", "The result will only be true at a location if all the labels match. If values is a Series, that\u2019s the index. If values is a dict, the keys must be the column names, which must match. If values is a DataFrame, then both the index and column labels must match.", "DataFrame of booleans showing whether each element in the DataFrame is contained in values.", "See also", "Equality test for DataFrame.", "Equivalent method on Series.", "Test if pattern or regex is contained within a string of a Series or Index.", "Examples", "When values is a list check whether every value in the DataFrame is present in the list (which animals have 0 or 2 legs or wings)", "To check if values is not in the DataFrame, use the ~ operator:", "When values is a dict, we can pass values to check for each column separately:", "When values is a Series or DataFrame the index and column must match. Note that \u2018falcon\u2019 does not match based on the number of legs in other."]}, {"name": "pandas.DataFrame.isna", "path": "reference/api/pandas.dataframe.isna", "type": "DataFrame", "text": ["Detect missing values.", "Return a boolean same-sized object indicating if the values are NA. NA values, such as None or numpy.NaN, gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True).", "Mask of bool values for each element in DataFrame that indicates whether an element is an NA value.", "See also", "Alias of isna.", "Boolean inverse of isna.", "Omit axes labels with missing values.", "Top-level isna.", "Examples", "Show which entries in a DataFrame are NA.", "Show which entries in a Series are NA."]}, {"name": "pandas.DataFrame.isnull", "path": "reference/api/pandas.dataframe.isnull", "type": "DataFrame", "text": ["DataFrame.isnull is an alias for DataFrame.isna.", "Detect missing values.", "Return a boolean same-sized object indicating if the values are NA. NA values, such as None or numpy.NaN, gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True).", "Mask of bool values for each element in DataFrame that indicates whether an element is an NA value.", "See also", "Alias of isna.", "Boolean inverse of isna.", "Omit axes labels with missing values.", "Top-level isna.", "Examples", "Show which entries in a DataFrame are NA.", "Show which entries in a Series are NA."]}, {"name": "pandas.DataFrame.items", "path": "reference/api/pandas.dataframe.items", "type": "DataFrame", "text": ["Iterate over (column name, Series) pairs.", "Iterates over the DataFrame columns, returning a tuple with the column name and the content as a Series.", "The column names for the DataFrame being iterated over.", "The column entries belonging to each label, as a Series.", "See also", "Iterate over DataFrame rows as (index, Series) pairs.", "Iterate over DataFrame rows as namedtuples of the values.", "Examples"]}, {"name": "pandas.DataFrame.iteritems", "path": "reference/api/pandas.dataframe.iteritems", "type": "DataFrame", "text": ["Iterate over (column name, Series) pairs.", "Iterates over the DataFrame columns, returning a tuple with the column name and the content as a Series.", "The column names for the DataFrame being iterated over.", "The column entries belonging to each label, as a Series.", "See also", "Iterate over DataFrame rows as (index, Series) pairs.", "Iterate over DataFrame rows as namedtuples of the values.", "Examples"]}, {"name": "pandas.DataFrame.iterrows", "path": "reference/api/pandas.dataframe.iterrows", "type": "DataFrame", "text": ["Iterate over DataFrame rows as (index, Series) pairs.", "The index of the row. A tuple for a MultiIndex.", "The data of the row as a Series.", "See also", "Iterate over DataFrame rows as namedtuples of the values.", "Iterate over (column name, Series) pairs.", "Notes", "Because iterrows returns a Series for each row, it does not preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). For example,", "To preserve dtypes while iterating over the rows, it is better to use itertuples() which returns namedtuples of the values and which is generally faster than iterrows.", "You should never modify something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect."]}, {"name": "pandas.DataFrame.itertuples", "path": "reference/api/pandas.dataframe.itertuples", "type": "DataFrame", "text": ["Iterate over DataFrame rows as namedtuples.", "If True, return the index as the first element of the tuple.", "The name of the returned namedtuples or None to return regular tuples.", "An object to iterate over namedtuples for each row in the DataFrame with the first field possibly being the index and following fields being the column values.", "See also", "Iterate over DataFrame rows as (index, Series) pairs.", "Iterate over (column name, Series) pairs.", "Notes", "The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. On python versions < 3.7 regular tuples are returned for DataFrames with a large number of columns (>254).", "Examples", "By setting the index parameter to False we can remove the index as the first element of the tuple:", "With the name parameter set we set a custom name for the yielded namedtuples:"]}, {"name": "pandas.DataFrame.join", "path": "reference/api/pandas.dataframe.join", "type": "DataFrame", "text": ["Join columns of another DataFrame.", "Join columns with other DataFrame either on index or on a key column. Efficiently join multiple DataFrame objects by index at once by passing a list.", "Index should be similar to one of the columns in this one. If a Series is passed, its name attribute must be set, and that will be used as the column name in the resulting joined DataFrame.", "Column or index level name(s) in the caller to join on the index in other, otherwise joins index-on-index. If multiple values given, the other DataFrame must have a MultiIndex. Can pass an array as the join key if it is not already contained in the calling DataFrame. Like an Excel VLOOKUP operation.", "How to handle the operation of the two objects.", "left: use calling frame\u2019s index (or column if on is specified)", "right: use other\u2019s index.", "outer: form union of calling frame\u2019s index (or column if on is specified) with other\u2019s index, and sort it. lexicographically.", "inner: form intersection of calling frame\u2019s index (or column if on is specified) with other\u2019s index, preserving the order of the calling\u2019s one.", "cross: creates the cartesian product from both frames, preserves the order of the left keys.", "New in version 1.2.0.", "Suffix to use from left frame\u2019s overlapping columns.", "Suffix to use from right frame\u2019s overlapping columns.", "Order result DataFrame lexicographically by the join key. If False, the order of the join key depends on the join type (how keyword).", "A dataframe containing columns from both the caller and other.", "See also", "For column(s)-on-column(s) operations.", "Notes", "Parameters on, lsuffix, and rsuffix are not supported when passing a list of DataFrame objects.", "Support for specifying index levels as the on parameter was added in version 0.23.0.", "Examples", "Join DataFrames using their indexes.", "If we want to join using the key columns, we need to set key to be the index in both df and other. The joined DataFrame will have key as its index.", "Another option to join using the key columns is to use the on parameter. DataFrame.join always uses other\u2019s index but we can use any column in df. This method preserves the original DataFrame\u2019s index in the result.", "Using non-unique key values shows how they are matched."]}, {"name": "pandas.DataFrame.keys", "path": "reference/api/pandas.dataframe.keys", "type": "DataFrame", "text": ["Get the \u2018info axis\u2019 (see Indexing for more).", "This is index for Series, columns for DataFrame.", "Info axis."]}, {"name": "pandas.DataFrame.kurt", "path": "reference/api/pandas.dataframe.kurt", "type": "DataFrame", "text": ["Return unbiased kurtosis over requested axis.", "Kurtosis obtained using Fisher\u2019s definition of kurtosis (kurtosis of normal == 0.0). Normalized by N-1.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Additional keyword arguments to be passed to the function."]}, {"name": "pandas.DataFrame.kurtosis", "path": "reference/api/pandas.dataframe.kurtosis", "type": "DataFrame", "text": ["Return unbiased kurtosis over requested axis.", "Kurtosis obtained using Fisher\u2019s definition of kurtosis (kurtosis of normal == 0.0). Normalized by N-1.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Additional keyword arguments to be passed to the function."]}, {"name": "pandas.DataFrame.last", "path": "reference/api/pandas.dataframe.last", "type": "DataFrame", "text": ["Select final periods of time series data based on a date offset.", "For a DataFrame with a sorted DatetimeIndex, this function selects the last few rows based on a date offset.", "The offset length of the data that will be selected. For instance, \u20183D\u2019 will display all the rows having their index within the last 3 days.", "A subset of the caller.", "If the index is not a DatetimeIndex", "See also", "Select initial periods of time series based on a date offset.", "Select values at a particular time of the day.", "Select values between particular times of the day.", "Examples", "Get the rows for the last 3 days:", "Notice the data for 3 last calendar days were returned, not the last 3 observed days in the dataset, and therefore data for 2018-04-11 was not returned."]}, {"name": "pandas.DataFrame.last_valid_index", "path": "reference/api/pandas.dataframe.last_valid_index", "type": "DataFrame", "text": ["Return index for last non-NA value or None, if no NA value is found.", "Notes", "If all elements are non-NA/null, returns None. Also returns None for empty Series/DataFrame."]}, {"name": "pandas.DataFrame.le", "path": "reference/api/pandas.dataframe.le", "type": "DataFrame", "text": ["Get Less than or equal to of dataframe and other, element-wise (binary operator le).", "Among flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.", "Equivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or columns) and level for comparison.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Result of the comparison.", "See also", "Compare DataFrames for equality elementwise.", "Compare DataFrames for inequality elementwise.", "Compare DataFrames for less than inequality or equality elementwise.", "Compare DataFrames for strictly less than inequality elementwise.", "Compare DataFrames for greater than inequality or equality elementwise.", "Compare DataFrames for strictly greater than inequality elementwise.", "Notes", "Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN).", "Examples", "Comparison with a scalar, using either the operator or method:", "When other is a Series, the columns of a DataFrame are aligned with the index of other and broadcast:", "Use the method to control the broadcast axis:", "When comparing to an arbitrary sequence, the number of columns must match the number elements in other:", "Use the method to control the axis:", "Compare to a DataFrame of different shape.", "Compare to a MultiIndex by level."]}, {"name": "pandas.DataFrame.loc", "path": "reference/api/pandas.dataframe.loc", "type": "DataFrame", "text": ["Access a group of rows and columns by label(s) or a boolean array.", ".loc[] is primarily label based, but may also be used with a boolean array.", "Allowed inputs are:", "A single label, e.g. 5 or 'a', (note that 5 is interpreted as a label of the index, and never as an integer position along the index).", "A list or array of labels, e.g. ['a', 'b', 'c'].", "A slice object with labels, e.g. 'a':'f'.", "Warning", "Note that contrary to usual python slices, both the start and the stop are included", "A boolean array of the same length as the axis being sliced, e.g. [True, False, True].", "An alignable boolean Series. The index of the key will be aligned before masking.", "An alignable Index. The Index of the returned selection will be the input.", "A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above)", "See more at Selection by Label.", "If any items are not found.", "If an indexed key is passed and its index is unalignable to the frame index.", "See also", "Access a single value for a row/column label pair.", "Access group of rows and columns by integer position(s).", "Returns a cross-section (row(s) or column(s)) from the Series/DataFrame.", "Access group of values using labels.", "Examples", "Getting values", "Single label. Note this returns the row as a Series.", "List of labels. Note using [[]] returns a DataFrame.", "Single label for row and column", "Slice with labels for row and single label for column. As mentioned above, note that both the start and stop of the slice are included.", "Boolean list with the same length as the row axis", "Alignable boolean Series:", "Index (same behavior as df.reindex)", "Conditional that returns a boolean Series", "Conditional that returns a boolean Series with column labels specified", "Callable that returns a boolean Series", "Setting values", "Set value for all items matching the list of labels", "Set value for an entire row", "Set value for an entire column", "Set value for rows matching callable condition", "Getting values on a DataFrame with an index that has integer labels", "Another example using integers for the index", "Slice with integer labels for rows. As mentioned above, note that both the start and stop of the slice are included.", "Getting values with a MultiIndex", "A number of examples using a DataFrame with a MultiIndex", "Single label. Note this returns a DataFrame with a single index.", "Single index tuple. Note this returns a Series.", "Single label for row and column. Similar to passing in a tuple, this returns a Series.", "Single tuple. Note using [[]] returns a DataFrame.", "Single tuple for the index with a single label for the column", "Slice from index tuple to single label", "Slice from index tuple to index tuple"]}, {"name": "pandas.DataFrame.lookup", "path": "reference/api/pandas.dataframe.lookup", "type": "DataFrame", "text": ["Label-based \u201cfancy indexing\u201d function for DataFrame. Given equal-length arrays of row and column labels, return an array of the values corresponding to each (row, col) pair.", "Deprecated since version 1.2.0: DataFrame.lookup is deprecated, use DataFrame.melt and DataFrame.loc instead. For further details see Looking up values by index/column labels.", "The row labels to use for lookup.", "The column labels to use for lookup.", "The found values."]}, {"name": "pandas.DataFrame.lt", "path": "reference/api/pandas.dataframe.lt", "type": "DataFrame", "text": ["Get Less than of dataframe and other, element-wise (binary operator lt).", "Among flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.", "Equivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or columns) and level for comparison.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Result of the comparison.", "See also", "Compare DataFrames for equality elementwise.", "Compare DataFrames for inequality elementwise.", "Compare DataFrames for less than inequality or equality elementwise.", "Compare DataFrames for strictly less than inequality elementwise.", "Compare DataFrames for greater than inequality or equality elementwise.", "Compare DataFrames for strictly greater than inequality elementwise.", "Notes", "Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN).", "Examples", "Comparison with a scalar, using either the operator or method:", "When other is a Series, the columns of a DataFrame are aligned with the index of other and broadcast:", "Use the method to control the broadcast axis:", "When comparing to an arbitrary sequence, the number of columns must match the number elements in other:", "Use the method to control the axis:", "Compare to a DataFrame of different shape.", "Compare to a MultiIndex by level."]}, {"name": "pandas.DataFrame.mad", "path": "reference/api/pandas.dataframe.mad", "type": "DataFrame", "text": ["Return the mean absolute deviation of the values over the requested axis.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series."]}, {"name": "pandas.DataFrame.mask", "path": "reference/api/pandas.dataframe.mask", "type": "DataFrame", "text": ["Replace values where the condition is True.", "Where cond is False, keep the original value. Where True, replace with corresponding value from other. If cond is callable, it is computed on the Series/DataFrame and should return boolean Series/DataFrame or array. The callable must not change input Series/DataFrame (though pandas doesn\u2019t check it).", "Entries where cond is True are replaced with corresponding value from other. If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn\u2019t check it).", "Whether to perform the operation in place on the data.", "Alignment axis if needed.", "Alignment level if needed.", "Note that currently this parameter won\u2019t affect the results and will always coerce to a suitable dtype.", "\u2018raise\u2019 : allow exceptions to be raised.", "\u2018ignore\u2019 : suppress exceptions. On error return original object.", "Try to cast the result back to the input type (if possible).", "Deprecated since version 1.3.0: Manually cast back if necessary.", "See also", "Return an object of same shape as self.", "Notes", "The mask method is an application of the if-then idiom. For each element in the calling DataFrame, if cond is False the element is used; otherwise the corresponding element from the DataFrame other is used.", "The signature for DataFrame.where() differs from numpy.where(). Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2).", "For further details and examples see the mask documentation in indexing.", "Examples"]}, {"name": "pandas.DataFrame.max", "path": "reference/api/pandas.dataframe.max", "type": "DataFrame", "text": ["Return the maximum of the values over the requested axis.", "If you want the index of the maximum, use idxmax. This is the equivalent of the numpy.ndarray method argmax.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Additional keyword arguments to be passed to the function.", "See also", "Return the sum.", "Return the minimum.", "Return the maximum.", "Return the index of the minimum.", "Return the index of the maximum.", "Return the sum over the requested axis.", "Return the minimum over the requested axis.", "Return the maximum over the requested axis.", "Return the index of the minimum over the requested axis.", "Return the index of the maximum over the requested axis.", "Examples"]}, {"name": "pandas.DataFrame.mean", "path": "reference/api/pandas.dataframe.mean", "type": "DataFrame", "text": ["Return the mean of the values over the requested axis.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Additional keyword arguments to be passed to the function."]}, {"name": "pandas.DataFrame.median", "path": "reference/api/pandas.dataframe.median", "type": "DataFrame", "text": ["Return the median of the values over the requested axis.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Additional keyword arguments to be passed to the function."]}, {"name": "pandas.DataFrame.melt", "path": "reference/api/pandas.dataframe.melt", "type": "DataFrame", "text": ["Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.", "This function is useful to massage a DataFrame into a format where one or more columns are identifier variables (id_vars), while all other columns, considered measured variables (value_vars), are \u201cunpivoted\u201d to the row axis, leaving just two non-identifier columns, \u2018variable\u2019 and \u2018value\u2019.", "Column(s) to use as identifier variables.", "Column(s) to unpivot. If not specified, uses all columns that are not set as id_vars.", "Name to use for the \u2018variable\u2019 column. If None it uses frame.columns.name or \u2018variable\u2019.", "Name to use for the \u2018value\u2019 column.", "If columns are a MultiIndex then use this level to melt.", "If True, original index is ignored. If False, the original index is retained. Index labels will be repeated as necessary.", "New in version 1.1.0.", "Unpivoted DataFrame.", "See also", "Identical method.", "Create a spreadsheet-style pivot table as a DataFrame.", "Return reshaped DataFrame organized by given index / column values.", "Explode a DataFrame from list-like columns to long format.", "Examples", "The names of \u2018variable\u2019 and \u2018value\u2019 columns can be customized:", "Original index values can be kept around:", "If you have multi-index columns:"]}, {"name": "pandas.DataFrame.memory_usage", "path": "reference/api/pandas.dataframe.memory_usage", "type": "DataFrame", "text": ["Return the memory usage of each column in bytes.", "The memory usage can optionally include the contribution of the index and elements of object dtype.", "This value is displayed in DataFrame.info by default. This can be suppressed by setting pandas.options.display.memory_usage to False.", "Specifies whether to include the memory usage of the DataFrame\u2019s index in returned Series. If index=True, the memory usage of the index is the first item in the output.", "If True, introspect the data deeply by interrogating object dtypes for system-level memory consumption, and include it in the returned values.", "A Series whose index is the original column names and whose values is the memory usage of each column in bytes.", "See also", "Total bytes consumed by the elements of an ndarray.", "Bytes consumed by a Series.", "Memory-efficient array for string values with many repeated values.", "Concise summary of a DataFrame.", "Examples", "The memory footprint of object dtype columns is ignored by default:", "Use a Categorical for efficient storage of an object-dtype column with many repeated values."]}, {"name": "pandas.DataFrame.merge", "path": "reference/api/pandas.dataframe.merge", "type": "DataFrame", "text": ["Merge DataFrame or named Series objects with a database-style join.", "A named Series object is treated as a DataFrame with a single named column.", "The join is done on columns or indexes. If joining columns on columns, the DataFrame indexes will be ignored. Otherwise if joining indexes on indexes or indexes on a column or columns, the index will be passed on. When performing a cross merge, no column specifications to merge on are allowed.", "Warning", "If both key columns contain rows where the key is a null value, those rows will be matched against each other. This is different from usual SQL join behaviour and can lead to unexpected results.", "Object to merge with.", "Type of merge to be performed.", "left: use only keys from left frame, similar to a SQL left outer join; preserve key order.", "right: use only keys from right frame, similar to a SQL right outer join; preserve key order.", "outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically.", "inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys.", "cross: creates the cartesian product from both frames, preserves the order of the left keys.", "New in version 1.2.0.", "Column or index level names to join on. These must be found in both DataFrames. If on is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames.", "Column or index level names to join on in the left DataFrame. Can also be an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns.", "Column or index level names to join on in the right DataFrame. Can also be an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns.", "Use the index from the left DataFrame as the join key(s). If it is a MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels.", "Use the index from the right DataFrame as the join key. Same caveats as left_index.", "Sort the join keys lexicographically in the result DataFrame. If False, the order of the join keys depends on the join type (how keyword).", "A length-2 sequence where each element is optionally a string indicating the suffix to add to overlapping column names in left and right respectively. Pass a value of None instead of a string to indicate that the column name from left or right should be left as-is, with no suffix. At least one of the values must not be None.", "If False, avoid copy if possible.", "If True, adds a column to the output DataFrame called \u201c_merge\u201d with information on the source of each row. The column can be given a different name by providing a string argument. The column will have a Categorical type with the value of \u201cleft_only\u201d for observations whose merge key only appears in the left DataFrame, \u201cright_only\u201d for observations whose merge key only appears in the right DataFrame, and \u201cboth\u201d if the observation\u2019s merge key is found in both DataFrames.", "If specified, checks if merge is of specified type.", "\u201cone_to_one\u201d or \u201c1:1\u201d: check if merge keys are unique in both left and right datasets.", "\u201cone_to_many\u201d or \u201c1:m\u201d: check if merge keys are unique in left dataset.", "\u201cmany_to_one\u201d or \u201cm:1\u201d: check if merge keys are unique in right dataset.", "\u201cmany_to_many\u201d or \u201cm:m\u201d: allowed, but does not result in checks.", "A DataFrame of the two merged objects.", "See also", "Merge with optional filling/interpolation.", "Merge on nearest keys.", "Similar method using indices.", "Notes", "Support for specifying index levels as the on, left_on, and right_on parameters was added in version 0.23.0 Support for merging named Series objects was added in version 0.24.0", "Examples", "Merge df1 and df2 on the lkey and rkey columns. The value columns have the default suffixes, _x and _y, appended.", "Merge DataFrames df1 and df2 with specified left and right suffixes appended to any overlapping columns.", "Merge DataFrames df1 and df2, but raise an exception if the DataFrames have any overlapping columns."]}, {"name": "pandas.DataFrame.min", "path": "reference/api/pandas.dataframe.min", "type": "DataFrame", "text": ["Return the minimum of the values over the requested axis.", "If you want the index of the minimum, use idxmin. This is the equivalent of the numpy.ndarray method argmin.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Additional keyword arguments to be passed to the function.", "See also", "Return the sum.", "Return the minimum.", "Return the maximum.", "Return the index of the minimum.", "Return the index of the maximum.", "Return the sum over the requested axis.", "Return the minimum over the requested axis.", "Return the maximum over the requested axis.", "Return the index of the minimum over the requested axis.", "Return the index of the maximum over the requested axis.", "Examples"]}, {"name": "pandas.DataFrame.mod", "path": "reference/api/pandas.dataframe.mod", "type": "DataFrame", "text": ["Get Modulo of dataframe and other, element-wise (binary operator mod).", "Equivalent to dataframe % other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rmod.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.mode", "path": "reference/api/pandas.dataframe.mode", "type": "DataFrame", "text": ["Get the mode(s) of each element along the selected axis.", "The mode of a set of values is the value that appears most often. It can be multiple values.", "The axis to iterate over while searching for the mode:", "0 or \u2018index\u2019 : get mode of each column", "1 or \u2018columns\u2019 : get mode of each row.", "If True, only apply to numeric columns.", "Don\u2019t consider counts of NaN/NaT.", "The modes of each column or row.", "See also", "Return the highest frequency value in a Series.", "Return the counts of values in a Series.", "Examples", "By default, missing values are not considered, and the mode of wings are both 0 and 2. Because the resulting DataFrame has two rows, the second row of species and legs contains NaN.", "Setting dropna=False NaN values are considered and they can be the mode (like for wings).", "Setting numeric_only=True, only the mode of numeric columns is computed, and columns of other types are ignored.", "To compute the mode over columns and not rows, use the axis parameter:"]}, {"name": "pandas.DataFrame.mul", "path": "reference/api/pandas.dataframe.mul", "type": "DataFrame", "text": ["Get Multiplication of dataframe and other, element-wise (binary operator mul).", "Equivalent to dataframe * other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rmul.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.multiply", "path": "reference/api/pandas.dataframe.multiply", "type": "DataFrame", "text": ["Get Multiplication of dataframe and other, element-wise (binary operator mul).", "Equivalent to dataframe * other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rmul.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.ndim", "path": "reference/api/pandas.dataframe.ndim", "type": "DataFrame", "text": ["Return an int representing the number of axes / array dimensions.", "Return 1 if Series. Otherwise return 2 if DataFrame.", "See also", "Number of array dimensions.", "Examples"]}, {"name": "pandas.DataFrame.ne", "path": "reference/api/pandas.dataframe.ne", "type": "DataFrame", "text": ["Get Not equal to of dataframe and other, element-wise (binary operator ne).", "Among flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.", "Equivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or columns) and level for comparison.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Result of the comparison.", "See also", "Compare DataFrames for equality elementwise.", "Compare DataFrames for inequality elementwise.", "Compare DataFrames for less than inequality or equality elementwise.", "Compare DataFrames for strictly less than inequality elementwise.", "Compare DataFrames for greater than inequality or equality elementwise.", "Compare DataFrames for strictly greater than inequality elementwise.", "Notes", "Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN).", "Examples", "Comparison with a scalar, using either the operator or method:", "When other is a Series, the columns of a DataFrame are aligned with the index of other and broadcast:", "Use the method to control the broadcast axis:", "When comparing to an arbitrary sequence, the number of columns must match the number elements in other:", "Use the method to control the axis:", "Compare to a DataFrame of different shape.", "Compare to a MultiIndex by level."]}, {"name": "pandas.DataFrame.nlargest", "path": "reference/api/pandas.dataframe.nlargest", "type": "DataFrame", "text": ["Return the first n rows ordered by columns in descending order.", "Return the first n rows with the largest values in columns, in descending order. The columns that are not specified are returned as well, but not used for ordering.", "This method is equivalent to df.sort_values(columns, ascending=False).head(n), but more performant.", "Number of rows to return.", "Column label(s) to order by.", "Where there are duplicate values:", "first : prioritize the first occurrence(s)", "last : prioritize the last occurrence(s)", "all : do not drop any duplicates, even it means selecting more than n items.", "The first n rows ordered by the given columns in descending order.", "See also", "Return the first n rows ordered by columns in ascending order.", "Sort DataFrame by the values.", "Return the first n rows without re-ordering.", "Notes", "This function cannot be used with all column types. For example, when specifying columns with object or category dtypes, TypeError is raised.", "Examples", "In the following example, we will use nlargest to select the three rows having the largest values in column \u201cpopulation\u201d.", "When using keep='last', ties are resolved in reverse order:", "When using keep='all', all duplicate items are maintained:", "To order by the largest values in column \u201cpopulation\u201d and then \u201cGDP\u201d, we can specify multiple columns like in the next example."]}, {"name": "pandas.DataFrame.notna", "path": "reference/api/pandas.dataframe.notna", "type": "DataFrame", "text": ["Detect existing (non-missing) values.", "Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True). NA values, such as None or numpy.NaN, get mapped to False values.", "Mask of bool values for each element in DataFrame that indicates whether an element is not an NA value.", "See also", "Alias of notna.", "Boolean inverse of notna.", "Omit axes labels with missing values.", "Top-level notna.", "Examples", "Show which entries in a DataFrame are not NA.", "Show which entries in a Series are not NA."]}, {"name": "pandas.DataFrame.notnull", "path": "reference/api/pandas.dataframe.notnull", "type": "DataFrame", "text": ["DataFrame.notnull is an alias for DataFrame.notna.", "Detect existing (non-missing) values.", "Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True). NA values, such as None or numpy.NaN, get mapped to False values.", "Mask of bool values for each element in DataFrame that indicates whether an element is not an NA value.", "See also", "Alias of notna.", "Boolean inverse of notna.", "Omit axes labels with missing values.", "Top-level notna.", "Examples", "Show which entries in a DataFrame are not NA.", "Show which entries in a Series are not NA."]}, {"name": "pandas.DataFrame.nsmallest", "path": "reference/api/pandas.dataframe.nsmallest", "type": "DataFrame", "text": ["Return the first n rows ordered by columns in ascending order.", "Return the first n rows with the smallest values in columns, in ascending order. The columns that are not specified are returned as well, but not used for ordering.", "This method is equivalent to df.sort_values(columns, ascending=True).head(n), but more performant.", "Number of items to retrieve.", "Column name or names to order by.", "Where there are duplicate values:", "first : take the first occurrence.", "last : take the last occurrence.", "all : do not drop any duplicates, even it means selecting more than n items.", "See also", "Return the first n rows ordered by columns in descending order.", "Sort DataFrame by the values.", "Return the first n rows without re-ordering.", "Examples", "In the following example, we will use nsmallest to select the three rows having the smallest values in column \u201cpopulation\u201d.", "When using keep='last', ties are resolved in reverse order:", "When using keep='all', all duplicate items are maintained:", "To order by the smallest values in column \u201cpopulation\u201d and then \u201cGDP\u201d, we can specify multiple columns like in the next example."]}, {"name": "pandas.DataFrame.nunique", "path": "reference/api/pandas.dataframe.nunique", "type": "DataFrame", "text": ["Count number of distinct elements in specified axis.", "Return Series with number of distinct elements. Can ignore NaN values.", "The axis to use. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.", "Don\u2019t include NaN in the counts.", "See also", "Method nunique for Series.", "Count non-NA cells for each column or row.", "Examples"]}, {"name": "pandas.DataFrame.pad", "path": "reference/api/pandas.dataframe.pad", "type": "DataFrame", "text": ["Synonym for DataFrame.fillna() with method='ffill'.", "Object with missing values filled or None if inplace=True."]}, {"name": "pandas.DataFrame.pct_change", "path": "reference/api/pandas.dataframe.pct_change", "type": "DataFrame", "text": ["Percentage change between the current and a prior element.", "Computes the percentage change from the immediately previous row by default. This is useful in comparing the percentage of change in a time series of elements.", "Periods to shift for forming percent change.", "How to handle NAs before computing percent changes.", "The number of consecutive NAs to fill before stopping.", "Increment to use from time series API (e.g. \u2018M\u2019 or BDay()).", "Additional keyword arguments are passed into DataFrame.shift or Series.shift.", "The same type as the calling object.", "See also", "Compute the difference of two elements in a Series.", "Compute the difference of two elements in a DataFrame.", "Shift the index by some number of periods.", "Shift the index by some number of periods.", "Examples", "Series", "See the percentage change in a Series where filling NAs with last valid observation forward to next valid.", "DataFrame", "Percentage change in French franc, Deutsche Mark, and Italian lira from 1980-01-01 to 1980-03-01.", "Percentage of change in GOOG and APPL stock volume. Shows computing the percentage change between columns."]}, {"name": "pandas.DataFrame.pipe", "path": "reference/api/pandas.dataframe.pipe", "type": "DataFrame", "text": ["Apply chainable functions that expect Series or DataFrames.", "Function to apply to the Series/DataFrame. args, and kwargs are passed into func. Alternatively a (callable, data_keyword) tuple where data_keyword is a string indicating the keyword of callable that expects the Series/DataFrame.", "Positional arguments passed into func.", "A dictionary of keyword arguments passed into func.", "See also", "Apply a function along input axis of DataFrame.", "Apply a function elementwise on a whole DataFrame.", "Apply a mapping correspondence on a Series.", "Notes", "Use .pipe when chaining together functions that expect Series, DataFrames or GroupBy objects. Instead of writing", "You can write", "If you have a function that takes the data as (say) the second argument, pass a tuple indicating which keyword expects the data. For example, suppose f takes its data as arg2:"]}, {"name": "pandas.DataFrame.pivot", "path": "reference/api/pandas.dataframe.pivot", "type": "DataFrame", "text": ["Return reshaped DataFrame organized by given index / column values.", "Reshape data (produce a \u201cpivot\u201d table) based on column values. Uses unique values from specified index / columns to form axes of the resulting DataFrame. This function does not support data aggregation, multiple values will result in a MultiIndex in the columns. See the User Guide for more on reshaping.", "Column to use to make new frame\u2019s index. If None, uses existing index.", "Changed in version 1.1.0: Also accept list of index names.", "Column to use to make new frame\u2019s columns.", "Changed in version 1.1.0: Also accept list of columns names.", "Column(s) to use for populating new frame\u2019s values. If not specified, all remaining columns will be used and the result will have hierarchically indexed columns.", "Returns reshaped DataFrame.", "When there are any index, columns combinations with multiple values. DataFrame.pivot_table when you need to aggregate.", "See also", "Generalization of pivot that can handle duplicate values for one index/column pair.", "Pivot based on the index values instead of a column.", "Wide panel to long format. Less flexible but more user-friendly than melt.", "Notes", "For finer-tuned control, see hierarchical indexing documentation along with the related stack/unstack methods.", "Examples", "You could also assign a list of column names or a list of index names.", "A ValueError is raised if there are any duplicates.", "Notice that the first two rows are the same for our index and columns arguments."]}, {"name": "pandas.DataFrame.pivot_table", "path": "reference/api/pandas.dataframe.pivot_table", "type": "DataFrame", "text": ["Create a spreadsheet-style pivot table as a DataFrame.", "The levels in the pivot table will be stored in MultiIndex objects (hierarchical indexes) on the index and columns of the result DataFrame.", "If an array is passed, it must be the same length as the data. The list can contain any of the other types (except list). Keys to group by on the pivot table index. If an array is passed, it is being used as the same manner as column values.", "If an array is passed, it must be the same length as the data. The list can contain any of the other types (except list). Keys to group by on the pivot table column. If an array is passed, it is being used as the same manner as column values.", "If list of functions passed, the resulting pivot table will have hierarchical columns whose top level are the function names (inferred from the function objects themselves) If dict is passed, the key is column to aggregate and value is function or list of functions.", "Value to replace missing values with (in the resulting pivot table, after aggregation).", "Add all row / columns (e.g. for subtotal / grand totals).", "Do not include columns whose entries are all NaN.", "Name of the row / column that will contain the totals when margins is True.", "This only applies if any of the groupers are Categoricals. If True: only show observed values for categorical groupers. If False: show all values for categorical groupers.", "Changed in version 0.25.0.", "Specifies if the result should be sorted.", "New in version 1.3.0.", "An Excel style pivot table.", "See also", "Pivot without aggregation that can handle non-numeric data.", "Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.", "Wide panel to long format. Less flexible but more user-friendly than melt.", "Examples", "This first example aggregates values by taking the sum.", "We can also fill missing values using the fill_value parameter.", "The next example aggregates by taking the mean across multiple columns.", "We can also calculate multiple types of aggregations for any given value column."]}, {"name": "pandas.DataFrame.plot", "path": "reference/api/pandas.dataframe.plot", "type": "DataFrame", "text": ["Make plots of Series or DataFrame.", "Uses the backend specified by the option plotting.backend. By default, matplotlib is used.", "The object for which the method is called.", "Only used if data is a DataFrame.", "Allows plotting of one column versus another. Only used if data is a DataFrame.", "The kind of plot to produce:", "\u2018line\u2019 : line plot (default)", "\u2018bar\u2019 : vertical bar plot", "\u2018barh\u2019 : horizontal bar plot", "\u2018hist\u2019 : histogram", "\u2018box\u2019 : boxplot", "\u2018kde\u2019 : Kernel Density Estimation plot", "\u2018density\u2019 : same as \u2018kde\u2019", "\u2018area\u2019 : area plot", "\u2018pie\u2019 : pie plot", "\u2018scatter\u2019 : scatter plot (DataFrame only)", "\u2018hexbin\u2019 : hexbin plot (DataFrame only)", "An axes of the current figure.", "Make separate subplots for each column.", "In case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in; Be aware, that passing in both an ax and sharex=True will alter all x axis labels for all axis in a figure.", "In case subplots=True, share y axis and set some y axis labels to invisible.", "(rows, columns) for the layout of subplots.", "Size of a figure object.", "Use index as ticks for x axis.", "Title to use for the plot. If a string is passed, print the string at the top of the figure. If a list is passed and subplots is True, print each item in the list above the corresponding subplot.", "Axis grid lines.", "Place legend on axis subplots.", "The matplotlib line style per column.", "Use log scaling or symlog scaling on x axis. .. versionchanged:: 0.25.0", "Use log scaling or symlog scaling on y axis. .. versionchanged:: 0.25.0", "Use log scaling or symlog scaling on both x and y axes. .. versionchanged:: 0.25.0", "Values to use for the xticks.", "Values to use for the yticks.", "Set the x limits of the current axes.", "Set the y limits of the current axes.", "Name to use for the xlabel on x-axis. Default uses index name as xlabel, or the x-column name for planar plots.", "New in version 1.1.0.", "Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).", "Name to use for the ylabel on y-axis. Default will show no ylabel, or the y-column name for planar plots.", "New in version 1.1.0.", "Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).", "Rotation for ticks (xticks for vertical, yticks for horizontal plots).", "Font size for xticks and yticks.", "Colormap to select colors from. If string, load colormap with that name from matplotlib.", "If True, plot colorbar (only relevant for \u2018scatter\u2019 and \u2018hexbin\u2019 plots).", "Specify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center).", "If True, draw a table using the data in the DataFrame and the data will be transposed to meet matplotlib\u2019s default layout. If a Series or DataFrame is passed, use passed data to draw a table.", "See Plotting with Error Bars for detail.", "Equivalent to yerr.", "If True, create stacked plot.", "Sort column names to determine plot ordering.", "Whether to plot on the secondary y-axis if a list/tuple, which columns to plot on secondary y-axis.", "When using a secondary_y axis, automatically mark the column labels with \u201c(right)\u201d in the legend.", "If True, boolean values can be plotted.", "Backend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.", "New in version 1.0.0.", "Options to pass to matplotlib plotting method.", "If the backend is not the default matplotlib one, the return value will be the object returned by the backend.", "Notes", "See matplotlib documentation online for more on this subject", "If kind = \u2018bar\u2019 or \u2018barh\u2019, you can specify relative alignments for bar plot layout by position keyword. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center)"]}, {"name": "pandas.DataFrame.plot.area", "path": "reference/api/pandas.dataframe.plot.area", "type": "DataFrame", "text": ["Draw a stacked area plot.", "An area plot displays quantitative data visually. This function wraps the matplotlib area function.", "Coordinates for the X axis. By default uses the index.", "Column to plot. By default uses all columns.", "Area plots are stacked by default. Set to False to create a unstacked plot.", "Additional keyword arguments are documented in DataFrame.plot().", "Area plot, or array of area plots if subplots is True.", "See also", "Make plots of DataFrame using matplotlib / pylab.", "Examples", "Draw an area plot based on basic business metrics:", "Area plots are stacked by default. To produce an unstacked plot, pass stacked=False:", "Draw an area plot for a single column:", "Draw with a different x:"]}, {"name": "pandas.DataFrame.plot.bar", "path": "reference/api/pandas.dataframe.plot.bar", "type": "DataFrame", "text": ["Vertical bar plot.", "A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.", "Allows plotting of one column versus another. If not specified, the index of the DataFrame is used.", "Allows plotting of one column versus another. If not specified, all numerical columns are used.", "The color for each of the DataFrame\u2019s columns. Possible values are:", "for instance \u2018red\u2019 or \u2018#a98d19\u2019.", "code, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.", "colored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.", "New in version 1.1.0.", "Additional keyword arguments are documented in DataFrame.plot().", "An ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.", "See also", "Horizontal bar plot.", "Make plots of a DataFrame.", "Make a bar plot with matplotlib.", "Examples", "Basic plot.", "Plot a whole dataframe to a bar plot. Each column is assigned a distinct color, and each row is nested in a group along the horizontal axis.", "Plot stacked bar charts for the DataFrame", "Instead of nesting, the figure can be split by column with subplots=True. In this case, a numpy.ndarray of matplotlib.axes.Axes are returned.", "If you don\u2019t like the default colours, you can specify how you\u2019d like each column to be colored.", "Plot a single column.", "Plot only selected categories for the DataFrame."]}, {"name": "pandas.DataFrame.plot.barh", "path": "reference/api/pandas.dataframe.plot.barh", "type": "DataFrame", "text": ["Make a horizontal bar plot.", "A horizontal bar plot is a plot that presents quantitative data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.", "Allows plotting of one column versus another. If not specified, the index of the DataFrame is used.", "Allows plotting of one column versus another. If not specified, all numerical columns are used.", "The color for each of the DataFrame\u2019s columns. Possible values are:", "for instance \u2018red\u2019 or \u2018#a98d19\u2019.", "code, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.", "colored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.", "New in version 1.1.0.", "Additional keyword arguments are documented in DataFrame.plot().", "An ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.", "See also", "Vertical bar plot.", "Make plots of DataFrame using matplotlib.", "Plot a vertical bar plot using matplotlib.", "Examples", "Basic example", "Plot a whole DataFrame to a horizontal bar plot", "Plot stacked barh charts for the DataFrame", "We can specify colors for each column", "Plot a column of the DataFrame to a horizontal bar plot", "Plot DataFrame versus the desired column"]}, {"name": "pandas.DataFrame.plot.box", "path": "reference/api/pandas.dataframe.plot.box", "type": "DataFrame", "text": ["Make a box plot of the DataFrame columns.", "A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of box to show the range of the data. The position of the whiskers is set by default to 1.5*IQR (IQR = Q3 - Q1) from the edges of the box. Outlier points are those past the end of the whiskers.", "For further details see Wikipedia\u2019s entry for boxplot.", "A consideration when using this chart is that the box and the whiskers can overlap, which is very common when plotting small sets of data.", "Column in the DataFrame to group by.", "Changed in version 1.4.0: Previously, by is silently ignore and makes no groupings", "Additional keywords are documented in DataFrame.plot().", "See also", "Another method to draw a box plot.", "Draw a box plot from a Series object.", "Draw a box plot in matplotlib.", "Examples", "Draw a box plot from a DataFrame with four columns of randomly generated data.", "You can also generate groupings if you specify the by parameter (which can take a column name, or a list or tuple of column names):", "Changed in version 1.4.0."]}, {"name": "pandas.DataFrame.plot.density", "path": "reference/api/pandas.dataframe.plot.density", "type": "DataFrame", "text": ["Generate Kernel Density Estimate plot using Gaussian kernels.", "In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function (PDF) of a random variable. This function uses Gaussian kernels and includes automatic bandwidth determination.", "The method used to calculate the estimator bandwidth. This can be \u2018scott\u2019, \u2018silverman\u2019, a scalar constant or a callable. If None (default), \u2018scott\u2019 is used. See scipy.stats.gaussian_kde for more information.", "Evaluation points for the estimated PDF. If None (default), 1000 equally spaced points are used. If ind is a NumPy array, the KDE is evaluated at the points passed. If ind is an integer, ind number of equally spaced points are used.", "Additional keyword arguments are documented in pandas.%(this-datatype)s.plot().", "See also", "Representation of a kernel-density estimate using Gaussian kernels. This is the function used internally to estimate the PDF.", "Examples", "Given a Series of points randomly sampled from an unknown distribution, estimate its PDF using KDE with automatic bandwidth determination and plot the results, evaluating them at 1000 equally spaced points (default):", "A scalar bandwidth can be specified. Using a small bandwidth value can lead to over-fitting, while using a large bandwidth value may result in under-fitting:", "Finally, the ind parameter determines the evaluation points for the plot of the estimated PDF:", "For DataFrame, it works in the same way:", "A scalar bandwidth can be specified. Using a small bandwidth value can lead to over-fitting, while using a large bandwidth value may result in under-fitting:", "Finally, the ind parameter determines the evaluation points for the plot of the estimated PDF:"]}, {"name": "pandas.DataFrame.plot.hexbin", "path": "reference/api/pandas.dataframe.plot.hexbin", "type": "DataFrame", "text": ["Generate a hexagonal binning plot.", "Generate a hexagonal binning plot of x versus y. If C is None (the default), this is a histogram of the number of occurrences of the observations at (x[i], y[i]).", "If C is specified, specifies values at given coordinates (x[i], y[i]). These values are accumulated for each hexagonal bin and then reduced according to reduce_C_function, having as default the NumPy\u2019s mean function (numpy.mean()). (If C is specified, it must also be a 1-D sequence of the same length as x and y, or a column label.)", "The column label or position for x points.", "The column label or position for y points.", "The column label or position for the value of (x, y) point.", "Function of one argument that reduces all the values in a bin to a single number (e.g. np.mean, np.max, np.sum, np.std).", "The number of hexagons in the x-direction. The corresponding number of hexagons in the y-direction is chosen in a way that the hexagons are approximately regular. Alternatively, gridsize can be a tuple with two elements specifying the number of hexagons in the x-direction and the y-direction.", "Additional keyword arguments are documented in DataFrame.plot().", "The matplotlib Axes on which the hexbin is plotted.", "See also", "Make plots of a DataFrame.", "Hexagonal binning plot using matplotlib, the matplotlib function that is used under the hood.", "Examples", "The following examples are generated with random data from a normal distribution.", "The next example uses C and np.sum as reduce_C_function. Note that \u2018observations\u2019 values ranges from 1 to 5 but the result plot shows values up to more than 25. This is because of the reduce_C_function."]}, {"name": "pandas.DataFrame.plot.hist", "path": "reference/api/pandas.dataframe.plot.hist", "type": "DataFrame", "text": ["Draw one histogram of the DataFrame\u2019s columns.", "A histogram is a representation of the distribution of data. This function groups the values of all given Series in the DataFrame into bins and draws all bins in one matplotlib.axes.Axes. This is useful when the DataFrame\u2019s Series are in a similar scale.", "Column in the DataFrame to group by.", "Changed in version 1.4.0: Previously, by is silently ignore and makes no groupings", "Number of histogram bins to be used.", "Additional keyword arguments are documented in DataFrame.plot().", "Return a histogram plot.", "See also", "Draw histograms per DataFrame\u2019s Series.", "Draw a histogram with Series\u2019 data.", "Examples", "When we roll a die 6000 times, we expect to get each value around 1000 times. But when we roll two dice and sum the result, the distribution is going to be quite different. A histogram illustrates those distributions.", "A grouped histogram can be generated by providing the parameter by (which can be a column name, or a list of column names):"]}, {"name": "pandas.DataFrame.plot.kde", "path": "reference/api/pandas.dataframe.plot.kde", "type": "DataFrame", "text": ["Generate Kernel Density Estimate plot using Gaussian kernels.", "In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function (PDF) of a random variable. This function uses Gaussian kernels and includes automatic bandwidth determination.", "The method used to calculate the estimator bandwidth. This can be \u2018scott\u2019, \u2018silverman\u2019, a scalar constant or a callable. If None (default), \u2018scott\u2019 is used. See scipy.stats.gaussian_kde for more information.", "Evaluation points for the estimated PDF. If None (default), 1000 equally spaced points are used. If ind is a NumPy array, the KDE is evaluated at the points passed. If ind is an integer, ind number of equally spaced points are used.", "Additional keyword arguments are documented in pandas.%(this-datatype)s.plot().", "See also", "Representation of a kernel-density estimate using Gaussian kernels. This is the function used internally to estimate the PDF.", "Examples", "Given a Series of points randomly sampled from an unknown distribution, estimate its PDF using KDE with automatic bandwidth determination and plot the results, evaluating them at 1000 equally spaced points (default):", "A scalar bandwidth can be specified. Using a small bandwidth value can lead to over-fitting, while using a large bandwidth value may result in under-fitting:", "Finally, the ind parameter determines the evaluation points for the plot of the estimated PDF:", "For DataFrame, it works in the same way:", "A scalar bandwidth can be specified. Using a small bandwidth value can lead to over-fitting, while using a large bandwidth value may result in under-fitting:", "Finally, the ind parameter determines the evaluation points for the plot of the estimated PDF:"]}, {"name": "pandas.DataFrame.plot.line", "path": "reference/api/pandas.dataframe.plot.line", "type": "DataFrame", "text": ["Plot Series or DataFrame as lines.", "This function is useful to plot lines using DataFrame\u2019s values as coordinates.", "Allows plotting of one column versus another. If not specified, the index of the DataFrame is used.", "Allows plotting of one column versus another. If not specified, all numerical columns are used.", "The color for each of the DataFrame\u2019s columns. Possible values are:", "for instance \u2018red\u2019 or \u2018#a98d19\u2019.", "code, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s line will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.", "colored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color lines for column a in green and lines for column b in red.", "New in version 1.1.0.", "Additional keyword arguments are documented in DataFrame.plot().", "An ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.", "See also", "Plot y versus x as lines and/or markers.", "Examples", "The following example shows the populations for some animals over the years.", "An example with subplots, so an array of axes is returned.", "Let\u2019s repeat the same example, but specifying colors for each column (in this case, for each animal).", "The following example shows the relationship between both populations."]}, {"name": "pandas.DataFrame.plot.pie", "path": "reference/api/pandas.dataframe.plot.pie", "type": "DataFrame", "text": ["Generate a pie plot.", "A pie plot is a proportional representation of the numerical data in a column. This function wraps matplotlib.pyplot.pie() for the specified column. If no column reference is passed and subplots=True a pie plot is drawn for each numerical column independently.", "Label or position of the column to plot. If not provided, subplots=True argument must be passed.", "Keyword arguments to pass on to DataFrame.plot().", "A NumPy array is returned when subplots is True.", "See also", "Generate a pie plot for a Series.", "Make plots of a DataFrame.", "Examples", "In the example below we have a DataFrame with the information about planet\u2019s mass and radius. We pass the \u2018mass\u2019 column to the pie function to get a pie plot."]}, {"name": "pandas.DataFrame.plot.scatter", "path": "reference/api/pandas.dataframe.plot.scatter", "type": "DataFrame", "text": ["Create a scatter plot with varying marker point size and color.", "The coordinates of each point are defined by two dataframe columns and filled circles are used to represent each point. This kind of plot is useful to see complex correlations between two variables. Points could be for instance natural 2D coordinates like longitude and latitude in a map or, in general, any pair of metrics that can be plotted against each other.", "The column name or column position to be used as horizontal coordinates for each point.", "The column name or column position to be used as vertical coordinates for each point.", "The size of each point. Possible values are:", "A string with the name of the column to be used for marker\u2019s size.", "A single scalar so all points have the same size.", "A sequence of scalars, which will be used for each point\u2019s size recursively. For instance, when passing [2,14] all points size will be either 2 or 14, alternatively.", "Changed in version 1.1.0.", "The color of each point. Possible values are:", "A single color string referred to by name, RGB or RGBA code, for instance \u2018red\u2019 or \u2018#a98d19\u2019.", "A sequence of color strings referred to by name, RGB or RGBA code, which will be used for each point\u2019s color recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] all points will be filled in green or yellow, alternatively.", "A column name or position whose values will be used to color the marker points according to a colormap.", "Keyword arguments to pass on to DataFrame.plot().", "See also", "Scatter plot using multiple input data formats.", "Examples", "Let\u2019s see how to draw a scatter plot using coordinates from the values in a DataFrame\u2019s columns.", "And now with the color determined by a column as well."]}, {"name": "pandas.DataFrame.pop", "path": "reference/api/pandas.dataframe.pop", "type": "DataFrame", "text": ["Return item and drop from frame. Raise KeyError if not found.", "Label of column to be popped.", "Examples"]}, {"name": "pandas.DataFrame.pow", "path": "reference/api/pandas.dataframe.pow", "type": "DataFrame", "text": ["Get Exponential power of dataframe and other, element-wise (binary operator pow).", "Equivalent to dataframe ** other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rpow.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.prod", "path": "reference/api/pandas.dataframe.prod", "type": "DataFrame", "text": ["Return the product of the values over the requested axis.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Additional keyword arguments to be passed to the function.", "See also", "Return the sum.", "Return the minimum.", "Return the maximum.", "Return the index of the minimum.", "Return the index of the maximum.", "Return the sum over the requested axis.", "Return the minimum over the requested axis.", "Return the maximum over the requested axis.", "Return the index of the minimum over the requested axis.", "Return the index of the maximum over the requested axis.", "Examples", "By default, the product of an empty or all-NA Series is 1", "This can be controlled with the min_count parameter", "Thanks to the skipna parameter, min_count handles all-NA and empty series identically."]}, {"name": "pandas.DataFrame.product", "path": "reference/api/pandas.dataframe.product", "type": "DataFrame", "text": ["Return the product of the values over the requested axis.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Additional keyword arguments to be passed to the function.", "See also", "Return the sum.", "Return the minimum.", "Return the maximum.", "Return the index of the minimum.", "Return the index of the maximum.", "Return the sum over the requested axis.", "Return the minimum over the requested axis.", "Return the maximum over the requested axis.", "Return the index of the minimum over the requested axis.", "Return the index of the maximum over the requested axis.", "Examples", "By default, the product of an empty or all-NA Series is 1", "This can be controlled with the min_count parameter", "Thanks to the skipna parameter, min_count handles all-NA and empty series identically."]}, {"name": "pandas.DataFrame.quantile", "path": "reference/api/pandas.dataframe.quantile", "type": "DataFrame", "text": ["Return values at the given quantile over requested axis.", "Value between 0 <= q <= 1, the quantile(s) to compute.", "Equals 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.", "If False, the quantile of datetime and timedelta data will be computed as well.", "This optional parameter specifies the interpolation method to use, when the desired quantile lies between two data points i and j:", "linear: i + (j - i) * fraction, where fraction is the fractional part of the index surrounded by i and j.", "lower: i.", "higher: j.", "nearest: i or j whichever is nearest.", "midpoint: (i + j) / 2.", "index is q, the columns are the columns of self, and the values are the quantiles.", "index is the columns of self and the values are the quantiles.", "See also", "Rolling quantile.", "Numpy function to compute the percentile.", "Examples", "Specifying numeric_only=False will also compute the quantile of datetime and timedelta data."]}, {"name": "pandas.DataFrame.query", "path": "reference/api/pandas.dataframe.query", "type": "DataFrame", "text": ["Query the columns of a DataFrame with a boolean expression.", "The query string to evaluate.", "You can refer to variables in the environment by prefixing them with an \u2018@\u2019 character like @a + b.", "You can refer to column names that are not valid Python variable names by surrounding them in backticks. Thus, column names containing spaces or punctuations (besides underscores) or starting with digits must be surrounded by backticks. (For example, a column named \u201cArea (cm^2)\u201d would be referenced as `Area (cm^2)`). Column names which are Python keywords (like \u201clist\u201d, \u201cfor\u201d, \u201cimport\u201d, etc) cannot be used.", "For example, if one of your columns is called a a and you want to sum it with b, your query should be `a a` + b.", "New in version 0.25.0: Backtick quoting introduced.", "New in version 1.0.0: Expanding functionality of backtick quoting for more than only spaces.", "Whether the query should modify the data in place or return a modified copy.", "See the documentation for eval() for complete details on the keyword arguments accepted by DataFrame.query().", "DataFrame resulting from the provided query expression or None if inplace=True.", "See also", "Evaluate a string describing operations on DataFrame columns.", "Evaluate a string describing operations on DataFrame columns.", "Notes", "The result of the evaluation of this expression is first passed to DataFrame.loc and if that fails because of a multidimensional key (e.g., a DataFrame) then the result will be passed to DataFrame.__getitem__().", "This method uses the top-level eval() function to evaluate the passed query.", "The query() method uses a slightly modified Python syntax by default. For example, the & and | (bitwise) operators have the precedence of their boolean cousins, and and or. This is syntactically valid Python, however the semantics are different.", "You can change the semantics of the expression by passing the keyword argument parser='python'. This enforces the same semantics as evaluation in Python space. Likewise, you can pass engine='python' to evaluate an expression using Python itself as a backend. This is not recommended as it is inefficient compared to using numexpr as the engine.", "The DataFrame.index and DataFrame.columns attributes of the DataFrame instance are placed in the query namespace by default, which allows you to treat both the index and columns of the frame as a column in the frame. The identifier index is used for the frame index; you can also use the name of the index to identify it in a query. Please note that Python keywords may not be used as identifiers.", "For further details and examples see the query documentation in indexing.", "Backtick quoted variables", "Backtick quoted variables are parsed as literal Python code and are converted internally to a Python valid identifier. This can lead to the following problems.", "During parsing a number of disallowed characters inside the backtick quoted string are replaced by strings that are allowed as a Python identifier. These characters include all operators in Python, the space character, the question mark, the exclamation mark, the dollar sign, and the euro sign. For other characters that fall outside the ASCII range (U+0001..U+007F) and those that are not further specified in PEP 3131, the query parser will raise an error. This excludes whitespace different than the space character, but also the hashtag (as it is used for comments) and the backtick itself (backtick can also not be escaped).", "In a special case, quotes that make a pair around a backtick can confuse the parser. For example, `it's` > `that's` will raise an error, as it forms a quoted string ('s > `that') with a backtick inside.", "See also the Python documentation about lexical analysis (https://docs.python.org/3/reference/lexical_analysis.html) in combination with the source code in pandas.core.computation.parsing.", "Examples", "The previous expression is equivalent to", "For columns with spaces in their name, you can use backtick quoting.", "The previous expression is equivalent to"]}, {"name": "pandas.DataFrame.radd", "path": "reference/api/pandas.dataframe.radd", "type": "DataFrame", "text": ["Get Addition of dataframe and other, element-wise (binary operator radd).", "Equivalent to other + dataframe, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, add.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.rank", "path": "reference/api/pandas.dataframe.rank", "type": "DataFrame", "text": ["Compute numerical data ranks (1 through n) along axis.", "By default, equal values are assigned a rank that is the average of the ranks of those values.", "Index to direct ranking.", "How to rank the group of records that have the same value (i.e. ties):", "average: average rank of the group", "min: lowest rank in the group", "max: highest rank in the group", "first: ranks assigned in order they appear in the array", "dense: like \u2018min\u2019, but rank always increases by 1 between groups.", "For DataFrame objects, rank only numeric columns if set to True.", "How to rank NaN values:", "keep: assign NaN rank to NaN values", "top: assign lowest rank to NaN values", "bottom: assign highest rank to NaN values", "Whether or not the elements should be ranked in ascending order.", "Whether or not to display the returned rankings in percentile form.", "Return a Series or DataFrame with data ranks as values.", "See also", "Rank of values within each group.", "Examples", "The following example shows how the method behaves with the above parameters:", "default_rank: this is the default behaviour obtained without using any parameter.", "max_rank: setting method = 'max' the records that have the same values are ranked using the highest rank (e.g.: since \u2018cat\u2019 and \u2018dog\u2019 are both in the 2nd and 3rd position, rank 3 is assigned.)", "NA_bottom: choosing na_option = 'bottom', if there are records with NaN values they are placed at the bottom of the ranking.", "pct_rank: when setting pct = True, the ranking is expressed as percentile rank."]}, {"name": "pandas.DataFrame.rdiv", "path": "reference/api/pandas.dataframe.rdiv", "type": "DataFrame", "text": ["Get Floating division of dataframe and other, element-wise (binary operator rtruediv).", "Equivalent to other / dataframe, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, truediv.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.reindex", "path": "reference/api/pandas.dataframe.reindex", "type": "DataFrame", "text": ["Conform Series/DataFrame to new index with optional filling logic.", "Places NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.", "New labels / index to conform to, should be specified using keywords. Preferably an Index object to avoid duplicating data.", "Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index.", "None (default): don\u2019t fill gaps", "pad / ffill: Propagate last valid observation forward to next valid.", "backfill / bfill: Use next valid observation to fill gap.", "nearest: Use nearest valid observations to fill gap.", "Return a new object, even if the passed indexes are the same.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Value to use for missing values. Defaults to NaN, but can be any \u201ccompatible\u201d value.", "Maximum number of consecutive elements to forward or backward fill.", "Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations most satisfy the equation abs(index[indexer] - target) <= tolerance.", "Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index\u2019s type.", "See also", "Set row labels.", "Remove row labels or move them to new columns.", "Change to same indices as other DataFrame.", "Examples", "DataFrame.reindex supports two calling conventions", "(index=index_labels, columns=column_labels, ...)", "(labels, axis={'index', 'columns'}, ...)", "We highly recommend using keyword arguments to clarify your intent.", "Create a dataframe with some fictional data.", "Create a new index and reindex the dataframe. By default values in the new index that do not have corresponding records in the dataframe are assigned NaN.", "We can fill in the missing values by passing a value to the keyword fill_value. Because the index is not monotonically increasing or decreasing, we cannot use arguments to the keyword method to fill the NaN values.", "We can also reindex the columns.", "Or we can use \u201caxis-style\u201d keyword arguments", "To further illustrate the filling functionality in reindex, we will create a dataframe with a monotonically increasing index (for example, a sequence of dates).", "Suppose we decide to expand the dataframe to cover a wider date range.", "The index entries that did not have a value in the original data frame (for example, \u20182009-12-29\u2019) are by default filled with NaN. If desired, we can fill in the missing values using one of several options.", "For example, to back-propagate the last valid value to fill the NaN values, pass bfill as an argument to the method keyword.", "Please note that the NaN value present in the original dataframe (at index value 2010-01-03) will not be filled by any of the value propagation schemes. This is because filling while reindexing does not look at dataframe values, but only compares the original and desired indexes. If you do want to fill in the NaN values present in the original dataframe, use the fillna() method.", "See the user guide for more."]}, {"name": "pandas.DataFrame.reindex_like", "path": "reference/api/pandas.dataframe.reindex_like", "type": "DataFrame", "text": ["Return an object with matching indices as other object.", "Conform the object to the same index on all axes. Optional filling logic, placing NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.", "Its row and column indices are used to define the new indices of this object.", "Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index.", "None (default): don\u2019t fill gaps", "pad / ffill: propagate last valid observation forward to next valid", "backfill / bfill: use next valid observation to fill gap", "nearest: use nearest valid observations to fill gap.", "Return a new object, even if the passed indexes are the same.", "Maximum number of consecutive labels to fill for inexact matches.", "Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance.", "Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index\u2019s type.", "Same type as caller, but with changed indices on each axis.", "See also", "Set row labels.", "Remove row labels or move them to new columns.", "Change to new indices or expand indices.", "Notes", "Same as calling .reindex(index=other.index, columns=other.columns,...).", "Examples"]}, {"name": "pandas.DataFrame.rename", "path": "reference/api/pandas.dataframe.rename", "type": "DataFrame", "text": ["Alter axes labels.", "Function / dict values must be unique (1-to-1). Labels not contained in a dict / Series will be left as-is. Extra labels listed don\u2019t throw an error.", "See the user guide for more.", "Dict-like or function transformations to apply to that axis\u2019 values. Use either mapper and axis to specify the axis to target with mapper, or index and columns.", "Alternative to specifying axis (mapper, axis=0 is equivalent to index=mapper).", "Alternative to specifying axis (mapper, axis=1 is equivalent to columns=mapper).", "Axis to target with mapper. Can be either the axis name (\u2018index\u2019, \u2018columns\u2019) or number (0, 1). The default is \u2018index\u2019.", "Also copy underlying data.", "Whether to return a new DataFrame. If True then value of copy is ignored.", "In case of a MultiIndex, only rename labels in the specified level.", "If \u2018raise\u2019, raise a KeyError when a dict-like mapper, index, or columns contains labels that are not present in the Index being transformed. If \u2018ignore\u2019, existing keys will be renamed and extra keys will be ignored.", "DataFrame with the renamed axis labels or None if inplace=True.", "If any of the labels is not found in the selected axis and \u201cerrors=\u2019raise\u2019\u201d.", "See also", "Set the name of the axis.", "Examples", "DataFrame.rename supports two calling conventions", "(index=index_mapper, columns=columns_mapper, ...)", "(mapper, axis={'index', 'columns'}, ...)", "We highly recommend using keyword arguments to clarify your intent.", "Rename columns using a mapping:", "Rename index using a mapping:", "Cast index labels to a different type:", "Using axis-style parameters:"]}, {"name": "pandas.DataFrame.rename_axis", "path": "reference/api/pandas.dataframe.rename_axis", "type": "DataFrame", "text": ["Set the name of the axis for the index or columns.", "Value to set the axis name attribute.", "A scalar, list-like, dict-like or functions transformations to apply to that axis\u2019 values. Note that the columns parameter is not allowed if the object is a Series. This parameter only apply for DataFrame type objects.", "Use either mapper and axis to specify the axis to target with mapper, or index and/or columns.", "The axis to rename.", "Also copy underlying data.", "Modifies the object directly, instead of creating a new Series or DataFrame.", "The same type as the caller or None if inplace=True.", "See also", "Alter Series index labels or name.", "Alter DataFrame index labels or name.", "Set new names on index.", "Notes", "DataFrame.rename_axis supports two calling conventions", "(index=index_mapper, columns=columns_mapper, ...)", "(mapper, axis={'index', 'columns'}, ...)", "The first calling convention will only modify the names of the index and/or the names of the Index object that is the columns. In this case, the parameter copy is ignored.", "The second calling convention will modify the names of the corresponding index if mapper is a list or a scalar. However, if mapper is dict-like or a function, it will use the deprecated behavior of modifying the axis labels.", "We highly recommend using keyword arguments to clarify your intent.", "Examples", "Series", "DataFrame", "MultiIndex"]}, {"name": "pandas.DataFrame.reorder_levels", "path": "reference/api/pandas.dataframe.reorder_levels", "type": "DataFrame", "text": ["Rearrange index levels using input order. May not drop or duplicate levels.", "List representing new level order. Reference level by number (position) or by key (label).", "Where to reorder levels.", "Examples", "Let\u2019s reorder the levels of the index:"]}, {"name": "pandas.DataFrame.replace", "path": "reference/api/pandas.dataframe.replace", "type": "DataFrame", "text": ["Replace values given in to_replace with value.", "Values of the DataFrame are replaced with other values dynamically.", "This differs from updating with .loc or .iloc, which require you to specify a location to update with some value.", "How to find the values that will be replaced.", "numeric, str or regex:", "numeric: numeric values equal to to_replace will be replaced with value", "str: string exactly matching to_replace will be replaced with value", "regex: regexs matching to_replace will be replaced with value", "list of str, regex, or numeric:", "First, if to_replace and value are both lists, they must be the same length.", "Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. This doesn\u2019t matter much for value since there are only a few possible substitution regexes you can use.", "str, regex and numeric rules apply as above.", "dict:", "Dicts can be used to specify different replacement values for different existing values. For example, {'a': 'b', 'y': 'z'} replaces the value \u2018a\u2019 with \u2018b\u2019 and \u2018y\u2019 with \u2018z\u2019. To use a dict in this way the value parameter should be None.", "For a DataFrame a dict can specify that different values should be replaced in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1 in column \u2018a\u2019 and the value \u2018z\u2019 in column \u2018b\u2019 and replaces these values with whatever is specified in value. The value parameter should not be None in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in.", "For a DataFrame nested dictionaries, e.g., {'a': {'b': np.nan}}, are read as follows: look in column \u2018a\u2019 for the value \u2018b\u2019 and replace it with NaN. The value parameter should be None to use a nested dict in this way. You can nest regular expressions as well. Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions.", "None:", "This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series.", "See the examples section for examples of each of these.", "Value to replace any values matching to_replace with. For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed.", "If True, performs operation inplace and returns None.", "Maximum size gap to forward or backward fill.", "Whether to interpret to_replace and/or value as regular expressions. If this is True then to_replace must be a string. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None.", "The method to use when for replacement, when to_replace is a scalar, list or tuple and value is None.", "Changed in version 0.23.0: Added to DataFrame.", "Object after replacement.", "If regex is not a bool and to_replace is not None.", "If to_replace is not a scalar, array-like, dict, or None", "If to_replace is a dict and value is not a list, dict, ndarray, or Series", "If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series.", "When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced", "If a list or an ndarray is passed to to_replace and value but they are not the same length.", "See also", "Fill NA values.", "Replace values based on boolean condition.", "Simple string replacement.", "Notes", "Regex substitution is performed under the hood with re.sub. The rules for substitution for re.sub are the same.", "Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this.", "This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works.", "When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter.", "Examples", "Scalar `to_replace` and `value`", "List-like `to_replace`", "dict-like `to_replace`", "Regular expression `to_replace`", "Compare the behavior of s.replace({'a': None}) and s.replace('a', None) to understand the peculiarities of the to_replace parameter:", "When one uses a dict as the to_replace value, it is like the value(s) in the dict are equal to the value parameter. s.replace({'a': None}) is equivalent to s.replace(to_replace={'a': None}, value=None, method=None):", "When value is not explicitly passed and to_replace is a scalar, list or tuple, replace uses the method parameter (default \u2018pad\u2019) to do the replacement. So this is why the \u2018a\u2019 values are being replaced by 10 in rows 1 and 2 and \u2018b\u2019 in row 4 in this case.", "On the other hand, if None is explicitly passed for value, it will be respected:", "Changed in version 1.4.0: Previously the explicit None was silently ignored."]}, {"name": "pandas.DataFrame.resample", "path": "reference/api/pandas.dataframe.resample", "type": "DataFrame", "text": ["Resample time-series data.", "Convenience method for frequency conversion and resampling of time series. The object must have a datetime-like index (DatetimeIndex, PeriodIndex, or TimedeltaIndex), or the caller must pass the label of a datetime-like series/index to the on/level keyword parameter.", "The offset string or object representing target conversion.", "Which axis to use for up- or down-sampling. For Series this will default to 0, i.e. along the rows. Must be DatetimeIndex, TimedeltaIndex or PeriodIndex.", "Which side of bin interval is closed. The default is \u2018left\u2019 for all frequency offsets except for \u2018M\u2019, \u2018A\u2019, \u2018Q\u2019, \u2018BM\u2019, \u2018BA\u2019, \u2018BQ\u2019, and \u2018W\u2019 which all have a default of \u2018right\u2019.", "Which bin edge label to label bucket with. The default is \u2018left\u2019 for all frequency offsets except for \u2018M\u2019, \u2018A\u2019, \u2018Q\u2019, \u2018BM\u2019, \u2018BA\u2019, \u2018BQ\u2019, and \u2018W\u2019 which all have a default of \u2018right\u2019.", "For PeriodIndex only, controls whether to use the start or end of rule.", "Pass \u2018timestamp\u2019 to convert the resulting index to a DateTimeIndex or \u2018period\u2019 to convert it to a PeriodIndex. By default the input representation is retained.", "Adjust the resampled time labels.", "Deprecated since version 1.1.0: You should add the loffset to the df.index after the resample. See below.", "For frequencies that evenly subdivide 1 day, the \u201corigin\u201d of the aggregated intervals. For example, for \u20185min\u2019 frequency, base could range from 0 through 4. Defaults to 0.", "Deprecated since version 1.1.0: The new arguments that you should use are \u2018offset\u2019 or \u2018origin\u2019.", "For a DataFrame, column to use instead of index for resampling. Column must be datetime-like.", "For a MultiIndex, level (name or number) to use for resampling. level must be datetime-like.", "The timestamp on which to adjust the grouping. The timezone of origin must match the timezone of the index. If string, must be one of the following:", "\u2018epoch\u2019: origin is 1970-01-01", "\u2018start\u2019: origin is the first value of the timeseries", "\u2018start_day\u2019: origin is the first day at midnight of the timeseries", "New in version 1.1.0.", "\u2018end\u2019: origin is the last value of the timeseries", "\u2018end_day\u2019: origin is the ceiling midnight of the last day", "New in version 1.3.0.", "An offset timedelta added to the origin.", "New in version 1.1.0.", "Resampler object.", "See also", "Resample a Series.", "Resample a DataFrame.", "Group DataFrame by mapping, function, label, or list of labels.", "Reindex a DataFrame with the given frequency without grouping.", "Notes", "See the user guide for more.", "To learn more about the offset strings, please see this link.", "Examples", "Start by creating a series with 9 one minute timestamps.", "Downsample the series into 3 minute bins and sum the values of the timestamps falling into a bin.", "Downsample the series into 3 minute bins as above, but label each bin using the right edge instead of the left. Please note that the value in the bucket used as the label is not included in the bucket, which it labels. For example, in the original series the bucket 2000-01-01 00:03:00 contains the value 3, but the summed value in the resampled bucket with the label 2000-01-01 00:03:00 does not include 3 (if it did, the summed value would be 6, not 3). To include this value close the right side of the bin interval as illustrated in the example below this one.", "Downsample the series into 3 minute bins as above, but close the right side of the bin interval.", "Upsample the series into 30 second bins.", "Upsample the series into 30 second bins and fill the NaN values using the pad method.", "Upsample the series into 30 second bins and fill the NaN values using the bfill method.", "Pass a custom function via apply", "For a Series with a PeriodIndex, the keyword convention can be used to control whether to use the start or end of rule.", "Resample a year by quarter using \u2018start\u2019 convention. Values are assigned to the first quarter of the period.", "Resample quarters by month using \u2018end\u2019 convention. Values are assigned to the last month of the period.", "For DataFrame objects, the keyword on can be used to specify the column instead of the index for resampling.", "For a DataFrame with MultiIndex, the keyword level can be used to specify on which level the resampling needs to take place.", "If you want to adjust the start of the bins based on a fixed timestamp:", "If you want to adjust the start of the bins with an offset Timedelta, the two following lines are equivalent:", "If you want to take the largest Timestamp as the end of the bins:", "In contrast with the start_day, you can use end_day to take the ceiling midnight of the largest Timestamp as the end of the bins and drop the bins not containing data:", "To replace the use of the deprecated base argument, you can now use offset, in this example it is equivalent to have base=2:", "To replace the use of the deprecated loffset argument:"]}, {"name": "pandas.DataFrame.reset_index", "path": "reference/api/pandas.dataframe.reset_index", "type": "DataFrame", "text": ["Reset the index, or a level of it.", "Reset the index of the DataFrame, and use the default one instead. If the DataFrame has a MultiIndex, this method can remove one or more levels.", "Only remove the given levels from the index. Removes all levels by default.", "Do not try to insert index into dataframe columns. This resets the index to the default integer index.", "Modify the DataFrame in place (do not create a new object).", "If the columns have multiple levels, determines which level the labels are inserted into. By default it is inserted into the first level.", "If the columns have multiple levels, determines how the other levels are named. If None then the index name is repeated.", "DataFrame with the new index or None if inplace=True.", "See also", "Opposite of reset_index.", "Change to new indices or expand indices.", "Change to same indices as other DataFrame.", "Examples", "When we reset the index, the old index is added as a column, and a new sequential index is used:", "We can use the drop parameter to avoid the old index being added as a column:", "You can also use reset_index with MultiIndex.", "If the index has multiple levels, we can reset a subset of them:", "If we are not dropping the index, by default, it is placed in the top level. We can place it in another level:", "When the index is inserted under another level, we can specify under which one with the parameter col_fill:", "If we specify a nonexistent level for col_fill, it is created:"]}, {"name": "pandas.DataFrame.rfloordiv", "path": "reference/api/pandas.dataframe.rfloordiv", "type": "DataFrame", "text": ["Get Integer division of dataframe and other, element-wise (binary operator rfloordiv).", "Equivalent to other // dataframe, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, floordiv.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.rmod", "path": "reference/api/pandas.dataframe.rmod", "type": "DataFrame", "text": ["Get Modulo of dataframe and other, element-wise (binary operator rmod).", "Equivalent to other % dataframe, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, mod.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.rmul", "path": "reference/api/pandas.dataframe.rmul", "type": "DataFrame", "text": ["Get Multiplication of dataframe and other, element-wise (binary operator rmul).", "Equivalent to other * dataframe, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, mul.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.rolling", "path": "reference/api/pandas.dataframe.rolling", "type": "DataFrame", "text": ["Provide rolling window calculations.", "Size of the moving window.", "If an integer, the fixed number of observations used for each window.", "If an offset, the time period of each window. Each window will be a variable sized based on the observations included in the time-period. This is only valid for datetimelike indexes. To learn more about the offsets & frequency strings, please see this link.", "If a BaseIndexer subclass, the window boundaries based on the defined get_window_bounds method. Additional rolling keyword arguments, namely min_periods, center, and closed will be passed to get_window_bounds.", "Minimum number of observations in window required to have a value; otherwise, result is np.nan.", "For a window that is specified by an offset, min_periods will default to 1.", "For a window that is specified by an integer, min_periods will default to the size of the window.", "If False, set the window labels as the right edge of the window index.", "If True, set the window labels as the center of the window index.", "If None, all points are evenly weighted.", "If a string, it must be a valid scipy.signal window function.", "Certain Scipy window types require additional parameters to be passed in the aggregation function. The additional parameters must match the keywords specified in the Scipy window type method signature.", "For a DataFrame, a column label or Index level on which to calculate the rolling window, rather than the DataFrame\u2019s index.", "Provided integer column is ignored and excluded from result since an integer index is not used to calculate the rolling window.", "If 0 or 'index', roll across the rows.", "If 1 or 'columns', roll across the columns.", "If 'right', the first point in the window is excluded from calculations.", "If 'left', the last point in the window is excluded from calculations.", "If 'both', the no points in the window are excluded from calculations.", "If 'neither', the first and last points in the window are excluded from calculations.", "Default None ('right').", "Changed in version 1.2.0: The closed parameter with fixed windows is now supported.", "New in version 1.3.0.", "Execute the rolling operation per single column or row ('single') or over the entire object ('table').", "This argument is only implemented when specifying engine='numba' in the method call.", "See also", "Provides expanding transformations.", "Provides exponential weighted functions.", "Notes", "See Windowing Operations for further usage details and examples.", "Examples", "window", "Rolling sum with a window length of 2 observations.", "Rolling sum with a window span of 2 seconds.", "Rolling sum with forward looking windows with 2 observations.", "min_periods", "Rolling sum with a window length of 2 observations, but only needs a minimum of 1 observation to calculate a value.", "center", "Rolling sum with the result assigned to the center of the window index.", "win_type", "Rolling sum with a window length of 2, using the Scipy 'gaussian' window type. std is required in the aggregation function."]}, {"name": "pandas.DataFrame.round", "path": "reference/api/pandas.dataframe.round", "type": "DataFrame", "text": ["Round a DataFrame to a variable number of decimal places.", "Number of decimal places to round each column to. If an int is given, round each column to the same number of places. Otherwise dict and Series round to variable numbers of places. Column names should be in the keys if decimals is a dict-like, or in the index if decimals is a Series. Any columns not included in decimals will be left as is. Elements of decimals which are not columns of the input will be ignored.", "Additional keywords have no effect but might be accepted for compatibility with numpy.", "Additional keywords have no effect but might be accepted for compatibility with numpy.", "A DataFrame with the affected columns rounded to the specified number of decimal places.", "See also", "Round a numpy array to the given number of decimals.", "Round a Series to the given number of decimals.", "Examples", "By providing an integer each column is rounded to the same number of decimal places", "With a dict, the number of places for specific columns can be specified with the column names as key and the number of decimal places as value", "Using a Series, the number of places for specific columns can be specified with the column names as index and the number of decimal places as value"]}, {"name": "pandas.DataFrame.rpow", "path": "reference/api/pandas.dataframe.rpow", "type": "DataFrame", "text": ["Get Exponential power of dataframe and other, element-wise (binary operator rpow).", "Equivalent to other ** dataframe, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, pow.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.rsub", "path": "reference/api/pandas.dataframe.rsub", "type": "DataFrame", "text": ["Get Subtraction of dataframe and other, element-wise (binary operator rsub).", "Equivalent to other - dataframe, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, sub.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.rtruediv", "path": "reference/api/pandas.dataframe.rtruediv", "type": "DataFrame", "text": ["Get Floating division of dataframe and other, element-wise (binary operator rtruediv).", "Equivalent to other / dataframe, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, truediv.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.sample", "path": "reference/api/pandas.dataframe.sample", "type": "DataFrame", "text": ["Return a random sample of items from an axis of object.", "You can use random_state for reproducibility.", "Number of items from axis to return. Cannot be used with frac. Default = 1 if frac = None.", "Fraction of axis items to return. Cannot be used with n.", "Allow or disallow sampling of the same row more than once.", "Default \u2018None\u2019 results in equal probability weighting. If passed a Series, will align with target object on index. Index values in weights not found in sampled object will be ignored and index values in sampled object not in weights will be assigned weights of zero. If called on a DataFrame, will accept the name of a column when axis = 0. Unless weights are a Series, weights must be same length as axis being sampled. If weights do not sum to 1, they will be normalized to sum to 1. Missing values in the weights column will be treated as zero. Infinite values not allowed.", "If int, array-like, or BitGenerator, seed for random number generator. If np.random.RandomState or np.random.Generator, use as given.", "Changed in version 1.1.0: array-like and BitGenerator object now passed to np.random.RandomState() as seed", "Changed in version 1.4.0: np.random.Generator objects now accepted", "Axis to sample. Accepts axis number or name. Default is stat axis for given data type (0 for Series and DataFrames).", "If True, the resulting index will be labeled 0, 1, \u2026, n - 1.", "New in version 1.3.0.", "A new object of same type as caller containing n items randomly sampled from the caller object.", "See also", "Generates random samples from each group of a DataFrame object.", "Generates random samples from each group of a Series object.", "Generates a random sample from a given 1-D numpy array.", "Notes", "If frac > 1, replacement should be set to True.", "Examples", "Extract 3 random elements from the Series df['num_legs']: Note that we use random_state to ensure the reproducibility of the examples.", "A random 50% sample of the DataFrame with replacement:", "An upsample sample of the DataFrame with replacement: Note that replace parameter has to be True for frac parameter > 1.", "Using a DataFrame column as weights. Rows with larger value in the num_specimen_seen column are more likely to be sampled."]}, {"name": "pandas.DataFrame.select_dtypes", "path": "reference/api/pandas.dataframe.select_dtypes", "type": "General utility functions", "text": ["Return a subset of the DataFrame\u2019s columns based on the column dtypes.", "A selection of dtypes or strings to be included/excluded. At least one of these parameters must be supplied.", "The subset of the frame including the dtypes in include and excluding the dtypes in exclude.", "If both of include and exclude are empty", "If include and exclude have overlapping elements", "If any kind of string dtype is passed in.", "See also", "Return Series with the data type of each column.", "Notes", "To select all numeric types, use np.number or 'number'", "To select strings you must use the object dtype, but note that this will return all object dtype columns", "See the numpy dtype hierarchy", "To select datetimes, use np.datetime64, 'datetime' or 'datetime64'", "To select timedeltas, use np.timedelta64, 'timedelta' or 'timedelta64'", "To select Pandas categorical dtypes, use 'category'", "To select Pandas datetimetz dtypes, use 'datetimetz' (new in 0.20.0) or 'datetime64[ns, tz]'", "Examples"]}, {"name": "pandas.DataFrame.sem", "path": "reference/api/pandas.dataframe.sem", "type": "DataFrame", "text": ["Return unbiased standard error of the mean over requested axis.", "Normalized by N-1 by default. This can be changed using the ddof argument", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.", "Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series."]}, {"name": "pandas.DataFrame.set_axis", "path": "reference/api/pandas.dataframe.set_axis", "type": "DataFrame", "text": ["Assign desired index to given axis.", "Indexes for column or row labels can be changed by assigning a list-like or Index.", "The values for the new index.", "The axis to update. The value 0 identifies the rows, and 1 identifies the columns.", "Whether to return a new DataFrame instance.", "An object of type DataFrame or None if inplace=True.", "See also", "Alter the name of the index or columns.", "Examples", "Change the row labels.", "Change the column labels.", "Now, update the labels inplace."]}, {"name": "pandas.DataFrame.set_flags", "path": "reference/api/pandas.dataframe.set_flags", "type": "DataFrame", "text": ["Return a new object with updated flags.", "Whether the returned object allows duplicate labels.", "The same type as the caller.", "See also", "Global metadata applying to this dataset.", "Global flags applying to this object.", "Notes", "This method returns a new object that\u2019s a view on the same data as the input. Mutating the input or the output values will be reflected in the other.", "This method is intended to be used in method chains.", "\u201cFlags\u201d differ from \u201cmetadata\u201d. Flags reflect properties of the pandas object (the Series or DataFrame). Metadata refer to properties of the dataset, and should be stored in DataFrame.attrs.", "Examples"]}, {"name": "pandas.DataFrame.set_index", "path": "reference/api/pandas.dataframe.set_index", "type": "DataFrame", "text": ["Set the DataFrame index using existing columns.", "Set the DataFrame index (row labels) using one or more existing columns or arrays (of the correct length). The index can replace the existing index or expand on it.", "This parameter can be either a single column key, a single array of the same length as the calling DataFrame, or a list containing an arbitrary combination of column keys and arrays. Here, \u201carray\u201d encompasses Series, Index, np.ndarray, and instances of Iterator.", "Delete columns to be used as the new index.", "Whether to append columns to existing index.", "If True, modifies the DataFrame in place (do not create a new object).", "Check the new index for duplicates. Otherwise defer the check until necessary. Setting to False will improve the performance of this method.", "Changed row labels or None if inplace=True.", "See also", "Opposite of set_index.", "Change to new indices or expand indices.", "Change to same indices as other DataFrame.", "Examples", "Set the index to become the \u2018month\u2019 column:", "Create a MultiIndex using columns \u2018year\u2019 and \u2018month\u2019:", "Create a MultiIndex using an Index and a column:", "Create a MultiIndex using two Series:"]}, {"name": "pandas.DataFrame.shape", "path": "reference/api/pandas.dataframe.shape", "type": "DataFrame", "text": ["Return a tuple representing the dimensionality of the DataFrame.", "See also", "Tuple of array dimensions.", "Examples"]}, {"name": "pandas.DataFrame.shift", "path": "reference/api/pandas.dataframe.shift", "type": "DataFrame", "text": ["Shift index by desired number of periods with an optional time freq.", "When freq is not passed, shift the index without realigning the data. If freq is passed (in this case, the index must be date or datetime, or it will raise a NotImplementedError), the index will be increased using the periods and the freq. freq can be inferred when specified as \u201cinfer\u201d as long as either freq or inferred_freq attribute is set in the index.", "Number of periods to shift. Can be positive or negative.", "Offset to use from the tseries module or time rule (e.g. \u2018EOM\u2019). If freq is specified then the index values are shifted but the data is not realigned. That is, use freq if you would like to extend the index when shifting and preserve the original data. If freq is specified as \u201cinfer\u201d then it will be inferred from the freq or inferred_freq attributes of the index. If neither of those attributes exist, a ValueError is thrown.", "Shift direction.", "The scalar value to use for newly introduced missing values. the default depends on the dtype of self. For numeric data, np.nan is used. For datetime, timedelta, or period data, etc. NaT is used. For extension dtypes, self.dtype.na_value is used.", "Changed in version 1.1.0.", "Copy of input object, shifted.", "See also", "Shift values of Index.", "Shift values of DatetimeIndex.", "Shift values of PeriodIndex.", "Shift the time index, using the index\u2019s frequency if available.", "Examples"]}, {"name": "pandas.DataFrame.size", "path": "reference/api/pandas.dataframe.size", "type": "DataFrame", "text": ["Return an int representing the number of elements in this object.", "Return the number of rows if Series. Otherwise return the number of rows times number of columns if DataFrame.", "See also", "Number of elements in the array.", "Examples"]}, {"name": "pandas.DataFrame.skew", "path": "reference/api/pandas.dataframe.skew", "type": "DataFrame", "text": ["Return unbiased skew over requested axis.", "Normalized by N-1.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Additional keyword arguments to be passed to the function."]}, {"name": "pandas.DataFrame.slice_shift", "path": "reference/api/pandas.dataframe.slice_shift", "type": "DataFrame", "text": ["Equivalent to shift without copying data. The shifted data will not include the dropped periods and the shifted axis will be smaller than the original.", "Deprecated since version 1.2.0: slice_shift is deprecated, use DataFrame/Series.shift instead.", "Number of periods to move, can be positive or negative.", "Notes", "While the slice_shift is faster than shift, you may pay for it later during alignment."]}, {"name": "pandas.DataFrame.sort_index", "path": "reference/api/pandas.dataframe.sort_index", "type": "DataFrame", "text": ["Sort object by labels (along an axis).", "Returns a new DataFrame sorted by label if inplace argument is False, otherwise updates the original DataFrame and returns None.", "The axis along which to sort. The value 0 identifies the rows, and 1 identifies the columns.", "If not None, sort on values in specified index level(s).", "Sort ascending vs. descending. When the index is a MultiIndex the sort direction can be controlled for each level individually.", "If True, perform operation in-place.", "Choice of sorting algorithm. See also numpy.sort() for more information. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label.", "Puts NaNs at the beginning if first; last puts NaNs at the end. Not implemented for MultiIndex.", "If True and sorting by level and index is multilevel, sort by other levels too (in order) after sorting by specified level.", "If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.", "New in version 1.0.0.", "If not None, apply the key function to the index values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect an Index and return an Index of the same shape. For MultiIndex inputs, the key is applied per level.", "New in version 1.1.0.", "The original DataFrame sorted by the labels or None if inplace=True.", "See also", "Sort Series by the index.", "Sort DataFrame by the value.", "Sort Series by the value.", "Examples", "By default, it sorts in ascending order, to sort in descending order, use ascending=False", "A key function can be specified which is applied to the index before sorting. For a MultiIndex this is applied to each level separately."]}, {"name": "pandas.DataFrame.sort_values", "path": "reference/api/pandas.dataframe.sort_values", "type": "DataFrame", "text": ["Sort by the values along either axis.", "Name or list of names to sort by.", "if axis is 0 or \u2018index\u2019 then by may contain index levels and/or column labels.", "if axis is 1 or \u2018columns\u2019 then by may contain column levels and/or index labels.", "Axis to be sorted.", "Sort ascending vs. descending. Specify list for multiple sort orders. If this is a list of bools, must match the length of the by.", "If True, perform operation in-place.", "Choice of sorting algorithm. See also numpy.sort() for more information. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label.", "Puts NaNs at the beginning if first; last puts NaNs at the end.", "If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.", "New in version 1.0.0.", "Apply the key function to the values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect a Series and return a Series with the same shape as the input. It will be applied to each column in by independently.", "New in version 1.1.0.", "DataFrame with sorted values or None if inplace=True.", "See also", "Sort a DataFrame by the index.", "Similar method for a Series.", "Examples", "Sort by col1", "Sort by multiple columns", "Sort Descending", "Putting NAs first", "Sorting with a key function", "Natural sort with the key argument, using the natsort <https://github.com/SethMMorton/natsort> package."]}, {"name": "pandas.DataFrame.sparse", "path": "reference/api/pandas.dataframe.sparse", "type": "DataFrame", "text": ["DataFrame accessor for sparse data.", "New in version 0.25.0."]}, {"name": "pandas.DataFrame.sparse.density", "path": "reference/api/pandas.dataframe.sparse.density", "type": "DataFrame", "text": ["Ratio of non-sparse points to total (dense) data points."]}, {"name": "pandas.DataFrame.sparse.from_spmatrix", "path": "reference/api/pandas.dataframe.sparse.from_spmatrix", "type": "DataFrame", "text": ["Create a new DataFrame from a scipy sparse matrix.", "New in version 0.25.0.", "Must be convertible to csc format.", "Row and column labels to use for the resulting DataFrame. Defaults to a RangeIndex.", "Each column of the DataFrame is stored as a arrays.SparseArray.", "Examples"]}, {"name": "pandas.DataFrame.sparse.to_coo", "path": "reference/api/pandas.dataframe.sparse.to_coo", "type": "DataFrame", "text": ["Return the contents of the frame as a sparse SciPy COO matrix.", "New in version 0.25.0.", "If the caller is heterogeneous and contains booleans or objects, the result will be of dtype=object. See Notes.", "Notes", "The dtype will be the lowest-common-denominator type (implicit upcasting); that is to say if the dtypes (even of numeric types) are mixed, the one that accommodates all will be chosen.", "e.g. If the dtypes are float16 and float32, dtype will be upcast to float32. By numpy.find_common_type convention, mixing int64 and and uint64 will result in a float64 dtype."]}, {"name": "pandas.DataFrame.sparse.to_dense", "path": "reference/api/pandas.dataframe.sparse.to_dense", "type": "DataFrame", "text": ["Convert a DataFrame with sparse values to dense.", "New in version 0.25.0.", "A DataFrame with the same values stored as dense arrays.", "Examples"]}, {"name": "pandas.DataFrame.squeeze", "path": "reference/api/pandas.dataframe.squeeze", "type": "DataFrame", "text": ["Squeeze 1 dimensional axis objects into scalars.", "Series or DataFrames with a single element are squeezed to a scalar. DataFrames with a single column or a single row are squeezed to a Series. Otherwise the object is unchanged.", "This method is most useful when you don\u2019t know if your object is a Series or DataFrame, but you do know it has just a single column. In that case you can safely call squeeze to ensure you have a Series.", "A specific axis to squeeze. By default, all length-1 axes are squeezed.", "The projection after squeezing axis or all the axes.", "See also", "Integer-location based indexing for selecting scalars.", "Integer-location based indexing for selecting Series.", "Inverse of DataFrame.squeeze for a single-column DataFrame.", "Examples", "Slicing might produce a Series with a single value:", "Squeezing objects with more than one value in every axis does nothing:", "Squeezing is even more effective when used with DataFrames.", "Slicing a single column will produce a DataFrame with the columns having only one value:", "So the columns can be squeezed down, resulting in a Series:", "Slicing a single row from a single column will produce a single scalar DataFrame:", "Squeezing the rows produces a single scalar Series:", "Squeezing all axes will project directly into a scalar:"]}, {"name": "pandas.DataFrame.stack", "path": "reference/api/pandas.dataframe.stack", "type": "DataFrame", "text": ["Stack the prescribed level(s) from columns to index.", "Return a reshaped DataFrame or Series having a multi-level index with one or more new inner-most levels compared to the current DataFrame. The new inner-most levels are created by pivoting the columns of the current dataframe:", "if the columns have a single level, the output is a Series;", "if the columns have multiple levels, the new index level(s) is (are) taken from the prescribed level(s) and the output is a DataFrame.", "Level(s) to stack from the column axis onto the index axis, defined as one index or label, or a list of indices or labels.", "Whether to drop rows in the resulting Frame/Series with missing values. Stacking a column level onto the index axis can create combinations of index and column values that are missing from the original dataframe. See Examples section.", "Stacked dataframe or series.", "See also", "Unstack prescribed level(s) from index axis onto column axis.", "Reshape dataframe from long format to wide format.", "Create a spreadsheet-style pivot table as a DataFrame.", "Notes", "The function is named by analogy with a collection of books being reorganized from being side by side on a horizontal position (the columns of the dataframe) to being stacked vertically on top of each other (in the index of the dataframe).", "Examples", "Single level columns", "Stacking a dataframe with a single level column axis returns a Series:", "Multi level columns: simple case", "Stacking a dataframe with a multi-level column axis:", "Missing values", "It is common to have missing values when stacking a dataframe with multi-level columns, as the stacked dataframe typically has more values than the original dataframe. Missing values are filled with NaNs:", "Prescribing the level(s) to be stacked", "The first parameter controls which level or levels are stacked:", "Dropping missing values", "Note that rows where all values are missing are dropped by default but this behaviour can be controlled via the dropna keyword parameter:"]}, {"name": "pandas.DataFrame.std", "path": "reference/api/pandas.dataframe.std", "type": "DataFrame", "text": ["Return sample standard deviation over requested axis.", "Normalized by N-1 by default. This can be changed using the ddof argument.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.", "Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Notes", "To have the same behaviour as numpy.std, use ddof=0 (instead of the default ddof=1)", "Examples", "The standard deviation of the columns can be found as follows:", "Alternatively, ddof=0 can be set to normalize by N instead of N-1:"]}, {"name": "pandas.DataFrame.style", "path": "reference/api/pandas.dataframe.style", "type": "Style", "text": ["Returns a Styler object.", "Contains methods for building a styled HTML representation of the DataFrame.", "See also", "Helps style a DataFrame or Series according to the data with HTML and CSS."]}, {"name": "pandas.DataFrame.sub", "path": "reference/api/pandas.dataframe.sub", "type": "DataFrame", "text": ["Get Subtraction of dataframe and other, element-wise (binary operator sub).", "Equivalent to dataframe - other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rsub.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.subtract", "path": "reference/api/pandas.dataframe.subtract", "type": "DataFrame", "text": ["Get Subtraction of dataframe and other, element-wise (binary operator sub).", "Equivalent to dataframe - other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rsub.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.sum", "path": "reference/api/pandas.dataframe.sum", "type": "DataFrame", "text": ["Return the sum of the values over the requested axis.", "This is equivalent to the method numpy.sum.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Additional keyword arguments to be passed to the function.", "See also", "Return the sum.", "Return the minimum.", "Return the maximum.", "Return the index of the minimum.", "Return the index of the maximum.", "Return the sum over the requested axis.", "Return the minimum over the requested axis.", "Return the maximum over the requested axis.", "Return the index of the minimum over the requested axis.", "Return the index of the maximum over the requested axis.", "Examples", "By default, the sum of an empty or all-NA Series is 0.", "This can be controlled with the min_count parameter. For example, if you\u2019d like the sum of an empty series to be NaN, pass min_count=1.", "Thanks to the skipna parameter, min_count handles all-NA and empty series identically."]}, {"name": "pandas.DataFrame.swapaxes", "path": "reference/api/pandas.dataframe.swapaxes", "type": "DataFrame", "text": ["Interchange axes and swap values axes appropriately."]}, {"name": "pandas.DataFrame.swaplevel", "path": "reference/api/pandas.dataframe.swaplevel", "type": "DataFrame", "text": ["Swap levels i and j in a MultiIndex.", "Default is to swap the two innermost levels of the index.", "Levels of the indices to be swapped. Can pass level name as string.", "The axis to swap levels on. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.", "DataFrame with levels swapped in MultiIndex.", "Examples", "In the following example, we will swap the levels of the indices. Here, we will swap the levels column-wise, but levels can be swapped row-wise in a similar manner. Note that column-wise is the default behaviour. By not supplying any arguments for i and j, we swap the last and second to last indices.", "By supplying one argument, we can choose which index to swap the last index with. We can for example swap the first index with the last one as follows.", "We can also define explicitly which indices we want to swap by supplying values for both i and j. Here, we for example swap the first and second indices."]}, {"name": "pandas.DataFrame.T", "path": "reference/api/pandas.dataframe.t", "type": "DataFrame", "text": []}, {"name": "pandas.DataFrame.tail", "path": "reference/api/pandas.dataframe.tail", "type": "DataFrame", "text": ["Return the last n rows.", "This function returns last n rows from the object based on position. It is useful for quickly verifying data, for example, after sorting or appending rows.", "For negative values of n, this function returns all rows except the first n rows, equivalent to df[n:].", "Number of rows to select.", "The last n rows of the caller object.", "See also", "The first n rows of the caller object.", "Examples", "Viewing the last 5 lines", "Viewing the last n lines (three in this case)", "For negative values of n"]}, {"name": "pandas.DataFrame.take", "path": "reference/api/pandas.dataframe.take", "type": "DataFrame", "text": ["Return the elements in the given positional indices along an axis.", "This means that we are not indexing according to actual values in the index attribute of the object. We are indexing according to the actual position of the element in the object.", "An array of ints indicating which positions to take.", "The axis on which to select elements. 0 means that we are selecting rows, 1 means that we are selecting columns.", "Before pandas 1.0, is_copy=False can be specified to ensure that the return value is an actual copy. Starting with pandas 1.0, take always returns a copy, and the keyword is therefore deprecated.", "Deprecated since version 1.0.0.", "For compatibility with numpy.take(). Has no effect on the output.", "An array-like containing the elements taken from the object.", "See also", "Select a subset of a DataFrame by labels.", "Select a subset of a DataFrame by positions.", "Take elements from an array along an axis.", "Examples", "Take elements at positions 0 and 3 along the axis 0 (default).", "Note how the actual indices selected (0 and 1) do not correspond to our selected indices 0 and 3. That\u2019s because we are selecting the 0th and 3rd rows, not rows whose indices equal 0 and 3.", "Take elements at indices 1 and 2 along the axis 1 (column selection).", "We may take elements using negative integers for positive indices, starting from the end of the object, just like with Python lists."]}, {"name": "pandas.DataFrame.to_clipboard", "path": "reference/api/pandas.dataframe.to_clipboard", "type": "DataFrame", "text": ["Copy object to the system clipboard.", "Write a text representation of object to the system clipboard. This can be pasted into Excel, for example.", "Produce output in a csv format for easy pasting into excel.", "True, use the provided separator for csv pasting.", "False, write a string representation of the object to the clipboard.", "Field delimiter.", "These parameters will be passed to DataFrame.to_csv.", "See also", "Write a DataFrame to a comma-separated values (csv) file.", "Read text from clipboard and pass to read_csv.", "Notes", "Requirements for your platform.", "Linux : xclip, or xsel (with PyQt4 modules)", "Windows : none", "macOS : none", "Examples", "Copy the contents of a DataFrame to the clipboard.", "We can omit the index by passing the keyword index and setting it to false."]}, {"name": "pandas.DataFrame.to_csv", "path": "reference/api/pandas.dataframe.to_csv", "type": "DataFrame", "text": ["Write object to a comma-separated values (csv) file.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a write() function. If None, the result is returned as a string. If a non-binary file object is passed, it should be opened with newline=\u2019\u2019, disabling universal newlines. If a binary file object is passed, mode might need to contain a \u2018b\u2019.", "Changed in version 1.2.0: Support for binary file objects was introduced.", "String of length 1. Field delimiter for the output file.", "Missing data representation.", "Format string for floating point numbers.", "Columns to write.", "Write out the column names. If a list of strings is given it is assumed to be aliases for the column names.", "Write row names (index).", "Column label for index column(s) if desired. If None is given, and header and index are True, then the index names are used. A sequence should be given if the object uses MultiIndex. If False do not print fields for index names. Use index_label=False for easier importing in R.", "Python write mode, default \u2018w\u2019.", "A string representing the encoding to use in the output file, defaults to \u2018utf-8\u2019. encoding is not supported if path_or_buf is a non-binary file object.", "For on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018%s\u2019 path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.", "Changed in version 1.0.0: May now be a dict with key \u2018method\u2019 as compression mode and other entries as additional compression options if compression mode is \u2018zip\u2019.", "Changed in version 1.1.0: Passing compression options as keys in dict is supported for compression modes \u2018gzip\u2019, \u2018bz2\u2019, \u2018zstd\u2019, and \u2018zip\u2019.", "Changed in version 1.2.0: Compression is supported for binary file objects.", "Changed in version 1.2.0: Previous versions forwarded dict entries for \u2018gzip\u2019 to gzip.open instead of gzip.GzipFile which prevented setting mtime.", "Defaults to csv.QUOTE_MINIMAL. If you have set a float_format then floats are converted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-numeric.", "String of length 1. Character used to quote fields.", "The newline character or character sequence to use in the output file. Defaults to os.linesep, which depends on the OS in which this method is called (\u2019\\n\u2019 for linux, \u2018\\r\\n\u2019 for Windows, i.e.).", "Rows to write at a time.", "Format string for datetime objects.", "Control quoting of quotechar inside a field.", "String of length 1. Character used to escape sep and quotechar when appropriate.", "Character recognized as decimal separator. E.g. use \u2018,\u2019 for European data.", "Specifies how encoding and decoding errors are to be handled. See the errors argument for open() for a full list of options.", "New in version 1.1.0.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.0.", "If path_or_buf is None, returns the resulting csv format as a string. Otherwise returns None.", "See also", "Load a CSV file into a DataFrame.", "Write DataFrame to an Excel file.", "Examples", "Create \u2018out.zip\u2019 containing \u2018out.csv\u2019", "To write a csv file to a new folder or nested folder you will first need to create it using either Pathlib or os:"]}, {"name": "pandas.DataFrame.to_dict", "path": "reference/api/pandas.dataframe.to_dict", "type": "DataFrame", "text": ["Convert the DataFrame to a dictionary.", "The type of the key-value pairs can be customized with the parameters (see below).", "Determines the type of the values of the dictionary.", "\u2018dict\u2019 (default) : dict like {column -> {index -> value}}", "\u2018list\u2019 : dict like {column -> [values]}", "\u2018series\u2019 : dict like {column -> Series(values)}", "\u2018split\u2019 : dict like {\u2018index\u2019 -> [index], \u2018columns\u2019 -> [columns], \u2018data\u2019 -> [values]}", "\u2018tight\u2019 : dict like {\u2018index\u2019 -> [index], \u2018columns\u2019 -> [columns], \u2018data\u2019 -> [values], \u2018index_names\u2019 -> [index.names], \u2018column_names\u2019 -> [column.names]}", "\u2018records\u2019 : list like [{column -> value}, \u2026 , {column -> value}]", "\u2018index\u2019 : dict like {index -> {column -> value}}", "Abbreviations are allowed. s indicates series and sp indicates split.", "New in version 1.4.0: \u2018tight\u2019 as an allowed value for the orient argument", "The collections.abc.Mapping subclass used for all Mappings in the return value. Can be the actual class or an empty instance of the mapping type you want. If you want a collections.defaultdict, you must pass it initialized.", "Return a collections.abc.Mapping object representing the DataFrame. The resulting transformation depends on the orient parameter.", "See also", "Create a DataFrame from a dictionary.", "Convert a DataFrame to JSON format.", "Examples", "You can specify the return orientation.", "You can also specify the mapping type.", "If you want a defaultdict, you need to initialize it:"]}, {"name": "pandas.DataFrame.to_excel", "path": "reference/api/pandas.dataframe.to_excel", "type": "DataFrame", "text": ["Write object to an Excel sheet.", "To write a single object to an Excel .xlsx file it is only necessary to specify a target file name. To write to multiple sheets it is necessary to create an ExcelWriter object with a target file name, and specify a sheet in the file to write to.", "Multiple sheets may be written to by specifying unique sheet_name. With all data written to the file it is necessary to save the changes. Note that creating an ExcelWriter object with a file name that already exists will result in the contents of the existing file being erased.", "File path or existing ExcelWriter.", "Name of sheet which will contain DataFrame.", "Missing data representation.", "Format string for floating point numbers. For example float_format=\"%.2f\" will format 0.1234 to 0.12.", "Columns to write.", "Write out the column names. If a list of string is given it is assumed to be aliases for the column names.", "Write row names (index).", "Column label for index column(s) if desired. If not specified, and header and index are True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex.", "Upper left cell row to dump data frame.", "Upper left cell column to dump data frame.", "Write engine to use, \u2018openpyxl\u2019 or \u2018xlsxwriter\u2019. You can also set this via the options io.excel.xlsx.writer, io.excel.xls.writer, and io.excel.xlsm.writer.", "Deprecated since version 1.2.0: As the xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas.", "Write MultiIndex and Hierarchical Rows as merged cells.", "Encoding of the resulting excel file. Only necessary for xlwt, other writers support unicode natively.", "Representation for infinity (there is no native representation for infinity in Excel).", "Display more information in the error logs.", "Specifies the one-based bottommost row and rightmost column that is to be frozen.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.0.", "See also", "Write DataFrame to a comma-separated values (csv) file.", "Class for writing DataFrame objects into excel sheets.", "Read an Excel file into a pandas DataFrame.", "Read a comma-separated values (csv) file into DataFrame.", "Notes", "For compatibility with to_csv(), to_excel serializes lists and dicts to strings before writing.", "Once a workbook has been saved it is not possible to write further data without rewriting the whole workbook.", "Examples", "Create, write to and save a workbook:", "To specify the sheet name:", "If you wish to write to more than one sheet in the workbook, it is necessary to specify an ExcelWriter object:", "ExcelWriter can also be used to append to an existing Excel file:", "To set the library that is used to write the Excel file, you can pass the engine keyword (the default engine is automatically chosen depending on the file extension):"]}, {"name": "pandas.DataFrame.to_feather", "path": "reference/api/pandas.dataframe.to_feather", "type": "DataFrame", "text": ["Write a DataFrame to the binary Feather format.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a binary write() function. If a string or a path, it will be used as Root Directory path when writing a partitioned dataset.", "Additional keywords passed to pyarrow.feather.write_feather(). Starting with pyarrow 0.17, this includes the compression, compression_level, chunksize and version keywords.", "New in version 1.1.0.", "Notes", "This function writes the dataframe as a feather file. Requires a default index. For saving the DataFrame with your custom index use a method that supports custom indices e.g. to_parquet."]}, {"name": "pandas.DataFrame.to_gbq", "path": "reference/api/pandas.dataframe.to_gbq", "type": "DataFrame", "text": ["Write a DataFrame to a Google BigQuery table.", "This function requires the pandas-gbq package.", "See the How to authenticate with Google BigQuery guide for authentication instructions.", "Name of table to be written, in the form dataset.tablename.", "Google BigQuery Account project ID. Optional when available from the environment.", "Number of rows to be inserted in each chunk from the dataframe. Set to None to load the whole dataframe at once.", "Force Google BigQuery to re-authenticate the user. This is useful if multiple accounts are used.", "Behavior when the destination table exists. Value can be one of:", "If table exists raise pandas_gbq.gbq.TableCreationError.", "If table exists, drop it, recreate it, and insert data.", "If table exists, insert data. Create if does not exist.", "Use the local webserver flow instead of the console flow when getting user credentials.", "New in version 0.2.0 of pandas-gbq.", "List of BigQuery table fields to which according DataFrame columns conform to, e.g. [{'name': 'col1', 'type':\n'STRING'},...]. If schema is not provided, it will be generated according to dtypes of DataFrame columns. See BigQuery API documentation on available names of a field.", "New in version 0.3.1 of pandas-gbq.", "Location where the load job should run. See the BigQuery locations documentation for a list of available locations. The location must match that of the target dataset.", "New in version 0.5.0 of pandas-gbq.", "Use the library tqdm to show the progress bar for the upload, chunk by chunk.", "New in version 0.5.0 of pandas-gbq.", "Credentials for accessing Google APIs. Use this parameter to override default credentials, such as to use Compute Engine google.auth.compute_engine.Credentials or Service Account google.oauth2.service_account.Credentials directly.", "New in version 0.8.0 of pandas-gbq.", "See also", "This function in the pandas-gbq library.", "Read a DataFrame from Google BigQuery."]}, {"name": "pandas.DataFrame.to_hdf", "path": "reference/api/pandas.dataframe.to_hdf", "type": "DataFrame", "text": ["Write the contained data to an HDF5 file using HDFStore.", "Hierarchical Data Format (HDF) is self-describing, allowing an application to interpret the structure and contents of a file with no outside information. One HDF file can hold a mix of related objects which can be accessed as a group or as individual objects.", "In order to add another DataFrame or Series to an existing HDF file please use append mode and a different a key.", "Warning", "One can store a subclass of DataFrame or Series to HDF5, but the type of the subclass is lost upon storing.", "For more information see the user guide.", "File path or HDFStore object.", "Identifier for the group in the store.", "Mode to open file:", "\u2018w\u2019: write, a new file is created (an existing file with the same name would be deleted).", "\u2018a\u2019: append, an existing file is opened for reading and writing, and if the file does not exist it is created.", "\u2018r+\u2019: similar to \u2018a\u2019, but the file must already exist.", "Specifies a compression level for data. A value of 0 or None disables compression.", "Specifies the compression library to be used. As of v0.20.2 these additional compressors for Blosc are supported (default if no compressor specified: \u2018blosc:blosclz\u2019): {\u2018blosc:blosclz\u2019, \u2018blosc:lz4\u2019, \u2018blosc:lz4hc\u2019, \u2018blosc:snappy\u2019, \u2018blosc:zlib\u2019, \u2018blosc:zstd\u2019}. Specifying a compression library which is not available issues a ValueError.", "For Table formats, append the input data to the existing.", "Possible values:", "\u2018fixed\u2019: Fixed format. Fast writing/reading. Not-appendable, nor searchable.", "\u2018table\u2019: Table format. Write as a PyTables Table structure which may perform worse but allow more flexible operations like searching / selecting subsets of the data.", "If None, pd.get_option(\u2018io.hdf.default_format\u2019) is checked, followed by fallback to \u201cfixed\u201d.", "Specifies how encoding and decoding errors are to be handled. See the errors argument for open() for a full list of options.", "Map column names to minimum string sizes for columns.", "How to represent null values as str. Not allowed with append=True.", "List of columns to create as indexed data columns for on-disk queries, or True to use all columns. By default only the axes of the object are indexed. See Query via data columns. Applicable only to format=\u2019table\u2019.", "See also", "Read from HDF file.", "Write a DataFrame to the binary parquet format.", "Write to a SQL table.", "Write out feather-format for DataFrames.", "Write out to a csv file.", "Examples", "We can add another object to the same file:", "Reading from HDF file:"]}, {"name": "pandas.DataFrame.to_html", "path": "reference/api/pandas.dataframe.to_html", "type": "DataFrame", "text": ["Render a DataFrame as an HTML table.", "Buffer to write to. If None, the output is returned as a string.", "The subset of columns to write. Writes all columns by default.", "The minimum width of each column in CSS length units. An int is assumed to be px units.", "New in version 0.25.0: Ability to use str.", "Whether to print column labels, default True.", "Whether to print index (row) labels.", "String representation of NaN to use.", "Formatter functions to apply to columns\u2019 elements by position or name. The result of each function must be a unicode string. List/tuple must be of length equal to the number of columns.", "Formatter function to apply to columns\u2019 elements if they are floats. This function must return a unicode string and will be applied only to the non-NaN elements, with NaN being handled by na_rep.", "Changed in version 1.2.0.", "Set to False for a DataFrame with a hierarchical index to print every multiindex key at each row.", "Prints the names of the indexes.", "How to justify the column labels. If None uses the option from the print configuration (controlled by set_option), \u2018right\u2019 out of the box. Valid values are", "left", "right", "center", "justify", "justify-all", "start", "end", "inherit", "match-parent", "initial", "unset.", "Maximum number of rows to display in the console.", "Maximum number of columns to display in the console.", "Display DataFrame dimensions (number of rows by number of columns).", "Character recognized as decimal separator, e.g. \u2018,\u2019 in Europe.", "Make the row labels bold in the output.", "CSS class(es) to apply to the resulting html table.", "Convert the characters <, >, and & to HTML-safe sequences.", "Whether the generated HTML is for IPython Notebook.", "A border=border attribute is included in the opening <table> tag. Default pd.options.display.html.border.", "A css id is included in the opening <table> tag if specified.", "Convert URLs to HTML links.", "Set character encoding.", "New in version 1.0.", "If buf is None, returns the result as a string. Otherwise returns None.", "See also", "Convert DataFrame to a string."]}, {"name": "pandas.DataFrame.to_json", "path": "reference/api/pandas.dataframe.to_json", "type": "DataFrame", "text": ["Convert the object to a JSON string.", "Note NaN\u2019s and None will be converted to null and datetime objects will be converted to UNIX timestamps.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a write() function. If None, the result is returned as a string.", "Indication of expected JSON string format.", "Series:", "default is \u2018index\u2019", "allowed values are: {\u2018split\u2019, \u2018records\u2019, \u2018index\u2019, \u2018table\u2019}.", "DataFrame:", "default is \u2018columns\u2019", "allowed values are: {\u2018split\u2019, \u2018records\u2019, \u2018index\u2019, \u2018columns\u2019, \u2018values\u2019, \u2018table\u2019}.", "The format of the JSON string:", "\u2018split\u2019 : dict like {\u2018index\u2019 -> [index], \u2018columns\u2019 -> [columns], \u2018data\u2019 -> [values]}", "\u2018records\u2019 : list like [{column -> value}, \u2026 , {column -> value}]", "\u2018index\u2019 : dict like {index -> {column -> value}}", "\u2018columns\u2019 : dict like {column -> {index -> value}}", "\u2018values\u2019 : just the values array", "\u2018table\u2019 : dict like {\u2018schema\u2019: {schema}, \u2018data\u2019: {data}}", "Describing the data, where data component is like orient='records'.", "Type of date conversion. \u2018epoch\u2019 = epoch milliseconds, \u2018iso\u2019 = ISO8601. The default depends on the orient. For orient='table', the default is \u2018iso\u2019. For all other orients, the default is \u2018epoch\u2019.", "The number of decimal places to use when encoding floating point values.", "Force encoded string to be ASCII.", "The time unit to encode to, governs timestamp and ISO8601 precision. One of \u2018s\u2019, \u2018ms\u2019, \u2018us\u2019, \u2018ns\u2019 for second, millisecond, microsecond, and nanosecond respectively.", "Handler to call if object cannot otherwise be converted to a suitable format for JSON. Should receive a single argument which is the object to convert and return a serialisable object.", "If \u2018orient\u2019 is \u2018records\u2019 write out line-delimited json format. Will throw ValueError if incorrect \u2018orient\u2019 since others are not list-like.", "For on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018path_or_buf\u2019 path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.", "Changed in version 1.4.0: Zstandard support.", "Whether to include the index values in the JSON string. Not including the index (index=False) is only supported when orient is \u2018split\u2019 or \u2018table\u2019.", "Length of whitespace used to indent each record.", "New in version 1.0.0.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.0.", "If path_or_buf is None, returns the resulting json format as a string. Otherwise returns None.", "See also", "Convert a JSON string to pandas object.", "Notes", "The behavior of indent=0 varies from the stdlib, which does not indent the output but does insert newlines. Currently, indent=0 and the default indent=None are equivalent in pandas, though this may change in a future release.", "orient='table' contains a \u2018pandas_version\u2019 field under \u2018schema\u2019. This stores the version of pandas used in the latest revision of the schema.", "Examples", "Encoding/decoding a Dataframe using 'records' formatted JSON. Note that index labels are not preserved with this encoding.", "Encoding/decoding a Dataframe using 'index' formatted JSON:", "Encoding/decoding a Dataframe using 'columns' formatted JSON:", "Encoding/decoding a Dataframe using 'values' formatted JSON:", "Encoding with Table Schema:"]}, {"name": "pandas.DataFrame.to_latex", "path": "reference/api/pandas.dataframe.to_latex", "type": "DataFrame", "text": ["Render object to a LaTeX tabular, longtable, or nested table.", "Requires \\usepackage{booktabs}. The output can be copy/pasted into a main LaTeX document or read from an external file with \\input{table.tex}.", "Changed in version 1.0.0: Added caption and label arguments.", "Changed in version 1.2.0: Added position argument, changed meaning of caption argument.", "Buffer to write to. If None, the output is returned as a string.", "The subset of columns to write. Writes all columns by default.", "The minimum width of each column.", "Write out the column names. If a list of strings is given, it is assumed to be aliases for the column names.", "Write row names (index).", "Missing data representation.", "Formatter functions to apply to columns\u2019 elements by position or name. The result of each function must be a unicode string. List must be of length equal to the number of columns.", "Formatter for floating point numbers. For example float_format=\"%.2f\" and float_format=\"{:0.2f}\".format will both result in 0.1234 being formatted as 0.12.", "Set to False for a DataFrame with a hierarchical index to print every multiindex key at each row. By default, the value will be read from the config module.", "Prints the names of the indexes.", "Make the row labels bold in the output.", "The columns format as specified in LaTeX table format e.g. \u2018rcl\u2019 for 3 columns. By default, \u2018l\u2019 will be used for all columns except columns of numbers, which default to \u2018r\u2019.", "By default, the value will be read from the pandas config module. Use a longtable environment instead of tabular. Requires adding a usepackage{longtable} to your LaTeX preamble.", "By default, the value will be read from the pandas config module. When set to False prevents from escaping latex special characters in column names.", "A string representing the encoding to use in the output file, defaults to \u2018utf-8\u2019.", "Character recognized as decimal separator, e.g. \u2018,\u2019 in Europe.", "Use multicolumn to enhance MultiIndex columns. The default will be read from the config module.", "The alignment for multicolumns, similar to column_format The default will be read from the config module.", "Use multirow to enhance MultiIndex rows. Requires adding a usepackage{multirow} to your LaTeX preamble. Will print centered labels (instead of top-aligned) across the contained rows, separating groups via clines. The default will be read from the pandas config module.", "Tuple (full_caption, short_caption), which results in \\caption[short_caption]{full_caption}; if a single string is passed, no short caption will be set.", "New in version 1.0.0.", "Changed in version 1.2.0: Optionally allow caption to be a tuple (full_caption, short_caption).", "The LaTeX label to be placed inside \\label{} in the output. This is used with \\ref{} in the main .tex file.", "New in version 1.0.0.", "The LaTeX positional argument for tables, to be placed after \\begin{} in the output.", "New in version 1.2.0.", "If buf is None, returns the result as a string. Otherwise returns None.", "See also", "Render a DataFrame to LaTeX with conditional formatting.", "Render a DataFrame to a console-friendly tabular output.", "Render a DataFrame as an HTML table.", "Examples"]}, {"name": "pandas.DataFrame.to_markdown", "path": "reference/api/pandas.dataframe.to_markdown", "type": "DataFrame", "text": ["Print DataFrame in Markdown-friendly format.", "New in version 1.0.0.", "Buffer to write to. If None, the output is returned as a string.", "Mode in which file is opened, \u201cwt\u201d by default.", "Add index (row) labels.", "New in version 1.1.0.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.0.", "These parameters will be passed to tabulate.", "DataFrame in Markdown-friendly format.", "Notes", "Requires the tabulate package.", "Examples", "Output markdown with a tabulate option."]}, {"name": "pandas.DataFrame.to_numpy", "path": "reference/api/pandas.dataframe.to_numpy", "type": "DataFrame", "text": ["Convert the DataFrame to a NumPy array.", "By default, the dtype of the returned array will be the common NumPy dtype of all types in the DataFrame. For example, if the dtypes are float16 and float32, the results dtype will be float32. This may require copying data and coercing values, which may be expensive.", "The dtype to pass to numpy.asarray().", "Whether to ensure that the returned value is not a view on another array. Note that copy=False does not ensure that to_numpy() is no-copy. Rather, copy=True ensure that a copy is made, even if not strictly necessary.", "The value to use for missing values. The default value depends on dtype and the dtypes of the DataFrame columns.", "New in version 1.1.0.", "See also", "Similar method for Series.", "Examples", "With heterogeneous data, the lowest common type will have to be used.", "For a mix of numeric and non-numeric types, the output array will have object dtype."]}, {"name": "pandas.DataFrame.to_parquet", "path": "reference/api/pandas.dataframe.to_parquet", "type": "DataFrame", "text": ["Write a DataFrame to the binary parquet format.", "This function writes the dataframe as a parquet file. You can choose different parquet backends, and have the option of compression. See the user guide for more details.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a binary write() function. If None, the result is returned as bytes. If a string or path, it will be used as Root Directory path when writing a partitioned dataset.", "Changed in version 1.2.0.", "Previously this was \u201cfname\u201d", "Parquet library to use. If \u2018auto\u2019, then the option io.parquet.engine is used. The default io.parquet.engine behavior is to try \u2018pyarrow\u2019, falling back to \u2018fastparquet\u2019 if \u2018pyarrow\u2019 is unavailable.", "Name of the compression to use. Use None for no compression.", "If True, include the dataframe\u2019s index(es) in the file output. If False, they will not be written to the file. If None, similar to True the dataframe\u2019s index(es) will be saved. However, instead of being saved as values, the RangeIndex will be stored as a range in the metadata so it doesn\u2019t require much space and is faster. Other indexes will be included as columns in the file output.", "Column names by which to partition the dataset. Columns are partitioned in the order they are given. Must be None if path is not a string.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.0.", "Additional arguments passed to the parquet library. See pandas io for more details.", "See also", "Read a parquet file.", "Write a csv file.", "Write to a sql table.", "Write to hdf.", "Notes", "This function requires either the fastparquet or pyarrow library.", "Examples", "If you want to get a buffer to the parquet content you can use a io.BytesIO object, as long as you don\u2019t use partition_cols, which creates multiple files."]}, {"name": "pandas.DataFrame.to_period", "path": "reference/api/pandas.dataframe.to_period", "type": "Input/output", "text": ["Convert DataFrame from DatetimeIndex to PeriodIndex.", "Convert DataFrame from DatetimeIndex to PeriodIndex with desired frequency (inferred from index if not passed).", "Frequency of the PeriodIndex.", "The axis to convert (the index by default).", "If False then underlying input data is not copied.", "Examples", "For the yearly frequency"]}, {"name": "pandas.DataFrame.to_pickle", "path": "reference/api/pandas.dataframe.to_pickle", "type": "DataFrame", "text": ["Pickle (serialize) object to file.", "File path where the pickled object will be stored.", "For on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018path\u2019 path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.", "Int which indicates which protocol should be used by the pickler, default HIGHEST_PROTOCOL (see [1] paragraph 12.1.2). The possible values are 0, 1, 2, 3, 4, 5. A negative value for the protocol parameter is equivalent to setting its value to HIGHEST_PROTOCOL.", "https://docs.python.org/3/library/pickle.html.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.0.", "See also", "Load pickled pandas object (or any object) from file.", "Write DataFrame to an HDF5 file.", "Write DataFrame to a SQL database.", "Write a DataFrame to the binary parquet format.", "Examples"]}, {"name": "pandas.DataFrame.to_records", "path": "reference/api/pandas.dataframe.to_records", "type": "DataFrame", "text": ["Convert DataFrame to a NumPy record array.", "Index will be included as the first field of the record array if requested.", "Include index in resulting record array, stored in \u2018index\u2019 field or using the index label, if set.", "If a string or type, the data type to store all columns. If a dictionary, a mapping of column names and indices (zero-indexed) to specific data types.", "If a string or type, the data type to store all index levels. If a dictionary, a mapping of index level names and indices (zero-indexed) to specific data types.", "This mapping is applied only if index=True.", "NumPy ndarray with the DataFrame labels as fields and each row of the DataFrame as entries.", "See also", "Convert structured or record ndarray to DataFrame.", "An ndarray that allows field access using attributes, analogous to typed columns in a spreadsheet.", "Examples", "If the DataFrame index has no label then the recarray field name is set to \u2018index\u2019. If the index has a label then this is used as the field name:", "The index can be excluded from the record array:", "Data types can be specified for the columns:", "As well as for the index:"]}, {"name": "pandas.DataFrame.to_sql", "path": "reference/api/pandas.dataframe.to_sql", "type": "DataFrame", "text": ["Write records stored in a DataFrame to a SQL database.", "Databases supported by SQLAlchemy [1] are supported. Tables can be newly created, appended to, or overwritten.", "Name of SQL table.", "Using SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable See here.", "Specify the schema (if database flavor supports this). If None, use default schema.", "How to behave if the table already exists.", "fail: Raise a ValueError.", "replace: Drop the table before inserting new values.", "append: Insert new values to the existing table.", "Write DataFrame index as a column. Uses index_label as the column name in the table.", "Column label for index column(s). If None is given (default) and index is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex.", "Specify the number of rows in each batch to be written at a time. By default, all rows will be written at once.", "Specifying the datatype for columns. If a dictionary is used, the keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. If a scalar is provided, it will be applied to all columns.", "Controls the SQL insertion clause used:", "None : Uses standard SQL INSERT clause (one per row).", "\u2018multi\u2019: Pass multiple values in a single INSERT clause.", "callable with signature (pd_table, conn, keys, data_iter).", "Details and a sample callable implementation can be found in the section insert method.", "Number of rows affected by to_sql. None is returned if the callable passed into method does not return the number of rows.", "The number of returned rows affected is the sum of the rowcount attribute of sqlite3.Cursor or SQLAlchemy connectable which may not reflect the exact number of written rows as stipulated in the sqlite3 or SQLAlchemy.", "New in version 1.4.0.", "When the table already exists and if_exists is \u2018fail\u2019 (the default).", "See also", "Read a DataFrame from a table.", "Notes", "Timezone aware datetime columns will be written as Timestamp with timezone type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone.", "References", "https://docs.sqlalchemy.org", "https://www.python.org/dev/peps/pep-0249/", "Examples", "Create an in-memory SQLite database.", "Create a table from scratch with 3 rows.", "An sqlalchemy.engine.Connection can also be passed to con:", "This is allowed to support operations that require that the same DBAPI connection is used for the entire operation.", "Overwrite the table with just df2.", "Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars."]}, {"name": "pandas.DataFrame.to_stata", "path": "reference/api/pandas.dataframe.to_stata", "type": "DataFrame", "text": ["Export DataFrame object to Stata dta format.", "Writes the DataFrame to a Stata dataset file. \u201cdta\u201d files contain a Stata dataset.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a binary write() function.", "Changed in version 1.0.0.", "Previously this was \u201cfname\u201d", "Dictionary mapping columns containing datetime types to stata internal format to use when writing the dates. Options are \u2018tc\u2019, \u2018td\u2019, \u2018tm\u2019, \u2018tw\u2019, \u2018th\u2019, \u2018tq\u2019, \u2018ty\u2019. Column can be either an integer or a name. Datetime columns that do not have a conversion type specified will be converted to \u2018tc\u2019. Raises NotImplementedError if a datetime column has timezone information.", "Write the index to Stata dataset.", "Can be \u201c>\u201d, \u201c<\u201d, \u201clittle\u201d, or \u201cbig\u201d. default is sys.byteorder.", "A datetime to use as file creation date. Default is the current time.", "A label for the data set. Must be 80 characters or smaller.", "Dictionary containing columns as keys and variable labels as values. Each label must be 80 characters or smaller.", "Version to use in the output dta file. Set to None to let pandas decide between 118 or 119 formats depending on the number of columns in the frame. Version 114 can be read by Stata 10 and later. Version 117 can be read by Stata 13 or later. Version 118 is supported in Stata 14 and later. Version 119 is supported in Stata 15 and later. Version 114 limits string variables to 244 characters or fewer while versions 117 and later allow strings with lengths up to 2,000,000 characters. Versions 118 and 119 support Unicode characters, and version 119 supports more than 32,767 variables.", "Version 119 should usually only be used when the number of variables exceeds the capacity of dta format 118. Exporting smaller datasets in format 119 may have unintended consequences, and, as of November 2020, Stata SE cannot read version 119 files.", "Changed in version 1.0.0: Added support for formats 118 and 119.", "List of column names to convert to string columns to Stata StrL format. Only available if version is 117. Storing strings in the StrL format can produce smaller dta files if strings have more than 8 characters and values are repeated.", "For on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018path\u2019 path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.", "New in version 1.1.0.", "Changed in version 1.4.0: Zstandard support.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.0.", "Dictionary containing columns as keys and dictionaries of column value to labels as values. Labels for a single variable must be 32,000 characters or smaller.", "New in version 1.4.0.", "If datetimes contain timezone information", "Column dtype is not representable in Stata", "Columns listed in convert_dates are neither datetime64[ns] or datetime.datetime", "Column listed in convert_dates is not in DataFrame", "Categorical label contains more than 32,000 characters", "See also", "Import Stata data files.", "Low-level writer for Stata data files.", "Low-level writer for version 117 files.", "Examples"]}, {"name": "pandas.DataFrame.to_string", "path": "reference/api/pandas.dataframe.to_string", "type": "DataFrame", "text": ["Render a DataFrame to a console-friendly tabular output.", "Buffer to write to. If None, the output is returned as a string.", "The subset of columns to write. Writes all columns by default.", "The minimum width of each column. If a list of ints is given every integers corresponds with one column. If a dict is given, the key references the column, while the value defines the space to use..", "Write out the column names. If a list of strings is given, it is assumed to be aliases for the column names.", "Whether to print index (row) labels.", "String representation of NaN to use.", "Formatter functions to apply to columns\u2019 elements by position or name. The result of each function must be a unicode string. List/tuple must be of length equal to the number of columns.", "Formatter function to apply to columns\u2019 elements if they are floats. This function must return a unicode string and will be applied only to the non-NaN elements, with NaN being handled by na_rep.", "Changed in version 1.2.0.", "Set to False for a DataFrame with a hierarchical index to print every multiindex key at each row.", "Prints the names of the indexes.", "How to justify the column labels. If None uses the option from the print configuration (controlled by set_option), \u2018right\u2019 out of the box. Valid values are", "left", "right", "center", "justify", "justify-all", "start", "end", "inherit", "match-parent", "initial", "unset.", "Maximum number of rows to display in the console.", "Maximum number of columns to display in the console.", "Display DataFrame dimensions (number of rows by number of columns).", "Character recognized as decimal separator, e.g. \u2018,\u2019 in Europe.", "Width to wrap a line in characters.", "The number of rows to display in the console in a truncated repr (when number of rows is above max_rows).", "Max width to truncate each column in characters. By default, no limit.", "New in version 1.0.0.", "Set character encoding.", "New in version 1.0.", "If buf is None, returns the result as a string. Otherwise returns None.", "See also", "Convert DataFrame to HTML.", "Examples"]}, {"name": "pandas.DataFrame.to_timestamp", "path": "reference/api/pandas.dataframe.to_timestamp", "type": "DataFrame", "text": ["Cast to DatetimeIndex of timestamps, at beginning of period.", "Desired frequency.", "Convention for converting period to timestamp; start of period vs. end.", "The axis to convert (the index by default).", "If False then underlying input data is not copied."]}, {"name": "pandas.DataFrame.to_xarray", "path": "reference/api/pandas.dataframe.to_xarray", "type": "DataFrame", "text": ["Return an xarray object from the pandas object.", "Data in the pandas structure converted to Dataset if the object is a DataFrame, or a DataArray if the object is a Series.", "See also", "Write DataFrame to an HDF5 file.", "Write a DataFrame to the binary parquet format.", "Notes", "See the xarray docs", "Examples"]}, {"name": "pandas.DataFrame.to_xml", "path": "reference/api/pandas.dataframe.to_xml", "type": "DataFrame", "text": ["Render a DataFrame to an XML document.", "New in version 1.3.0.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a write() function. If None, the result is returned as a string.", "Whether to include index in XML document.", "The name of root element in XML document.", "The name of row element in XML document.", "Missing data representation.", "List of columns to write as attributes in row element. Hierarchical columns will be flattened with underscore delimiting the different levels.", "List of columns to write as children in row element. By default, all columns output as children of row element. Hierarchical columns will be flattened with underscore delimiting the different levels.", "All namespaces to be defined in root element. Keys of dict should be prefix names and values of dict corresponding URIs. Default namespaces should be given empty string key. For example,", "Namespace prefix to be used for every element and/or attribute in document. This should be one of the keys in namespaces dict.", "Encoding of the resulting document.", "Whether to include the XML declaration at start of document.", "Whether output should be pretty printed with indentation and line breaks.", "Parser module to use for building of tree. Only \u2018lxml\u2019 and \u2018etree\u2019 are supported. With \u2018lxml\u2019, the ability to use XSLT stylesheet is supported.", "A URL, file-like object, or a raw string containing an XSLT script used to transform the raw XML output. Script should use layout of elements and attributes from original output. This argument requires lxml to be installed. Only XSLT 1.0 scripts and not later versions is currently supported.", "For on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018path_or_buffer\u2019 path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.", "Changed in version 1.4.0: Zstandard support.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "If io is None, returns the resulting XML format as a string. Otherwise returns None.", "See also", "Convert the pandas object to a JSON string.", "Convert DataFrame to a html.", "Examples"]}, {"name": "pandas.DataFrame.transform", "path": "reference/api/pandas.dataframe.transform", "type": "DataFrame", "text": ["Call func on self producing a DataFrame with the same axis shape as self.", "Function to use for transforming the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. If func is both list-like and dict-like, dict-like behavior takes precedence.", "Accepted combinations are:", "function", "string function name", "list-like of functions and/or function names, e.g. [np.exp, 'sqrt']", "dict-like of axis labels -> functions, function names or list-like of such.", "If 0 or \u2018index\u2019: apply function to each column. If 1 or \u2018columns\u2019: apply function to each row.", "Positional arguments to pass to func.", "Keyword arguments to pass to func.", "A DataFrame that must have the same length as self.", "See also", "Only perform aggregating type operations.", "Invoke function on a DataFrame.", "Notes", "Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.", "Examples", "Even though the resulting DataFrame must have the same length as the input DataFrame, it is possible to provide several input functions:", "You can call transform on a GroupBy object:"]}, {"name": "pandas.DataFrame.transpose", "path": "reference/api/pandas.dataframe.transpose", "type": "DataFrame", "text": ["Transpose index and columns.", "Reflect the DataFrame over its main diagonal by writing rows as columns and vice-versa. The property T is an accessor to the method transpose().", "Accepted for compatibility with NumPy.", "Whether to copy the data after transposing, even for DataFrames with a single dtype.", "Note that a copy is always required for mixed dtype DataFrames, or for DataFrames with any extension types.", "The transposed DataFrame.", "See also", "Permute the dimensions of a given array.", "Notes", "Transposing a DataFrame with mixed dtypes will result in a homogeneous DataFrame with the object dtype. In such a case, a copy of the data is always made.", "Examples", "Square DataFrame with homogeneous dtype", "When the dtype is homogeneous in the original DataFrame, we get a transposed DataFrame with the same dtype:", "Non-square DataFrame with mixed dtypes", "When the DataFrame has mixed dtypes, we get a transposed DataFrame with the object dtype:"]}, {"name": "pandas.DataFrame.truediv", "path": "reference/api/pandas.dataframe.truediv", "type": "DataFrame", "text": ["Get Floating division of dataframe and other, element-wise (binary operator truediv).", "Equivalent to dataframe / other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rtruediv.", "Among flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic operators: +, -, *, /, //, %, **.", "Any single or multiple element data structure, or list-like object.", "Whether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019). For Series input, axis to match Series index on.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.", "Result of the arithmetic operation.", "See also", "Add DataFrames.", "Subtract DataFrames.", "Multiply DataFrames.", "Divide DataFrames (float division).", "Divide DataFrames (float division).", "Divide DataFrames (integer division).", "Calculate modulo (remainder after division).", "Calculate exponential power.", "Notes", "Mismatched indices will be unioned together.", "Examples", "Add a scalar with operator version which return the same results.", "Divide by constant with reverse version.", "Subtract a list and Series by axis with operator version.", "Multiply a DataFrame of different shape with operator version.", "Divide by a MultiIndex by level."]}, {"name": "pandas.DataFrame.truncate", "path": "reference/api/pandas.dataframe.truncate", "type": "DataFrame", "text": ["Truncate a Series or DataFrame before and after some index value.", "This is a useful shorthand for boolean indexing based on index values above or below certain thresholds.", "Truncate all rows before this index value.", "Truncate all rows after this index value.", "Axis to truncate. Truncates the index (rows) by default.", "Return a copy of the truncated section.", "The truncated Series or DataFrame.", "See also", "Select a subset of a DataFrame by label.", "Select a subset of a DataFrame by position.", "Notes", "If the index being truncated contains only datetime values, before and after may be specified as strings instead of Timestamps.", "Examples", "The columns of a DataFrame can be truncated.", "For Series, only rows can be truncated.", "The index values in truncate can be datetimes or string dates.", "Because the index is a DatetimeIndex containing only dates, we can specify before and after as strings. They will be coerced to Timestamps before truncation.", "Note that truncate assumes a 0 value for any unspecified time component (midnight). This differs from partial string slicing, which returns any partially matching dates."]}, {"name": "pandas.DataFrame.tshift", "path": "reference/api/pandas.dataframe.tshift", "type": "DataFrame", "text": ["Shift the time index, using the index\u2019s frequency if available.", "Deprecated since version 1.1.0: Use shift instead.", "Number of periods to move, can be positive or negative.", "Increment to use from the tseries module or time rule expressed as a string (e.g. \u2018EOM\u2019).", "Corresponds to the axis that contains the Index.", "Notes", "If freq is not specified then tries to use the freq or inferred_freq attributes of the index. If neither of those attributes exist, a ValueError is thrown"]}, {"name": "pandas.DataFrame.tz_convert", "path": "reference/api/pandas.dataframe.tz_convert", "type": "DataFrame", "text": ["Convert tz-aware axis to target time zone.", "If axis is a MultiIndex, convert a specific level. Otherwise must be None.", "Also make a copy of the underlying data.", "Object with time zone converted axis.", "If the axis is tz-naive."]}, {"name": "pandas.DataFrame.tz_localize", "path": "reference/api/pandas.dataframe.tz_localize", "type": "DataFrame", "text": ["Localize tz-naive index of a Series or DataFrame to target time zone.", "This operation localizes the Index. To localize the values in a timezone-naive Series, use Series.dt.tz_localize().", "If axis ia a MultiIndex, localize a specific level. Otherwise must be None.", "Also make a copy of the underlying data.", "When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled.", "\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order", "bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times)", "\u2018NaT\u2019 will return NaT where there are ambiguous times", "\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. Valid values are:", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time", "\u2018NaT\u2019 will return NaT where there are nonexistent times", "timedelta objects will shift nonexistent times by the timedelta", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "Same type as the input.", "If the TimeSeries is tz-aware and tz is not None.", "Examples", "Localize local times:", "Be careful with DST changes. When there is sequential data, pandas can infer the DST time:", "In some cases, inferring the DST is impossible. In such cases, you can pass an ndarray to the ambiguous parameter to set the DST explicitly", "If the DST transition causes nonexistent times, you can shift these dates forward or backward with a timedelta object or \u2018shift_forward\u2019 or \u2018shift_backward\u2019."]}, {"name": "pandas.DataFrame.unstack", "path": "reference/api/pandas.dataframe.unstack", "type": "DataFrame", "text": ["Pivot a level of the (necessarily hierarchical) index labels.", "Returns a DataFrame having a new level of column labels whose inner-most level consists of the pivoted index labels.", "If the index is not a MultiIndex, the output will be a Series (the analogue of stack when the columns are not a MultiIndex).", "Level(s) of index to unstack, can pass level name.", "Replace NaN with this value if the unstack produces missing values.", "See also", "Pivot a table based on column values.", "Pivot a level of the column labels (inverse operation from unstack).", "Examples"]}, {"name": "pandas.DataFrame.update", "path": "reference/api/pandas.dataframe.update", "type": "DataFrame", "text": ["Modify in place using non-NA values from another DataFrame.", "Aligns on indices. There is no return value.", "Should have at least one matching index/column label with the original DataFrame. If a Series is passed, its name attribute must be set, and that will be used as the column name to align with the original DataFrame.", "Only left join is implemented, keeping the index and columns of the original object.", "How to handle non-NA values for overlapping keys:", "True: overwrite original DataFrame\u2019s values with values from other.", "False: only update values that are NA in the original DataFrame.", "Can choose to replace values other than NA. Return True for values that should be updated.", "If \u2018raise\u2019, will raise a ValueError if the DataFrame and other both contain non-NA data in the same place.", "When errors=\u2019raise\u2019 and there\u2019s overlapping non-NA data.", "When errors is not either \u2018ignore\u2019 or \u2018raise\u2019", "If join != \u2018left\u2019", "See also", "Similar method for dictionaries.", "For column(s)-on-column(s) operations.", "Examples", "The DataFrame\u2019s length does not increase as a result of the update, only values at matching index/column labels are updated.", "For Series, its name attribute must be set.", "If other contains NaNs the corresponding values are not updated in the original dataframe."]}, {"name": "pandas.DataFrame.value_counts", "path": "reference/api/pandas.dataframe.value_counts", "type": "DataFrame", "text": ["Return a Series containing counts of unique rows in the DataFrame.", "New in version 1.1.0.", "Columns to use when counting unique combinations.", "Return proportions rather than frequencies.", "Sort by frequencies.", "Sort in ascending order.", "Don\u2019t include counts of rows that contain NA values.", "New in version 1.3.0.", "See also", "Equivalent method on Series.", "Notes", "The returned Series will have a MultiIndex with one level per input column. By default, rows that contain any NA values are omitted from the result. By default, the resulting Series will be in descending order so that the first element is the most frequently-occurring row.", "Examples", "With dropna set to False we can also count rows with NA values."]}, {"name": "pandas.DataFrame.values", "path": "reference/api/pandas.dataframe.values", "type": "DataFrame", "text": ["Return a Numpy representation of the DataFrame.", "Warning", "We recommend using DataFrame.to_numpy() instead.", "Only the values in the DataFrame will be returned, the axes labels will be removed.", "The values of the DataFrame.", "See also", "Recommended alternative to this method.", "Retrieve the index labels.", "Retrieving the column names.", "Notes", "The dtype will be a lower-common-denominator dtype (implicit upcasting); that is to say if the dtypes (even of numeric types) are mixed, the one that accommodates all will be chosen. Use this with care if you are not dealing with the blocks.", "e.g. If the dtypes are float16 and float32, dtype will be upcast to float32. If dtypes are int32 and uint8, dtype will be upcast to int32. By numpy.find_common_type() convention, mixing int64 and uint64 will result in a float64 dtype.", "Examples", "A DataFrame where all columns are the same type (e.g., int64) results in an array of the same type.", "A DataFrame with mixed type columns(e.g., str/object, int64, float32) results in an ndarray of the broadest type that accommodates these mixed types (e.g., object)."]}, {"name": "pandas.DataFrame.var", "path": "reference/api/pandas.dataframe.var", "type": "DataFrame", "text": ["Return unbiased variance over requested axis.", "Normalized by N-1 by default. This can be changed using the ddof argument.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.", "Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Examples", "Alternatively, ddof=0 can be set to normalize by N instead of N-1:"]}, {"name": "pandas.DataFrame.where", "path": "reference/api/pandas.dataframe.where", "type": "DataFrame", "text": ["Replace values where the condition is False.", "Where cond is True, keep the original value. Where False, replace with corresponding value from other. If cond is callable, it is computed on the Series/DataFrame and should return boolean Series/DataFrame or array. The callable must not change input Series/DataFrame (though pandas doesn\u2019t check it).", "Entries where cond is False are replaced with corresponding value from other. If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn\u2019t check it).", "Whether to perform the operation in place on the data.", "Alignment axis if needed.", "Alignment level if needed.", "Note that currently this parameter won\u2019t affect the results and will always coerce to a suitable dtype.", "\u2018raise\u2019 : allow exceptions to be raised.", "\u2018ignore\u2019 : suppress exceptions. On error return original object.", "Try to cast the result back to the input type (if possible).", "Deprecated since version 1.3.0: Manually cast back if necessary.", "See also", "Return an object of same shape as self.", "Notes", "The where method is an application of the if-then idiom. For each element in the calling DataFrame, if cond is True the element is used; otherwise the corresponding element from the DataFrame other is used.", "The signature for DataFrame.where() differs from numpy.where(). Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2).", "For further details and examples see the where documentation in indexing.", "Examples"]}, {"name": "pandas.DataFrame.xs", "path": "reference/api/pandas.dataframe.xs", "type": "DataFrame", "text": ["Return cross-section from the Series/DataFrame.", "This method takes a key argument to select data at a particular level of a MultiIndex.", "Label contained in the index, or partially in a MultiIndex.", "Axis to retrieve cross-section on.", "In case of a key partially contained in a MultiIndex, indicate which levels are used. Levels can be referred by label or position.", "If False, returns object with same levels as self.", "Cross-section from the original Series or DataFrame corresponding to the selected index levels.", "See also", "Access a group of rows and columns by label(s) or a boolean array.", "Purely integer-location based indexing for selection by position.", "Notes", "xs can not be used to set values.", "MultiIndex Slicers is a generic way to get/set values on any level or levels. It is a superset of xs functionality, see MultiIndex Slicers.", "Examples", "Get values at specified index", "Get values at several indexes", "Get values at specified index and level", "Get values at several indexes and levels", "Get values at specified column and axis"]}, {"name": "pandas.date_range", "path": "reference/api/pandas.date_range", "type": "General functions", "text": ["Return a fixed frequency DatetimeIndex.", "Returns the range of equally spaced time points (where the difference between any two adjacent points is specified by the given frequency) such that they all satisfy start <[=] x <[=] end, where the first one and the last one are, resp., the first and last time points in that range that fall on the boundary of freq (if given as a frequency string) or that are valid for freq (if given as a pandas.tseries.offsets.DateOffset). (If exactly one of start, end, or freq is not specified, this missing parameter can be computed given periods, the number of timesteps in the range. See the note below.)", "Left bound for generating dates.", "Right bound for generating dates.", "Number of periods to generate.", "Frequency strings can have multiples, e.g. \u20185H\u2019. See here for a list of frequency aliases.", "Time zone name for returning localized DatetimeIndex, for example \u2018Asia/Hong_Kong\u2019. By default, the resulting DatetimeIndex is timezone-naive.", "Normalize start/end dates to midnight before generating date range.", "Name of the resulting DatetimeIndex.", "Make the interval closed with respect to the given frequency to the \u2018left\u2019, \u2018right\u2019, or both sides (None, the default).", "Deprecated since version 1.4.0: Argument closed has been deprecated to standardize boundary inputs. Use inclusive instead, to set each bound as closed or open.", "Include boundaries; Whether to set each bound as closed or open.", "New in version 1.4.0.", "For compatibility. Has no effect on the result.", "See also", "An immutable container for datetimes.", "Return a fixed frequency TimedeltaIndex.", "Return a fixed frequency PeriodIndex.", "Return a fixed frequency IntervalIndex.", "Notes", "Of the four parameters start, end, periods, and freq, exactly three must be specified. If freq is omitted, the resulting DatetimeIndex will have periods linearly spaced elements between start and end (closed on both sides).", "To learn more about the frequency strings, please see this link.", "Examples", "Specifying the values", "The next four examples generate the same DatetimeIndex, but vary the combination of start, end and periods.", "Specify start and end, with the default daily frequency.", "Specify start and periods, the number of periods (days).", "Specify end and periods, the number of periods (days).", "Specify start, end, and periods; the frequency is generated automatically (linearly spaced).", "Other Parameters", "Changed the freq (frequency) to 'M' (month end frequency).", "Multiples are allowed", "freq can also be specified as an Offset object.", "Specify tz to set the timezone.", "inclusive controls whether to include start and end that are on the boundary. The default, \u201cboth\u201d, includes boundary points on either end.", "Use inclusive='left' to exclude end if it falls on the boundary.", "Use inclusive='right' to exclude start if it falls on the boundary, and similarly inclusive='neither' will exclude both start and end."]}, {"name": "pandas.DatetimeIndex", "path": "reference/api/pandas.datetimeindex", "type": "Index Objects", "text": ["Immutable ndarray-like of datetime64 data.", "Represented internally as int64, and which can be boxed to Timestamp objects that are subclasses of datetime and carry metadata.", "Optional datetime-like data to construct index with.", "One of pandas date offset strings or corresponding objects. The string \u2018infer\u2019 can be passed in order to set the frequency of the index as the inferred frequency upon creation.", "Set the Timezone of the data.", "Normalize start/end dates to midnight before generating date range.", "Set whether to include start and end that are on the boundary. The default includes boundary points on either end.", "When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled.", "\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order", "bool-ndarray where True signifies a DST time, False signifies a non-DST time (note that this flag is only applicable for ambiguous times)", "\u2018NaT\u2019 will return NaT where there are ambiguous times", "\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.", "If True, parse dates in data with the day first order.", "If True parse dates in data with the year first order.", "Note that the only NumPy dtype allowed is \u2018datetime64[ns]\u2019.", "Make a copy of input ndarray.", "Name to be stored in the index.", "See also", "The base pandas Index type.", "Index of timedelta64 data.", "Index of Period data.", "Convert argument to datetime.", "Create a fixed-frequency DatetimeIndex.", "Notes", "To learn more about the frequency strings, please see this link.", "Attributes", "year", "The year of the datetime.", "month", "The month as January=1, December=12.", "day", "The day of the datetime.", "hour", "The hours of the datetime.", "minute", "The minutes of the datetime.", "second", "The seconds of the datetime.", "microsecond", "The microseconds of the datetime.", "nanosecond", "The nanoseconds of the datetime.", "date", "Returns numpy array of python datetime.date objects.", "time", "Returns numpy array of datetime.time objects.", "timetz", "Returns numpy array of datetime.time objects with timezone information.", "dayofyear", "The ordinal day of the year.", "day_of_year", "The ordinal day of the year.", "weekofyear", "(DEPRECATED) The week ordinal of the year.", "week", "(DEPRECATED) The week ordinal of the year.", "dayofweek", "The day of the week with Monday=0, Sunday=6.", "day_of_week", "The day of the week with Monday=0, Sunday=6.", "weekday", "The day of the week with Monday=0, Sunday=6.", "quarter", "The quarter of the date.", "tz", "Return the timezone.", "freq", "Return the frequency object if it is set, otherwise None.", "freqstr", "Return the frequency object as a string if its set, otherwise None.", "is_month_start", "Indicates whether the date is the first day of the month.", "is_month_end", "Indicates whether the date is the last day of the month.", "is_quarter_start", "Indicator for whether the date is the first day of a quarter.", "is_quarter_end", "Indicator for whether the date is the last day of a quarter.", "is_year_start", "Indicate whether the date is the first day of a year.", "is_year_end", "Indicate whether the date is the last day of the year.", "is_leap_year", "Boolean indicator if the date belongs to a leap year.", "inferred_freq", "Tries to return a string representing a frequency guess, generated by infer_freq.", "Methods", "normalize(*args, **kwargs)", "Convert times to midnight.", "strftime(*args, **kwargs)", "Convert to Index using specified date_format.", "snap([freq])", "Snap time stamps to nearest occurring frequency.", "tz_convert(tz)", "Convert tz-aware Datetime Array/Index from one time zone to another.", "tz_localize(tz[, ambiguous, nonexistent])", "Localize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index.", "round(*args, **kwargs)", "Perform round operation on the data to the specified freq.", "floor(*args, **kwargs)", "Perform floor operation on the data to the specified freq.", "ceil(*args, **kwargs)", "Perform ceil operation on the data to the specified freq.", "to_period(*args, **kwargs)", "Cast to PeriodArray/Index at a particular frequency.", "to_perioddelta(freq)", "Calculate TimedeltaArray of difference between index values and index converted to PeriodArray at specified freq.", "to_pydatetime(*args, **kwargs)", "Return Datetime Array/Index as object ndarray of datetime.datetime objects.", "to_series([keep_tz, index, name])", "Create a Series with both index and values equal to the index keys useful with map for returning an indexer based on an index.", "to_frame([index, name])", "Create a DataFrame with a column containing the Index.", "month_name(*args, **kwargs)", "Return the month names of the DateTimeIndex with specified locale.", "day_name(*args, **kwargs)", "Return the day names of the DateTimeIndex with specified locale.", "mean(*args, **kwargs)", "Return the mean value of the Array.", "std(*args, **kwargs)", "Return sample standard deviation over requested axis."]}, {"name": "pandas.DatetimeIndex.ceil", "path": "reference/api/pandas.datetimeindex.ceil", "type": "Index Objects", "text": ["Perform ceil operation on the data to the specified freq.", "The frequency level to ceil the index to. Must be a fixed frequency like \u2018S\u2019 (second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible freq values.", "Only relevant for DatetimeIndex:", "\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order", "bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times)", "\u2018NaT\u2019 will return NaT where there are ambiguous times", "\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST.", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time", "\u2018NaT\u2019 will return NaT where there are nonexistent times", "timedelta objects will shift nonexistent times by the timedelta", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "Index of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series.", "Notes", "If the timestamps have a timezone, ceiling will take place relative to the local (\u201cwall\u201d) time and re-localized to the same timezone. When ceiling near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.", "Examples", "DatetimeIndex", "Series", "When rounding near a daylight savings time transition, use ambiguous or nonexistent to control how the timestamp should be re-localized."]}, {"name": "pandas.DatetimeIndex.date", "path": "reference/api/pandas.datetimeindex.date", "type": "Index Objects", "text": ["Returns numpy array of python datetime.date objects.", "Namely, the date part of Timestamps without time and timezone information."]}, {"name": "pandas.DatetimeIndex.day", "path": "reference/api/pandas.datetimeindex.day", "type": "Index Objects", "text": ["The day of the datetime.", "Examples"]}, {"name": "pandas.DatetimeIndex.day_name", "path": "reference/api/pandas.datetimeindex.day_name", "type": "Index Objects", "text": ["Return the day names of the DateTimeIndex with specified locale.", "Locale determining the language in which to return the day name. Default is English locale.", "Index of day names.", "Examples"]}, {"name": "pandas.DatetimeIndex.day_of_week", "path": "reference/api/pandas.datetimeindex.day_of_week", "type": "Index Objects", "text": ["The day of the week with Monday=0, Sunday=6.", "Return the day of the week. It is assumed the week starts on Monday, which is denoted by 0 and ends on Sunday which is denoted by 6. This method is available on both Series with datetime values (using the dt accessor) or DatetimeIndex.", "Containing integers indicating the day number.", "See also", "Alias.", "Alias.", "Returns the name of the day of the week.", "Examples"]}, {"name": "pandas.DatetimeIndex.day_of_year", "path": "reference/api/pandas.datetimeindex.day_of_year", "type": "Index Objects", "text": ["The ordinal day of the year."]}, {"name": "pandas.DatetimeIndex.dayofweek", "path": "reference/api/pandas.datetimeindex.dayofweek", "type": "Index Objects", "text": ["The day of the week with Monday=0, Sunday=6.", "Return the day of the week. It is assumed the week starts on Monday, which is denoted by 0 and ends on Sunday which is denoted by 6. This method is available on both Series with datetime values (using the dt accessor) or DatetimeIndex.", "Containing integers indicating the day number.", "See also", "Alias.", "Alias.", "Returns the name of the day of the week.", "Examples"]}, {"name": "pandas.DatetimeIndex.dayofyear", "path": "reference/api/pandas.datetimeindex.dayofyear", "type": "Index Objects", "text": ["The ordinal day of the year."]}, {"name": "pandas.DatetimeIndex.floor", "path": "reference/api/pandas.datetimeindex.floor", "type": "Index Objects", "text": ["Perform floor operation on the data to the specified freq.", "The frequency level to floor the index to. Must be a fixed frequency like \u2018S\u2019 (second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible freq values.", "Only relevant for DatetimeIndex:", "\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order", "bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times)", "\u2018NaT\u2019 will return NaT where there are ambiguous times", "\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST.", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time", "\u2018NaT\u2019 will return NaT where there are nonexistent times", "timedelta objects will shift nonexistent times by the timedelta", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "Index of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series.", "Notes", "If the timestamps have a timezone, flooring will take place relative to the local (\u201cwall\u201d) time and re-localized to the same timezone. When flooring near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.", "Examples", "DatetimeIndex", "Series", "When rounding near a daylight savings time transition, use ambiguous or nonexistent to control how the timestamp should be re-localized."]}, {"name": "pandas.DatetimeIndex.freq", "path": "reference/api/pandas.datetimeindex.freq", "type": "Index Objects", "text": ["Return the frequency object if it is set, otherwise None."]}, {"name": "pandas.DatetimeIndex.freqstr", "path": "reference/api/pandas.datetimeindex.freqstr", "type": "Index Objects", "text": ["Return the frequency object as a string if its set, otherwise None."]}, {"name": "pandas.DatetimeIndex.hour", "path": "reference/api/pandas.datetimeindex.hour", "type": "Index Objects", "text": ["The hours of the datetime.", "Examples"]}, {"name": "pandas.DatetimeIndex.indexer_at_time", "path": "reference/api/pandas.datetimeindex.indexer_at_time", "type": "Index Objects", "text": ["Return index locations of values at particular time of day (e.g. 9:30AM).", "Time passed in either as object (datetime.time) or as string in appropriate format (\u201c%H:%M\u201d, \u201c%H%M\u201d, \u201c%I:%M%p\u201d, \u201c%I%M%p\u201d, \u201c%H:%M:%S\u201d, \u201c%H%M%S\u201d, \u201c%I:%M:%S%p\u201d, \u201c%I%M%S%p\u201d).", "See also", "Get index locations of values between particular times of day.", "Select values at particular time of day."]}, {"name": "pandas.DatetimeIndex.indexer_between_time", "path": "reference/api/pandas.datetimeindex.indexer_between_time", "type": "Index Objects", "text": ["Return index locations of values between particular times of day (e.g., 9:00-9:30AM).", "Time passed either as object (datetime.time) or as string in appropriate format (\u201c%H:%M\u201d, \u201c%H%M\u201d, \u201c%I:%M%p\u201d, \u201c%I%M%p\u201d, \u201c%H:%M:%S\u201d, \u201c%H%M%S\u201d, \u201c%I:%M:%S%p\u201d,\u201d%I%M%S%p\u201d).", "See also", "Get index locations of values at particular time of day.", "Select values between particular times of day."]}, {"name": "pandas.DatetimeIndex.inferred_freq", "path": "reference/api/pandas.datetimeindex.inferred_freq", "type": "Index Objects", "text": ["Tries to return a string representing a frequency guess, generated by infer_freq. Returns None if it can\u2019t autodetect the frequency."]}, {"name": "pandas.DatetimeIndex.is_leap_year", "path": "reference/api/pandas.datetimeindex.is_leap_year", "type": "Index Objects", "text": ["Boolean indicator if the date belongs to a leap year.", "A leap year is a year, which has 366 days (instead of 365) including 29th of February as an intercalary day. Leap years are years which are multiples of four with the exception of years divisible by 100 but not by 400.", "Booleans indicating if dates belong to a leap year.", "Examples", "This method is available on Series with datetime values under the .dt accessor, and directly on DatetimeIndex."]}, {"name": "pandas.DatetimeIndex.is_month_end", "path": "reference/api/pandas.datetimeindex.is_month_end", "type": "Index Objects", "text": ["Indicates whether the date is the last day of the month.", "For Series, returns a Series with boolean values. For DatetimeIndex, returns a boolean array.", "See also", "Return a boolean indicating whether the date is the first day of the month.", "Return a boolean indicating whether the date is the last day of the month.", "Examples", "This method is available on Series with datetime values under the .dt accessor, and directly on DatetimeIndex."]}, {"name": "pandas.DatetimeIndex.is_month_start", "path": "reference/api/pandas.datetimeindex.is_month_start", "type": "Index Objects", "text": ["Indicates whether the date is the first day of the month.", "For Series, returns a Series with boolean values. For DatetimeIndex, returns a boolean array.", "See also", "Return a boolean indicating whether the date is the first day of the month.", "Return a boolean indicating whether the date is the last day of the month.", "Examples", "This method is available on Series with datetime values under the .dt accessor, and directly on DatetimeIndex."]}, {"name": "pandas.DatetimeIndex.is_quarter_end", "path": "reference/api/pandas.datetimeindex.is_quarter_end", "type": "Index Objects", "text": ["Indicator for whether the date is the last day of a quarter.", "The same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.", "See also", "Return the quarter of the date.", "Similar property indicating the quarter start.", "Examples", "This method is available on Series with datetime values under the .dt accessor, and directly on DatetimeIndex."]}, {"name": "pandas.DatetimeIndex.is_quarter_start", "path": "reference/api/pandas.datetimeindex.is_quarter_start", "type": "Index Objects", "text": ["Indicator for whether the date is the first day of a quarter.", "The same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.", "See also", "Return the quarter of the date.", "Similar property for indicating the quarter start.", "Examples", "This method is available on Series with datetime values under the .dt accessor, and directly on DatetimeIndex."]}, {"name": "pandas.DatetimeIndex.is_year_end", "path": "reference/api/pandas.datetimeindex.is_year_end", "type": "Index Objects", "text": ["Indicate whether the date is the last day of the year.", "The same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.", "See also", "Similar property indicating the start of the year.", "Examples", "This method is available on Series with datetime values under the .dt accessor, and directly on DatetimeIndex."]}, {"name": "pandas.DatetimeIndex.is_year_start", "path": "reference/api/pandas.datetimeindex.is_year_start", "type": "Index Objects", "text": ["Indicate whether the date is the first day of a year.", "The same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.", "See also", "Similar property indicating the last day of the year.", "Examples", "This method is available on Series with datetime values under the .dt accessor, and directly on DatetimeIndex."]}, {"name": "pandas.DatetimeIndex.mean", "path": "reference/api/pandas.datetimeindex.mean", "type": "Index Objects", "text": ["Return the mean value of the Array.", "New in version 0.25.0.", "Whether to ignore any NaT elements.", "Timestamp or Timedelta.", "See also", "Returns the average of array elements along a given axis.", "Return the mean value in a Series.", "Notes", "mean is only defined for Datetime and Timedelta dtypes, not for Period."]}, {"name": "pandas.DatetimeIndex.microsecond", "path": "reference/api/pandas.datetimeindex.microsecond", "type": "Index Objects", "text": ["The microseconds of the datetime.", "Examples"]}, {"name": "pandas.DatetimeIndex.minute", "path": "reference/api/pandas.datetimeindex.minute", "type": "Index Objects", "text": ["The minutes of the datetime.", "Examples"]}, {"name": "pandas.DatetimeIndex.month", "path": "reference/api/pandas.datetimeindex.month", "type": "Index Objects", "text": ["The month as January=1, December=12.", "Examples"]}, {"name": "pandas.DatetimeIndex.month_name", "path": "reference/api/pandas.datetimeindex.month_name", "type": "Index Objects", "text": ["Return the month names of the DateTimeIndex with specified locale.", "Locale determining the language in which to return the month name. Default is English locale.", "Index of month names.", "Examples"]}, {"name": "pandas.DatetimeIndex.nanosecond", "path": "reference/api/pandas.datetimeindex.nanosecond", "type": "Index Objects", "text": ["The nanoseconds of the datetime.", "Examples"]}, {"name": "pandas.DatetimeIndex.normalize", "path": "reference/api/pandas.datetimeindex.normalize", "type": "Index Objects", "text": ["Convert times to midnight.", "The time component of the date-time is converted to midnight i.e. 00:00:00. This is useful in cases, when the time does not matter. Length is unaltered. The timezones are unaffected.", "This method is available on Series with datetime values under the .dt accessor, and directly on Datetime Array/Index.", "The same type as the original data. Series will have the same name and index. DatetimeIndex will have the same name.", "See also", "Floor the datetimes to the specified freq.", "Ceil the datetimes to the specified freq.", "Round the datetimes to the specified freq.", "Examples"]}, {"name": "pandas.DatetimeIndex.quarter", "path": "reference/api/pandas.datetimeindex.quarter", "type": "Index Objects", "text": ["The quarter of the date."]}, {"name": "pandas.DatetimeIndex.round", "path": "reference/api/pandas.datetimeindex.round", "type": "Index Objects", "text": ["Perform round operation on the data to the specified freq.", "The frequency level to round the index to. Must be a fixed frequency like \u2018S\u2019 (second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible freq values.", "Only relevant for DatetimeIndex:", "\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order", "bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times)", "\u2018NaT\u2019 will return NaT where there are ambiguous times", "\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST.", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time", "\u2018NaT\u2019 will return NaT where there are nonexistent times", "timedelta objects will shift nonexistent times by the timedelta", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "Index of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series.", "Notes", "If the timestamps have a timezone, rounding will take place relative to the local (\u201cwall\u201d) time and re-localized to the same timezone. When rounding near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.", "Examples", "DatetimeIndex", "Series", "When rounding near a daylight savings time transition, use ambiguous or nonexistent to control how the timestamp should be re-localized."]}, {"name": "pandas.DatetimeIndex.second", "path": "reference/api/pandas.datetimeindex.second", "type": "Index Objects", "text": ["The seconds of the datetime.", "Examples"]}, {"name": "pandas.DatetimeIndex.snap", "path": "reference/api/pandas.datetimeindex.snap", "type": "Index Objects", "text": ["Snap time stamps to nearest occurring frequency."]}, {"name": "pandas.DatetimeIndex.std", "path": "reference/api/pandas.datetimeindex.std", "type": "Index Objects", "text": ["Return sample standard deviation over requested axis.", "Normalized by N-1 by default. This can be changed using the ddof argument", "Axis for the function to be applied on.", "Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA."]}, {"name": "pandas.DatetimeIndex.strftime", "path": "reference/api/pandas.datetimeindex.strftime", "type": "Index Objects", "text": ["Convert to Index using specified date_format.", "Return an Index of formatted strings specified by date_format, which supports the same string format as the python standard library. Details of the string format can be found in python string format doc.", "Date format string (e.g. \u201c%Y-%m-%d\u201d).", "NumPy ndarray of formatted strings.", "See also", "Convert the given argument to datetime.", "Return DatetimeIndex with times to midnight.", "Round the DatetimeIndex to the specified freq.", "Floor the DatetimeIndex to the specified freq.", "Examples"]}, {"name": "pandas.DatetimeIndex.time", "path": "reference/api/pandas.datetimeindex.time", "type": "Index Objects", "text": ["Returns numpy array of datetime.time objects.", "The time part of the Timestamps."]}, {"name": "pandas.DatetimeIndex.timetz", "path": "reference/api/pandas.datetimeindex.timetz", "type": "Index Objects", "text": ["Returns numpy array of datetime.time objects with timezone information.", "The time part of the Timestamps."]}, {"name": "pandas.DatetimeIndex.to_frame", "path": "reference/api/pandas.datetimeindex.to_frame", "type": "DataFrame", "text": ["Create a DataFrame with a column containing the Index.", "Set the index of the returned DataFrame as the original Index.", "The passed name should substitute for the index name (if it has one).", "DataFrame containing the original Index data.", "See also", "Convert an Index to a Series.", "Convert Series to DataFrame.", "Examples", "By default, the original Index is reused. To enforce a new Index:", "To override the name of the resulting column, specify name:"]}, {"name": "pandas.DatetimeIndex.to_period", "path": "reference/api/pandas.datetimeindex.to_period", "type": "Input/output", "text": ["Cast to PeriodArray/Index at a particular frequency.", "Converts DatetimeArray/Index to PeriodArray/Index.", "One of pandas\u2019 offset strings or an Offset object. Will be inferred by default.", "When converting a DatetimeArray/Index with non-regular values, so that a frequency cannot be inferred.", "See also", "Immutable ndarray holding ordinal values.", "Return DatetimeIndex as object.", "Examples", "Infer the daily frequency"]}, {"name": "pandas.DatetimeIndex.to_perioddelta", "path": "reference/api/pandas.datetimeindex.to_perioddelta", "type": "Input/output", "text": ["Calculate TimedeltaArray of difference between index values and index converted to PeriodArray at specified freq. Used for vectorized offsets."]}, {"name": "pandas.DatetimeIndex.to_pydatetime", "path": "reference/api/pandas.datetimeindex.to_pydatetime", "type": "Index Objects", "text": ["Return Datetime Array/Index as object ndarray of datetime.datetime objects."]}, {"name": "pandas.DatetimeIndex.to_series", "path": "reference/api/pandas.datetimeindex.to_series", "type": "Index Objects", "text": ["Create a Series with both index and values equal to the index keys useful with map for returning an indexer based on an index.", "Return the data keeping the timezone.", "If keep_tz is True:", "If the timezone is not set, the resulting Series will have a datetime64[ns] dtype.", "Otherwise the Series will have an datetime64[ns, tz] dtype; the tz will be preserved.", "If keep_tz is False:", "Series will have a datetime64[ns] dtype. TZ aware objects will have the tz removed.", "Changed in version 1.0.0: The default value is now True. In a future version, this keyword will be removed entirely. Stop passing the argument to obtain the future behavior and silence the warning.", "Index of resulting Series. If None, defaults to original index.", "Name of resulting Series. If None, defaults to name of original index."]}, {"name": "pandas.DatetimeIndex.tz", "path": "reference/api/pandas.datetimeindex.tz", "type": "Index Objects", "text": ["Return the timezone.", "Returns None when the array is tz-naive."]}, {"name": "pandas.DatetimeIndex.tz_convert", "path": "reference/api/pandas.datetimeindex.tz_convert", "type": "Index Objects", "text": ["Convert tz-aware Datetime Array/Index from one time zone to another.", "Time zone for time. Corresponding timestamps would be converted to this time zone of the Datetime Array/Index. A tz of None will convert to UTC and remove the timezone information.", "If Datetime Array/Index is tz-naive.", "See also", "A timezone that has a variable offset from UTC.", "Localize tz-naive DatetimeIndex to a given time zone, or remove timezone from a tz-aware DatetimeIndex.", "Examples", "With the tz parameter, we can change the DatetimeIndex to other time zones:", "With the tz=None, we can remove the timezone (after converting to UTC if necessary):"]}, {"name": "pandas.DatetimeIndex.tz_localize", "path": "reference/api/pandas.datetimeindex.tz_localize", "type": "Index Objects", "text": ["Localize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index.", "This method takes a time zone (tz) naive Datetime Array/Index object and makes this time zone aware. It does not move the time to another time zone.", "This method can also be used to do the inverse \u2013 to create a time zone unaware object from an aware object. To that end, pass tz=None.", "Time zone to convert timestamps to. Passing None will remove the time zone information preserving local time.", "When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled.", "\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order", "bool-ndarray where True signifies a DST time, False signifies a non-DST time (note that this flag is only applicable for ambiguous times)", "\u2018NaT\u2019 will return NaT where there are ambiguous times", "\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST.", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time", "\u2018NaT\u2019 will return NaT where there are nonexistent times", "timedelta objects will shift nonexistent times by the timedelta", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "Array/Index converted to the specified time zone.", "If the Datetime Array/Index is tz-aware and tz is not None.", "See also", "Convert tz-aware DatetimeIndex from one time zone to another.", "Examples", "Localize DatetimeIndex in US/Eastern time zone:", "With the tz=None, we can remove the time zone information while keeping the local time (not converted to UTC):", "Be careful with DST changes. When there is sequential data, pandas can infer the DST time:", "In some cases, inferring the DST is impossible. In such cases, you can pass an ndarray to the ambiguous parameter to set the DST explicitly", "If the DST transition causes nonexistent times, you can shift these dates forward or backwards with a timedelta object or \u2018shift_forward\u2019 or \u2018shift_backwards\u2019."]}, {"name": "pandas.DatetimeIndex.week", "path": "reference/api/pandas.datetimeindex.week", "type": "Index Objects", "text": ["The week ordinal of the year.", "Deprecated since version 1.1.0.", "weekofyear and week have been deprecated. Please use DatetimeIndex.isocalendar().week instead."]}, {"name": "pandas.DatetimeIndex.weekday", "path": "reference/api/pandas.datetimeindex.weekday", "type": "Index Objects", "text": ["The day of the week with Monday=0, Sunday=6.", "Return the day of the week. It is assumed the week starts on Monday, which is denoted by 0 and ends on Sunday which is denoted by 6. This method is available on both Series with datetime values (using the dt accessor) or DatetimeIndex.", "Containing integers indicating the day number.", "See also", "Alias.", "Alias.", "Returns the name of the day of the week.", "Examples"]}, {"name": "pandas.DatetimeIndex.weekofyear", "path": "reference/api/pandas.datetimeindex.weekofyear", "type": "Index Objects", "text": ["The week ordinal of the year.", "Deprecated since version 1.1.0.", "weekofyear and week have been deprecated. Please use DatetimeIndex.isocalendar().week instead."]}, {"name": "pandas.DatetimeIndex.year", "path": "reference/api/pandas.datetimeindex.year", "type": "Index Objects", "text": ["The year of the datetime.", "Examples"]}, {"name": "pandas.DatetimeTZDtype", "path": "reference/api/pandas.datetimetzdtype", "type": "Pandas arrays", "text": ["An ExtensionDtype for timezone-aware datetime data.", "This is not an actual numpy dtype, but a duck type.", "The precision of the datetime data. Currently limited to \"ns\".", "The timezone.", "When the requested timezone cannot be found.", "Examples", "Attributes", "unit", "The precision of the datetime data.", "tz", "The timezone.", "Methods", "None"]}, {"name": "pandas.DatetimeTZDtype.tz", "path": "reference/api/pandas.datetimetzdtype.tz", "type": "Pandas arrays", "text": ["The timezone."]}, {"name": "pandas.DatetimeTZDtype.unit", "path": "reference/api/pandas.datetimetzdtype.unit", "type": "Pandas arrays", "text": ["The precision of the datetime data."]}, {"name": "pandas.describe_option", "path": "reference/api/pandas.describe_option", "type": "General utility functions", "text": ["Prints the description for one or more registered options.", "Call with no arguments to get a listing for all registered options.", "Available options:", "compute.[use_bottleneck, use_numba, use_numexpr]", "display.[chop_threshold, colheader_justify, column_space, date_dayfirst, date_yearfirst, encoding, expand_frame_repr, float_format]", "display.html.[border, table_schema, use_mathjax]", "display.[large_repr]", "display.latex.[escape, longtable, multicolumn, multicolumn_format, multirow, repr]", "display.[max_categories, max_columns, max_colwidth, max_dir_items, max_info_columns, max_info_rows, max_rows, max_seq_items, memory_usage, min_rows, multi_sparse, notebook_repr_html, pprint_nest_depth, precision, show_dimensions]", "display.unicode.[ambiguous_as_wide, east_asian_width]", "display.[width]", "io.excel.ods.[reader, writer]", "io.excel.xls.[reader, writer]", "io.excel.xlsb.[reader]", "io.excel.xlsm.[reader, writer]", "io.excel.xlsx.[reader, writer]", "io.hdf.[default_format, dropna_table]", "io.parquet.[engine]", "io.sql.[engine]", "mode.[chained_assignment, data_manager, sim_interactive, string_storage, use_inf_as_na, use_inf_as_null]", "plotting.[backend]", "plotting.matplotlib.[register_converters]", "styler.format.[decimal, escape, formatter, na_rep, precision, thousands]", "styler.html.[mathjax]", "styler.latex.[environment, hrules, multicol_align, multirow_align]", "styler.render.[encoding, max_columns, max_elements, max_rows, repr]", "styler.sparse.[columns, index]", "Regexp pattern. All matching keys will have their description displayed.", "If True (default) the description(s) will be printed to stdout. Otherwise, the description(s) will be returned as a unicode string (for testing).", "Notes", "The available options with its descriptions:", "Use the bottleneck library to accelerate if it is installed, the default is True Valid values: False,True [default: True] [currently: True]", "Use the numba engine option for select operations if it is installed, the default is False Valid values: False,True [default: False] [currently: False]", "Use the numexpr library to accelerate computation if it is installed, the default is True Valid values: False,True [default: True] [currently: True]", "if set to a float value, all float values smaller then the given threshold will be displayed as exactly 0 by repr and friends. [default: None] [currently: None]", "Controls the justification of column headers. used by DataFrameFormatter. [default: right] [currently: right]", "[default: 12] [currently: 12]", "When True, prints and parses dates with the day first, eg 20/01/2005 [default: False] [currently: False]", "When True, prints and parses dates with the year first, eg 2005/01/20 [default: False] [currently: False]", "Defaults to the detected encoding of the console. Specifies the encoding to be used for strings returned by to_string, these are generally strings meant to be displayed on the console. [default: utf-8] [currently: utf-8]", "Whether to print out the full DataFrame repr for wide DataFrames across multiple lines, max_columns is still respected, but the output will wrap-around across multiple \u201cpages\u201d if its width exceeds display.width. [default: True] [currently: True]", "The callable should accept a floating point number and return a string with the desired format of the number. This is used in some places like SeriesFormatter. See formats.format.EngFormatter for an example. [default: None] [currently: None]", "A border=value attribute is inserted in the <table> tag for the DataFrame HTML repr. [default: 1] [currently: 1]", "Whether to publish a Table Schema representation for frontends that support it. (default: False) [default: False] [currently: False]", "When True, Jupyter notebook will process table contents using MathJax, rendering mathematical expressions enclosed by the dollar symbol. (default: True) [default: True] [currently: True]", "For DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show a truncated table (the default from 0.13), or switch to the view from df.info() (the behaviour in earlier versions of pandas). [default: truncate] [currently: truncate]", "This specifies if the to_latex method of a Dataframe uses escapes special characters. Valid values: False,True [default: True] [currently: True]", "This specifies if the to_latex method of a Dataframe uses the longtable format. Valid values: False,True [default: False] [currently: False]", "This specifies if the to_latex method of a Dataframe uses multicolumns to pretty-print MultiIndex columns. Valid values: False,True [default: True] [currently: True]", "This specifies if the to_latex method of a Dataframe uses multicolumns to pretty-print MultiIndex columns. Valid values: False,True [default: l] [currently: l]", "This specifies if the to_latex method of a Dataframe uses multirows to pretty-print MultiIndex rows. Valid values: False,True [default: False] [currently: False]", "Whether to produce a latex DataFrame representation for jupyter environments that support it. (default: False) [default: False] [currently: False]", "This sets the maximum number of categories pandas should output when printing out a Categorical or a Series of dtype \u201ccategory\u201d. [default: 8] [currently: 8]", "If max_cols is exceeded, switch to truncate view. Depending on large_repr, objects are either centrally truncated or printed as a summary view. \u2018None\u2019 value means unlimited.", "In case python/IPython is running in a terminal and large_repr equals \u2018truncate\u2019 this can be set to 0 and pandas will auto-detect the width of the terminal and print a truncated object which fits the screen width. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection. [default: 0] [currently: 0]", "The maximum width in characters of a column in the repr of a pandas data structure. When the column overflows, a \u201c\u2026\u201d placeholder is embedded in the output. A \u2018None\u2019 value means unlimited. [default: 50] [currently: 50]", "The number of items that will be added to dir(\u2026). \u2018None\u2019 value means unlimited. Because dir is cached, changing this option will not immediately affect already existing dataframes until a column is deleted or added.", "This is for instance used to suggest columns from a dataframe to tab completion. [default: 100] [currently: 100]", "max_info_columns is used in DataFrame.info method to decide if per column information will be printed. [default: 100] [currently: 100]", "df.info() will usually show null-counts for each column. For large frames this can be quite slow. max_info_rows and max_info_cols limit this null check only to frames with smaller dimensions than specified. [default: 1690785] [currently: 1690785]", "If max_rows is exceeded, switch to truncate view. Depending on large_repr, objects are either centrally truncated or printed as a summary view. \u2018None\u2019 value means unlimited.", "In case python/IPython is running in a terminal and large_repr equals \u2018truncate\u2019 this can be set to 0 and pandas will auto-detect the height of the terminal and print a truncated object which fits the screen height. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection. [default: 60] [currently: 60]", "When pretty-printing a long sequence, no more then max_seq_items will be printed. If items are omitted, they will be denoted by the addition of \u201c\u2026\u201d to the resulting string.", "If set to None, the number of items to be printed is unlimited. [default: 100] [currently: 100]", "This specifies if the memory usage of a DataFrame should be displayed when df.info() is called. Valid values True,False,\u2019deep\u2019 [default: True] [currently: True]", "The numbers of rows to show in a truncated view (when max_rows is exceeded). Ignored when max_rows is set to None or 0. When set to None, follows the value of max_rows. [default: 10] [currently: 10]", "\u201csparsify\u201d MultiIndex display (don\u2019t display repeated elements in outer levels within groups) [default: True] [currently: True]", "When True, IPython notebook will use html representation for pandas objects (if it is available). [default: True] [currently: True]", "Controls the number of nested levels to process when pretty-printing [default: 3] [currently: 3]", "Floating point output precision in terms of number of places after the decimal, for regular formatting as well as scientific notation. Similar to precision in numpy.set_printoptions(). [default: 6] [currently: 6]", "Whether to print out dimensions at the end of DataFrame repr. If \u2018truncate\u2019 is specified, only print out the dimensions if the frame is truncated (e.g. not display all rows and/or columns) [default: truncate] [currently: truncate]", "Whether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False]", "Whether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False]", "Width of the display in characters. In case python/IPython is running in a terminal this can be set to None and pandas will correctly auto-detect the width. Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to correctly detect the width. [default: 80] [currently: 80]", "The default Excel reader engine for \u2018ods\u2019 files. Available options: auto, odf. [default: auto] [currently: auto]", "The default Excel writer engine for \u2018ods\u2019 files. Available options: auto, odf. [default: auto] [currently: auto]", "The default Excel reader engine for \u2018xls\u2019 files. Available options: auto, xlrd. [default: auto] [currently: auto]", "The default Excel writer engine for \u2018xls\u2019 files. Available options: auto, xlwt. [default: auto] [currently: auto] (Deprecated, use `` instead.)", "The default Excel reader engine for \u2018xlsb\u2019 files. Available options: auto, pyxlsb. [default: auto] [currently: auto]", "The default Excel reader engine for \u2018xlsm\u2019 files. Available options: auto, xlrd, openpyxl. [default: auto] [currently: auto]", "The default Excel writer engine for \u2018xlsm\u2019 files. Available options: auto, openpyxl. [default: auto] [currently: auto]", "The default Excel reader engine for \u2018xlsx\u2019 files. Available options: auto, xlrd, openpyxl. [default: auto] [currently: auto]", "The default Excel writer engine for \u2018xlsx\u2019 files. Available options: auto, openpyxl, xlsxwriter. [default: auto] [currently: auto]", "default format writing format, if None, then put will default to \u2018fixed\u2019 and append will default to \u2018table\u2019 [default: None] [currently: None]", "drop ALL nan rows when appending to a table [default: False] [currently: False]", "The default parquet reader/writer engine. Available options: \u2018auto\u2019, \u2018pyarrow\u2019, \u2018fastparquet\u2019, the default is \u2018auto\u2019 [default: auto] [currently: auto]", "The default sql reader/writer engine. Available options: \u2018auto\u2019, \u2018sqlalchemy\u2019, the default is \u2018auto\u2019 [default: auto] [currently: auto]", "Raise an exception, warn, or no action if trying to use chained assignment, The default is warn [default: warn] [currently: warn]", "Internal data manager type; can be \u201cblock\u201d or \u201carray\u201d. Defaults to \u201cblock\u201d, unless overridden by the \u2018PANDAS_DATA_MANAGER\u2019 environment variable (needs to be set before pandas is imported). [default: block] [currently: block]", "Whether to simulate interactive mode for purposes of testing [default: False] [currently: False]", "The default storage for StringDtype. [default: python] [currently: python]", "True means treat None, NaN, INF, -INF as NA (old way), False means None and NaN are null, but INF, -INF are not NA (new way). [default: False] [currently: False]", "use_inf_as_null had been deprecated and will be removed in a future version. Use use_inf_as_na instead. [default: False] [currently: False] (Deprecated, use mode.use_inf_as_na instead.)", "The plotting backend to use. The default value is \u201cmatplotlib\u201d, the backend provided with pandas. Other backends can be specified by providing the name of the module that implements the backend. [default: matplotlib] [currently: matplotlib]", "Whether to register converters with matplotlib\u2019s units registry for dates, times, datetimes, and Periods. Toggling to False will remove the converters, restoring any converters that pandas overwrote. [default: auto] [currently: auto]", "The character representation for the decimal separator for floats and complex. [default: .] [currently: .]", "Whether to escape certain characters according to the given context; html or latex. [default: None] [currently: None]", "A formatter object to be used as default within Styler.format. [default: None] [currently: None]", "The string representation for values identified as missing. [default: None] [currently: None]", "The precision for floats and complex numbers. [default: 6] [currently: 6]", "The character representation for thousands separator for floats, int and complex. [default: None] [currently: None]", "If False will render special CSS classes to table attributes that indicate Mathjax will not be used in Jupyter Notebook. [default: True] [currently: True]", "The environment to replace \\begin{table}. If \u201clongtable\u201d is used results in a specific longtable environment format. [default: None] [currently: None]", "Whether to add horizontal rules on top and bottom and below the headers. [default: False] [currently: False]", "The specifier for horizontal alignment of sparsified LaTeX multicolumns. Pipe decorators can also be added to non-naive values to draw vertical rules, e.g. \u201c|r\u201d will draw a rule on the left side of right aligned merged cells. [default: r] [currently: r]", "The specifier for vertical alignment of sparsified LaTeX multirows. [default: c] [currently: c]", "The encoding used for output HTML and LaTeX files. [default: utf-8] [currently: utf-8]", "The maximum number of columns that will be rendered. May still be reduced to satsify max_elements, which takes precedence. [default: None] [currently: None]", "The maximum number of data-cell (<td>) elements that will be rendered before trimming will occur over columns, rows or both if needed. [default: 262144] [currently: 262144]", "The maximum number of rows that will be rendered. May still be reduced to satsify max_elements, which takes precedence. [default: None] [currently: None]", "Determine which output to use in Jupyter Notebook in {\u201chtml\u201d, \u201clatex\u201d}. [default: html] [currently: html]", "Whether to sparsify the display of hierarchical columns. Setting to False will display each explicit level element in a hierarchical key for each column. [default: True] [currently: True]", "Whether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. [default: True] [currently: True]"]}, {"name": "pandas.errors.AbstractMethodError", "path": "reference/api/pandas.errors.abstractmethoderror", "type": "General utility functions", "text": ["Raise this error instead of NotImplementedError for abstract methods while keeping compatibility with Python 2 and Python 3."]}, {"name": "pandas.errors.AccessorRegistrationWarning", "path": "reference/api/pandas.errors.accessorregistrationwarning", "type": "General utility functions", "text": ["Warning for attribute conflicts in accessor registration."]}, {"name": "pandas.errors.DtypeWarning", "path": "reference/api/pandas.errors.dtypewarning", "type": "General utility functions", "text": ["Warning raised when reading different dtypes in a column from a file.", "Raised for a dtype incompatibility. This can happen whenever read_csv or read_table encounter non-uniform dtypes in a column(s) of a given CSV file.", "See also", "Read CSV (comma-separated) file into a DataFrame.", "Read general delimited file into a DataFrame.", "Notes", "This warning is issued when dealing with larger files because the dtype checking happens per chunk read.", "Despite the warning, the CSV file is read with mixed types in a single column which will be an object type. See the examples below to better understand this issue.", "Examples", "This example creates and reads a large CSV file with a column that contains int and str.", "Important to notice that df2 will contain both str and int for the same input, \u20181\u2019.", "One way to solve this issue is using the dtype parameter in the read_csv and read_table functions to explicit the conversion:", "No warning was issued."]}, {"name": "pandas.errors.DuplicateLabelError", "path": "reference/api/pandas.errors.duplicatelabelerror", "type": "General utility functions", "text": ["Error raised when an operation would introduce duplicate labels.", "New in version 1.2.0.", "Examples"]}, {"name": "pandas.errors.EmptyDataError", "path": "reference/api/pandas.errors.emptydataerror", "type": "General utility functions", "text": ["Exception that is thrown in pd.read_csv (by both the C and Python engines) when empty data or header is encountered."]}, {"name": "pandas.errors.IntCastingNaNError", "path": "reference/api/pandas.errors.intcastingnanerror", "type": "General utility functions", "text": ["Raised when attempting an astype operation on an array with NaN to an integer dtype."]}, {"name": "pandas.errors.InvalidIndexError", "path": "reference/api/pandas.errors.invalidindexerror", "type": "General utility functions", "text": ["Exception raised when attempting to use an invalid index key.", "New in version 1.1.0."]}, {"name": "pandas.errors.MergeError", "path": "reference/api/pandas.errors.mergeerror", "type": "General utility functions", "text": ["Error raised when problems arise during merging due to problems with input data. Subclass of ValueError."]}, {"name": "pandas.errors.NullFrequencyError", "path": "reference/api/pandas.errors.nullfrequencyerror", "type": "General utility functions", "text": ["Error raised when a null freq attribute is used in an operation that needs a non-null frequency, particularly DatetimeIndex.shift, TimedeltaIndex.shift, PeriodIndex.shift."]}, {"name": "pandas.errors.NumbaUtilError", "path": "reference/api/pandas.errors.numbautilerror", "type": "General utility functions", "text": ["Error raised for unsupported Numba engine routines."]}, {"name": "pandas.errors.OptionError", "path": "reference/api/pandas.errors.optionerror", "type": "General utility functions", "text": ["Exception for pandas.options, backwards compatible with KeyError checks."]}, {"name": "pandas.errors.OutOfBoundsDatetime", "path": "reference/api/pandas.errors.outofboundsdatetime", "type": "General utility functions", "text": []}, {"name": "pandas.errors.OutOfBoundsTimedelta", "path": "reference/api/pandas.errors.outofboundstimedelta", "type": "General utility functions", "text": ["Raised when encountering a timedelta value that cannot be represented as a timedelta64[ns]."]}, {"name": "pandas.errors.ParserError", "path": "reference/api/pandas.errors.parsererror", "type": "General utility functions", "text": ["Exception that is raised by an error encountered in parsing file contents.", "This is a generic error raised for errors encountered when functions like read_csv or read_html are parsing contents of a file.", "See also", "Read CSV (comma-separated) file into a DataFrame.", "Read HTML table into a DataFrame."]}, {"name": "pandas.errors.ParserWarning", "path": "reference/api/pandas.errors.parserwarning", "type": "General utility functions", "text": ["Warning raised when reading a file that doesn\u2019t use the default \u2018c\u2019 parser.", "Raised by pd.read_csv and pd.read_table when it is necessary to change parsers, generally from the default \u2018c\u2019 parser to \u2018python\u2019.", "It happens due to a lack of support or functionality for parsing a particular attribute of a CSV file with the requested engine.", "Currently, \u2018c\u2019 unsupported options include the following parameters:", "sep other than a single character (e.g. regex separators)", "skipfooter higher than 0", "sep=None with delim_whitespace=False", "The warning can be avoided by adding engine=\u2019python\u2019 as a parameter in pd.read_csv and pd.read_table methods.", "See also", "Read CSV (comma-separated) file into DataFrame.", "Read general delimited file into DataFrame.", "Examples", "Using a sep in pd.read_csv other than a single character:", "Adding engine=\u2019python\u2019 to pd.read_csv removes the Warning:"]}, {"name": "pandas.errors.PerformanceWarning", "path": "reference/api/pandas.errors.performancewarning", "type": "General utility functions", "text": ["Warning raised when there is a possible performance impact."]}, {"name": "pandas.errors.UnsortedIndexError", "path": "reference/api/pandas.errors.unsortedindexerror", "type": "General utility functions", "text": ["Error raised when attempting to get a slice of a MultiIndex, and the index has not been lexsorted. Subclass of KeyError."]}, {"name": "pandas.errors.UnsupportedFunctionCall", "path": "reference/api/pandas.errors.unsupportedfunctioncall", "type": "General utility functions", "text": ["Exception raised when attempting to call a numpy function on a pandas object, but that function is not supported by the object e.g. np.cumsum(groupby_object)."]}, {"name": "pandas.eval", "path": "reference/api/pandas.eval", "type": "General functions", "text": ["Evaluate a Python expression as a string using various backends.", "The following arithmetic operations are supported: +, -, *, /, **, %, // (python engine only) along with the following boolean operations: | (or), & (and), and ~ (not). Additionally, the 'pandas' parser allows the use of and, or, and not with the same semantics as the corresponding bitwise operators. Series and DataFrame objects are supported and behave as they would with plain ol\u2019 Python evaluation.", "The expression to evaluate. This string cannot contain any Python statements, only Python expressions.", "The parser to use to construct the syntax tree from the expression. The default of 'pandas' parses code slightly different than standard Python. Alternatively, you can parse an expression using the 'python' parser to retain strict Python semantics. See the enhancing performance documentation for more details.", "The engine used to evaluate the expression. Supported engines are", "None : tries to use numexpr, falls back to python", "numexpr for large speed ups in complex expressions with large frames.", "level python. This engine is generally not that useful.", "More backends may be available in the future.", "Whether to use true division, like in Python >= 3.", "Deprecated since version 1.0.0.", "A dictionary of local variables, taken from locals() by default.", "A dictionary of global variables, taken from globals() by default.", "A list of objects implementing the __getitem__ special method that you can use to inject an additional collection of namespaces to use for variable lookup. For example, this is used in the query() method to inject the DataFrame.index and DataFrame.columns variables that refer to their respective DataFrame instance attributes.", "The number of prior stack frames to traverse and add to the current scope. Most users will not need to change this parameter.", "This is the target object for assignment. It is used when there is variable assignment in the expression. If so, then target must support item assignment with string keys, and if a copy is being returned, it must also support .copy().", "If target is provided, and the expression mutates target, whether to modify target inplace. Otherwise, return a copy of target with the mutation.", "The completion value of evaluating the given code or None if inplace=True.", "There are many instances where such an error can be raised:", "target=None, but the expression is multiline.", "The expression is multiline, but not all them have item assignment. An example of such an arrangement is this:", "a = b + 1 a + 2", "Here, there are expressions on different lines, making it multiline, but the last line has no variable assigned to the output of a + 2.", "inplace=True, but the expression is missing item assignment.", "Item assignment is provided, but the target does not support string item assignment.", "Item assignment is provided and inplace=False, but the target does not support the .copy() method", "See also", "Evaluates a boolean expression to query the columns of a frame.", "Evaluate a string describing operations on DataFrame columns.", "Notes", "The dtype of any objects involved in an arithmetic % operation are recursively cast to float64.", "See the enhancing performance documentation for more details.", "Examples", "We can add a new column using pd.eval:"]}, {"name": "pandas.ExcelFile.parse", "path": "reference/api/pandas.excelfile.parse", "type": "General functions", "text": ["Parse specified sheet(s) into a DataFrame.", "Equivalent to read_excel(ExcelFile, \u2026) See the read_excel docstring for more info on accepted parameters.", "DataFrame from the passed in Excel file."]}, {"name": "pandas.ExcelWriter", "path": "reference/api/pandas.excelwriter", "type": "General functions", "text": ["Class for writing DataFrame objects into excel sheets.", "Default is to use : * xlwt for xls * xlsxwriter for xlsx if xlsxwriter is installed otherwise openpyxl * odf for ods. See DataFrame.to_excel for typical usage.", "The writer should be used as a context manager. Otherwise, call close() to save and close any opened file handles.", "Path to xls or xlsx or ods file.", "Engine to use for writing. If None, defaults to io.excel.<extension>.writer. NOTE: can only be passed as a keyword argument.", "Deprecated since version 1.2.0: As the xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas.", "Format string for dates written into Excel files (e.g. \u2018YYYY-MM-DD\u2019).", "Format string for datetime objects written into Excel files. (e.g. \u2018YYYY-MM-DD HH:MM:SS\u2019).", "File mode to use (write or append). Append does not work with fsspec URLs.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc., if using a URL that will be parsed by fsspec, e.g., starting \u201cs3://\u201d, \u201cgcs://\u201d.", "New in version 1.2.0.", "How to behave when trying to write to a sheet that already exists (append mode only).", "error: raise a ValueError.", "new: Create a new sheet, with a name determined by the engine.", "replace: Delete the contents of the sheet before writing to it.", "overlay: Write contents to the existing sheet without removing the old contents.", "New in version 1.3.0.", "Changed in version 1.4.0: Added overlay option", "Keyword arguments to be passed into the engine. These will be passed to the following functions of the respective engines:", "xlsxwriter: xlsxwriter.Workbook(file, **engine_kwargs)", "openpyxl (write mode): openpyxl.Workbook(**engine_kwargs)", "openpyxl (append mode): openpyxl.load_workbook(file, **engine_kwargs)", "odswriter: odf.opendocument.OpenDocumentSpreadsheet(**engine_kwargs)", "New in version 1.3.0.", "Keyword arguments to be passed into the engine.", "Deprecated since version 1.3.0: Use engine_kwargs instead.", "Notes", "None of the methods and properties are considered public.", "For compatibility with CSV writers, ExcelWriter serializes lists and dicts to strings before writing.", "Examples", "Default usage:", "To write to separate sheets in a single file:", "You can set the date format or datetime format:", "You can also append to an existing Excel file:", "Here, the if_sheet_exists parameter can be set to replace a sheet if it already exists:", "You can also write multiple DataFrames to a single sheet. Note that the if_sheet_exists parameter needs to be set to overlay:", "You can store Excel file in RAM:", "You can pack Excel file into zip archive:", "You can specify additional arguments to the underlying engine:", "In append mode, engine_kwargs are passed through to openpyxl\u2019s load_workbook:", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.factorize", "path": "reference/api/pandas.factorize", "type": "General functions", "text": ["Encode the object as an enumerated type or categorical variable.", "This method is useful for obtaining a numeric representation of an array when all that matters is identifying distinct values. factorize is available as both a top-level function pandas.factorize(), and as a method Series.factorize() and Index.factorize().", "A 1-D sequence. Sequences that aren\u2019t pandas objects are coerced to ndarrays before factorization.", "Sort uniques and shuffle codes to maintain the relationship.", "Value to mark \u201cnot found\u201d. If None, will not drop the NaN from the uniques of the values.", "Changed in version 1.1.2.", "Hint to the hashtable sizer.", "An integer ndarray that\u2019s an indexer into uniques. uniques.take(codes) will have the same values as values.", "The unique valid values. When values is Categorical, uniques is a Categorical. When values is some other pandas object, an Index is returned. Otherwise, a 1-D ndarray is returned.", "Note", "Even if there\u2019s a missing value in values, uniques will not contain an entry for it.", "See also", "Discretize continuous-valued array.", "Find the unique value in an array.", "Examples", "These examples all show factorize as a top-level method like pd.factorize(values). The results are identical for methods like Series.factorize().", "With sort=True, the uniques will be sorted, and codes will be shuffled so that the relationship is the maintained.", "Missing values are indicated in codes with na_sentinel (-1 by default). Note that missing values are never included in uniques.", "Thus far, we\u2019ve only factorized lists (which are internally coerced to NumPy arrays). When factorizing pandas objects, the type of uniques will differ. For Categoricals, a Categorical is returned.", "Notice that 'b' is in uniques.categories, despite not being present in cat.values.", "For all other pandas objects, an Index of the appropriate type is returned.", "If NaN is in the values, and we want to include NaN in the uniques of the values, it can be achieved by setting na_sentinel=None."]}, {"name": "pandas.Flags", "path": "reference/api/pandas.flags", "type": "General functions", "text": ["Flags that apply to pandas objects.", "New in version 1.2.0.", "The object these flags are associated with.", "Whether to allow duplicate labels in this object. By default, duplicate labels are permitted. Setting this to False will cause an errors.DuplicateLabelError to be raised when index (or columns for DataFrame) is not unique, or any subsequent operation on introduces duplicates. See Disallowing Duplicate Labels for more.", "Warning", "This is an experimental feature. Currently, many methods fail to propagate the allows_duplicate_labels value. In future versions it is expected that every method taking or returning one or more DataFrame or Series objects will propagate allows_duplicate_labels.", "Notes", "Attributes can be set in two ways", "Attributes", "allows_duplicate_labels", "Whether this object allows duplicate labels."]}, {"name": "pandas.Flags.allows_duplicate_labels", "path": "reference/api/pandas.flags.allows_duplicate_labels", "type": "General functions", "text": ["Whether this object allows duplicate labels.", "Setting allows_duplicate_labels=False ensures that the index (and columns of a DataFrame) are unique. Most methods that accept and return a Series or DataFrame will propagate the value of allows_duplicate_labels.", "See Duplicate Labels for more.", "See also", "Set global metadata on this object.", "Set global flags on this object.", "Examples"]}, {"name": "pandas.Float64Index", "path": "reference/api/pandas.float64index", "type": "Index Objects", "text": ["Immutable sequence used for indexing and alignment. The basic object storing axis labels for all pandas objects. Float64Index is a special case of Index with purely float labels. .", "Deprecated since version 1.4.0: In pandas v2.0 Float64Index will be removed and NumericIndex used instead. Float64Index will remain fully functional for the duration of pandas 1.x.", "Make a copy of input ndarray.", "Name to be stored in the index.", "See also", "The base pandas Index type.", "Index of numpy int/uint/float data.", "Notes", "An Index instance can only contain hashable objects.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.get_dummies", "path": "reference/api/pandas.get_dummies", "type": "General functions", "text": ["Convert categorical variable into dummy/indicator variables.", "Data of which to get dummy indicators.", "String to append DataFrame column names. Pass a list with length equal to the number of columns when calling get_dummies on a DataFrame. Alternatively, prefix can be a dictionary mapping column names to prefixes.", "If appending prefix, separator/delimiter to use. Or pass a list or dictionary as with prefix.", "Add a column to indicate NaNs, if False NaNs are ignored.", "Column names in the DataFrame to be encoded. If columns is None then all the columns with object or category dtype will be converted.", "Whether the dummy-encoded columns should be backed by a SparseArray (True) or a regular NumPy array (False).", "Whether to get k-1 dummies out of k categorical levels by removing the first level.", "Data type for new columns. Only a single dtype is allowed.", "Dummy-coded data.", "See also", "Convert Series to dummy codes.", "Examples"]}, {"name": "pandas.get_option", "path": "reference/api/pandas.get_option", "type": "General utility functions", "text": ["Retrieves the value of the specified option.", "Available options:", "compute.[use_bottleneck, use_numba, use_numexpr]", "display.[chop_threshold, colheader_justify, column_space, date_dayfirst, date_yearfirst, encoding, expand_frame_repr, float_format]", "display.html.[border, table_schema, use_mathjax]", "display.[large_repr]", "display.latex.[escape, longtable, multicolumn, multicolumn_format, multirow, repr]", "display.[max_categories, max_columns, max_colwidth, max_dir_items, max_info_columns, max_info_rows, max_rows, max_seq_items, memory_usage, min_rows, multi_sparse, notebook_repr_html, pprint_nest_depth, precision, show_dimensions]", "display.unicode.[ambiguous_as_wide, east_asian_width]", "display.[width]", "io.excel.ods.[reader, writer]", "io.excel.xls.[reader, writer]", "io.excel.xlsb.[reader]", "io.excel.xlsm.[reader, writer]", "io.excel.xlsx.[reader, writer]", "io.hdf.[default_format, dropna_table]", "io.parquet.[engine]", "io.sql.[engine]", "mode.[chained_assignment, data_manager, sim_interactive, string_storage, use_inf_as_na, use_inf_as_null]", "plotting.[backend]", "plotting.matplotlib.[register_converters]", "styler.format.[decimal, escape, formatter, na_rep, precision, thousands]", "styler.html.[mathjax]", "styler.latex.[environment, hrules, multicol_align, multirow_align]", "styler.render.[encoding, max_columns, max_elements, max_rows, repr]", "styler.sparse.[columns, index]", "Regexp which should match a single option. Note: partial matches are supported for convenience, but unless you use the full option name (e.g. x.y.z.option_name), your code may break in future versions if new options with similar names are introduced.", "Notes", "The available options with its descriptions:", "Use the bottleneck library to accelerate if it is installed, the default is True Valid values: False,True [default: True] [currently: True]", "Use the numba engine option for select operations if it is installed, the default is False Valid values: False,True [default: False] [currently: False]", "Use the numexpr library to accelerate computation if it is installed, the default is True Valid values: False,True [default: True] [currently: True]", "if set to a float value, all float values smaller then the given threshold will be displayed as exactly 0 by repr and friends. [default: None] [currently: None]", "Controls the justification of column headers. used by DataFrameFormatter. [default: right] [currently: right]", "[default: 12] [currently: 12]", "When True, prints and parses dates with the day first, eg 20/01/2005 [default: False] [currently: False]", "When True, prints and parses dates with the year first, eg 2005/01/20 [default: False] [currently: False]", "Defaults to the detected encoding of the console. Specifies the encoding to be used for strings returned by to_string, these are generally strings meant to be displayed on the console. [default: utf-8] [currently: utf-8]", "Whether to print out the full DataFrame repr for wide DataFrames across multiple lines, max_columns is still respected, but the output will wrap-around across multiple \u201cpages\u201d if its width exceeds display.width. [default: True] [currently: True]", "The callable should accept a floating point number and return a string with the desired format of the number. This is used in some places like SeriesFormatter. See formats.format.EngFormatter for an example. [default: None] [currently: None]", "A border=value attribute is inserted in the <table> tag for the DataFrame HTML repr. [default: 1] [currently: 1]", "Whether to publish a Table Schema representation for frontends that support it. (default: False) [default: False] [currently: False]", "When True, Jupyter notebook will process table contents using MathJax, rendering mathematical expressions enclosed by the dollar symbol. (default: True) [default: True] [currently: True]", "For DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show a truncated table (the default from 0.13), or switch to the view from df.info() (the behaviour in earlier versions of pandas). [default: truncate] [currently: truncate]", "This specifies if the to_latex method of a Dataframe uses escapes special characters. Valid values: False,True [default: True] [currently: True]", "This specifies if the to_latex method of a Dataframe uses the longtable format. Valid values: False,True [default: False] [currently: False]", "This specifies if the to_latex method of a Dataframe uses multicolumns to pretty-print MultiIndex columns. Valid values: False,True [default: True] [currently: True]", "This specifies if the to_latex method of a Dataframe uses multicolumns to pretty-print MultiIndex columns. Valid values: False,True [default: l] [currently: l]", "This specifies if the to_latex method of a Dataframe uses multirows to pretty-print MultiIndex rows. Valid values: False,True [default: False] [currently: False]", "Whether to produce a latex DataFrame representation for jupyter environments that support it. (default: False) [default: False] [currently: False]", "This sets the maximum number of categories pandas should output when printing out a Categorical or a Series of dtype \u201ccategory\u201d. [default: 8] [currently: 8]", "If max_cols is exceeded, switch to truncate view. Depending on large_repr, objects are either centrally truncated or printed as a summary view. \u2018None\u2019 value means unlimited.", "In case python/IPython is running in a terminal and large_repr equals \u2018truncate\u2019 this can be set to 0 and pandas will auto-detect the width of the terminal and print a truncated object which fits the screen width. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection. [default: 0] [currently: 0]", "The maximum width in characters of a column in the repr of a pandas data structure. When the column overflows, a \u201c\u2026\u201d placeholder is embedded in the output. A \u2018None\u2019 value means unlimited. [default: 50] [currently: 50]", "The number of items that will be added to dir(\u2026). \u2018None\u2019 value means unlimited. Because dir is cached, changing this option will not immediately affect already existing dataframes until a column is deleted or added.", "This is for instance used to suggest columns from a dataframe to tab completion. [default: 100] [currently: 100]", "max_info_columns is used in DataFrame.info method to decide if per column information will be printed. [default: 100] [currently: 100]", "df.info() will usually show null-counts for each column. For large frames this can be quite slow. max_info_rows and max_info_cols limit this null check only to frames with smaller dimensions than specified. [default: 1690785] [currently: 1690785]", "If max_rows is exceeded, switch to truncate view. Depending on large_repr, objects are either centrally truncated or printed as a summary view. \u2018None\u2019 value means unlimited.", "In case python/IPython is running in a terminal and large_repr equals \u2018truncate\u2019 this can be set to 0 and pandas will auto-detect the height of the terminal and print a truncated object which fits the screen height. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection. [default: 60] [currently: 60]", "When pretty-printing a long sequence, no more then max_seq_items will be printed. If items are omitted, they will be denoted by the addition of \u201c\u2026\u201d to the resulting string.", "If set to None, the number of items to be printed is unlimited. [default: 100] [currently: 100]", "This specifies if the memory usage of a DataFrame should be displayed when df.info() is called. Valid values True,False,\u2019deep\u2019 [default: True] [currently: True]", "The numbers of rows to show in a truncated view (when max_rows is exceeded). Ignored when max_rows is set to None or 0. When set to None, follows the value of max_rows. [default: 10] [currently: 10]", "\u201csparsify\u201d MultiIndex display (don\u2019t display repeated elements in outer levels within groups) [default: True] [currently: True]", "When True, IPython notebook will use html representation for pandas objects (if it is available). [default: True] [currently: True]", "Controls the number of nested levels to process when pretty-printing [default: 3] [currently: 3]", "Floating point output precision in terms of number of places after the decimal, for regular formatting as well as scientific notation. Similar to precision in numpy.set_printoptions(). [default: 6] [currently: 6]", "Whether to print out dimensions at the end of DataFrame repr. If \u2018truncate\u2019 is specified, only print out the dimensions if the frame is truncated (e.g. not display all rows and/or columns) [default: truncate] [currently: truncate]", "Whether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False]", "Whether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False]", "Width of the display in characters. In case python/IPython is running in a terminal this can be set to None and pandas will correctly auto-detect the width. Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to correctly detect the width. [default: 80] [currently: 80]", "The default Excel reader engine for \u2018ods\u2019 files. Available options: auto, odf. [default: auto] [currently: auto]", "The default Excel writer engine for \u2018ods\u2019 files. Available options: auto, odf. [default: auto] [currently: auto]", "The default Excel reader engine for \u2018xls\u2019 files. Available options: auto, xlrd. [default: auto] [currently: auto]", "The default Excel writer engine for \u2018xls\u2019 files. Available options: auto, xlwt. [default: auto] [currently: auto] (Deprecated, use `` instead.)", "The default Excel reader engine for \u2018xlsb\u2019 files. Available options: auto, pyxlsb. [default: auto] [currently: auto]", "The default Excel reader engine for \u2018xlsm\u2019 files. Available options: auto, xlrd, openpyxl. [default: auto] [currently: auto]", "The default Excel writer engine for \u2018xlsm\u2019 files. Available options: auto, openpyxl. [default: auto] [currently: auto]", "The default Excel reader engine for \u2018xlsx\u2019 files. Available options: auto, xlrd, openpyxl. [default: auto] [currently: auto]", "The default Excel writer engine for \u2018xlsx\u2019 files. Available options: auto, openpyxl, xlsxwriter. [default: auto] [currently: auto]", "default format writing format, if None, then put will default to \u2018fixed\u2019 and append will default to \u2018table\u2019 [default: None] [currently: None]", "drop ALL nan rows when appending to a table [default: False] [currently: False]", "The default parquet reader/writer engine. Available options: \u2018auto\u2019, \u2018pyarrow\u2019, \u2018fastparquet\u2019, the default is \u2018auto\u2019 [default: auto] [currently: auto]", "The default sql reader/writer engine. Available options: \u2018auto\u2019, \u2018sqlalchemy\u2019, the default is \u2018auto\u2019 [default: auto] [currently: auto]", "Raise an exception, warn, or no action if trying to use chained assignment, The default is warn [default: warn] [currently: warn]", "Internal data manager type; can be \u201cblock\u201d or \u201carray\u201d. Defaults to \u201cblock\u201d, unless overridden by the \u2018PANDAS_DATA_MANAGER\u2019 environment variable (needs to be set before pandas is imported). [default: block] [currently: block]", "Whether to simulate interactive mode for purposes of testing [default: False] [currently: False]", "The default storage for StringDtype. [default: python] [currently: python]", "True means treat None, NaN, INF, -INF as NA (old way), False means None and NaN are null, but INF, -INF are not NA (new way). [default: False] [currently: False]", "use_inf_as_null had been deprecated and will be removed in a future version. Use use_inf_as_na instead. [default: False] [currently: False] (Deprecated, use mode.use_inf_as_na instead.)", "The plotting backend to use. The default value is \u201cmatplotlib\u201d, the backend provided with pandas. Other backends can be specified by providing the name of the module that implements the backend. [default: matplotlib] [currently: matplotlib]", "Whether to register converters with matplotlib\u2019s units registry for dates, times, datetimes, and Periods. Toggling to False will remove the converters, restoring any converters that pandas overwrote. [default: auto] [currently: auto]", "The character representation for the decimal separator for floats and complex. [default: .] [currently: .]", "Whether to escape certain characters according to the given context; html or latex. [default: None] [currently: None]", "A formatter object to be used as default within Styler.format. [default: None] [currently: None]", "The string representation for values identified as missing. [default: None] [currently: None]", "The precision for floats and complex numbers. [default: 6] [currently: 6]", "The character representation for thousands separator for floats, int and complex. [default: None] [currently: None]", "If False will render special CSS classes to table attributes that indicate Mathjax will not be used in Jupyter Notebook. [default: True] [currently: True]", "The environment to replace \\begin{table}. If \u201clongtable\u201d is used results in a specific longtable environment format. [default: None] [currently: None]", "Whether to add horizontal rules on top and bottom and below the headers. [default: False] [currently: False]", "The specifier for horizontal alignment of sparsified LaTeX multicolumns. Pipe decorators can also be added to non-naive values to draw vertical rules, e.g. \u201c|r\u201d will draw a rule on the left side of right aligned merged cells. [default: r] [currently: r]", "The specifier for vertical alignment of sparsified LaTeX multirows. [default: c] [currently: c]", "The encoding used for output HTML and LaTeX files. [default: utf-8] [currently: utf-8]", "The maximum number of columns that will be rendered. May still be reduced to satsify max_elements, which takes precedence. [default: None] [currently: None]", "The maximum number of data-cell (<td>) elements that will be rendered before trimming will occur over columns, rows or both if needed. [default: 262144] [currently: 262144]", "The maximum number of rows that will be rendered. May still be reduced to satsify max_elements, which takes precedence. [default: None] [currently: None]", "Determine which output to use in Jupyter Notebook in {\u201chtml\u201d, \u201clatex\u201d}. [default: html] [currently: html]", "Whether to sparsify the display of hierarchical columns. Setting to False will display each explicit level element in a hierarchical key for each column. [default: True] [currently: True]", "Whether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. [default: True] [currently: True]"]}, {"name": "pandas.Grouper", "path": "reference/api/pandas.grouper", "type": "GroupBy", "text": ["A Grouper allows the user to specify a groupby instruction for an object.", "This specification will select a column via the key parameter, or if the level and/or axis parameters are given, a level of the index of the target object.", "If axis and/or level are passed as keywords to both Grouper and groupby, the values passed to Grouper take precedence.", "Groupby key, which selects the grouping column of the target.", "The level for the target index.", "This will groupby the specified frequency if the target selection (via key or level) is a datetime-like object. For full specification of available frequencies, please see here.", "Number/name of the axis.", "Whether to sort the resulting labels.", "Closed end of interval. Only when freq parameter is passed.", "Interval boundary to use for labeling. Only when freq parameter is passed.", "If grouper is PeriodIndex and freq parameter is passed.", "Only when freq parameter is passed. For frequencies that evenly subdivide 1 day, the \u201corigin\u201d of the aggregated intervals. For example, for \u20185min\u2019 frequency, base could range from 0 through 4. Defaults to 0.", "Deprecated since version 1.1.0: The new arguments that you should use are \u2018offset\u2019 or \u2018origin\u2019.", "Only when freq parameter is passed.", "Deprecated since version 1.1.0: loffset is only working for .resample(...) and not for Grouper (GH28302). However, loffset is also deprecated for .resample(...) See: DataFrame.resample", "The timestamp on which to adjust the grouping. The timezone of origin must match the timezone of the index. If string, must be one of the following:", "\u2018epoch\u2019: origin is 1970-01-01", "\u2018start\u2019: origin is the first value of the timeseries", "\u2018start_day\u2019: origin is the first day at midnight of the timeseries", "New in version 1.1.0.", "\u2018end\u2019: origin is the last value of the timeseries", "\u2018end_day\u2019: origin is the ceiling midnight of the last day", "New in version 1.3.0.", "An offset timedelta added to the origin.", "New in version 1.1.0.", "If True, and if group keys contain NA values, NA values together with row/column will be dropped. If False, NA values will also be treated as the key in groups.", "New in version 1.2.0.", "Examples", "Syntactic sugar for df.groupby('A')", "Specify a resample operation on the column \u2018Publish date\u2019", "If you want to adjust the start of the bins based on a fixed timestamp:", "If you want to adjust the start of the bins with an offset Timedelta, the two following lines are equivalent:", "To replace the use of the deprecated base argument, you can now use offset, in this example it is equivalent to have base=2:", "Attributes", "ax", "groups"]}, {"name": "pandas.HDFStore.append", "path": "reference/api/pandas.hdfstore.append", "type": "Input/output", "text": ["Append to Table in file. Node must already exist and be Table format.", "Format to use when storing object in HDFStore. Value can be one of:", "Table format. Write as a PyTables Table structure which may perform worse but allow more flexible operations like searching / selecting subsets of the data.", "Append the input data to the existing.", "List of columns to create as indexed data columns for on-disk queries, or True to use all columns. By default only the axes of the object are indexed. See here.", "Do not write an ALL nan row to the store settable by the option \u2018io.hdf.dropna_table\u2019.", "Notes", "Does not check if data being appended overlaps with existing data in the table, so be careful"]}, {"name": "pandas.HDFStore.get", "path": "reference/api/pandas.hdfstore.get", "type": "Input/output", "text": ["Retrieve pandas object stored in file.", "Same type as object stored in file."]}, {"name": "pandas.HDFStore.groups", "path": "reference/api/pandas.hdfstore.groups", "type": "Input/output", "text": ["Return a list of all the top-level nodes.", "Each node returned is not a pandas storage object.", "List of objects."]}, {"name": "pandas.HDFStore.info", "path": "reference/api/pandas.hdfstore.info", "type": "Input/output", "text": ["Print detailed information on the store."]}, {"name": "pandas.HDFStore.keys", "path": "reference/api/pandas.hdfstore.keys", "type": "Input/output", "text": ["Return a list of keys corresponding to objects stored in HDFStore.", "When kind equals \u2018pandas\u2019 return pandas objects. When kind equals \u2018native\u2019 return native HDF5 Table objects.", "New in version 1.1.0.", "List of ABSOLUTE path-names (e.g. have the leading \u2018/\u2019)."]}, {"name": "pandas.HDFStore.put", "path": "reference/api/pandas.hdfstore.put", "type": "Input/output", "text": ["Store object in HDFStore.", "Format to use when storing object in HDFStore. Value can be one of:", "Fixed format. Fast writing/reading. Not-appendable, nor searchable.", "Table format. Write as a PyTables Table structure which may perform worse but allow more flexible operations like searching / selecting subsets of the data.", "This will force Table format, append the input data to the existing.", "List of columns to create as data columns, or True to use all columns. See here.", "Provide an encoding for strings.", "Parameter is propagated to \u2018create_table\u2019 method of \u2018PyTables\u2019. If set to False it enables to have the same h5 files (same hashes) independent on creation time.", "New in version 1.1.0."]}, {"name": "pandas.HDFStore.select", "path": "reference/api/pandas.hdfstore.select", "type": "Input/output", "text": ["Retrieve pandas object stored in file, optionally based on where criteria.", "Warning", "Pandas uses PyTables for reading and writing HDF5 files, which allows serializing object-dtype data with pickle when using the \u201cfixed\u201d format. Loading pickled data received from untrusted sources can be unsafe.", "See: https://docs.python.org/3/library/pickle.html for more.", "Object being retrieved from file.", "List of Term (or convertible) objects, optional.", "Row number to start selection.", "Row number to stop selection.", "A list of columns that if not None, will limit the return columns.", "Returns an iterator.", "Number or rows to include in iteration, return an iterator.", "Should automatically close the store when finished.", "Retrieved object from file."]}, {"name": "pandas.HDFStore.walk", "path": "reference/api/pandas.hdfstore.walk", "type": "Input/output", "text": ["Walk the pytables group hierarchy for pandas objects.", "This generator will yield the group path, subgroups and pandas object names for each group.", "Any non-pandas PyTables objects that are not a group will be ignored.", "The where group itself is listed first (preorder), then each of its child groups (following an alphanumerical order) is also traversed, following the same procedure.", "Group where to start walking.", "Full path to a group (without trailing \u2018/\u2019).", "Names (strings) of the groups contained in path.", "Names (strings) of the pandas objects contained in path."]}, {"name": "pandas.Index", "path": "reference/api/pandas.index", "type": "Index Objects", "text": ["Immutable sequence used for indexing and alignment. The basic object storing axis labels for all pandas objects.", "If dtype is None, we find the dtype that best fits the data. If an actual dtype is provided, we coerce to that dtype if it\u2019s safe. Otherwise, an error will be raised.", "Make a copy of input ndarray.", "Name to be stored in the index.", "When True, attempt to create a MultiIndex if possible.", "See also", "Index implementing a monotonic integer range.", "Index of Categorical s.", "A multi-level, or hierarchical Index.", "An Index of Interval s.", "Index of datetime64 data.", "Index of timedelta64 data.", "Index of Period data.", "Index of numpy int/uint/float data.", "Index of purely int64 labels (deprecated).", "Index of purely uint64 labels (deprecated).", "Index of purely float64 labels (deprecated).", "Notes", "An Index instance can only contain hashable objects", "Examples", "Attributes", "T", "Return the transpose, which is by definition self.", "array", "The ExtensionArray of the data backing this Series or Index.", "asi8", "Integer representation of the values.", "dtype", "Return the dtype object of the underlying data.", "has_duplicates", "Check if the Index has duplicate values.", "hasnans", "Return True if there are any NaNs.", "inferred_type", "Return a string of the type inferred from the values.", "is_all_dates", "Whether or not the index values only consist of dates.", "is_monotonic", "Alias for is_monotonic_increasing.", "is_monotonic_decreasing", "Return if the index is monotonic decreasing (only equal or decreasing) values.", "is_monotonic_increasing", "Return if the index is monotonic increasing (only equal or increasing) values.", "is_unique", "Return if the index has unique values.", "name", "Return Index or MultiIndex name.", "nbytes", "Return the number of bytes in the underlying data.", "ndim", "Number of dimensions of the underlying data, by definition 1.", "nlevels", "Number of levels.", "shape", "Return a tuple of the shape of the underlying data.", "size", "Return the number of elements in the underlying data.", "values", "Return an array representing the data in the Index.", "empty", "names", "Methods", "all(*args, **kwargs)", "Return whether all elements are Truthy.", "any(*args, **kwargs)", "Return whether any element is Truthy.", "append(other)", "Append a collection of Index options together.", "argmax([axis, skipna])", "Return int position of the largest value in the Series.", "argmin([axis, skipna])", "Return int position of the smallest value in the Series.", "argsort(*args, **kwargs)", "Return the integer indices that would sort the index.", "asof(label)", "Return the label from the index, or, if not present, the previous one.", "asof_locs(where, mask)", "Return the locations (indices) of labels in the index.", "astype(dtype[, copy])", "Create an Index with values cast to dtypes.", "copy([name, deep, dtype, names])", "Make a copy of this object.", "delete(loc)", "Make new Index with passed location(-s) deleted.", "difference(other[, sort])", "Return a new Index with elements of index not in other.", "drop(labels[, errors])", "Make new Index with passed list of labels deleted.", "drop_duplicates([keep])", "Return Index with duplicate values removed.", "droplevel([level])", "Return index with requested level(s) removed.", "dropna([how])", "Return Index without NA/NaN values.", "duplicated([keep])", "Indicate duplicate index values.", "equals(other)", "Determine if two Index object are equal.", "factorize([sort, na_sentinel])", "Encode the object as an enumerated type or categorical variable.", "fillna([value, downcast])", "Fill NA/NaN values with the specified value.", "format([name, formatter, na_rep])", "Render a string representation of the Index.", "get_indexer(target[, method, limit, tolerance])", "Compute indexer and mask for new index given the current index.", "get_indexer_for(target)", "Guaranteed return of an indexer even when non-unique.", "get_indexer_non_unique(target)", "Compute indexer and mask for new index given the current index.", "get_level_values(level)", "Return an Index of values for requested level.", "get_loc(key[, method, tolerance])", "Get integer location, slice or boolean mask for requested label.", "get_slice_bound(label, side[, kind])", "Calculate slice bound that corresponds to given label.", "get_value(series, key)", "Fast lookup of value from 1-dimensional ndarray.", "groupby(values)", "Group the index labels by a given array of values.", "holds_integer()", "Whether the type is an integer type.", "identical(other)", "Similar to equals, but checks that object attributes and types are also equal.", "insert(loc, item)", "Make new Index inserting new item at location.", "intersection(other[, sort])", "Form the intersection of two Index objects.", "is_(other)", "More flexible, faster check like is but that works through views.", "is_boolean()", "Check if the Index only consists of booleans.", "is_categorical()", "Check if the Index holds categorical data.", "is_floating()", "Check if the Index is a floating type.", "is_integer()", "Check if the Index only consists of integers.", "is_interval()", "Check if the Index holds Interval objects.", "is_mixed()", "Check if the Index holds data with mixed data types.", "is_numeric()", "Check if the Index only consists of numeric data.", "is_object()", "Check if the Index is of the object dtype.", "is_type_compatible(kind)", "Whether the index type is compatible with the provided type.", "isin(values[, level])", "Return a boolean array where the index values are in values.", "isna()", "Detect missing values.", "isnull()", "Detect missing values.", "item()", "Return the first element of the underlying data as a Python scalar.", "join(other[, how, level, return_indexers, sort])", "Compute join_index and indexers to conform data structures to the new index.", "map(mapper[, na_action])", "Map values using an input mapping or function.", "max([axis, skipna])", "Return the maximum value of the Index.", "memory_usage([deep])", "Memory usage of the values.", "min([axis, skipna])", "Return the minimum value of the Index.", "notna()", "Detect existing (non-missing) values.", "notnull()", "Detect existing (non-missing) values.", "nunique([dropna])", "Return number of unique elements in the object.", "putmask(mask, value)", "Return a new Index of the values set with the mask.", "ravel([order])", "Return an ndarray of the flattened values of the underlying data.", "reindex(target[, method, level, limit, ...])", "Create index with target's values.", "rename(name[, inplace])", "Alter Index or MultiIndex name.", "repeat(repeats[, axis])", "Repeat elements of a Index.", "searchsorted(value[, side, sorter])", "Find indices where elements should be inserted to maintain order.", "set_names(names[, level, inplace])", "Set Index or MultiIndex name.", "set_value(arr, key, value)", "(DEPRECATED) Fast lookup of value from 1-dimensional ndarray.", "shift([periods, freq])", "Shift index by desired number of time frequency increments.", "slice_indexer([start, end, step, kind])", "Compute the slice indexer for input labels and step.", "slice_locs([start, end, step, kind])", "Compute slice locations for input labels.", "sort(*args, **kwargs)", "Use sort_values instead.", "sort_values([return_indexer, ascending, ...])", "Return a sorted copy of the index.", "sortlevel([level, ascending, sort_remaining])", "For internal compatibility with the Index API.", "str", "alias of pandas.core.strings.accessor.StringMethods", "symmetric_difference(other[, result_name, sort])", "Compute the symmetric difference of two Index objects.", "take(indices[, axis, allow_fill, fill_value])", "Return a new Index of the values selected by the indices.", "to_flat_index()", "Identity method.", "to_frame([index, name])", "Create a DataFrame with a column containing the Index.", "to_list()", "Return a list of the values.", "to_native_types([slicer])", "(DEPRECATED) Format specified values of self and return them.", "to_numpy([dtype, copy, na_value])", "A NumPy ndarray representing the values in this Series or Index.", "to_series([index, name])", "Create a Series with both index and values equal to the index keys.", "tolist()", "Return a list of the values.", "transpose(*args, **kwargs)", "Return the transpose, which is by definition self.", "union(other[, sort])", "Form the union of two Index objects.", "unique([level])", "Return unique values in the index.", "value_counts([normalize, sort, ascending, ...])", "Return a Series containing counts of unique values.", "where(cond[, other])", "Replace values where the condition is False.", "view"]}, {"name": "pandas.Index.all", "path": "reference/api/pandas.index.all", "type": "Index Objects", "text": ["Return whether all elements are Truthy.", "Required for compatibility with numpy.", "Required for compatibility with numpy.", "A single element array-like may be converted to bool.", "See also", "Return whether any element in an Index is True.", "Return whether any element in a Series is True.", "Return whether all elements in a Series are True.", "Notes", "Not a Number (NaN), positive infinity and negative infinity evaluate to True because these are not equal to zero.", "Examples", "True, because nonzero integers are considered True.", "False, because 0 is considered False."]}, {"name": "pandas.Index.any", "path": "reference/api/pandas.index.any", "type": "Index Objects", "text": ["Return whether any element is Truthy.", "Required for compatibility with numpy.", "Required for compatibility with numpy.", "A single element array-like may be converted to bool.", "See also", "Return whether all elements are True.", "Return whether all elements are True.", "Notes", "Not a Number (NaN), positive infinity and negative infinity evaluate to True because these are not equal to zero.", "Examples"]}, {"name": "pandas.Index.append", "path": "reference/api/pandas.index.append", "type": "Index Objects", "text": ["Append a collection of Index options together."]}, {"name": "pandas.Index.argmax", "path": "reference/api/pandas.index.argmax", "type": "Index Objects", "text": ["Return int position of the largest value in the Series.", "If the maximum is achieved in multiple locations, the first row position is returned.", "Dummy argument for consistency with Series.", "Exclude NA/null values when showing the result.", "Additional arguments and keywords for compatibility with NumPy.", "Row position of the maximum value.", "See also", "Return position of the maximum value.", "Return position of the minimum value.", "Equivalent method for numpy arrays.", "Return index label of the maximum values.", "Return index label of the minimum values.", "Examples", "Consider dataset containing cereal calories", "The maximum cereal calories is the third element and the minimum cereal calories is the first element, since series is zero-indexed."]}, {"name": "pandas.Index.argmin", "path": "reference/api/pandas.index.argmin", "type": "Index Objects", "text": ["Return int position of the smallest value in the Series.", "If the minimum is achieved in multiple locations, the first row position is returned.", "Dummy argument for consistency with Series.", "Exclude NA/null values when showing the result.", "Additional arguments and keywords for compatibility with NumPy.", "Row position of the minimum value.", "See also", "Return position of the minimum value.", "Return position of the maximum value.", "Equivalent method for numpy arrays.", "Return index label of the maximum values.", "Return index label of the minimum values.", "Examples", "Consider dataset containing cereal calories", "The maximum cereal calories is the third element and the minimum cereal calories is the first element, since series is zero-indexed."]}, {"name": "pandas.Index.argsort", "path": "reference/api/pandas.index.argsort", "type": "Index Objects", "text": ["Return the integer indices that would sort the index.", "Passed to numpy.ndarray.argsort.", "Passed to numpy.ndarray.argsort.", "Integer indices that would sort the index if used as an indexer.", "See also", "Similar method for NumPy arrays.", "Return sorted copy of Index.", "Examples"]}, {"name": "pandas.Index.array", "path": "reference/api/pandas.index.array", "type": "Index Objects", "text": ["The ExtensionArray of the data backing this Series or Index.", "An ExtensionArray of the values stored within. For extension types, this is the actual array. For NumPy native types, this is a thin (no copy) wrapper around numpy.ndarray.", ".array differs .values which may require converting the data to a different form.", "See also", "Similar method that always returns a NumPy array.", "Similar method that always returns a NumPy array.", "Notes", "This table lays out the different array types for each extension dtype within pandas.", "dtype", "array type", "category", "Categorical", "period", "PeriodArray", "interval", "IntervalArray", "IntegerNA", "IntegerArray", "string", "StringArray", "boolean", "BooleanArray", "datetime64[ns, tz]", "DatetimeArray", "For any 3rd-party extension types, the array type will be an ExtensionArray.", "For all remaining dtypes .array will be a arrays.NumpyExtensionArray wrapping the actual ndarray stored within. If you absolutely need a NumPy array (possibly with copying / coercing data), then use Series.to_numpy() instead.", "Examples", "For regular NumPy types like int, and float, a PandasArray is returned.", "For extension types, like Categorical, the actual ExtensionArray is returned"]}, {"name": "pandas.Index.asi8", "path": "reference/api/pandas.index.asi8", "type": "Index Objects", "text": ["Integer representation of the values.", "An ndarray with int64 dtype."]}, {"name": "pandas.Index.asof", "path": "reference/api/pandas.index.asof", "type": "Index Objects", "text": ["Return the label from the index, or, if not present, the previous one.", "Assuming that the index is sorted, return the passed index label if it is in the index, or return the previous index label if the passed one is not in the index.", "The label up to which the method returns the latest index label.", "The passed label if it is in the index. The previous label if the passed label is not in the sorted index or NaN if there is no such label.", "See also", "Return the latest value in a Series up to the passed index.", "Perform an asof merge (similar to left join but it matches on nearest key rather than equal key).", "An asof is a thin wrapper around get_loc with method=\u2019pad\u2019.", "Examples", "Index.asof returns the latest index label up to the passed label.", "If the label is in the index, the method returns the passed label.", "If all of the labels in the index are later than the passed label, NaN is returned.", "If the index is not sorted, an error is raised."]}, {"name": "pandas.Index.asof_locs", "path": "reference/api/pandas.index.asof_locs", "type": "Index Objects", "text": ["Return the locations (indices) of labels in the index.", "As in the asof function, if the label (a particular entry in where) is not in the index, the latest index label up to the passed label is chosen and its index returned.", "If all of the labels in the index are later than a label in where, -1 is returned.", "mask is used to ignore NA values in the index during calculation.", "An Index consisting of an array of timestamps.", "Array of booleans denoting where values in the original data are not NA.", "An array of locations (indices) of the labels from the Index which correspond to the return values of the asof function for every element in where."]}, {"name": "pandas.Index.astype", "path": "reference/api/pandas.index.astype", "type": "Index Objects", "text": ["Create an Index with values cast to dtypes.", "The class of a new Index is determined by dtype. When conversion is impossible, a TypeError exception is raised.", "Note that any signed integer dtype is treated as 'int64', and any unsigned integer dtype is treated as 'uint64', regardless of the size.", "By default, astype always returns a newly allocated object. If copy is set to False and internal requirements on dtype are satisfied, the original data is used to create a new Index or the original Index is returned.", "Index with values cast to specified dtype."]}, {"name": "pandas.Index.copy", "path": "reference/api/pandas.index.copy", "type": "Index Objects", "text": ["Make a copy of this object.", "Name and dtype sets those attributes on the new object.", "Set name for new object.", "Set dtype for new object.", "Deprecated since version 1.2.0: use astype method instead.", "Kept for compatibility with MultiIndex. Should not be used.", "Deprecated since version 1.4.0: use name instead.", "Index refer to new object which is a copy of this object.", "Notes", "In most cases, there should be no functional difference from using deep, but if deep is passed it will attempt to deepcopy."]}, {"name": "pandas.Index.delete", "path": "reference/api/pandas.index.delete", "type": "Index Objects", "text": ["Make new Index with passed location(-s) deleted.", "Location of item(-s) which will be deleted. Use a list of locations to delete more than one value at the same time.", "Will be same type as self, except for RangeIndex.", "See also", "Delete any rows and column from NumPy array (ndarray).", "Examples"]}, {"name": "pandas.Index.difference", "path": "reference/api/pandas.index.difference", "type": "Index Objects", "text": ["Return a new Index with elements of index not in other.", "This is the set difference of two Index objects.", "Whether to sort the resulting index. By default, the values are attempted to be sorted, but any TypeError from incomparable elements is caught by pandas.", "None : Attempt to sort the result, but catch any TypeErrors from comparing incomparable elements.", "False : Do not sort the result.", "Examples"]}, {"name": "pandas.Index.drop", "path": "reference/api/pandas.index.drop", "type": "Index Objects", "text": ["Make new Index with passed list of labels deleted.", "If \u2018ignore\u2019, suppress error and existing labels are dropped.", "Will be same type as self, except for RangeIndex.", "If not all of the labels are found in the selected axis"]}, {"name": "pandas.Index.drop_duplicates", "path": "reference/api/pandas.index.drop_duplicates", "type": "Index Objects", "text": ["Return Index with duplicate values removed.", "\u2018first\u2019 : Drop duplicates except for the first occurrence.", "\u2018last\u2019 : Drop duplicates except for the last occurrence.", "False : Drop all duplicates.", "See also", "Equivalent method on Series.", "Equivalent method on DataFrame.", "Related method on Index, indicating duplicate Index values.", "Examples", "Generate an pandas.Index with duplicate values.", "The keep parameter controls which duplicate values are removed. The value \u2018first\u2019 keeps the first occurrence for each set of duplicated entries. The default value of keep is \u2018first\u2019.", "The value \u2018last\u2019 keeps the last occurrence for each set of duplicated entries.", "The value False discards all sets of duplicated entries."]}, {"name": "pandas.Index.droplevel", "path": "reference/api/pandas.index.droplevel", "type": "Index Objects", "text": ["Return index with requested level(s) removed.", "If resulting index has only 1 level left, the result will be of Index type, not MultiIndex.", "If a string is given, must be the name of a level If list-like, elements must be names or indexes of levels.", "Examples"]}, {"name": "pandas.Index.dropna", "path": "reference/api/pandas.index.dropna", "type": "Index Objects", "text": ["Return Index without NA/NaN values.", "If the Index is a MultiIndex, drop the value when any or all levels are NaN."]}, {"name": "pandas.Index.dtype", "path": "reference/api/pandas.index.dtype", "type": "Index Objects", "text": ["Return the dtype object of the underlying data."]}, {"name": "pandas.Index.duplicated", "path": "reference/api/pandas.index.duplicated", "type": "Index Objects", "text": ["Indicate duplicate index values.", "Duplicated values are indicated as True values in the resulting array. Either all duplicates, all except the first, or all except the last occurrence of duplicates can be indicated.", "The value or values in a set of duplicates to mark as missing.", "\u2018first\u2019 : Mark duplicates as True except for the first occurrence.", "\u2018last\u2019 : Mark duplicates as True except for the last occurrence.", "False : Mark all duplicates as True.", "See also", "Equivalent method on pandas.Series.", "Equivalent method on pandas.DataFrame.", "Remove duplicate values from Index.", "Examples", "By default, for each set of duplicated values, the first occurrence is set to False and all others to True:", "which is equivalent to", "By using \u2018last\u2019, the last occurrence of each set of duplicated values is set on False and all others on True:", "By setting keep on False, all duplicates are True:"]}, {"name": "pandas.Index.empty", "path": "reference/api/pandas.index.empty", "type": "Index Objects", "text": []}, {"name": "pandas.Index.equals", "path": "reference/api/pandas.index.equals", "type": "Index Objects", "text": ["Determine if two Index object are equal.", "The things that are being compared are:", "The elements inside the Index object.", "The order of the elements inside the Index object.", "The other object to compare against.", "True if \u201cother\u201d is an Index and it has the same elements and order as the calling index; False otherwise.", "Examples", "The elements inside are compared", "The order is compared", "The dtype is not compared"]}, {"name": "pandas.Index.factorize", "path": "reference/api/pandas.index.factorize", "type": "Index Objects", "text": ["Encode the object as an enumerated type or categorical variable.", "This method is useful for obtaining a numeric representation of an array when all that matters is identifying distinct values. factorize is available as both a top-level function pandas.factorize(), and as a method Series.factorize() and Index.factorize().", "Sort uniques and shuffle codes to maintain the relationship.", "Value to mark \u201cnot found\u201d. If None, will not drop the NaN from the uniques of the values.", "Changed in version 1.1.2.", "An integer ndarray that\u2019s an indexer into uniques. uniques.take(codes) will have the same values as values.", "The unique valid values. When values is Categorical, uniques is a Categorical. When values is some other pandas object, an Index is returned. Otherwise, a 1-D ndarray is returned.", "Note", "Even if there\u2019s a missing value in values, uniques will not contain an entry for it.", "See also", "Discretize continuous-valued array.", "Find the unique value in an array.", "Examples", "These examples all show factorize as a top-level method like pd.factorize(values). The results are identical for methods like Series.factorize().", "With sort=True, the uniques will be sorted, and codes will be shuffled so that the relationship is the maintained.", "Missing values are indicated in codes with na_sentinel (-1 by default). Note that missing values are never included in uniques.", "Thus far, we\u2019ve only factorized lists (which are internally coerced to NumPy arrays). When factorizing pandas objects, the type of uniques will differ. For Categoricals, a Categorical is returned.", "Notice that 'b' is in uniques.categories, despite not being present in cat.values.", "For all other pandas objects, an Index of the appropriate type is returned.", "If NaN is in the values, and we want to include NaN in the uniques of the values, it can be achieved by setting na_sentinel=None."]}, {"name": "pandas.Index.fillna", "path": "reference/api/pandas.index.fillna", "type": "Index Objects", "text": ["Fill NA/NaN values with the specified value.", "Scalar value to use to fill holes (e.g. 0). This value cannot be a list-likes.", "A dict of item->dtype of what to downcast if possible, or the string \u2018infer\u2019 which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible).", "See also", "Fill NaN values of a DataFrame.", "Fill NaN Values of a Series."]}, {"name": "pandas.Index.format", "path": "reference/api/pandas.index.format", "type": "Index Objects", "text": ["Render a string representation of the Index."]}, {"name": "pandas.Index.get_indexer", "path": "reference/api/pandas.index.get_indexer", "type": "Index Objects", "text": ["Compute indexer and mask for new index given the current index. The indexer should be then used as an input to ndarray.take to align the current data to the new index.", "default: exact matches only.", "pad / ffill: find the PREVIOUS index value if no exact match.", "backfill / bfill: use NEXT index value if no exact match", "nearest: use the NEAREST index value if no exact match. Tied distances are broken by preferring the larger index value.", "Maximum number of consecutive labels in target to match for inexact matches.", "Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance.", "Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index\u2019s type.", "Integers from 0 to n - 1 indicating that the index at these positions matches the corresponding target values. Missing values in the target are marked by -1.", "Notes", "Returns -1 for unmatched values, for further explanation see the example below.", "Examples", "Notice that the return value is an array of locations in index and x is marked by -1, as it is not in index."]}, {"name": "pandas.Index.get_indexer_for", "path": "reference/api/pandas.index.get_indexer_for", "type": "Index Objects", "text": ["Guaranteed return of an indexer even when non-unique.", "This dispatches to get_indexer or get_indexer_non_unique as appropriate.", "List of indices.", "Examples"]}, {"name": "pandas.Index.get_indexer_non_unique", "path": "reference/api/pandas.index.get_indexer_non_unique", "type": "Index Objects", "text": ["Compute indexer and mask for new index given the current index. The indexer should be then used as an input to ndarray.take to align the current data to the new index.", "Integers from 0 to n - 1 indicating that the index at these positions matches the corresponding target values. Missing values in the target are marked by -1.", "An indexer into the target of the values not found. These correspond to the -1 in the indexer array."]}, {"name": "pandas.Index.get_level_values", "path": "reference/api/pandas.index.get_level_values", "type": "Index Objects", "text": ["Return an Index of values for requested level.", "This is primarily useful to get an individual level of values from a MultiIndex, but is provided on Index as well for compatibility.", "It is either the integer position or the name of the level.", "Calling object, as there is only one level in the Index.", "See also", "Get values for a level of a MultiIndex.", "Notes", "For Index, level should be 0, since there are no multiple levels.", "Examples", "Get level values by supplying level as integer:"]}, {"name": "pandas.Index.get_loc", "path": "reference/api/pandas.index.get_loc", "type": "Index Objects", "text": ["Get integer location, slice or boolean mask for requested label.", "default: exact matches only.", "pad / ffill: find the PREVIOUS index value if no exact match.", "backfill / bfill: use NEXT index value if no exact match", "nearest: use the NEAREST index value if no exact match. Tied distances are broken by preferring the larger index value.", "Maximum distance from index value for inexact matches. The value of the index at the matching location must satisfy the equation abs(index[loc] - key) <= tolerance.", "Examples"]}, {"name": "pandas.Index.get_slice_bound", "path": "reference/api/pandas.index.get_slice_bound", "type": "Index Objects", "text": ["Calculate slice bound that corresponds to given label.", "Returns leftmost (one-past-the-rightmost if side=='right') position of given label.", "Deprecated since version 1.4.0.", "Index of label."]}, {"name": "pandas.Index.get_value", "path": "reference/api/pandas.index.get_value", "type": "Index Objects", "text": ["Fast lookup of value from 1-dimensional ndarray.", "Only use this if you know what you\u2019re doing."]}, {"name": "pandas.Index.groupby", "path": "reference/api/pandas.index.groupby", "type": "GroupBy", "text": ["Group the index labels by a given array of values.", "Values used to determine the groups.", "{group name -> group labels}"]}, {"name": "pandas.Index.has_duplicates", "path": "reference/api/pandas.index.has_duplicates", "type": "Index Objects", "text": ["Check if the Index has duplicate values.", "Whether or not the Index has duplicate values.", "Examples"]}, {"name": "pandas.Index.hasnans", "path": "reference/api/pandas.index.hasnans", "type": "Index Objects", "text": ["Return True if there are any NaNs.", "Enables various performance speedups."]}, {"name": "pandas.Index.holds_integer", "path": "reference/api/pandas.index.holds_integer", "type": "Index Objects", "text": ["Whether the type is an integer type."]}, {"name": "pandas.Index.identical", "path": "reference/api/pandas.index.identical", "type": "Index Objects", "text": ["Similar to equals, but checks that object attributes and types are also equal.", "If two Index objects have equal elements and same type True, otherwise False."]}, {"name": "pandas.Index.inferred_type", "path": "reference/api/pandas.index.inferred_type", "type": "Index Objects", "text": ["Return a string of the type inferred from the values."]}, {"name": "pandas.Index.insert", "path": "reference/api/pandas.index.insert", "type": "Index Objects", "text": ["Make new Index inserting new item at location.", "Follows Python numpy.insert semantics for negative values."]}, {"name": "pandas.Index.intersection", "path": "reference/api/pandas.index.intersection", "type": "Input/output", "text": ["Form the intersection of two Index objects.", "This returns a new Index with elements common to the index and other.", "Whether to sort the resulting index.", "False : do not sort the result.", "None : sort the result, except when self and other are equal or when the values cannot be compared.", "Examples"]}, {"name": "pandas.Index.is_", "path": "reference/api/pandas.index.is_", "type": "Index Objects", "text": ["More flexible, faster check like is but that works through views.", "Note: this is not the same as Index.identical(), which checks that metadata is also the same.", "Other object to compare against.", "True if both have same underlying data, False otherwise.", "See also", "Works like Index.is_ but also checks metadata."]}, {"name": "pandas.Index.is_all_dates", "path": "reference/api/pandas.index.is_all_dates", "type": "Index Objects", "text": ["Whether or not the index values only consist of dates."]}, {"name": "pandas.Index.is_boolean", "path": "reference/api/pandas.index.is_boolean", "type": "Index Objects", "text": ["Check if the Index only consists of booleans.", "Whether or not the Index only consists of booleans.", "See also", "Check if the Index only consists of integers.", "Check if the Index is a floating type.", "Check if the Index only consists of numeric data.", "Check if the Index is of the object dtype.", "Check if the Index holds categorical data.", "Check if the Index holds Interval objects.", "Check if the Index holds data with mixed data types.", "Examples"]}, {"name": "pandas.Index.is_categorical", "path": "reference/api/pandas.index.is_categorical", "type": "Index Objects", "text": ["Check if the Index holds categorical data.", "True if the Index is categorical.", "See also", "Index for categorical data.", "Check if the Index only consists of booleans.", "Check if the Index only consists of integers.", "Check if the Index is a floating type.", "Check if the Index only consists of numeric data.", "Check if the Index is of the object dtype.", "Check if the Index holds Interval objects.", "Check if the Index holds data with mixed data types.", "Examples"]}, {"name": "pandas.Index.is_floating", "path": "reference/api/pandas.index.is_floating", "type": "Index Objects", "text": ["Check if the Index is a floating type.", "The Index may consist of only floats, NaNs, or a mix of floats, integers, or NaNs.", "Whether or not the Index only consists of only consists of floats, NaNs, or a mix of floats, integers, or NaNs.", "See also", "Check if the Index only consists of booleans.", "Check if the Index only consists of integers.", "Check if the Index only consists of numeric data.", "Check if the Index is of the object dtype.", "Check if the Index holds categorical data.", "Check if the Index holds Interval objects.", "Check if the Index holds data with mixed data types.", "Examples"]}, {"name": "pandas.Index.is_integer", "path": "reference/api/pandas.index.is_integer", "type": "Index Objects", "text": ["Check if the Index only consists of integers.", "Whether or not the Index only consists of integers.", "See also", "Check if the Index only consists of booleans.", "Check if the Index is a floating type.", "Check if the Index only consists of numeric data.", "Check if the Index is of the object dtype.", "Check if the Index holds categorical data.", "Check if the Index holds Interval objects.", "Check if the Index holds data with mixed data types.", "Examples"]}, {"name": "pandas.Index.is_interval", "path": "reference/api/pandas.index.is_interval", "type": "Index Objects", "text": ["Check if the Index holds Interval objects.", "Whether or not the Index holds Interval objects.", "See also", "Index for Interval objects.", "Check if the Index only consists of booleans.", "Check if the Index only consists of integers.", "Check if the Index is a floating type.", "Check if the Index only consists of numeric data.", "Check if the Index is of the object dtype.", "Check if the Index holds categorical data.", "Check if the Index holds data with mixed data types.", "Examples"]}, {"name": "pandas.Index.is_mixed", "path": "reference/api/pandas.index.is_mixed", "type": "Index Objects", "text": ["Check if the Index holds data with mixed data types.", "Whether or not the Index holds data with mixed data types.", "See also", "Check if the Index only consists of booleans.", "Check if the Index only consists of integers.", "Check if the Index is a floating type.", "Check if the Index only consists of numeric data.", "Check if the Index is of the object dtype.", "Check if the Index holds categorical data.", "Check if the Index holds Interval objects.", "Examples"]}, {"name": "pandas.Index.is_monotonic", "path": "reference/api/pandas.index.is_monotonic", "type": "Index Objects", "text": ["Alias for is_monotonic_increasing."]}, {"name": "pandas.Index.is_monotonic_decreasing", "path": "reference/api/pandas.index.is_monotonic_decreasing", "type": "Index Objects", "text": ["Return if the index is monotonic decreasing (only equal or decreasing) values.", "Examples"]}, {"name": "pandas.Index.is_monotonic_increasing", "path": "reference/api/pandas.index.is_monotonic_increasing", "type": "Index Objects", "text": ["Return if the index is monotonic increasing (only equal or increasing) values.", "Examples"]}, {"name": "pandas.Index.is_numeric", "path": "reference/api/pandas.index.is_numeric", "type": "Index Objects", "text": ["Check if the Index only consists of numeric data.", "Whether or not the Index only consists of numeric data.", "See also", "Check if the Index only consists of booleans.", "Check if the Index only consists of integers.", "Check if the Index is a floating type.", "Check if the Index is of the object dtype.", "Check if the Index holds categorical data.", "Check if the Index holds Interval objects.", "Check if the Index holds data with mixed data types.", "Examples"]}, {"name": "pandas.Index.is_object", "path": "reference/api/pandas.index.is_object", "type": "Index Objects", "text": ["Check if the Index is of the object dtype.", "Whether or not the Index is of the object dtype.", "See also", "Check if the Index only consists of booleans.", "Check if the Index only consists of integers.", "Check if the Index is a floating type.", "Check if the Index only consists of numeric data.", "Check if the Index holds categorical data.", "Check if the Index holds Interval objects.", "Check if the Index holds data with mixed data types.", "Examples"]}, {"name": "pandas.Index.is_type_compatible", "path": "reference/api/pandas.index.is_type_compatible", "type": "Index Objects", "text": ["Whether the index type is compatible with the provided type."]}, {"name": "pandas.Index.is_unique", "path": "reference/api/pandas.index.is_unique", "type": "Index Objects", "text": ["Return if the index has unique values."]}, {"name": "pandas.Index.isin", "path": "reference/api/pandas.index.isin", "type": "Index Objects", "text": ["Return a boolean array where the index values are in values.", "Compute boolean array of whether each index value is found in the passed set of values. The length of the returned boolean array matches the length of the index.", "Sought values.", "Name or position of the index level to use (if the index is a MultiIndex).", "NumPy array of boolean values.", "See also", "Same for Series.", "Same method for DataFrames.", "Notes", "In the case of MultiIndex you must either specify values as a list-like object containing tuples that are the same length as the number of levels, or specify level. Otherwise it will raise a ValueError.", "If level is specified:", "if it is the name of one and only one index level, use that level;", "otherwise it should be a number indicating level position.", "Examples", "Check whether each index value in a list of values.", "Check whether the strings in the \u2018color\u2019 level of the MultiIndex are in a list of colors.", "To check across the levels of a MultiIndex, pass a list of tuples:", "For a DatetimeIndex, string values in values are converted to Timestamps."]}, {"name": "pandas.Index.isna", "path": "reference/api/pandas.index.isna", "type": "Index Objects", "text": ["Detect missing values.", "Return a boolean same-sized object indicating if the values are NA. NA values, such as None, numpy.NaN or pd.NaT, get mapped to True values. Everything else get mapped to False values. Characters such as empty strings \u2018\u2019 or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True).", "A boolean array of whether my values are NA.", "See also", "Boolean inverse of isna.", "Omit entries with missing values.", "Top-level isna.", "Detect missing values in Series object.", "Examples", "Show which entries in a pandas.Index are NA. The result is an array.", "Empty strings are not considered NA values. None is considered an NA value.", "For datetimes, NaT (Not a Time) is considered as an NA value."]}, {"name": "pandas.Index.isnull", "path": "reference/api/pandas.index.isnull", "type": "Index Objects", "text": ["Detect missing values.", "Return a boolean same-sized object indicating if the values are NA. NA values, such as None, numpy.NaN or pd.NaT, get mapped to True values. Everything else get mapped to False values. Characters such as empty strings \u2018\u2019 or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True).", "A boolean array of whether my values are NA.", "See also", "Boolean inverse of isna.", "Omit entries with missing values.", "Top-level isna.", "Detect missing values in Series object.", "Examples", "Show which entries in a pandas.Index are NA. The result is an array.", "Empty strings are not considered NA values. None is considered an NA value.", "For datetimes, NaT (Not a Time) is considered as an NA value."]}, {"name": "pandas.Index.item", "path": "reference/api/pandas.index.item", "type": "Index Objects", "text": ["Return the first element of the underlying data as a Python scalar.", "The first element of %(klass)s.", "If the data is not length-1."]}, {"name": "pandas.Index.join", "path": "reference/api/pandas.index.join", "type": "Index Objects", "text": ["Compute join_index and indexers to conform data structures to the new index.", "Sort the join keys lexicographically in the result Index. If False, the order of the join keys depends on the join type (how keyword)."]}, {"name": "pandas.Index.map", "path": "reference/api/pandas.index.map", "type": "Index Objects", "text": ["Map values using an input mapping or function.", "Mapping correspondence.", "If \u2018ignore\u2019, propagate NA values, without passing them to the mapping correspondence.", "The output of the mapping function applied to the index. If the function returns a tuple with more than one element a MultiIndex will be returned."]}, {"name": "pandas.Index.max", "path": "reference/api/pandas.index.max", "type": "Index Objects", "text": ["Return the maximum value of the Index.", "For compatibility with NumPy. Only 0 or None are allowed.", "Exclude NA/null values when showing the result.", "Additional arguments and keywords for compatibility with NumPy.", "Maximum value.", "See also", "Return the minimum value in an Index.", "Return the maximum value in a Series.", "Return the maximum values in a DataFrame.", "Examples", "For a MultiIndex, the maximum is determined lexicographically."]}, {"name": "pandas.Index.memory_usage", "path": "reference/api/pandas.index.memory_usage", "type": "Index Objects", "text": ["Memory usage of the values.", "Introspect the data deeply, interrogate object dtypes for system-level memory consumption.", "See also", "Total bytes consumed by the elements of the array.", "Notes", "Memory usage does not include memory consumed by elements that are not components of the array if deep=False or if used on PyPy"]}, {"name": "pandas.Index.min", "path": "reference/api/pandas.index.min", "type": "Index Objects", "text": ["Return the minimum value of the Index.", "Dummy argument for consistency with Series.", "Exclude NA/null values when showing the result.", "Additional arguments and keywords for compatibility with NumPy.", "Minimum value.", "See also", "Return the maximum value of the object.", "Return the minimum value in a Series.", "Return the minimum values in a DataFrame.", "Examples", "For a MultiIndex, the minimum is determined lexicographically."]}, {"name": "pandas.Index.name", "path": "reference/api/pandas.index.name", "type": "Index Objects", "text": ["Return Index or MultiIndex name."]}, {"name": "pandas.Index.names", "path": "reference/api/pandas.index.names", "type": "Index Objects", "text": []}, {"name": "pandas.Index.nbytes", "path": "reference/api/pandas.index.nbytes", "type": "Index Objects", "text": ["Return the number of bytes in the underlying data."]}, {"name": "pandas.Index.ndim", "path": "reference/api/pandas.index.ndim", "type": "Index Objects", "text": ["Number of dimensions of the underlying data, by definition 1."]}, {"name": "pandas.Index.nlevels", "path": "reference/api/pandas.index.nlevels", "type": "Index Objects", "text": ["Number of levels."]}, {"name": "pandas.Index.notna", "path": "reference/api/pandas.index.notna", "type": "Index Objects", "text": ["Detect existing (non-missing) values.", "Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True). NA values, such as None or numpy.NaN, get mapped to False values.", "Boolean array to indicate which entries are not NA.", "See also", "Alias of notna.", "Inverse of notna.", "Top-level notna.", "Examples", "Show which entries in an Index are not NA. The result is an array.", "Empty strings are not considered NA values. None is considered a NA value."]}, {"name": "pandas.Index.notnull", "path": "reference/api/pandas.index.notnull", "type": "Index Objects", "text": ["Detect existing (non-missing) values.", "Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True). NA values, such as None or numpy.NaN, get mapped to False values.", "Boolean array to indicate which entries are not NA.", "See also", "Alias of notna.", "Inverse of notna.", "Top-level notna.", "Examples", "Show which entries in an Index are not NA. The result is an array.", "Empty strings are not considered NA values. None is considered a NA value."]}, {"name": "pandas.Index.nunique", "path": "reference/api/pandas.index.nunique", "type": "Index Objects", "text": ["Return number of unique elements in the object.", "Excludes NA values by default.", "Don\u2019t include NaN in the count.", "See also", "Method nunique for DataFrame.", "Count non-NA/null observations in the Series.", "Examples"]}, {"name": "pandas.Index.putmask", "path": "reference/api/pandas.index.putmask", "type": "Index Objects", "text": ["Return a new Index of the values set with the mask.", "See also", "Changes elements of an array based on conditional and input values."]}, {"name": "pandas.Index.ravel", "path": "reference/api/pandas.index.ravel", "type": "Index Objects", "text": ["Return an ndarray of the flattened values of the underlying data.", "Flattened array.", "See also", "Return a flattened array."]}, {"name": "pandas.Index.reindex", "path": "reference/api/pandas.index.reindex", "type": "Index Objects", "text": ["Create index with target\u2019s values.", "default: exact matches only.", "pad / ffill: find the PREVIOUS index value if no exact match.", "backfill / bfill: use NEXT index value if no exact match", "nearest: use the NEAREST index value if no exact match. Tied distances are broken by preferring the larger index value.", "Level of multiindex.", "Maximum number of consecutive labels in target to match for inexact matches.", "Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance.", "Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index\u2019s type.", "Resulting index.", "Indices of output values in original index.", "If method passed along with level.", "If non-unique multi-index", "If non-unique index and method or limit passed.", "See also", "Examples"]}, {"name": "pandas.Index.rename", "path": "reference/api/pandas.index.rename", "type": "Index Objects", "text": ["Alter Index or MultiIndex name.", "Able to set new names without level. Defaults to returning new index. Length of names must match number of levels in MultiIndex.", "Name(s) to set.", "Modifies the object directly, instead of creating a new Index or MultiIndex.", "The same type as the caller or None if inplace=True.", "See also", "Able to set new names partially and by level.", "Examples"]}, {"name": "pandas.Index.repeat", "path": "reference/api/pandas.index.repeat", "type": "Index Objects", "text": ["Repeat elements of a Index.", "Returns a new Index where each element of the current Index is repeated consecutively a given number of times.", "The number of repetitions for each element. This should be a non-negative integer. Repeating 0 times will return an empty Index.", "Must be None. Has no effect but is accepted for compatibility with numpy.", "Newly created Index with repeated elements.", "See also", "Equivalent function for Series.", "Similar method for numpy.ndarray.", "Examples"]}, {"name": "pandas.Index.searchsorted", "path": "reference/api/pandas.index.searchsorted", "type": "Index Objects", "text": ["Find indices where elements should be inserted to maintain order.", "Find the indices into a sorted Index self such that, if the corresponding elements in value were inserted before the indices, the order of self would be preserved.", "Note", "The Index must be monotonically sorted, otherwise wrong locations will likely be returned. Pandas does not check this for you.", "Values to insert into self.", "If \u2018left\u2019, the index of the first suitable location found is given. If \u2018right\u2019, return the last such index. If there is no suitable index, return either 0 or N (where N is the length of self).", "Optional array of integer indices that sort self into ascending order. They are typically the result of np.argsort.", "A scalar or array of insertion points with the same shape as value.", "See also", "Sort by the values along either axis.", "Similar method from NumPy.", "Notes", "Binary search is used to find the required insertion points.", "Examples", "If the values are not monotonically sorted, wrong locations may be returned:"]}, {"name": "pandas.Index.set_names", "path": "reference/api/pandas.index.set_names", "type": "Index Objects", "text": ["Set Index or MultiIndex name.", "Able to set new names partially and by level.", "Name(s) to set.", "Changed in version 1.3.0.", "If the index is a MultiIndex and names is not dict-like, level(s) to set (None for all levels). Otherwise level must be None.", "Changed in version 1.3.0.", "Modifies the object directly, instead of creating a new Index or MultiIndex.", "The same type as the caller or None if inplace=True.", "See also", "Able to set new names without level.", "Examples", "When renaming levels with a dict, levels can not be passed."]}, {"name": "pandas.Index.set_value", "path": "reference/api/pandas.index.set_value", "type": "Index Objects", "text": ["Fast lookup of value from 1-dimensional ndarray.", "Deprecated since version 1.0.", "Notes", "Only use this if you know what you\u2019re doing."]}, {"name": "pandas.Index.shape", "path": "reference/api/pandas.index.shape", "type": "Index Objects", "text": ["Return a tuple of the shape of the underlying data."]}, {"name": "pandas.Index.shift", "path": "reference/api/pandas.index.shift", "type": "Index Objects", "text": ["Shift index by desired number of time frequency increments.", "This method is for shifting the values of datetime-like indexes by a specified time increment a given number of times.", "Number of periods (or increments) to shift by, can be positive or negative.", "Frequency increment to shift by. If None, the index is shifted by its own freq attribute. Offset aliases are valid strings, e.g., \u2018D\u2019, \u2018W\u2019, \u2018M\u2019 etc.", "Shifted index.", "See also", "Shift values of Series.", "Notes", "This method is only implemented for datetime-like index classes, i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.", "Examples", "Put the first 5 month starts of 2011 into an index.", "Shift the index by 10 days.", "The default value of freq is the freq attribute of the index, which is \u2018MS\u2019 (month start) in this example."]}, {"name": "pandas.Index.size", "path": "reference/api/pandas.index.size", "type": "Index Objects", "text": ["Return the number of elements in the underlying data."]}, {"name": "pandas.Index.slice_indexer", "path": "reference/api/pandas.index.slice_indexer", "type": "Index Objects", "text": ["Compute the slice indexer for input labels and step.", "Index needs to be ordered and unique.", "If None, defaults to the beginning.", "If None, defaults to the end.", "Deprecated since version 1.4.0.", "not ordered.", "Notes", "This function assumes that the data is sorted, so use at your own peril", "Examples", "This is a method on all index types. For example you can do:"]}, {"name": "pandas.Index.slice_locs", "path": "reference/api/pandas.index.slice_locs", "type": "Index Objects", "text": ["Compute slice locations for input labels.", "If None, defaults to the beginning.", "If None, defaults to the end.", "If None, defaults to 1.", "Deprecated since version 1.4.0.", "See also", "Get location for a single label.", "Notes", "This method only works if the index is monotonic or unique.", "Examples"]}, {"name": "pandas.Index.sort", "path": "reference/api/pandas.index.sort", "type": "Index Objects", "text": ["Use sort_values instead."]}, {"name": "pandas.Index.sort_values", "path": "reference/api/pandas.index.sort_values", "type": "Index Objects", "text": ["Return a sorted copy of the index.", "Return a sorted copy of the index, and optionally return the indices that sorted the index itself.", "Should the indices that would sort the index be returned.", "Should the index values be sorted in an ascending order.", "Argument \u2018first\u2019 puts NaNs at the beginning, \u2018last\u2019 puts NaNs at the end.", "New in version 1.2.0.", "If not None, apply the key function to the index values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect an Index and return an Index of the same shape.", "New in version 1.1.0.", "Sorted copy of the index.", "The indices that the index itself was sorted by.", "See also", "Sort values of a Series.", "Sort values in a DataFrame.", "Examples", "Sort values in ascending order (default behavior).", "Sort values in descending order, and also get the indices idx was sorted by."]}, {"name": "pandas.Index.sortlevel", "path": "reference/api/pandas.index.sortlevel", "type": "Index Objects", "text": ["For internal compatibility with the Index API.", "Sort the Index. This is for compat with MultiIndex", "False to sort in descending order"]}, {"name": "pandas.Index.str", "path": "reference/api/pandas.index.str", "type": "Index Objects", "text": ["Vectorized string functions for Series and Index.", "NAs stay NA unless handled otherwise by a particular method. Patterned after Python\u2019s string methods, with some inspiration from R\u2019s stringr package.", "Examples"]}, {"name": "pandas.Index.symmetric_difference", "path": "reference/api/pandas.index.symmetric_difference", "type": "Index Objects", "text": ["Compute the symmetric difference of two Index objects.", "Whether to sort the resulting index. By default, the values are attempted to be sorted, but any TypeError from incomparable elements is caught by pandas.", "None : Attempt to sort the result, but catch any TypeErrors from comparing incomparable elements.", "False : Do not sort the result.", "Notes", "symmetric_difference contains elements that appear in either idx1 or idx2 but not both. Equivalent to the Index created by idx1.difference(idx2) | idx2.difference(idx1) with duplicates dropped.", "Examples"]}, {"name": "pandas.Index.T", "path": "reference/api/pandas.index.t", "type": "Index Objects", "text": ["Return the transpose, which is by definition self."]}, {"name": "pandas.Index.take", "path": "reference/api/pandas.index.take", "type": "Index Objects", "text": ["Return a new Index of the values selected by the indices.", "For internal compatibility with numpy arrays.", "Indices to be taken.", "The axis over which to select values, always 0.", "If allow_fill=True and fill_value is not None, indices specified by -1 are regarded as NA. If Index doesn\u2019t hold NA, raise ValueError.", "An index formed of elements at the given indices. Will be the same type as self, except for RangeIndex.", "See also", "Return an array formed from the elements of a at the given indices."]}, {"name": "pandas.Index.to_flat_index", "path": "reference/api/pandas.index.to_flat_index", "type": "Index Objects", "text": ["Identity method.", "This is implemented for compatibility with subclass implementations when chaining.", "Caller.", "See also", "Subclass implementation."]}, {"name": "pandas.Index.to_frame", "path": "reference/api/pandas.index.to_frame", "type": "DataFrame", "text": ["Create a DataFrame with a column containing the Index.", "Set the index of the returned DataFrame as the original Index.", "The passed name should substitute for the index name (if it has one).", "DataFrame containing the original Index data.", "See also", "Convert an Index to a Series.", "Convert Series to DataFrame.", "Examples", "By default, the original Index is reused. To enforce a new Index:", "To override the name of the resulting column, specify name:"]}, {"name": "pandas.Index.to_list", "path": "reference/api/pandas.index.to_list", "type": "Index Objects", "text": ["Return a list of the values.", "These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)", "See also", "Return the array as an a.ndim-levels deep nested list of Python scalars."]}, {"name": "pandas.Index.to_native_types", "path": "reference/api/pandas.index.to_native_types", "type": "General utility functions", "text": ["Format specified values of self and return them.", "Deprecated since version 1.2.0.", "An indexer into self that specifies which values are used in the formatting process.", "Options for specifying how the values should be formatted. These options include the following:", "The value that serves as a placeholder for NULL values", "Whether or not there are quoted values in self", "The format used to represent date-like values.", "Formatted values."]}, {"name": "pandas.Index.to_numpy", "path": "reference/api/pandas.index.to_numpy", "type": "Index Objects", "text": ["A NumPy ndarray representing the values in this Series or Index.", "The dtype to pass to numpy.asarray().", "Whether to ensure that the returned value is not a view on another array. Note that copy=False does not ensure that to_numpy() is no-copy. Rather, copy=True ensure that a copy is made, even if not strictly necessary.", "The value to use for missing values. The default value depends on dtype and the type of the array.", "New in version 1.0.0.", "Additional keywords passed through to the to_numpy method of the underlying array (for extension arrays).", "New in version 1.0.0.", "See also", "Get the actual data stored within.", "Get the actual data stored within.", "Similar method for DataFrame.", "Notes", "The returned array will be the same up to equality (values equal in self will be equal in the returned array; likewise for values that are not equal). When self contains an ExtensionArray, the dtype may be different. For example, for a category-dtype Series, to_numpy() will return a NumPy array and the categorical dtype will be lost.", "For NumPy dtypes, this will be a reference to the actual data stored in this Series or Index (assuming copy=False). Modifying the result in place will modify the data stored in the Series or Index (not that we recommend doing that).", "For extension types, to_numpy() may require copying data and coercing the result to a NumPy type (possibly object), which may be expensive. When you need a no-copy reference to the underlying data, Series.array should be used instead.", "This table lays out the different dtypes and default return types of to_numpy() for various dtypes within pandas.", "dtype", "array type", "category[T]", "ndarray[T] (same dtype as input)", "period", "ndarray[object] (Periods)", "interval", "ndarray[object] (Intervals)", "IntegerNA", "ndarray[object]", "datetime64[ns]", "datetime64[ns]", "datetime64[ns, tz]", "ndarray[object] (Timestamps)", "Examples", "Specify the dtype to control how datetime-aware data is represented. Use dtype=object to return an ndarray of pandas Timestamp objects, each with the correct tz.", "Or dtype='datetime64[ns]' to return an ndarray of native datetime64 values. The values are converted to UTC and the timezone info is dropped."]}, {"name": "pandas.Index.to_series", "path": "reference/api/pandas.index.to_series", "type": "Index Objects", "text": ["Create a Series with both index and values equal to the index keys.", "Useful with map for returning an indexer based on an index.", "Index of resulting Series. If None, defaults to original index.", "Name of resulting Series. If None, defaults to name of original index.", "The dtype will be based on the type of the Index values.", "See also", "Convert an Index to a DataFrame.", "Convert Series to DataFrame.", "Examples", "By default, the original Index and original name is reused.", "To enforce a new Index, specify new labels to index:", "To override the name of the resulting column, specify name:"]}, {"name": "pandas.Index.tolist", "path": "reference/api/pandas.index.tolist", "type": "Index Objects", "text": ["Return a list of the values.", "These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)", "See also", "Return the array as an a.ndim-levels deep nested list of Python scalars."]}, {"name": "pandas.Index.transpose", "path": "reference/api/pandas.index.transpose", "type": "Index Objects", "text": ["Return the transpose, which is by definition self."]}, {"name": "pandas.Index.union", "path": "reference/api/pandas.index.union", "type": "Input/output", "text": ["Form the union of two Index objects.", "If the Index objects are incompatible, both Index objects will be cast to dtype(\u2018object\u2019) first.", "Changed in version 0.25.0.", "Whether to sort the resulting Index.", "None : Sort the result, except when", "self and other are equal.", "self or other has length 0.", "Some values in self or other cannot be compared. A RuntimeWarning is issued in this case.", "False : do not sort the result.", "Examples", "Union matching dtypes", "Union mismatched dtypes", "MultiIndex case"]}, {"name": "pandas.Index.unique", "path": "reference/api/pandas.index.unique", "type": "Index Objects", "text": ["Return unique values in the index.", "Unique values are returned in order of appearance, this does NOT sort.", "Only return values from specified level (for MultiIndex). If int, gets the level by integer position, else by level name.", "See also", "Numpy array of unique values in that column.", "Return unique values of Series object."]}, {"name": "pandas.Index.value_counts", "path": "reference/api/pandas.index.value_counts", "type": "Index Objects", "text": ["Return a Series containing counts of unique values.", "The resulting object will be in descending order so that the first element is the most frequently-occurring element. Excludes NA values by default.", "If True then the object returned will contain the relative frequencies of the unique values.", "Sort by frequencies.", "Sort in ascending order.", "Rather than count values, group them into half-open bins, a convenience for pd.cut, only works with numeric data.", "Don\u2019t include counts of NaN.", "See also", "Number of non-NA elements in a Series.", "Number of non-NA elements in a DataFrame.", "Equivalent method on DataFrames.", "Examples", "With normalize set to True, returns the relative frequency by dividing all values by the sum of values.", "bins", "Bins can be useful for going from a continuous variable to a categorical variable; instead of counting unique apparitions of values, divide the index in the specified number of half-open bins.", "dropna", "With dropna set to False we can also see NaN index values."]}, {"name": "pandas.Index.values", "path": "reference/api/pandas.index.values", "type": "Index Objects", "text": ["Return an array representing the data in the Index.", "Warning", "We recommend using Index.array or Index.to_numpy(), depending on whether you need a reference to the underlying data or a NumPy array.", "See also", "Reference to the underlying data.", "A NumPy array representing the underlying data."]}, {"name": "pandas.Index.view", "path": "reference/api/pandas.index.view", "type": "Index Objects", "text": []}, {"name": "pandas.Index.where", "path": "reference/api/pandas.index.where", "type": "Index Objects", "text": ["Replace values where the condition is False.", "The replacement is taken from other.", "Condition to select the values on.", "Replacement if the condition is False.", "A copy of self with values replaced from other where the condition is False.", "See also", "Same method for Series.", "Same method for DataFrame.", "Examples"]}, {"name": "pandas.IndexSlice", "path": "reference/api/pandas.indexslice", "type": "Index Objects", "text": ["Create an object to more easily perform multi-index slicing.", "See also", "New MultiIndex with no unused levels.", "Notes", "See Defined Levels for further info on slicing a MultiIndex.", "Examples", "Using the default slice command:", "Using the IndexSlice class for a more intuitive command:"]}, {"name": "pandas.infer_freq", "path": "reference/api/pandas.infer_freq", "type": "General functions", "text": ["Infer the most likely frequency given the input index. If the frequency is uncertain, a warning will be printed.", "If passed a Series will use the values of the series (NOT THE INDEX).", "None if no discernible frequency.", "If the index is not datetime-like.", "If there are fewer than three values.", "Examples"]}, {"name": "pandas.Int16Dtype", "path": "reference/api/pandas.int16dtype", "type": "Pandas arrays", "text": ["An ExtensionDtype for int16 integer data.", "Changed in version 1.0.0: Now uses pandas.NA as its missing value, rather than numpy.nan.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.Int32Dtype", "path": "reference/api/pandas.int32dtype", "type": "Pandas arrays", "text": ["An ExtensionDtype for int32 integer data.", "Changed in version 1.0.0: Now uses pandas.NA as its missing value, rather than numpy.nan.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.Int64Dtype", "path": "reference/api/pandas.int64dtype", "type": "Pandas arrays", "text": ["An ExtensionDtype for int64 integer data.", "Changed in version 1.0.0: Now uses pandas.NA as its missing value, rather than numpy.nan.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.Int64Index", "path": "reference/api/pandas.int64index", "type": "Index Objects", "text": ["Immutable sequence used for indexing and alignment. The basic object storing axis labels for all pandas objects. Int64Index is a special case of Index with purely integer labels. .", "Deprecated since version 1.4.0: In pandas v2.0 Int64Index will be removed and NumericIndex used instead. Int64Index will remain fully functional for the duration of pandas 1.x.", "Make a copy of input ndarray.", "Name to be stored in the index.", "See also", "The base pandas Index type.", "Index of numpy int/uint/float data.", "Notes", "An Index instance can only contain hashable objects.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.Int8Dtype", "path": "reference/api/pandas.int8dtype", "type": "Pandas arrays", "text": ["An ExtensionDtype for int8 integer data.", "Changed in version 1.0.0: Now uses pandas.NA as its missing value, rather than numpy.nan.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.Interval", "path": "reference/api/pandas.interval", "type": "Pandas arrays", "text": ["Immutable object implementing an Interval, a bounded slice-like interval.", "Left bound for the interval.", "Right bound for the interval.", "Whether the interval is closed on the left-side, right-side, both or neither. See the Notes for more detailed explanation.", "See also", "An Index of Interval objects that are all closed on the same side.", "Convert continuous data into discrete bins (Categorical of Interval objects).", "Convert continuous data into bins (Categorical of Interval objects) based on quantiles.", "Represents a period of time.", "Notes", "The parameters left and right must be from the same type, you must be able to compare them and they must satisfy left <= right.", "A closed interval (in mathematics denoted by square brackets) contains its endpoints, i.e. the closed interval [0, 5] is characterized by the conditions 0 <= x <= 5. This is what closed='both' stands for. An open interval (in mathematics denoted by parentheses) does not contain its endpoints, i.e. the open interval (0, 5) is characterized by the conditions 0 < x < 5. This is what closed='neither' stands for. Intervals can also be half-open or half-closed, i.e. [0, 5) is described by 0 <= x < 5 (closed='left') and (0, 5] is described by 0 < x <= 5 (closed='right').", "Examples", "It is possible to build Intervals of different types, like numeric ones:", "You can check if an element belongs to it", "You can test the bounds (closed='right', so 0 < x <= 5):", "Calculate its length", "You can operate with + and * over an Interval and the operation is applied to each of its bounds, so the result depends on the type of the bound elements", "To create a time interval you can use Timestamps as the bounds", "Attributes", "closed", "Whether the interval is closed on the left-side, right-side, both or neither.", "closed_left", "Check if the interval is closed on the left side.", "closed_right", "Check if the interval is closed on the right side.", "is_empty", "Indicates if an interval is empty, meaning it contains no points.", "left", "Left bound for the interval.", "length", "Return the length of the Interval.", "mid", "Return the midpoint of the Interval.", "open_left", "Check if the interval is open on the left side.", "open_right", "Check if the interval is open on the right side.", "right", "Right bound for the interval.", "Methods", "overlaps", "Check whether two Interval objects overlap."]}, {"name": "pandas.Interval.closed", "path": "reference/api/pandas.interval.closed", "type": "Pandas arrays", "text": ["Whether the interval is closed on the left-side, right-side, both or neither."]}, {"name": "pandas.Interval.closed_left", "path": "reference/api/pandas.interval.closed_left", "type": "Pandas arrays", "text": ["Check if the interval is closed on the left side.", "For the meaning of closed and open see Interval.", "True if the Interval is closed on the left-side."]}, {"name": "pandas.Interval.closed_right", "path": "reference/api/pandas.interval.closed_right", "type": "Pandas arrays", "text": ["Check if the interval is closed on the right side.", "For the meaning of closed and open see Interval.", "True if the Interval is closed on the left-side."]}, {"name": "pandas.Interval.is_empty", "path": "reference/api/pandas.interval.is_empty", "type": "Pandas arrays", "text": ["Indicates if an interval is empty, meaning it contains no points.", "New in version 0.25.0.", "A boolean indicating if a scalar Interval is empty, or a boolean ndarray positionally indicating if an Interval in an IntervalArray or IntervalIndex is empty.", "Examples", "An Interval that contains points is not empty:", "An Interval that does not contain any points is empty:", "An Interval that contains a single point is not empty:", "An IntervalArray or IntervalIndex returns a boolean ndarray positionally indicating if an Interval is empty:", "Missing values are not considered empty:"]}, {"name": "pandas.Interval.left", "path": "reference/api/pandas.interval.left", "type": "Pandas arrays", "text": ["Left bound for the interval."]}, {"name": "pandas.Interval.length", "path": "reference/api/pandas.interval.length", "type": "Pandas arrays", "text": ["Return the length of the Interval."]}, {"name": "pandas.Interval.mid", "path": "reference/api/pandas.interval.mid", "type": "Pandas arrays", "text": ["Return the midpoint of the Interval."]}, {"name": "pandas.Interval.open_left", "path": "reference/api/pandas.interval.open_left", "type": "Pandas arrays", "text": ["Check if the interval is open on the left side.", "For the meaning of closed and open see Interval.", "True if the Interval is closed on the left-side."]}, {"name": "pandas.Interval.open_right", "path": "reference/api/pandas.interval.open_right", "type": "Pandas arrays", "text": ["Check if the interval is open on the right side.", "For the meaning of closed and open see Interval.", "True if the Interval is closed on the left-side."]}, {"name": "pandas.Interval.overlaps", "path": "reference/api/pandas.interval.overlaps", "type": "Pandas arrays", "text": ["Check whether two Interval objects overlap.", "Two intervals overlap if they share a common point, including closed endpoints. Intervals that only have an open endpoint in common do not overlap.", "Interval to check against for an overlap.", "True if the two intervals overlap.", "See also", "The corresponding method for IntervalArray.", "The corresponding method for IntervalIndex.", "Examples", "Intervals that share closed endpoints overlap:", "Intervals that only have an open endpoint in common do not overlap:"]}, {"name": "pandas.Interval.right", "path": "reference/api/pandas.interval.right", "type": "Pandas arrays", "text": ["Right bound for the interval."]}, {"name": "pandas.interval_range", "path": "reference/api/pandas.interval_range", "type": "General functions", "text": ["Return a fixed frequency IntervalIndex.", "Left bound for generating intervals.", "Right bound for generating intervals.", "Number of periods to generate.", "The length of each interval. Must be consistent with the type of start and end, e.g. 2 for numeric, or \u20185H\u2019 for datetime-like. Default is 1 for numeric and \u2018D\u2019 for datetime-like.", "Name of the resulting IntervalIndex.", "Whether the intervals are closed on the left-side, right-side, both or neither.", "See also", "An Index of intervals that are all closed on the same side.", "Notes", "Of the four parameters start, end, periods, and freq, exactly three must be specified. If freq is omitted, the resulting IntervalIndex will have periods linearly spaced elements between start and end, inclusively.", "To learn more about datetime-like frequency strings, please see this link.", "Examples", "Numeric start and end is supported.", "Additionally, datetime-like input is also supported.", "The freq parameter specifies the frequency between the left and right. endpoints of the individual intervals within the IntervalIndex. For numeric start and end, the frequency must also be numeric.", "Similarly, for datetime-like start and end, the frequency must be convertible to a DateOffset.", "Specify start, end, and periods; the frequency is generated automatically (linearly spaced).", "The closed parameter specifies which endpoints of the individual intervals within the IntervalIndex are closed."]}, {"name": "pandas.IntervalDtype", "path": "reference/api/pandas.intervaldtype", "type": "Pandas arrays", "text": ["An ExtensionDtype for Interval data.", "This is not an actual numpy dtype, but a duck type.", "The dtype of the Interval bounds.", "Examples", "Attributes", "subtype", "The dtype of the Interval bounds.", "Methods", "None"]}, {"name": "pandas.IntervalDtype.subtype", "path": "reference/api/pandas.intervaldtype.subtype", "type": "Pandas arrays", "text": ["The dtype of the Interval bounds."]}, {"name": "pandas.IntervalIndex", "path": "reference/api/pandas.intervalindex", "type": "Index Objects", "text": ["Immutable index of intervals that are closed on the same side.", "New in version 0.20.0.", "Array-like containing Interval objects from which to build the IntervalIndex.", "Whether the intervals are closed on the left-side, right-side, both or neither.", "If None, dtype will be inferred.", "Copy the input data.", "Name to be stored in the index.", "Verify that the IntervalIndex is valid.", "See also", "The base pandas Index type.", "A bounded slice-like interval; the elements of an IntervalIndex.", "Function to create a fixed frequency IntervalIndex.", "Bin values into discrete Intervals.", "Bin values into equal-sized Intervals based on rank or sample quantiles.", "Notes", "See the user guide for more.", "Examples", "A new IntervalIndex is typically constructed using interval_range():", "It may also be constructed using one of the constructor methods: IntervalIndex.from_arrays(), IntervalIndex.from_breaks(), and IntervalIndex.from_tuples().", "See further examples in the doc strings of interval_range and the mentioned constructor methods.", "Attributes", "closed", "Whether the intervals are closed on the left-side, right-side, both or neither.", "is_empty", "Indicates if an interval is empty, meaning it contains no points.", "is_non_overlapping_monotonic", "Return True if the IntervalArray is non-overlapping (no Intervals share points) and is either monotonic increasing or monotonic decreasing, else False.", "is_overlapping", "Return True if the IntervalIndex has overlapping intervals, else False.", "values", "Return an array representing the data in the Index.", "left", "right", "mid", "length", "Methods", "from_arrays(left, right[, closed, name, ...])", "Construct from two arrays defining the left and right bounds.", "from_tuples(data[, closed, name, copy, dtype])", "Construct an IntervalIndex from an array-like of tuples.", "from_breaks(breaks[, closed, name, copy, dtype])", "Construct an IntervalIndex from an array of splits.", "contains(*args, **kwargs)", "Check elementwise if the Intervals contain the value.", "overlaps(*args, **kwargs)", "Check elementwise if an Interval overlaps the values in the IntervalArray.", "set_closed(*args, **kwargs)", "Return an IntervalArray identical to the current one, but closed on the specified side.", "to_tuples(*args, **kwargs)", "Return an ndarray of tuples of the form (left, right)."]}, {"name": "pandas.IntervalIndex.closed", "path": "reference/api/pandas.intervalindex.closed", "type": "Index Objects", "text": ["Whether the intervals are closed on the left-side, right-side, both or neither."]}, {"name": "pandas.IntervalIndex.contains", "path": "reference/api/pandas.intervalindex.contains", "type": "Index Objects", "text": ["Check elementwise if the Intervals contain the value.", "Return a boolean mask whether the value is contained in the Intervals of the IntervalArray.", "New in version 0.25.0.", "The value to check whether it is contained in the Intervals.", "See also", "Check whether Interval object contains value.", "Check if an Interval overlaps the values in the IntervalArray.", "Examples"]}, {"name": "pandas.IntervalIndex.from_arrays", "path": "reference/api/pandas.intervalindex.from_arrays", "type": "Index Objects", "text": ["Construct from two arrays defining the left and right bounds.", "Left bounds for each interval.", "Right bounds for each interval.", "Whether the intervals are closed on the left-side, right-side, both or neither.", "Copy the data.", "If None, dtype will be inferred.", "When a value is missing in only one of left or right. When a value in left is greater than the corresponding value in right.", "See also", "Function to create a fixed frequency IntervalIndex.", "Construct an IntervalIndex from an array of splits.", "Construct an IntervalIndex from an array-like of tuples.", "Notes", "Each element of left must be less than or equal to the right element at the same position. If an element is missing, it must be missing in both left and right. A TypeError is raised when using an unsupported type for left or right. At the moment, \u2018category\u2019, \u2018object\u2019, and \u2018string\u2019 subtypes are not supported.", "Examples"]}, {"name": "pandas.IntervalIndex.from_breaks", "path": "reference/api/pandas.intervalindex.from_breaks", "type": "Index Objects", "text": ["Construct an IntervalIndex from an array of splits.", "Left and right bounds for each interval.", "Whether the intervals are closed on the left-side, right-side, both or neither.", "Copy the data.", "If None, dtype will be inferred.", "See also", "Function to create a fixed frequency IntervalIndex.", "Construct from a left and right array.", "Construct from a sequence of tuples.", "Examples"]}, {"name": "pandas.IntervalIndex.from_tuples", "path": "reference/api/pandas.intervalindex.from_tuples", "type": "Index Objects", "text": ["Construct an IntervalIndex from an array-like of tuples.", "Array of tuples.", "Whether the intervals are closed on the left-side, right-side, both or neither.", "By-default copy the data, this is compat only and ignored.", "If None, dtype will be inferred.", "See also", "Function to create a fixed frequency IntervalIndex.", "Construct an IntervalIndex from a left and right array.", "Construct an IntervalIndex from an array of splits.", "Examples"]}, {"name": "pandas.IntervalIndex.get_indexer", "path": "reference/api/pandas.intervalindex.get_indexer", "type": "Index Objects", "text": ["Compute indexer and mask for new index given the current index. The indexer should be then used as an input to ndarray.take to align the current data to the new index.", "default: exact matches only.", "pad / ffill: find the PREVIOUS index value if no exact match.", "backfill / bfill: use NEXT index value if no exact match", "nearest: use the NEAREST index value if no exact match. Tied distances are broken by preferring the larger index value.", "Maximum number of consecutive labels in target to match for inexact matches.", "Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance.", "Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index\u2019s type.", "Integers from 0 to n - 1 indicating that the index at these positions matches the corresponding target values. Missing values in the target are marked by -1.", "Notes", "Returns -1 for unmatched values, for further explanation see the example below.", "Examples", "Notice that the return value is an array of locations in index and x is marked by -1, as it is not in index."]}, {"name": "pandas.IntervalIndex.get_loc", "path": "reference/api/pandas.intervalindex.get_loc", "type": "Index Objects", "text": ["Get integer location, slice or boolean mask for requested label.", "default: matches where the label is within an interval only.", "Examples", "You can also supply a point inside an interval.", "If a label is in several intervals, you get the locations of all the relevant intervals.", "Only exact matches will be returned if an interval is provided."]}, {"name": "pandas.IntervalIndex.is_empty", "path": "reference/api/pandas.intervalindex.is_empty", "type": "Index Objects", "text": ["Indicates if an interval is empty, meaning it contains no points.", "New in version 0.25.0.", "A boolean indicating if a scalar Interval is empty, or a boolean ndarray positionally indicating if an Interval in an IntervalArray or IntervalIndex is empty.", "Examples", "An Interval that contains points is not empty:", "An Interval that does not contain any points is empty:", "An Interval that contains a single point is not empty:", "An IntervalArray or IntervalIndex returns a boolean ndarray positionally indicating if an Interval is empty:", "Missing values are not considered empty:"]}, {"name": "pandas.IntervalIndex.is_non_overlapping_monotonic", "path": "reference/api/pandas.intervalindex.is_non_overlapping_monotonic", "type": "Index Objects", "text": ["Return True if the IntervalArray is non-overlapping (no Intervals share points) and is either monotonic increasing or monotonic decreasing, else False."]}, {"name": "pandas.IntervalIndex.is_overlapping", "path": "reference/api/pandas.intervalindex.is_overlapping", "type": "Index Objects", "text": ["Return True if the IntervalIndex has overlapping intervals, else False.", "Two intervals overlap if they share a common point, including closed endpoints. Intervals that only have an open endpoint in common do not overlap.", "Boolean indicating if the IntervalIndex has overlapping intervals.", "See also", "Check whether two Interval objects overlap.", "Check an IntervalIndex elementwise for overlaps.", "Examples", "Intervals that share closed endpoints overlap:", "Intervals that only have an open endpoint in common do not overlap:"]}, {"name": "pandas.IntervalIndex.left", "path": "reference/api/pandas.intervalindex.left", "type": "Index Objects", "text": []}, {"name": "pandas.IntervalIndex.length", "path": "reference/api/pandas.intervalindex.length", "type": "Index Objects", "text": []}, {"name": "pandas.IntervalIndex.mid", "path": "reference/api/pandas.intervalindex.mid", "type": "Index Objects", "text": []}, {"name": "pandas.IntervalIndex.overlaps", "path": "reference/api/pandas.intervalindex.overlaps", "type": "Index Objects", "text": ["Check elementwise if an Interval overlaps the values in the IntervalArray.", "Two intervals overlap if they share a common point, including closed endpoints. Intervals that only have an open endpoint in common do not overlap.", "Interval to check against for an overlap.", "Boolean array positionally indicating where an overlap occurs.", "See also", "Check whether two Interval objects overlap.", "Examples", "Intervals that share closed endpoints overlap:", "Intervals that only have an open endpoint in common do not overlap:"]}, {"name": "pandas.IntervalIndex.right", "path": "reference/api/pandas.intervalindex.right", "type": "Index Objects", "text": []}, {"name": "pandas.IntervalIndex.set_closed", "path": "reference/api/pandas.intervalindex.set_closed", "type": "Index Objects", "text": ["Return an IntervalArray identical to the current one, but closed on the specified side.", "Whether the intervals are closed on the left-side, right-side, both or neither.", "Examples"]}, {"name": "pandas.IntervalIndex.to_tuples", "path": "reference/api/pandas.intervalindex.to_tuples", "type": "Index Objects", "text": ["Return an ndarray of tuples of the form (left, right).", "Returns NA as a tuple if True, (nan, nan), or just as the NA value itself if False, nan."]}, {"name": "pandas.IntervalIndex.values", "path": "reference/api/pandas.intervalindex.values", "type": "Index Objects", "text": ["Return an array representing the data in the Index.", "Warning", "We recommend using Index.array or Index.to_numpy(), depending on whether you need a reference to the underlying data or a NumPy array.", "See also", "Reference to the underlying data.", "A NumPy array representing the underlying data."]}, {"name": "pandas.io.formats.style.Styler", "path": "reference/api/pandas.io.formats.style.styler", "type": "Style", "text": ["Helps style a DataFrame or Series according to the data with HTML and CSS.", "Data to be styled - either a Series or DataFrame.", "Precision to round floats to. If not given defaults to pandas.options.styler.format.precision.", "Changed in version 1.4.0.", "List of {selector: (attr, value)} dicts; see Notes.", "A unique identifier to avoid CSS collisions; generated automatically.", "String caption to attach to the table. Tuple only used for LaTeX dual captions.", "Items that show up in the opening <table> tag in addition to automatic (by default) id.", "If True, each cell will have an id attribute in their HTML tag. The id takes the form T_<uuid>_row<num_row>_col<num_col> where <uuid> is the unique identifier, <num_row> is the row number and <num_col> is the column number.", "Representation for missing values. If na_rep is None, no special formatting is applied, and falls back to pandas.options.styler.format.na_rep.", "New in version 1.0.0.", "If uuid is not specified, the length of the uuid to randomly generate expressed in hex characters, in range [0, 32].", "New in version 1.2.0.", "Character used as decimal separator for floats, complex and integers. If not given uses pandas.options.styler.format.decimal.", "New in version 1.3.0.", "Character used as thousands separator for floats, complex and integers. If not given uses pandas.options.styler.format.thousands.", "New in version 1.3.0.", "Use \u2018html\u2019 to replace the characters &, <, >, ', and \" in cell display string with HTML-safe sequences. Use \u2018latex\u2019 to replace the characters &, %, $, #, _, {, }, ~, ^, and \\ in the cell display string with LaTeX-safe sequences. If not given uses pandas.options.styler.format.escape.", "New in version 1.3.0.", "Object to define how values are displayed. See Styler.format. If not given uses pandas.options.styler.format.formatter.", "New in version 1.4.0.", "See also", "Return a Styler object containing methods for building a styled HTML representation for the DataFrame.", "Notes", "Most styling will be done by passing style functions into Styler.apply or Styler.applymap. Style functions should return values with strings containing CSS 'attr: value' that will be applied to the indicated cells.", "If using in the Jupyter notebook, Styler has defined a _repr_html_ to automatically render itself. Otherwise call Styler.to_html to get the generated HTML.", "CSS classes are attached to the generated HTML", "Index and Column names include index_name and level<k> where k is its level in a MultiIndex", "Index label cells include", "row_heading", "row<n> where n is the numeric position of the row", "level<k> where k is the level in a MultiIndex", "Column label cells include * col_heading * col<n> where n is the numeric position of the column * level<k> where k is the level in a MultiIndex", "Blank cells include blank", "Data cells include data", "Trimmed cells include col_trim or row_trim.", "Any, or all, or these classes can be renamed by using the css_class_names argument in Styler.set_table_classes, giving a value such as {\u201crow\u201d: \u201cMY_ROW_CLASS\u201d, \u201ccol_trim\u201d: \u201c\u201d, \u201crow_trim\u201d: \u201c\u201d}.", "Attributes", "env", "(Jinja2 jinja2.Environment)", "template_html", "(Jinja2 Template)", "template_html_table", "(Jinja2 Template)", "template_html_style", "(Jinja2 Template)", "template_latex", "(Jinja2 Template)", "loader", "(Jinja2 Loader)", "Methods", "apply(func[, axis, subset])", "Apply a CSS-styling function column-wise, row-wise, or table-wise.", "apply_index(func[, axis, level])", "Apply a CSS-styling function to the index or column headers, level-wise.", "applymap(func[, subset])", "Apply a CSS-styling function elementwise.", "applymap_index(func[, axis, level])", "Apply a CSS-styling function to the index or column headers, elementwise.", "background_gradient([cmap, low, high, axis, ...])", "Color the background in a gradient style.", "bar([subset, axis, color, cmap, width, ...])", "Draw bar chart in the cell backgrounds.", "clear()", "Reset the Styler, removing any previously applied styles.", "export()", "Export the styles applied to the current Styler.", "format([formatter, subset, na_rep, ...])", "Format the text display value of cells.", "format_index([formatter, axis, level, ...])", "Format the text display value of index labels or column headers.", "from_custom_template(searchpath[, ...])", "Factory function for creating a subclass of Styler.", "hide([subset, axis, level, names])", "Hide the entire index / column headers, or specific rows / columns from display.", "hide_columns([subset, level, names])", "Hide the column headers or specific keys in the columns from rendering.", "hide_index([subset, level, names])", "(DEPRECATED) Hide the entire index, or specific keys in the index from rendering.", "highlight_between([subset, color, axis, ...])", "Highlight a defined range with a style.", "highlight_max([subset, color, axis, props])", "Highlight the maximum with a style.", "highlight_min([subset, color, axis, props])", "Highlight the minimum with a style.", "highlight_null([null_color, subset, props])", "Highlight missing values with a style.", "highlight_quantile([subset, color, axis, ...])", "Highlight values defined by a quantile with a style.", "pipe(func, *args, **kwargs)", "Apply func(self, *args, **kwargs), and return the result.", "render([sparse_index, sparse_columns])", "(DEPRECATED) Render the Styler including all applied styles to HTML.", "set_caption(caption)", "Set the text added to a <caption> HTML element.", "set_na_rep(na_rep)", "(DEPRECATED) Set the missing data representation on a Styler.", "set_precision(precision)", "(DEPRECATED) Set the precision used to display values.", "set_properties([subset])", "Set defined CSS-properties to each <td> HTML element within the given subset.", "set_sticky([axis, pixel_size, levels])", "Add CSS to permanently display the index or column headers in a scrolling frame.", "set_table_attributes(attributes)", "Set the table attributes added to the <table> HTML element.", "set_table_styles([table_styles, axis, ...])", "Set the table styles included within the <style> HTML element.", "set_td_classes(classes)", "Set the DataFrame of strings added to the class attribute of <td> HTML elements.", "set_tooltips(ttips[, props, css_class])", "Set the DataFrame of strings on Styler generating :hover tooltips.", "set_uuid(uuid)", "Set the uuid applied to id attributes of HTML elements.", "text_gradient([cmap, low, high, axis, ...])", "Color the text in a gradient style.", "to_excel(excel_writer[, sheet_name, na_rep, ...])", "Write Styler to an Excel sheet.", "to_html([buf, table_uuid, table_attributes, ...])", "Write Styler to a file, buffer or string in HTML-CSS format.", "to_latex([buf, column_format, position, ...])", "Write Styler to a file, buffer or string in LaTeX format.", "use(styles)", "Set the styles on the current Styler.", "where(cond, value[, other, subset])", "(DEPRECATED) Apply CSS-styles based on a conditional function elementwise."]}, {"name": "pandas.io.formats.style.Styler.apply", "path": "reference/api/pandas.io.formats.style.styler.apply", "type": "Style", "text": ["Apply a CSS-styling function column-wise, row-wise, or table-wise.", "Updates the HTML representation with the result.", "func should take a Series if axis in [0,1] and return a list-like object of same length, or a Series, not necessarily of same length, with valid index labels considering subset. func should take a DataFrame if axis is None and return either an ndarray with the same shape or a DataFrame, not necessarily of the same shape, with valid index and columns labels considering subset.", "Changed in version 1.3.0.", "Changed in version 1.4.0.", "Apply to each column (axis=0 or 'index'), to each row (axis=1 or 'columns'), or to the entire DataFrame at once with axis=None.", "A valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function.", "Pass along to func.", "See also", "Apply a CSS-styling function to headers elementwise.", "Apply a CSS-styling function to headers level-wise.", "Apply a CSS-styling function elementwise.", "Notes", "The elements of the output of func should be CSS styles as strings, in the format \u2018attribute: value; attribute2: value2; \u2026\u2019 or, if nothing is to be applied to that element, an empty string or None.", "This is similar to DataFrame.apply, except that axis=None applies the function to the entire DataFrame at once, rather than column-wise or row-wise.", "Examples", "Using subset to restrict application to a single column or multiple columns", "Using a 2d input to subset to select rows in addition to columns", "Using a function which returns a Series / DataFrame of unequal length but containing valid index labels", "See Table Visualization user guide for more details."]}, {"name": "pandas.io.formats.style.Styler.apply_index", "path": "reference/api/pandas.io.formats.style.styler.apply_index", "type": "Style", "text": ["Apply a CSS-styling function to the index or column headers, level-wise.", "Updates the HTML representation with the result.", "New in version 1.4.0.", "func should take a Series and return a string array of the same length.", "The headers over which to apply the function.", "If index is MultiIndex the level(s) over which to apply the function.", "Pass along to func.", "See also", "Apply a CSS-styling function to headers elementwise.", "Apply a CSS-styling function column-wise, row-wise, or table-wise.", "Apply a CSS-styling function elementwise.", "Notes", "Each input to func will be the index as a Series, if an Index, or a level of a MultiIndex. The output of func should be an identically sized array of CSS styles as strings, in the format \u2018attribute: value; attribute2: value2; \u2026\u2019 or, if nothing is to be applied to that element, an empty string or None.", "Examples", "Basic usage to conditionally highlight values in the index.", "Selectively applying to specific levels of MultiIndex columns."]}, {"name": "pandas.io.formats.style.Styler.applymap", "path": "reference/api/pandas.io.formats.style.styler.applymap", "type": "Style", "text": ["Apply a CSS-styling function elementwise.", "Updates the HTML representation with the result.", "func should take a scalar and return a string.", "A valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function.", "Pass along to func.", "See also", "Apply a CSS-styling function to headers elementwise.", "Apply a CSS-styling function to headers level-wise.", "Apply a CSS-styling function column-wise, row-wise, or table-wise.", "Notes", "The elements of the output of func should be CSS styles as strings, in the format \u2018attribute: value; attribute2: value2; \u2026\u2019 or, if nothing is to be applied to that element, an empty string or None.", "Examples", "Using subset to restrict application to a single column or multiple columns", "Using a 2d input to subset to select rows in addition to columns", "See Table Visualization user guide for more details."]}, {"name": "pandas.io.formats.style.Styler.applymap_index", "path": "reference/api/pandas.io.formats.style.styler.applymap_index", "type": "Style", "text": ["Apply a CSS-styling function to the index or column headers, elementwise.", "Updates the HTML representation with the result.", "New in version 1.4.0.", "func should take a scalar and return a string.", "The headers over which to apply the function.", "If index is MultiIndex the level(s) over which to apply the function.", "Pass along to func.", "See also", "Apply a CSS-styling function to headers level-wise.", "Apply a CSS-styling function column-wise, row-wise, or table-wise.", "Apply a CSS-styling function elementwise.", "Notes", "Each input to func will be an index value, if an Index, or a level value of a MultiIndex. The output of func should be CSS styles as a string, in the format \u2018attribute: value; attribute2: value2; \u2026\u2019 or, if nothing is to be applied to that element, an empty string or None.", "Examples", "Basic usage to conditionally highlight values in the index.", "Selectively applying to specific levels of MultiIndex columns."]}, {"name": "pandas.io.formats.style.Styler.background_gradient", "path": "reference/api/pandas.io.formats.style.styler.background_gradient", "type": "Style", "text": ["Color the background in a gradient style.", "The background color is determined according to the data in each column, row or frame, or by a given gradient map. Requires matplotlib.", "Matplotlib colormap.", "Compress the color range at the low end. This is a multiple of the data range to extend below the minimum; good values usually in [0, 1], defaults to 0.", "Compress the color range at the high end. This is a multiple of the data range to extend above the maximum; good values usually in [0, 1], defaults to 0.", "Apply to each column (axis=0 or 'index'), to each row (axis=1 or 'columns'), or to the entire DataFrame at once with axis=None.", "A valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function.", "Luminance threshold for determining text color in [0, 1]. Facilitates text visibility across varying background colors. All text is dark if 0, and light if 1, defaults to 0.408.", "Minimum data value that corresponds to colormap minimum value. If not specified the minimum value of the data (or gmap) will be used.", "New in version 1.0.0.", "Maximum data value that corresponds to colormap maximum value. If not specified the maximum value of the data (or gmap) will be used.", "New in version 1.0.0.", "Gradient map for determining the background colors. If not supplied will use the underlying data from rows, columns or frame. If given as an ndarray or list-like must be an identical shape to the underlying data considering axis and subset. If given as DataFrame or Series must have same index and column labels considering axis and subset. If supplied, vmin and vmax should be given relative to this gradient map.", "New in version 1.3.0.", "See also", "Color the text in a gradient style.", "Notes", "When using low and high the range of the gradient, given by the data if gmap is not given or by gmap, is extended at the low end effectively by map.min - low * map.range and at the high end by map.max + high * map.range before the colors are normalized and determined.", "If combining with vmin and vmax the map.min, map.max and map.range are replaced by values according to the values derived from vmin and vmax.", "This method will preselect numeric columns and ignore non-numeric columns unless a gmap is supplied in which case no preselection occurs.", "Examples", "Shading the values column-wise, with axis=0, preselecting numeric columns", "Shading all values collectively using axis=None", "Compress the color map from the both low and high ends", "Manually setting vmin and vmax gradient thresholds", "Setting a gmap and applying to all columns with another cmap", "Setting the gradient map for a dataframe (i.e. axis=None), we need to explicitly state subset to match the gmap shape"]}, {"name": "pandas.io.formats.style.Styler.bar", "path": "reference/api/pandas.io.formats.style.styler.bar", "type": "Style", "text": ["Draw bar chart in the cell backgrounds.", "Changed in version 1.4.0.", "A valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function.", "Apply to each column (axis=0 or 'index'), to each row (axis=1 or 'columns'), or to the entire DataFrame at once with axis=None.", "If a str is passed, the color is the same for both negative and positive numbers. If 2-tuple/list is used, the first element is the color_negative and the second is the color_positive (eg: [\u2018#d65f5f\u2019, \u2018#5fba7d\u2019]).", "A string name of a matplotlib Colormap, or a Colormap object. Cannot be used together with color.", "New in version 1.4.0.", "The percentage of the cell, measured from the left, in which to draw the bars, in [0, 100].", "The percentage height of the bar in the cell, centrally aligned, in [0,100].", "New in version 1.4.0.", "How to align the bars within the cells relative to a width adjusted center. If string must be one of:", "\u2018left\u2019 : bars are drawn rightwards from the minimum data value.", "\u2018right\u2019 : bars are drawn leftwards from the maximum data value.", "\u2018zero\u2019 : a value of zero is located at the center of the cell.", "\u2018mid\u2019 : a value of (max-min)/2 is located at the center of the cell, or if all values are negative (positive) the zero is aligned at the right (left) of the cell.", "\u2018mean\u2019 : the mean value of the data is located at the center of the cell.", "If a float or integer is given this will indicate the center of the cell.", "If a callable should take a 1d or 2d array and return a scalar.", "Changed in version 1.4.0.", "Minimum bar value, defining the left hand limit of the bar drawing range, lower values are clipped to vmin. When None (default): the minimum value of the data will be used.", "Maximum bar value, defining the right hand limit of the bar drawing range, higher values are clipped to vmax. When None (default): the maximum value of the data will be used.", "The base CSS of the cell that is extended to add the bar chart. Defaults to \u201cwidth: 10em;\u201d.", "New in version 1.4.0.", "Notes", "This section of the user guide: Table Visualization gives a number of examples for different settings and color coordination."]}, {"name": "pandas.io.formats.style.Styler.clear", "path": "reference/api/pandas.io.formats.style.styler.clear", "type": "Style", "text": ["Reset the Styler, removing any previously applied styles.", "Returns None."]}, {"name": "pandas.io.formats.style.Styler.env", "path": "reference/api/pandas.io.formats.style.styler.env", "type": "Style", "text": []}, {"name": "pandas.io.formats.style.Styler.export", "path": "reference/api/pandas.io.formats.style.styler.export", "type": "Style", "text": ["Export the styles applied to the current Styler.", "Can be applied to a second Styler with Styler.use.", "See also", "Set the styles on the current Styler.", "Create a copy of the current Styler.", "Notes", "This method is designed to copy non-data dependent attributes of one Styler to another. It differs from Styler.copy where data and data dependent attributes are also copied.", "The following items are exported since they are not generally data dependent:", "Styling functions added by the apply and applymap", "Whether axes and names are hidden from the display, if unambiguous.", "Table attributes", "Table styles", "The following attributes are considered data dependent and therefore not exported:", "Caption", "UUID", "Tooltips", "Any hidden rows or columns identified by Index labels", "Any formatting applied using Styler.format", "Any CSS classes added using Styler.set_td_classes", "Examples"]}, {"name": "pandas.io.formats.style.Styler.format", "path": "reference/api/pandas.io.formats.style.styler.format", "type": "Style", "text": ["Format the text display value of cells.", "Object to define how values are displayed. See notes.", "A valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function.", "Representation for missing values. If na_rep is None, no special formatting is applied.", "New in version 1.0.0.", "Floating point precision to use for display purposes, if not determined by the specified formatter.", "New in version 1.3.0.", "Character used as decimal separator for floats, complex and integers.", "New in version 1.3.0.", "Character used as thousands separator for floats, complex and integers.", "New in version 1.3.0.", "Use \u2018html\u2019 to replace the characters &, <, >, ', and \" in cell display string with HTML-safe sequences. Use \u2018latex\u2019 to replace the characters &, %, $, #, _, {, }, ~, ^, and \\ in the cell display string with LaTeX-safe sequences. Escaping is done before formatter.", "New in version 1.3.0.", "Convert string patterns containing https://, http://, ftp:// or www. to HTML <a> tags as clickable URL hyperlinks if \u201chtml\u201d, or LaTeX href commands if \u201clatex\u201d.", "New in version 1.4.0.", "Notes", "This method assigns a formatting function, formatter, to each cell in the DataFrame. If formatter is None, then the default formatter is used. If a callable then that function should take a data value as input and return a displayable representation, such as a string. If formatter is given as a string this is assumed to be a valid Python format specification and is wrapped to a callable as string.format(x). If a dict is given, keys should correspond to column names, and values should be string or callable, as above.", "The default formatter currently expresses floats and complex numbers with the pandas display precision unless using the precision argument here. The default formatter does not adjust the representation of missing values unless the na_rep argument is used.", "The subset argument defines which region to apply the formatting function to. If the formatter argument is given in dict form but does not include all columns within the subset then these columns will have the default formatter applied. Any columns in the formatter dict excluded from the subset will be ignored.", "When using a formatter string the dtypes must be compatible, otherwise a ValueError will be raised.", "When instantiating a Styler, default formatting can be applied be setting the pandas.options:", "styler.format.formatter: default None.", "styler.format.na_rep: default None.", "styler.format.precision: default 6.", "styler.format.decimal: default \u201c.\u201d.", "styler.format.thousands: default None.", "styler.format.escape: default None.", "Examples", "Using na_rep and precision with the default formatter", "Using a formatter specification on consistent column dtypes", "Using the default formatter for unspecified columns", "Multiple na_rep or precision specifications under the default formatter.", "Using a callable formatter function.", "Using a formatter with HTML escape and na_rep.", "Using a formatter with LaTeX escape."]}, {"name": "pandas.io.formats.style.Styler.format_index", "path": "reference/api/pandas.io.formats.style.styler.format_index", "type": "Style", "text": ["Format the text display value of index labels or column headers.", "New in version 1.4.0.", "Object to define how values are displayed. See notes.", "Whether to apply the formatter to the index or column headers.", "The level(s) over which to apply the generic formatter.", "Representation for missing values. If na_rep is None, no special formatting is applied.", "Floating point precision to use for display purposes, if not determined by the specified formatter.", "Character used as decimal separator for floats, complex and integers.", "Character used as thousands separator for floats, complex and integers.", "Use \u2018html\u2019 to replace the characters &, <, >, ', and \" in cell display string with HTML-safe sequences. Use \u2018latex\u2019 to replace the characters &, %, $, #, _, {, }, ~, ^, and \\ in the cell display string with LaTeX-safe sequences. Escaping is done before formatter.", "Convert string patterns containing https://, http://, ftp:// or www. to HTML <a> tags as clickable URL hyperlinks if \u201chtml\u201d, or LaTeX href commands if \u201clatex\u201d.", "Notes", "This method assigns a formatting function, formatter, to each level label in the DataFrame\u2019s index or column headers. If formatter is None, then the default formatter is used. If a callable then that function should take a label value as input and return a displayable representation, such as a string. If formatter is given as a string this is assumed to be a valid Python format specification and is wrapped to a callable as string.format(x). If a dict is given, keys should correspond to MultiIndex level numbers or names, and values should be string or callable, as above.", "The default formatter currently expresses floats and complex numbers with the pandas display precision unless using the precision argument here. The default formatter does not adjust the representation of missing values unless the na_rep argument is used.", "The level argument defines which levels of a MultiIndex to apply the method to. If the formatter argument is given in dict form but does not include all levels within the level argument then these unspecified levels will have the default formatter applied. Any levels in the formatter dict specifically excluded from the level argument will be ignored.", "When using a formatter string the dtypes must be compatible, otherwise a ValueError will be raised.", "Examples", "Using na_rep and precision with the default formatter", "Using a formatter specification on consistent dtypes in a level", "Using the default formatter for unspecified levels", "Using a callable formatter function.", "Using a formatter with HTML escape and na_rep.", "Using a formatter with LaTeX escape."]}, {"name": "pandas.io.formats.style.Styler.from_custom_template", "path": "reference/api/pandas.io.formats.style.styler.from_custom_template", "type": "Style", "text": ["Factory function for creating a subclass of Styler.", "Uses custom templates and Jinja environment.", "Changed in version 1.3.0.", "Path or paths of directories containing the templates.", "Name of your custom template to replace the html_table template.", "New in version 1.3.0.", "Name of your custom template to replace the html_style template.", "New in version 1.3.0.", "Has the correct env,``template_html``, template_html_table and template_html_style class attributes set."]}, {"name": "pandas.io.formats.style.Styler.hide", "path": "reference/api/pandas.io.formats.style.styler.hide", "type": "Style", "text": ["Hide the entire index / column headers, or specific rows / columns from display.", "New in version 1.4.0.", "A valid 1d input or single key along the axis within DataFrame.loc[<subset>, :] or DataFrame.loc[:, <subset>] depending upon axis, to limit data to select hidden rows / columns.", "Apply to the index or columns.", "The level(s) to hide in a MultiIndex if hiding the entire index / column headers. Cannot be used simultaneously with subset.", "Whether to hide the level name(s) of the index / columns headers in the case it (or at least one the levels) remains visible.", "Notes", "This method has multiple functionality depending upon the combination of the subset, level and names arguments (see examples). The axis argument is used only to control whether the method is applied to row or column headers:", "subset", "level", "names", "Effect", "None", "None", "False", "The axis-Index is hidden entirely.", "None", "None", "True", "Only the axis-Index names are hidden.", "None", "Int, Str, List", "False", "Specified axis-MultiIndex levels are hidden entirely.", "None", "Int, Str, List", "True", "Specified axis-MultiIndex levels are hidden entirely and the names of remaining axis-MultiIndex levels.", "Subset", "None", "False", "The specified data rows/columns are hidden, but the axis-Index itself, and names, remain unchanged.", "Subset", "None", "True", "The specified data rows/columns and axis-Index names are hidden, but the axis-Index itself remains unchanged.", "Subset", "Int, Str, List", "Boolean", "ValueError: cannot supply subset and level simultaneously.", "Note this method only hides the identifed elements so can be chained to hide multiple elements in sequence.", "Examples", "Simple application hiding specific rows:", "Hide the index and retain the data values:", "Hide specific rows in a MultiIndex but retain the index:", "Hide specific rows and the index through chaining:", "Hide a specific level:", "Hiding just the index level names:", "Examples all produce equivalently transposed effects with axis=\"columns\"."]}, {"name": "pandas.io.formats.style.Styler.hide_columns", "path": "reference/api/pandas.io.formats.style.styler.hide_columns", "type": "Style", "text": ["Hide the column headers or specific keys in the columns from rendering.", "This method has dual functionality:", "if subset is None then the entire column headers row, or specific levels, will be hidden whilst the data-values remain visible.", "if a subset is given then those specific columns, including the data-values will be hidden, whilst the column headers row remains visible.", "Changed in version 1.3.0.", "This method should be replaced by hide(axis=\"columns\", **kwargs)", "A valid 1d input or single key along the columns axis within DataFrame.loc[:, <subset>], to limit data to before applying the function.", "The level(s) to hide in a MultiIndex if hiding the entire column headers row. Cannot be used simultaneously with subset.", "New in version 1.4.0.", "Whether to hide the column index name(s), in the case all column headers, or some levels, are visible.", "New in version 1.4.0.", "See also", "Hide the entire index / columns, or specific rows / columns."]}, {"name": "pandas.io.formats.style.Styler.hide_index", "path": "reference/api/pandas.io.formats.style.styler.hide_index", "type": "Style", "text": ["Hide the entire index, or specific keys in the index from rendering.", "This method has dual functionality:", "if subset is None then the entire index, or specified levels, will be hidden whilst displaying all data-rows.", "if a subset is given then those specific rows will be hidden whilst the index itself remains visible.", "Changed in version 1.3.0.", "Deprecated since version 1.4.0: This method should be replaced by hide(axis=\"index\", **kwargs)", "A valid 1d input or single key along the index axis within DataFrame.loc[<subset>, :], to limit data to before applying the function.", "The level(s) to hide in a MultiIndex if hiding the entire index. Cannot be used simultaneously with subset.", "New in version 1.4.0.", "Whether to hide the index name(s), in the case the index or part of it remains visible.", "New in version 1.4.0.", "See also", "Hide the entire index / columns, or specific rows / columns."]}, {"name": "pandas.io.formats.style.Styler.highlight_between", "path": "reference/api/pandas.io.formats.style.styler.highlight_between", "type": "Style", "text": ["Highlight a defined range with a style.", "New in version 1.3.0.", "A valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function.", "Background color to use for highlighting.", "If left or right given as sequence, axis along which to apply those boundaries. See examples.", "Left bound for defining the range.", "Right bound for defining the range.", "Identify whether bounds are closed or open.", "CSS properties to use for highlighting. If props is given, color is not used.", "See also", "Highlight missing values with a style.", "Highlight the maximum with a style.", "Highlight the minimum with a style.", "Highlight values defined by a quantile with a style.", "Notes", "If left is None only the right bound is applied. If right is None only the left bound is applied. If both are None all values are highlighted.", "axis is only needed if left or right are provided as a sequence or an array-like object for aligning the shapes. If left and right are both scalars then all axis inputs will give the same result.", "This function only works with compatible dtypes. For example a datetime-like region can only use equivalent datetime-like left and right arguments. Use subset to control regions which have multiple dtypes.", "Examples", "Basic usage", "Using a range input sequnce along an axis, in this case setting a left and right for each column individually", "Using axis=None and providing the left argument as an array that matches the input DataFrame, with a constant right", "Using props instead of default background coloring"]}, {"name": "pandas.io.formats.style.Styler.highlight_max", "path": "reference/api/pandas.io.formats.style.styler.highlight_max", "type": "Style", "text": ["Highlight the maximum with a style.", "A valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function.", "Background color to use for highlighting.", "Apply to each column (axis=0 or 'index'), to each row (axis=1 or 'columns'), or to the entire DataFrame at once with axis=None.", "CSS properties to use for highlighting. If props is given, color is not used.", "New in version 1.3.0.", "See also", "Highlight missing values with a style.", "Highlight the minimum with a style.", "Highlight a defined range with a style.", "Highlight values defined by a quantile with a style."]}, {"name": "pandas.io.formats.style.Styler.highlight_min", "path": "reference/api/pandas.io.formats.style.styler.highlight_min", "type": "Style", "text": ["Highlight the minimum with a style.", "A valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function.", "Background color to use for highlighting.", "Apply to each column (axis=0 or 'index'), to each row (axis=1 or 'columns'), or to the entire DataFrame at once with axis=None.", "CSS properties to use for highlighting. If props is given, color is not used.", "New in version 1.3.0.", "See also", "Highlight missing values with a style.", "Highlight the maximum with a style.", "Highlight a defined range with a style.", "Highlight values defined by a quantile with a style."]}, {"name": "pandas.io.formats.style.Styler.highlight_null", "path": "reference/api/pandas.io.formats.style.styler.highlight_null", "type": "Style", "text": ["Highlight missing values with a style.", "A valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function.", "New in version 1.1.0.", "CSS properties to use for highlighting. If props is given, color is not used.", "New in version 1.3.0.", "See also", "Highlight the maximum with a style.", "Highlight the minimum with a style.", "Highlight a defined range with a style.", "Highlight values defined by a quantile with a style."]}, {"name": "pandas.io.formats.style.Styler.highlight_quantile", "path": "reference/api/pandas.io.formats.style.styler.highlight_quantile", "type": "Style", "text": ["Highlight values defined by a quantile with a style.", "New in version 1.3.0.", "A valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function.", "Background color to use for highlighting.", "Axis along which to determine and highlight quantiles. If None quantiles are measured over the entire DataFrame. See examples.", "Left bound, in [0, q_right), for the target quantile range.", "Right bound, in (q_left, 1], for the target quantile range.", "Argument passed to Series.quantile or DataFrame.quantile for quantile estimation.", "Identify whether quantile bounds are closed or open.", "CSS properties to use for highlighting. If props is given, color is not used.", "See also", "Highlight missing values with a style.", "Highlight the maximum with a style.", "Highlight the minimum with a style.", "Highlight a defined range with a style.", "Notes", "This function does not work with str dtypes.", "Examples", "Using axis=None and apply a quantile to all collective data", "Or highlight quantiles row-wise or column-wise, in this case by row-wise", "Use props instead of default background coloring"]}, {"name": "pandas.io.formats.style.Styler.loader", "path": "reference/api/pandas.io.formats.style.styler.loader", "type": "Style", "text": []}, {"name": "pandas.io.formats.style.Styler.pipe", "path": "reference/api/pandas.io.formats.style.styler.pipe", "type": "Style", "text": ["Apply func(self, *args, **kwargs), and return the result.", "Function to apply to the Styler. Alternatively, a (callable, keyword) tuple where keyword is a string indicating the keyword of callable that expects the Styler.", "Arguments passed to func.", "A dictionary of keyword arguments passed into func.", "The value returned by func.", "See also", "Analogous method for DataFrame.", "Apply a CSS-styling function column-wise, row-wise, or table-wise.", "Notes", "Like DataFrame.pipe(), this method can simplify the application of several user-defined functions to a styler. Instead of writing:", "users can write:", "In particular, this allows users to define functions that take a styler object, along with other parameters, and return the styler after making styling changes (such as calling Styler.apply() or Styler.set_properties()). Using .pipe, these user-defined style \u201ctransformations\u201d can be interleaved with calls to the built-in Styler interface.", "Examples", "The user-defined format_conversion function above can be called within a sequence of other style modifications:"]}, {"name": "pandas.io.formats.style.Styler.render", "path": "reference/api/pandas.io.formats.style.styler.render", "type": "Style", "text": ["Render the Styler including all applied styles to HTML.", "Deprecated since version 1.4.0.", "Whether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. Defaults to pandas.options.styler.sparse.index value.", "Whether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. Defaults to pandas.options.styler.sparse.columns value.", "Any additional keyword arguments are passed through to self.template.render. This is useful when you need to provide additional variables for a custom template.", "The rendered HTML.", "Notes", "This method is deprecated in favour of Styler.to_html.", "Styler objects have defined the _repr_html_ method which automatically calls self.to_html() when it\u2019s the last item in a Notebook cell.", "When calling Styler.render() directly, wrap the result in IPython.display.HTML to view the rendered HTML in the notebook.", "Pandas uses the following keys in render. Arguments passed in **kwargs take precedence, so think carefully if you want to override them:", "head", "cellstyle", "body", "uuid", "table_styles", "caption", "table_attributes"]}, {"name": "pandas.io.formats.style.Styler.set_caption", "path": "reference/api/pandas.io.formats.style.styler.set_caption", "type": "Style", "text": ["Set the text added to a <caption> HTML element.", "For HTML output either the string input is used or the first element of the tuple. For LaTeX the string input provides a caption and the additional tuple input allows for full captions and short captions, in that order."]}, {"name": "pandas.io.formats.style.Styler.set_na_rep", "path": "reference/api/pandas.io.formats.style.styler.set_na_rep", "type": "Style", "text": ["Set the missing data representation on a Styler.", "New in version 1.0.0.", "Deprecated since version 1.3.0.", "Notes", "This method is deprecated. See Styler.format()"]}, {"name": "pandas.io.formats.style.Styler.set_precision", "path": "reference/api/pandas.io.formats.style.styler.set_precision", "type": "Style", "text": ["Set the precision used to display values.", "Deprecated since version 1.3.0.", "Notes", "This method is deprecated see Styler.format."]}, {"name": "pandas.io.formats.style.Styler.set_properties", "path": "reference/api/pandas.io.formats.style.styler.set_properties", "type": "Style", "text": ["Set defined CSS-properties to each <td> HTML element within the given subset.", "A valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function.", "A dictionary of property, value pairs to be set for each cell.", "Notes", "This is a convenience methods which wraps the Styler.applymap() calling a function returning the CSS-properties independently of the data.", "Examples", "See Table Visualization user guide for more details."]}, {"name": "pandas.io.formats.style.Styler.set_sticky", "path": "reference/api/pandas.io.formats.style.styler.set_sticky", "type": "Style", "text": ["Add CSS to permanently display the index or column headers in a scrolling frame.", "Whether to make the index or column headers sticky.", "Required to configure the width of index cells or the height of column header cells when sticking a MultiIndex (or with a named Index). Defaults to 75 and 25 respectively.", "If axis is a MultiIndex the specific levels to stick. If None will stick all levels.", "Notes", "This method uses the CSS \u2018position: sticky;\u2019 property to display. It is designed to work with visible axes, therefore both:", "styler.set_sticky(axis=\u201dindex\u201d).hide(axis=\u201dindex\u201d)", "styler.set_sticky(axis=\u201dcolumns\u201d).hide(axis=\u201dcolumns\u201d)", "may produce strange behaviour due to CSS controls with missing elements."]}, {"name": "pandas.io.formats.style.Styler.set_table_attributes", "path": "reference/api/pandas.io.formats.style.styler.set_table_attributes", "type": "Style", "text": ["Set the table attributes added to the <table> HTML element.", "These are items in addition to automatic (by default) id attribute.", "See also", "Set the table styles included within the <style> HTML element.", "Set the DataFrame of strings added to the class attribute of <td> HTML elements.", "Examples"]}, {"name": "pandas.io.formats.style.Styler.set_table_styles", "path": "reference/api/pandas.io.formats.style.styler.set_table_styles", "type": "Style", "text": ["Set the table styles included within the <style> HTML element.", "This function can be used to style the entire table, columns, rows or specific HTML selectors.", "If supplying a list, each individual table_style should be a dictionary with selector and props keys. selector should be a CSS selector that the style will be applied to (automatically prefixed by the table\u2019s UUID) and props should be a list of tuples with (attribute, value). If supplying a dict, the dict keys should correspond to column names or index values, depending upon the specified axis argument. These will be mapped to row or col CSS selectors. MultiIndex values as dict keys should be in their respective tuple form. The dict values should be a list as specified in the form with CSS selectors and props that will be applied to the specified row or column.", "Changed in version 1.2.0.", "Apply to each column (axis=0 or 'index'), to each row (axis=1 or 'columns'). Only used if table_styles is dict.", "New in version 1.2.0.", "Styles are replaced if True, or extended if False. CSS rules are preserved so most recent styles set will dominate if selectors intersect.", "New in version 1.2.0.", "A dict of strings used to replace the default CSS classes described below.", "New in version 1.4.0.", "See also", "Set the DataFrame of strings added to the class attribute of <td> HTML elements.", "Set the table attributes added to the <table> HTML element.", "Notes", "The default CSS classes dict, whose values can be replaced is as follows:", "Examples", "Or with CSS strings", "Adding column styling by name", "Adding row styling", "See Table Visualization user guide for more details."]}, {"name": "pandas.io.formats.style.Styler.set_td_classes", "path": "reference/api/pandas.io.formats.style.styler.set_td_classes", "type": "Style", "text": ["Set the DataFrame of strings added to the class attribute of <td> HTML elements.", "DataFrame containing strings that will be translated to CSS classes, mapped by identical column and index key values that must exist on the underlying Styler data. None, NaN values, and empty strings will be ignored and not affect the rendered HTML.", "See also", "Set the table styles included within the <style> HTML element.", "Set the table attributes added to the <table> HTML element.", "Notes", "Can be used in combination with Styler.set_table_styles to define an internal CSS solution without reference to external CSS files.", "Examples", "Using MultiIndex columns and a classes DataFrame as a subset of the underlying,", "Form of the output with new additional css classes,"]}, {"name": "pandas.io.formats.style.Styler.set_tooltips", "path": "reference/api/pandas.io.formats.style.styler.set_tooltips", "type": "Style", "text": ["Set the DataFrame of strings on Styler generating :hover tooltips.", "These string based tooltips are only applicable to <td> HTML elements, and cannot be used for column or index headers.", "New in version 1.3.0.", "DataFrame containing strings that will be translated to tooltips, mapped by identical column and index values that must exist on the underlying Styler data. None, NaN values, and empty strings will be ignored and not affect the rendered HTML.", "List of (attr, value) tuples or a valid CSS string. If None adopts the internal default values described in notes.", "Name of the tooltip class used in CSS, should conform to HTML standards. Only useful if integrating tooltips with external CSS. If None uses the internal default value \u2018pd-t\u2019.", "Notes", "Tooltips are created by adding <span class=\u201dpd-t\u201d></span> to each data cell and then manipulating the table level CSS to attach pseudo hover and pseudo after selectors to produce the required the results.", "The default properties for the tooltip CSS class are:", "visibility: hidden", "position: absolute", "z-index: 1", "background-color: black", "color: white", "transform: translate(-20px, -20px)", "The property \u2018visibility: hidden;\u2019 is a key prerequisite to the hover functionality, and should always be included in any manual properties specification, using the props argument.", "Tooltips are not designed to be efficient, and can add large amounts of additional HTML for larger tables, since they also require that cell_ids is forced to True.", "Examples", "Basic application", "Optionally controlling the tooltip visual display"]}, {"name": "pandas.io.formats.style.Styler.set_uuid", "path": "reference/api/pandas.io.formats.style.styler.set_uuid", "type": "Style", "text": ["Set the uuid applied to id attributes of HTML elements.", "Notes", "Almost all HTML elements within the table, and including the <table> element are assigned id attributes. The format is T_uuid_<extra> where <extra> is typically a more specific identifier, such as row1_col2."]}, {"name": "pandas.io.formats.style.Styler.template_html", "path": "reference/api/pandas.io.formats.style.styler.template_html", "type": "Style", "text": []}, {"name": "pandas.io.formats.style.Styler.template_html_style", "path": "reference/api/pandas.io.formats.style.styler.template_html_style", "type": "Style", "text": []}, {"name": "pandas.io.formats.style.Styler.template_html_table", "path": "reference/api/pandas.io.formats.style.styler.template_html_table", "type": "Style", "text": []}, {"name": "pandas.io.formats.style.Styler.template_latex", "path": "reference/api/pandas.io.formats.style.styler.template_latex", "type": "Style", "text": []}, {"name": "pandas.io.formats.style.Styler.text_gradient", "path": "reference/api/pandas.io.formats.style.styler.text_gradient", "type": "Style", "text": ["Color the text in a gradient style.", "The text color is determined according to the data in each column, row or frame, or by a given gradient map. Requires matplotlib.", "Matplotlib colormap.", "Compress the color range at the low end. This is a multiple of the data range to extend below the minimum; good values usually in [0, 1], defaults to 0.", "Compress the color range at the high end. This is a multiple of the data range to extend above the maximum; good values usually in [0, 1], defaults to 0.", "Apply to each column (axis=0 or 'index'), to each row (axis=1 or 'columns'), or to the entire DataFrame at once with axis=None.", "A valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function.", "This argument is ignored (only used in background_gradient). Luminance threshold for determining text color in [0, 1]. Facilitates text visibility across varying background colors. All text is dark if 0, and light if 1, defaults to 0.408.", "Minimum data value that corresponds to colormap minimum value. If not specified the minimum value of the data (or gmap) will be used.", "New in version 1.0.0.", "Maximum data value that corresponds to colormap maximum value. If not specified the maximum value of the data (or gmap) will be used.", "New in version 1.0.0.", "Gradient map for determining the text colors. If not supplied will use the underlying data from rows, columns or frame. If given as an ndarray or list-like must be an identical shape to the underlying data considering axis and subset. If given as DataFrame or Series must have same index and column labels considering axis and subset. If supplied, vmin and vmax should be given relative to this gradient map.", "New in version 1.3.0.", "See also", "Color the background in a gradient style.", "Notes", "When using low and high the range of the gradient, given by the data if gmap is not given or by gmap, is extended at the low end effectively by map.min - low * map.range and at the high end by map.max + high * map.range before the colors are normalized and determined.", "If combining with vmin and vmax the map.min, map.max and map.range are replaced by values according to the values derived from vmin and vmax.", "This method will preselect numeric columns and ignore non-numeric columns unless a gmap is supplied in which case no preselection occurs.", "Examples", "Shading the values column-wise, with axis=0, preselecting numeric columns", "Shading all values collectively using axis=None", "Compress the color map from the both low and high ends", "Manually setting vmin and vmax gradient thresholds", "Setting a gmap and applying to all columns with another cmap", "Setting the gradient map for a dataframe (i.e. axis=None), we need to explicitly state subset to match the gmap shape"]}, {"name": "pandas.io.formats.style.Styler.to_excel", "path": "reference/api/pandas.io.formats.style.styler.to_excel", "type": "Style", "text": ["Write Styler to an Excel sheet.", "To write a single Styler to an Excel .xlsx file it is only necessary to specify a target file name. To write to multiple sheets it is necessary to create an ExcelWriter object with a target file name, and specify a sheet in the file to write to.", "Multiple sheets may be written to by specifying unique sheet_name. With all data written to the file it is necessary to save the changes. Note that creating an ExcelWriter object with a file name that already exists will result in the contents of the existing file being erased.", "File path or existing ExcelWriter.", "Name of sheet which will contain DataFrame.", "Missing data representation.", "Format string for floating point numbers. For example float_format=\"%.2f\" will format 0.1234 to 0.12.", "Columns to write.", "Write out the column names. If a list of string is given it is assumed to be aliases for the column names.", "Write row names (index).", "Column label for index column(s) if desired. If not specified, and header and index are True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex.", "Upper left cell row to dump data frame.", "Upper left cell column to dump data frame.", "Write engine to use, \u2018openpyxl\u2019 or \u2018xlsxwriter\u2019. You can also set this via the options io.excel.xlsx.writer, io.excel.xls.writer, and io.excel.xlsm.writer.", "Deprecated since version 1.2.0: As the xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas.", "Write MultiIndex and Hierarchical Rows as merged cells.", "Encoding of the resulting excel file. Only necessary for xlwt, other writers support unicode natively.", "Representation for infinity (there is no native representation for infinity in Excel).", "Display more information in the error logs.", "Specifies the one-based bottommost row and rightmost column that is to be frozen.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.0.", "See also", "Write DataFrame to a comma-separated values (csv) file.", "Class for writing DataFrame objects into excel sheets.", "Read an Excel file into a pandas DataFrame.", "Read a comma-separated values (csv) file into DataFrame.", "Notes", "For compatibility with to_csv(), to_excel serializes lists and dicts to strings before writing.", "Once a workbook has been saved it is not possible to write further data without rewriting the whole workbook.", "Examples", "Create, write to and save a workbook:", "To specify the sheet name:", "If you wish to write to more than one sheet in the workbook, it is necessary to specify an ExcelWriter object:", "ExcelWriter can also be used to append to an existing Excel file:", "To set the library that is used to write the Excel file, you can pass the engine keyword (the default engine is automatically chosen depending on the file extension):"]}, {"name": "pandas.io.formats.style.Styler.to_html", "path": "reference/api/pandas.io.formats.style.styler.to_html", "type": "Style", "text": ["Write Styler to a file, buffer or string in HTML-CSS format.", "New in version 1.3.0.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a string write() function. If None, the result is returned as a string.", "Id attribute assigned to the <table> HTML element in the format:", "<table id=\"T_<table_uuid>\" ..>", "If not given uses Styler\u2019s initially assigned value.", "Attributes to assign within the <table> HTML element in the format:", "<table .. <table_attributes> >", "If not given defaults to Styler\u2019s preexisting value.", "Whether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. Defaults to pandas.options.styler.sparse.index value.", "New in version 1.4.0.", "Whether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each column. Defaults to pandas.options.styler.sparse.columns value.", "New in version 1.4.0.", "Adds \u201cfont-weight: bold;\u201d as a CSS property to table style header cells.", "New in version 1.4.0.", "Set, or overwrite, the caption on Styler before rendering.", "New in version 1.4.0.", "The maximum number of rows that will be rendered. Defaults to pandas.options.styler.render.max_rows/max_columns.", "New in version 1.4.0.", "The maximum number of columns that will be rendered. Defaults to pandas.options.styler.render.max_columns, which is None.", "Rows and columns may be reduced if the number of total elements is large. This value is set to pandas.options.styler.render.max_elements, which is 262144 (18 bit browser rendering).", "New in version 1.4.0.", "Character encoding setting for file output, and HTML meta tags. Defaults to pandas.options.styler.render.encoding value of \u201cutf-8\u201d.", "Whether to output a fully structured HTML file including all HTML elements, or just the core <style> and <table> elements.", "Whether to include the <style> element and all associated element class and id identifiers, or solely the <table> element without styling identifiers.", "Any additional keyword arguments are passed through to the jinja2 self.template.render process. This is useful when you need to provide additional variables for a custom template.", "If buf is None, returns the result as a string. Otherwise returns None.", "See also", "Write a DataFrame to a file, buffer or string in HTML format."]}, {"name": "pandas.io.formats.style.Styler.to_latex", "path": "reference/api/pandas.io.formats.style.styler.to_latex", "type": "Style", "text": ["Write Styler to a file, buffer or string in LaTeX format.", "New in version 1.3.0.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a string write() function. If None, the result is returned as a string.", "The LaTeX column specification placed in location:", "\\begin{tabular}{<column_format>}", "Defaults to \u2018l\u2019 for index and non-numeric data columns, and, for numeric data columns, to \u2018r\u2019 by default, or \u2018S\u2019 if siunitx is True.", "The LaTeX positional argument (e.g. \u2018h!\u2019) for tables, placed in location:", "\\\\begin{table}[<position>].", "The LaTeX float command placed in location:", "\\begin{table}[<position>]", "\\<position_float>", "Cannot be used if environment is \u201clongtable\u201d.", "Set to True to add \\toprule, \\midrule and \\bottomrule from the {booktabs} LaTeX package. Defaults to pandas.options.styler.latex.hrules, which is False.", "Changed in version 1.4.0.", "Use to control adding \\cline commands for the index labels separation. Possible values are:", "None: no cline commands are added (default).", "\u201call;data\u201d: a cline is added for every index value extending the width of the table, including data entries.", "\u201call;index\u201d: as above with lines extending only the width of the index entries.", "\u201cskip-last;data\u201d: a cline is added for each index value except the last level (which is never sparsified), extending the widtn of the table.", "\u201cskip-last;index\u201d: as above with lines extending only the width of the index entries.", "New in version 1.4.0.", "The LaTeX label included as: \\label{<label>}. This is used with \\ref{<label>} in the main .tex file.", "If string, the LaTeX table caption included as: \\caption{<caption>}. If tuple, i.e (\u201cfull caption\u201d, \u201cshort caption\u201d), the caption included as: \\caption[<caption[1]>]{<caption[0]>}.", "Whether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. Defaults to pandas.options.styler.sparse.index, which is True.", "Whether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each column. Defaults to pandas.options.styler.sparse.columns, which is True.", "If sparsifying hierarchical MultiIndexes whether to align text centrally, at the top or bottom using the multirow package. If not given defaults to pandas.options.styler.latex.multirow_align, which is \u201cc\u201d. If \u201cnaive\u201d is given renders without multirow.", "Changed in version 1.4.0.", "If sparsifying hierarchical MultiIndex columns whether to align text at the left, centrally, or at the right. If not given defaults to pandas.options.styler.latex.multicol_align, which is \u201cr\u201d. If a naive option is given renders without multicol. Pipe decorators can also be added to non-naive values to draw vertical rules, e.g. \u201c|r\u201d will draw a rule on the left side of right aligned merged cells.", "Changed in version 1.4.0.", "Set to True to structure LaTeX compatible with the {siunitx} package.", "If given, the environment that will replace \u2018table\u2019 in \\\\begin{table}. If \u2018longtable\u2019 is specified then a more suitable template is rendered. If not given defaults to pandas.options.styler.latex.environment, which is None.", "New in version 1.4.0.", "Character encoding setting. Defaults to pandas.options.styler.render.encoding, which is \u201cutf-8\u201d.", "Convert simple cell-styles from CSS to LaTeX format. Any CSS not found in conversion table is dropped. A style can be forced by adding option \u2013latex. See notes.", "If buf is None, returns the result as a string. Otherwise returns None.", "See also", "Format the text display value of cells.", "Notes", "Latex Packages", "For the following features we recommend the following LaTeX inclusions:", "Feature", "Inclusion", "sparse columns", "none: included within default {tabular} environment", "sparse rows", "\\usepackage{multirow}", "hrules", "\\usepackage{booktabs}", "colors", "\\usepackage[table]{xcolor}", "siunitx", "\\usepackage{siunitx}", "bold (with siunitx)", "italic (with siunitx)", "environment", "\\usepackage{longtable} if arg is \u201clongtable\u201d | or any other relevant environment package", "hyperlinks", "\\usepackage{hyperref}", "Cell Styles", "LaTeX styling can only be rendered if the accompanying styling functions have been constructed with appropriate LaTeX commands. All styling functionality is built around the concept of a CSS (<attribute>, <value>) pair (see Table Visualization), and this should be replaced by a LaTeX (<command>, <options>) approach. Each cell will be styled individually using nested LaTeX commands with their accompanied options.", "For example the following code will highlight and bold a cell in HTML-CSS:", "The equivalent using LaTeX only commands is the following:", "Internally these structured LaTeX (<command>, <options>) pairs are translated to the display_value with the default structure: \\<command><options> <display_value>. Where there are multiple commands the latter is nested recursively, so that the above example highlighed cell is rendered as \\cellcolor{red} \\bfseries 4.", "Occasionally this format does not suit the applied command, or combination of LaTeX packages that is in use, so additional flags can be added to the <options>, within the tuple, to result in different positions of required braces (the default being the same as --nowrap):", "Tuple Format", "Output Structure", "(<command>,<options>)", "\\<command><options> <display_value>", "(<command>,<options> --nowrap)", "\\<command><options> <display_value>", "(<command>,<options> --rwrap)", "\\<command><options>{<display_value>}", "(<command>,<options> --wrap)", "{\\<command><options> <display_value>}", "(<command>,<options> --lwrap)", "{\\<command><options>} <display_value>", "(<command>,<options> --dwrap)", "{\\<command><options>}{<display_value>}", "For example the textbf command for font-weight should always be used with \u2013rwrap so ('textbf', '--rwrap') will render a working cell, wrapped with braces, as \\textbf{<display_value>}.", "A more comprehensive example is as follows:", "Table Styles", "Internally Styler uses its table_styles object to parse the column_format, position, position_float, and label input arguments. These arguments are added to table styles in the format:", "Exception is made for the hrules argument which, in fact, controls all three commands: toprule, bottomrule and midrule simultaneously. Instead of setting hrules to True, it is also possible to set each individual rule definition, by manually setting the table_styles, for example below we set a regular toprule, set an hline for bottomrule and exclude the midrule:", "If other commands are added to table styles they will be detected, and positioned immediately above the \u2018\\begin{tabular}\u2019 command. For example to add odd and even row coloring, from the {colortbl} package, in format \\rowcolors{1}{pink}{red}, use:", "A more comprehensive example using these arguments is as follows:", "Formatting", "To format values Styler.format() should be used prior to calling Styler.to_latex, as well as other methods such as Styler.hide() for example:", "CSS Conversion", "This method can convert a Styler constructured with HTML-CSS to LaTeX using the following limited conversions.", "CSS Attribute", "CSS value", "LaTeX Command", "LaTeX Options", "font-weight", "font-style", "background-color", "cellcolor", "color", "color", "It is also possible to add user-defined LaTeX only styles to a HTML-CSS Styler using the --latex flag, and to add LaTeX parsing options that the converter will detect within a CSS-comment.", "Examples", "Below we give a complete step by step example adding some advanced features and noting some common gotchas.", "First we create the DataFrame and Styler as usual, including MultiIndex rows and columns, which allow for more advanced formatting options:", "Second we will format the display and, since our table is quite wide, will hide the repeated level-0 of the index:", "Note that one of the string entries of the index and column headers is \u201cH&M\u201d. Without applying the escape=\u201dlatex\u201d option to the format_index method the resultant LaTeX will fail to render, and the error returned is quite difficult to debug. Using the appropriate escape the \u201c&\u201d is converted to \u201c\\&\u201d.", "Thirdly we will apply some (CSS-HTML) styles to our object. We will use a builtin method and also define our own method to highlight the stock recommendation:", "All the above styles will work with HTML (see below) and LaTeX upon conversion:", "However, we finally want to add one LaTeX only style (from the {graphicx} package), that is not easy to convert from CSS and pandas does not support it. Notice the \u2013latex flag used here, as well as \u2013rwrap to ensure this is formatted correctly and not ignored upon conversion.", "Finally we render our LaTeX adding in other options as required:"]}, {"name": "pandas.io.formats.style.Styler.use", "path": "reference/api/pandas.io.formats.style.styler.use", "type": "Style", "text": ["Set the styles on the current Styler.", "Possibly uses styles from Styler.export.", "\u201capply\u201d: list of styler functions, typically added with apply or applymap.", "\u201ctable_attributes\u201d: HTML attributes, typically added with set_table_attributes.", "\u201ctable_styles\u201d: CSS selectors and properties, typically added with set_table_styles.", "\u201chide_index\u201d: whether the index is hidden, typically added with hide_index, or a boolean list for hidden levels.", "\u201chide_columns\u201d: whether column headers are hidden, typically added with hide_columns, or a boolean list for hidden levels.", "\u201chide_index_names\u201d: whether index names are hidden.", "\u201chide_column_names\u201d: whether column header names are hidden.", "\u201ccss\u201d: the css class names used.", "See also", "Export the non data dependent attributes to the current Styler.", "Examples"]}, {"name": "pandas.io.formats.style.Styler.where", "path": "reference/api/pandas.io.formats.style.styler.where", "type": "Style", "text": ["Apply CSS-styles based on a conditional function elementwise.", "Deprecated since version 1.3.0.", "Updates the HTML representation with a style which is selected in accordance with the return value of a function.", "cond should take a scalar, and optional keyword arguments, and return a boolean.", "Applied when cond returns true.", "Applied when cond returns false.", "A valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function.", "Pass along to cond.", "See also", "Apply a CSS-styling function elementwise.", "Apply a CSS-styling function column-wise, row-wise, or table-wise.", "Notes", "This method is deprecated.", "This method is a convenience wrapper for Styler.applymap(), which we recommend using instead.", "The example:", "should be refactored to:"]}, {"name": "pandas.io.json.build_table_schema", "path": "reference/api/pandas.io.json.build_table_schema", "type": "Input/output", "text": ["Create a Table schema from data.", "Whether to include data.index in the schema.", "Column names to designate as the primary key. The default None will set \u2018primaryKey\u2019 to the index level or levels if the index is unique.", "Whether to include a field pandas_version with the version of pandas that last revised the table schema. This version can be different from the installed pandas version.", "Notes", "See Table Schema for conversion types. Timedeltas as converted to ISO8601 duration format with 9 decimal places after the seconds field for nanosecond precision.", "Categoricals are converted to the any dtype, and use the enum field constraint to list the allowed values. The ordered attribute is included in an ordered field.", "Examples"]}, {"name": "pandas.io.stata.StataReader.data_label", "path": "reference/api/pandas.io.stata.statareader.data_label", "type": "Input/output", "text": ["Return data label of Stata file."]}, {"name": "pandas.io.stata.StataReader.value_labels", "path": "reference/api/pandas.io.stata.statareader.value_labels", "type": "Input/output", "text": ["Return a dict, associating each variable name a dict, associating each value its corresponding label."]}, {"name": "pandas.io.stata.StataReader.variable_labels", "path": "reference/api/pandas.io.stata.statareader.variable_labels", "type": "Input/output", "text": ["Return variable labels as a dict, associating each variable name with corresponding label."]}, {"name": "pandas.io.stata.StataWriter.write_file", "path": "reference/api/pandas.io.stata.statawriter.write_file", "type": "Input/output", "text": ["Export DataFrame object to Stata dta format."]}, {"name": "pandas.isna", "path": "reference/api/pandas.isna", "type": "General functions", "text": ["Detect missing values for an array-like object.", "This function takes a scalar or array-like object and indicates whether values are missing (NaN in numeric arrays, None or NaN in object arrays, NaT in datetimelike).", "Object to check for null or missing values.", "For scalar input, returns a scalar boolean. For array input, returns an array of boolean indicating whether each corresponding element is missing.", "See also", "Boolean inverse of pandas.isna.", "Detect missing values in a Series.", "Detect missing values in a DataFrame.", "Detect missing values in an Index.", "Examples", "Scalar arguments (including strings) result in a scalar boolean.", "ndarrays result in an ndarray of booleans.", "For indexes, an ndarray of booleans is returned.", "For Series and DataFrame, the same type is returned, containing booleans."]}, {"name": "pandas.isnull", "path": "reference/api/pandas.isnull", "type": "General functions", "text": ["Detect missing values for an array-like object.", "This function takes a scalar or array-like object and indicates whether values are missing (NaN in numeric arrays, None or NaN in object arrays, NaT in datetimelike).", "Object to check for null or missing values.", "For scalar input, returns a scalar boolean. For array input, returns an array of boolean indicating whether each corresponding element is missing.", "See also", "Boolean inverse of pandas.isna.", "Detect missing values in a Series.", "Detect missing values in a DataFrame.", "Detect missing values in an Index.", "Examples", "Scalar arguments (including strings) result in a scalar boolean.", "ndarrays result in an ndarray of booleans.", "For indexes, an ndarray of booleans is returned.", "For Series and DataFrame, the same type is returned, containing booleans."]}, {"name": "pandas.json_normalize", "path": "reference/api/pandas.json_normalize", "type": "General functions", "text": ["Normalize semi-structured JSON data into a flat table.", "Unserialized JSON objects.", "Path in each object to list of records. If not passed, data will be assumed to be an array of records.", "Fields to use as metadata for each record in resulting table.", "If True, prefix records with dotted (?) path, e.g. foo.bar.field if meta is [\u2018foo\u2019, \u2018bar\u2019].", "If True, prefix records with dotted (?) path, e.g. foo.bar.field if path to records is [\u2018foo\u2019, \u2018bar\u2019].", "Configures error handling.", "\u2018ignore\u2019 : will ignore KeyError if keys listed in meta are not always present.", "\u2018raise\u2019 : will raise KeyError if keys listed in meta are not always present.", "Nested records will generate names separated by sep. e.g., for sep=\u2019.\u2019, {\u2018foo\u2019: {\u2018bar\u2019: 0}} -> foo.bar.", "Max number of levels(depth of dict) to normalize. if None, normalizes all levels.", "New in version 0.25.0.", "Examples", "Normalizes nested data up to level 1.", "Returns normalized data with columns prefixed with the given string."]}, {"name": "pandas.melt", "path": "reference/api/pandas.melt", "type": "General functions", "text": ["Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.", "This function is useful to massage a DataFrame into a format where one or more columns are identifier variables (id_vars), while all other columns, considered measured variables (value_vars), are \u201cunpivoted\u201d to the row axis, leaving just two non-identifier columns, \u2018variable\u2019 and \u2018value\u2019.", "Column(s) to use as identifier variables.", "Column(s) to unpivot. If not specified, uses all columns that are not set as id_vars.", "Name to use for the \u2018variable\u2019 column. If None it uses frame.columns.name or \u2018variable\u2019.", "Name to use for the \u2018value\u2019 column.", "If columns are a MultiIndex then use this level to melt.", "If True, original index is ignored. If False, the original index is retained. Index labels will be repeated as necessary.", "New in version 1.1.0.", "Unpivoted DataFrame.", "See also", "Identical method.", "Create a spreadsheet-style pivot table as a DataFrame.", "Return reshaped DataFrame organized by given index / column values.", "Explode a DataFrame from list-like columns to long format.", "Examples", "The names of \u2018variable\u2019 and \u2018value\u2019 columns can be customized:", "Original index values can be kept around:", "If you have multi-index columns:"]}, {"name": "pandas.merge", "path": "reference/api/pandas.merge", "type": "General functions", "text": ["Merge DataFrame or named Series objects with a database-style join.", "A named Series object is treated as a DataFrame with a single named column.", "The join is done on columns or indexes. If joining columns on columns, the DataFrame indexes will be ignored. Otherwise if joining indexes on indexes or indexes on a column or columns, the index will be passed on. When performing a cross merge, no column specifications to merge on are allowed.", "Warning", "If both key columns contain rows where the key is a null value, those rows will be matched against each other. This is different from usual SQL join behaviour and can lead to unexpected results.", "Object to merge with.", "Type of merge to be performed.", "left: use only keys from left frame, similar to a SQL left outer join; preserve key order.", "right: use only keys from right frame, similar to a SQL right outer join; preserve key order.", "outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically.", "inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys.", "cross: creates the cartesian product from both frames, preserves the order of the left keys.", "New in version 1.2.0.", "Column or index level names to join on. These must be found in both DataFrames. If on is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames.", "Column or index level names to join on in the left DataFrame. Can also be an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns.", "Column or index level names to join on in the right DataFrame. Can also be an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns.", "Use the index from the left DataFrame as the join key(s). If it is a MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels.", "Use the index from the right DataFrame as the join key. Same caveats as left_index.", "Sort the join keys lexicographically in the result DataFrame. If False, the order of the join keys depends on the join type (how keyword).", "A length-2 sequence where each element is optionally a string indicating the suffix to add to overlapping column names in left and right respectively. Pass a value of None instead of a string to indicate that the column name from left or right should be left as-is, with no suffix. At least one of the values must not be None.", "If False, avoid copy if possible.", "If True, adds a column to the output DataFrame called \u201c_merge\u201d with information on the source of each row. The column can be given a different name by providing a string argument. The column will have a Categorical type with the value of \u201cleft_only\u201d for observations whose merge key only appears in the left DataFrame, \u201cright_only\u201d for observations whose merge key only appears in the right DataFrame, and \u201cboth\u201d if the observation\u2019s merge key is found in both DataFrames.", "If specified, checks if merge is of specified type.", "\u201cone_to_one\u201d or \u201c1:1\u201d: check if merge keys are unique in both left and right datasets.", "\u201cone_to_many\u201d or \u201c1:m\u201d: check if merge keys are unique in left dataset.", "\u201cmany_to_one\u201d or \u201cm:1\u201d: check if merge keys are unique in right dataset.", "\u201cmany_to_many\u201d or \u201cm:m\u201d: allowed, but does not result in checks.", "A DataFrame of the two merged objects.", "See also", "Merge with optional filling/interpolation.", "Merge on nearest keys.", "Similar method using indices.", "Notes", "Support for specifying index levels as the on, left_on, and right_on parameters was added in version 0.23.0 Support for merging named Series objects was added in version 0.24.0", "Examples", "Merge df1 and df2 on the lkey and rkey columns. The value columns have the default suffixes, _x and _y, appended.", "Merge DataFrames df1 and df2 with specified left and right suffixes appended to any overlapping columns.", "Merge DataFrames df1 and df2, but raise an exception if the DataFrames have any overlapping columns."]}, {"name": "pandas.merge_asof", "path": "reference/api/pandas.merge_asof", "type": "General functions", "text": ["Perform a merge by key distance.", "This is similar to a left-join except that we match on nearest key rather than equal keys. Both DataFrames must be sorted by the key.", "For each row in the left DataFrame:", "A \u201cbackward\u201d search selects the last row in the right DataFrame whose \u2018on\u2019 key is less than or equal to the left\u2019s key.", "A \u201cforward\u201d search selects the first row in the right DataFrame whose \u2018on\u2019 key is greater than or equal to the left\u2019s key.", "A \u201cnearest\u201d search selects the row in the right DataFrame whose \u2018on\u2019 key is closest in absolute distance to the left\u2019s key.", "The default is \u201cbackward\u201d and is compatible in versions below 0.20.0. The direction parameter was added in version 0.20.0 and introduces \u201cforward\u201d and \u201cnearest\u201d.", "Optionally match on equivalent keys with \u2018by\u2019 before searching with \u2018on\u2019.", "Field name to join on. Must be found in both DataFrames. The data MUST be ordered. Furthermore this must be a numeric column, such as datetimelike, integer, or float. On or left_on/right_on must be given.", "Field name to join on in left DataFrame.", "Field name to join on in right DataFrame.", "Use the index of the left DataFrame as the join key.", "Use the index of the right DataFrame as the join key.", "Match on these columns before performing merge operation.", "Field names to match on in the left DataFrame.", "Field names to match on in the right DataFrame.", "Suffix to apply to overlapping column names in the left and right side, respectively.", "Select asof tolerance within this range; must be compatible with the merge index.", "If True, allow matching with the same \u2018on\u2019 value (i.e. less-than-or-equal-to / greater-than-or-equal-to)", "If False, don\u2019t match the same \u2018on\u2019 value (i.e., strictly less-than / strictly greater-than).", "Whether to search for prior, subsequent, or closest matches.", "See also", "Merge with a database-style join.", "Merge with optional filling/interpolation.", "Examples", "We can use indexed DataFrames as well.", "Here is a real-world times-series example", "By default we are taking the asof of the quotes", "We only asof within 2ms between the quote time and the trade time", "We only asof within 10ms between the quote time and the trade time and we exclude exact matches on time. However prior data will propagate forward"]}, {"name": "pandas.merge_ordered", "path": "reference/api/pandas.merge_ordered", "type": "General functions", "text": ["Perform a merge for ordered data with optional filling/interpolation.", "Designed for ordered data like time series data. Optionally perform group-wise merge (see examples).", "Field names to join on. Must be found in both DataFrames.", "Field names to join on in left DataFrame. Can be a vector or list of vectors of the length of the DataFrame to use a particular vector as the join key instead of columns.", "Field names to join on in right DataFrame or vector/list of vectors per left_on docs.", "Group left DataFrame by group columns and merge piece by piece with right DataFrame.", "Group right DataFrame by group columns and merge piece by piece with left DataFrame.", "Interpolation method for data.", "A length-2 sequence where each element is optionally a string indicating the suffix to add to overlapping column names in left and right respectively. Pass a value of None instead of a string to indicate that the column name from left or right should be left as-is, with no suffix. At least one of the values must not be None.", "Changed in version 0.25.0.", "left: use only keys from left frame (SQL: left outer join)", "right: use only keys from right frame (SQL: right outer join)", "outer: use union of keys from both frames (SQL: full outer join)", "inner: use intersection of keys from both frames (SQL: inner join).", "The merged DataFrame output type will the be same as \u2018left\u2019, if it is a subclass of DataFrame.", "See also", "Merge with a database-style join.", "Merge on nearest keys.", "Examples"]}, {"name": "pandas.MultiIndex", "path": "reference/api/pandas.multiindex", "type": "Index Objects", "text": ["A multi-level, or hierarchical, index object for pandas objects.", "The unique labels for each level.", "Integers for each level designating which label at each location.", "Level of sortedness (must be lexicographically sorted by that level).", "Names for each of the index levels. (name is accepted for compat).", "Copy the meta-data.", "Check that the levels/codes are consistent and valid.", "See also", "Convert list of arrays to MultiIndex.", "Create a MultiIndex from the cartesian product of iterables.", "Convert list of tuples to a MultiIndex.", "Make a MultiIndex from a DataFrame.", "The base pandas Index type.", "Notes", "See the user guide for more.", "Examples", "A new MultiIndex is typically constructed using one of the helper methods MultiIndex.from_arrays(), MultiIndex.from_product() and MultiIndex.from_tuples(). For example (using .from_arrays):", "See further examples for how to construct a MultiIndex in the doc strings of the mentioned helper methods.", "Attributes", "names", "Names of levels in MultiIndex.", "nlevels", "Integer number of levels in this MultiIndex.", "levshape", "A tuple with the length of each level.", "levels", "codes", "Methods", "from_arrays(arrays[, sortorder, names])", "Convert arrays to MultiIndex.", "from_tuples(tuples[, sortorder, names])", "Convert list of tuples to MultiIndex.", "from_product(iterables[, sortorder, names])", "Make a MultiIndex from the cartesian product of multiple iterables.", "from_frame(df[, sortorder, names])", "Make a MultiIndex from a DataFrame.", "set_levels(levels[, level, inplace, ...])", "Set new levels on MultiIndex.", "set_codes(codes[, level, inplace, ...])", "Set new codes on MultiIndex.", "to_frame([index, name])", "Create a DataFrame with the levels of the MultiIndex as columns.", "to_flat_index()", "Convert a MultiIndex to an Index of Tuples containing the level values.", "sortlevel([level, ascending, sort_remaining])", "Sort MultiIndex at the requested level.", "droplevel([level])", "Return index with requested level(s) removed.", "swaplevel([i, j])", "Swap level i with level j.", "reorder_levels(order)", "Rearrange levels using input order.", "remove_unused_levels()", "Create new MultiIndex from current that removes unused levels.", "get_locs(seq)", "Get location for a sequence of labels."]}, {"name": "pandas.MultiIndex.codes", "path": "reference/api/pandas.multiindex.codes", "type": "Index Objects", "text": []}, {"name": "pandas.MultiIndex.droplevel", "path": "reference/api/pandas.multiindex.droplevel", "type": "Index Objects", "text": ["Return index with requested level(s) removed.", "If resulting index has only 1 level left, the result will be of Index type, not MultiIndex.", "If a string is given, must be the name of a level If list-like, elements must be names or indexes of levels.", "Examples"]}, {"name": "pandas.MultiIndex.dtypes", "path": "reference/api/pandas.multiindex.dtypes", "type": "General utility functions", "text": ["Return the dtypes as a Series for the underlying MultiIndex."]}, {"name": "pandas.MultiIndex.from_arrays", "path": "reference/api/pandas.multiindex.from_arrays", "type": "Index Objects", "text": ["Convert arrays to MultiIndex.", "Each array-like gives one level\u2019s value for each data point. len(arrays) is the number of levels.", "Level of sortedness (must be lexicographically sorted by that level).", "Names for the levels in the index.", "See also", "Convert list of tuples to MultiIndex.", "Make a MultiIndex from cartesian product of iterables.", "Make a MultiIndex from a DataFrame.", "Examples"]}, {"name": "pandas.MultiIndex.from_frame", "path": "reference/api/pandas.multiindex.from_frame", "type": "DataFrame", "text": ["Make a MultiIndex from a DataFrame.", "DataFrame to be converted to MultiIndex.", "Level of sortedness (must be lexicographically sorted by that level).", "If no names are provided, use the column names, or tuple of column names if the columns is a MultiIndex. If a sequence, overwrite names with the given sequence.", "The MultiIndex representation of the given DataFrame.", "See also", "Convert list of arrays to MultiIndex.", "Convert list of tuples to MultiIndex.", "Make a MultiIndex from cartesian product of iterables.", "Examples", "Using explicit names, instead of the column names"]}, {"name": "pandas.MultiIndex.from_product", "path": "reference/api/pandas.multiindex.from_product", "type": "Index Objects", "text": ["Make a MultiIndex from the cartesian product of multiple iterables.", "Each iterable has unique labels for each level of the index.", "Level of sortedness (must be lexicographically sorted by that level).", "Names for the levels in the index.", "Changed in version 1.0.0: If not explicitly provided, names will be inferred from the elements of iterables if an element has a name attribute", "See also", "Convert list of arrays to MultiIndex.", "Convert list of tuples to MultiIndex.", "Make a MultiIndex from a DataFrame.", "Examples"]}, {"name": "pandas.MultiIndex.from_tuples", "path": "reference/api/pandas.multiindex.from_tuples", "type": "Index Objects", "text": ["Convert list of tuples to MultiIndex.", "Each tuple is the index of one row/column.", "Level of sortedness (must be lexicographically sorted by that level).", "Names for the levels in the index.", "See also", "Convert list of arrays to MultiIndex.", "Make a MultiIndex from cartesian product of iterables.", "Make a MultiIndex from a DataFrame.", "Examples"]}, {"name": "pandas.MultiIndex.get_indexer", "path": "reference/api/pandas.multiindex.get_indexer", "type": "Index Objects", "text": ["Compute indexer and mask for new index given the current index. The indexer should be then used as an input to ndarray.take to align the current data to the new index.", "default: exact matches only.", "pad / ffill: find the PREVIOUS index value if no exact match.", "backfill / bfill: use NEXT index value if no exact match", "nearest: use the NEAREST index value if no exact match. Tied distances are broken by preferring the larger index value.", "Maximum number of consecutive labels in target to match for inexact matches.", "Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance.", "Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index\u2019s type.", "Integers from 0 to n - 1 indicating that the index at these positions matches the corresponding target values. Missing values in the target are marked by -1.", "Notes", "Returns -1 for unmatched values, for further explanation see the example below.", "Examples", "Notice that the return value is an array of locations in index and x is marked by -1, as it is not in index."]}, {"name": "pandas.MultiIndex.get_level_values", "path": "reference/api/pandas.multiindex.get_level_values", "type": "Index Objects", "text": ["Return vector of label values for requested level.", "Length of returned vector is equal to the length of the index.", "level is either the integer position of the level in the MultiIndex, or the name of the level.", "Values is a level of this MultiIndex converted to a single Index (or subclass thereof).", "Notes", "If the level contains missing values, the result may be casted to float with missing values specified as NaN. This is because the level is converted to a regular Index.", "Examples", "Create a MultiIndex:", "Get level values by supplying level as either integer or name:", "If a level contains missing values, the return type of the level maybe casted to float."]}, {"name": "pandas.MultiIndex.get_loc", "path": "reference/api/pandas.multiindex.get_loc", "type": "Index Objects", "text": ["Get location for a label or a tuple of labels.", "The location is returned as an integer/slice or boolean mask.", "If the key is past the lexsort depth, the return may be a boolean mask array, otherwise it is always a slice or int.", "See also", "The get_loc method for (single-level) index.", "Get slice location given start label(s) and end label(s).", "Get location for a label/slice/list/mask or a sequence of such.", "Notes", "The key cannot be a slice, list of same-level labels, a boolean mask, or a sequence of such. If you want to use those, use MultiIndex.get_locs() instead.", "Examples"]}, {"name": "pandas.MultiIndex.get_loc_level", "path": "reference/api/pandas.multiindex.get_loc_level", "type": "Index Objects", "text": ["Get location and sliced index for requested label(s)/level(s).", "If False, the resulting index will not drop any level.", "Element 0: int, slice object or boolean array Element 1: The resulting sliced multiindex/index. If the key contains all levels, this will be None.", "See also", "Get location for a label or a tuple of labels.", "Get location for a label/slice/list/mask or a sequence of such.", "Examples"]}, {"name": "pandas.MultiIndex.get_locs", "path": "reference/api/pandas.multiindex.get_locs", "type": "Index Objects", "text": ["Get location for a sequence of labels.", "You should use one of the above for each level. If a level should not be used, set it to slice(None).", "NumPy array of integers suitable for passing to iloc.", "See also", "Get location for a label or a tuple of labels.", "Get slice location given start label(s) and end label(s).", "Examples"]}, {"name": "pandas.MultiIndex.levels", "path": "reference/api/pandas.multiindex.levels", "type": "Index Objects", "text": []}, {"name": "pandas.MultiIndex.levshape", "path": "reference/api/pandas.multiindex.levshape", "type": "Index Objects", "text": ["A tuple with the length of each level.", "Examples"]}, {"name": "pandas.MultiIndex.names", "path": "reference/api/pandas.multiindex.names", "type": "Index Objects", "text": ["Names of levels in MultiIndex.", "Examples"]}, {"name": "pandas.MultiIndex.nlevels", "path": "reference/api/pandas.multiindex.nlevels", "type": "Index Objects", "text": ["Integer number of levels in this MultiIndex.", "Examples"]}, {"name": "pandas.MultiIndex.remove_unused_levels", "path": "reference/api/pandas.multiindex.remove_unused_levels", "type": "Index Objects", "text": ["Create new MultiIndex from current that removes unused levels.", "Unused level(s) means levels that are not expressed in the labels. The resulting MultiIndex will have the same outward appearance, meaning the same .values and ordering. It will also be .equals() to the original.", "Examples", "The 0 from the first level is not represented and can be removed"]}, {"name": "pandas.MultiIndex.reorder_levels", "path": "reference/api/pandas.multiindex.reorder_levels", "type": "Index Objects", "text": ["Rearrange levels using input order. May not drop or duplicate levels.", "List representing new level order. Reference level by number (position) or by key (label).", "Examples"]}, {"name": "pandas.MultiIndex.set_codes", "path": "reference/api/pandas.multiindex.set_codes", "type": "Index Objects", "text": ["Set new codes on MultiIndex. Defaults to returning new index.", "New codes to apply.", "Level(s) to set (None for all levels).", "If True, mutates in place.", "Deprecated since version 1.2.0.", "If True, checks that levels and codes are compatible.", "The same type as the caller or None if inplace=True.", "Examples"]}, {"name": "pandas.MultiIndex.set_levels", "path": "reference/api/pandas.multiindex.set_levels", "type": "Index Objects", "text": ["Set new levels on MultiIndex. Defaults to returning new index.", "New level(s) to apply.", "Level(s) to set (None for all levels).", "If True, mutates in place.", "Deprecated since version 1.2.0.", "If True, checks that levels and codes are compatible.", "The same type as the caller or None if inplace=True.", "Examples", "If any of the levels passed to set_levels() exceeds the existing length, all of the values from that argument will be stored in the MultiIndex levels, though the values will be truncated in the MultiIndex output."]}, {"name": "pandas.MultiIndex.sortlevel", "path": "reference/api/pandas.multiindex.sortlevel", "type": "Index Objects", "text": ["Sort MultiIndex at the requested level.", "The result will respect the original ordering of the associated factor at that level.", "If a string is given, must be a name of the level. If list-like must be names or ints of levels.", "False to sort in descending order. Can also be a list to specify a directed ordering.", "Resulting index.", "Indices of output values in original index.", "Examples"]}, {"name": "pandas.MultiIndex.swaplevel", "path": "reference/api/pandas.multiindex.swaplevel", "type": "Index Objects", "text": ["Swap level i with level j.", "Calling this method does not change the ordering of the values.", "First level of index to be swapped. Can pass level name as string. Type of parameters can be mixed.", "Second level of index to be swapped. Can pass level name as string. Type of parameters can be mixed.", "A new MultiIndex.", "See also", "Swap levels i and j in a MultiIndex.", "Swap levels i and j in a MultiIndex on a particular axis.", "Examples"]}, {"name": "pandas.MultiIndex.to_flat_index", "path": "reference/api/pandas.multiindex.to_flat_index", "type": "Index Objects", "text": ["Convert a MultiIndex to an Index of Tuples containing the level values.", "Index with the MultiIndex data represented in Tuples.", "See also", "Convert flat index back to MultiIndex.", "Notes", "This method will simply return the caller if called by anything other than a MultiIndex.", "Examples"]}, {"name": "pandas.MultiIndex.to_frame", "path": "reference/api/pandas.multiindex.to_frame", "type": "DataFrame", "text": ["Create a DataFrame with the levels of the MultiIndex as columns.", "Column ordering is determined by the DataFrame constructor with data as a dict.", "Set the index of the returned DataFrame as the original MultiIndex.", "The passed names should substitute index level names.", "See also", "Two-dimensional, size-mutable, potentially heterogeneous tabular data.", "Examples"]}, {"name": "pandas.notna", "path": "reference/api/pandas.notna", "type": "General functions", "text": ["Detect non-missing values for an array-like object.", "This function takes a scalar or array-like object and indicates whether values are valid (not missing, which is NaN in numeric arrays, None or NaN in object arrays, NaT in datetimelike).", "Object to check for not null or non-missing values.", "For scalar input, returns a scalar boolean. For array input, returns an array of boolean indicating whether each corresponding element is valid.", "See also", "Boolean inverse of pandas.notna.", "Detect valid values in a Series.", "Detect valid values in a DataFrame.", "Detect valid values in an Index.", "Examples", "Scalar arguments (including strings) result in a scalar boolean.", "ndarrays result in an ndarray of booleans.", "For indexes, an ndarray of booleans is returned.", "For Series and DataFrame, the same type is returned, containing booleans."]}, {"name": "pandas.notnull", "path": "reference/api/pandas.notnull", "type": "General functions", "text": ["Detect non-missing values for an array-like object.", "This function takes a scalar or array-like object and indicates whether values are valid (not missing, which is NaN in numeric arrays, None or NaN in object arrays, NaT in datetimelike).", "Object to check for not null or non-missing values.", "For scalar input, returns a scalar boolean. For array input, returns an array of boolean indicating whether each corresponding element is valid.", "See also", "Boolean inverse of pandas.notna.", "Detect valid values in a Series.", "Detect valid values in a DataFrame.", "Detect valid values in an Index.", "Examples", "Scalar arguments (including strings) result in a scalar boolean.", "ndarrays result in an ndarray of booleans.", "For indexes, an ndarray of booleans is returned.", "For Series and DataFrame, the same type is returned, containing booleans."]}, {"name": "pandas.option_context", "path": "reference/api/pandas.option_context", "type": "General utility functions", "text": ["Context manager to temporarily set options in the with statement context.", "You need to invoke as option_context(pat, val, [(pat, val), ...]).", "Examples", "Methods", "__call__(func)", "Call self as a function."]}, {"name": "pandas.option_context.__call__", "path": "reference/api/pandas.option_context.__call__", "type": "General utility functions", "text": ["Call self as a function."]}, {"name": "pandas.Period", "path": "reference/api/pandas.period", "type": "Input/output", "text": ["Represents a period of time.", "The time period represented (e.g., \u20184Q2005\u2019).", "One of pandas period strings or corresponding objects.", "The period offset from the proleptic Gregorian epoch.", "Year value of the period.", "Month value of the period.", "Quarter value of the period.", "Day value of the period.", "Hour value of the period.", "Minute value of the period.", "Second value of the period.", "Attributes", "day", "Get day of the month that a Period falls on.", "day_of_week", "Day of the week the period lies in, with Monday=0 and Sunday=6.", "day_of_year", "Return the day of the year.", "dayofweek", "Day of the week the period lies in, with Monday=0 and Sunday=6.", "dayofyear", "Return the day of the year.", "days_in_month", "Get the total number of days in the month that this period falls on.", "daysinmonth", "Get the total number of days of the month that the Period falls in.", "end_time", "Get the Timestamp for the end of the period.", "freqstr", "Return a string representation of the frequency.", "hour", "Get the hour of the day component of the Period.", "is_leap_year", "Return True if the period's year is in a leap year.", "minute", "Get minute of the hour component of the Period.", "month", "Return the month this Period falls on.", "quarter", "Return the quarter this Period falls on.", "qyear", "Fiscal year the Period lies in according to its starting-quarter.", "second", "Get the second component of the Period.", "start_time", "Get the Timestamp for the start of the period.", "week", "Get the week of the year on the given Period.", "weekday", "Day of the week the period lies in, with Monday=0 and Sunday=6.", "weekofyear", "Get the week of the year on the given Period.", "year", "Return the year this Period falls on.", "freq", "ordinal", "Methods", "asfreq", "Convert Period to desired frequency, at the start or end of the interval.", "now", "Return the period of now's date.", "strftime", "Returns the string representation of the Period, depending on the selected fmt.", "to_timestamp", "Return the Timestamp representation of the Period."]}, {"name": "pandas.Period.asfreq", "path": "reference/api/pandas.period.asfreq", "type": "Input/output", "text": ["Convert Period to desired frequency, at the start or end of the interval.", "The desired frequency.", "Start or end of the timespan."]}, {"name": "pandas.Period.day", "path": "reference/api/pandas.period.day", "type": "Input/output", "text": ["Get day of the month that a Period falls on.", "See also", "Get the day of the week.", "Get the day of the year.", "Examples"]}, {"name": "pandas.Period.day_of_week", "path": "reference/api/pandas.period.day_of_week", "type": "Input/output", "text": ["Day of the week the period lies in, with Monday=0 and Sunday=6.", "If the period frequency is lower than daily (e.g. hourly), and the period spans over multiple days, the day at the start of the period is used.", "If the frequency is higher than daily (e.g. monthly), the last day of the period is used.", "Day of the week.", "See also", "Day of the week the period lies in.", "Alias of Period.day_of_week.", "Day of the month.", "Day of the year.", "Examples", "For periods that span over multiple days, the day at the beginning of the period is returned.", "For periods with a frequency higher than days, the last day of the period is returned."]}, {"name": "pandas.Period.day_of_year", "path": "reference/api/pandas.period.day_of_year", "type": "Input/output", "text": ["Return the day of the year.", "This attribute returns the day of the year on which the particular date occurs. The return value ranges between 1 to 365 for regular years and 1 to 366 for leap years.", "The day of year.", "See also", "Return the day of the month.", "Return the day of week.", "Return the day of year of all indexes.", "Examples"]}, {"name": "pandas.Period.dayofweek", "path": "reference/api/pandas.period.dayofweek", "type": "Input/output", "text": ["Day of the week the period lies in, with Monday=0 and Sunday=6.", "If the period frequency is lower than daily (e.g. hourly), and the period spans over multiple days, the day at the start of the period is used.", "If the frequency is higher than daily (e.g. monthly), the last day of the period is used.", "Day of the week.", "See also", "Day of the week the period lies in.", "Alias of Period.day_of_week.", "Day of the month.", "Day of the year.", "Examples", "For periods that span over multiple days, the day at the beginning of the period is returned.", "For periods with a frequency higher than days, the last day of the period is returned."]}, {"name": "pandas.Period.dayofyear", "path": "reference/api/pandas.period.dayofyear", "type": "Input/output", "text": ["Return the day of the year.", "This attribute returns the day of the year on which the particular date occurs. The return value ranges between 1 to 365 for regular years and 1 to 366 for leap years.", "The day of year.", "See also", "Return the day of the month.", "Return the day of week.", "Return the day of year of all indexes.", "Examples"]}, {"name": "pandas.Period.days_in_month", "path": "reference/api/pandas.period.days_in_month", "type": "Input/output", "text": ["Get the total number of days in the month that this period falls on.", "See also", "Gets the number of days in the month.", "Gets the number of days in the month.", "Returns a tuple containing weekday (0-6 ~ Mon-Sun) and number of days (28-31).", "Examples", "Handles the leap year case as well:"]}, {"name": "pandas.Period.daysinmonth", "path": "reference/api/pandas.period.daysinmonth", "type": "Input/output", "text": ["Get the total number of days of the month that the Period falls in.", "See also", "Return the days of the month.", "Return the day of the year.", "Examples"]}, {"name": "pandas.Period.end_time", "path": "reference/api/pandas.period.end_time", "type": "Input/output", "text": ["Get the Timestamp for the end of the period.", "See also", "Return the start Timestamp.", "Return the day of year.", "Return the days in that month.", "Return the day of the week."]}, {"name": "pandas.Period.freq", "path": "reference/api/pandas.period.freq", "type": "Input/output", "text": []}, {"name": "pandas.Period.freqstr", "path": "reference/api/pandas.period.freqstr", "type": "Input/output", "text": ["Return a string representation of the frequency."]}, {"name": "pandas.Period.hour", "path": "reference/api/pandas.period.hour", "type": "Input/output", "text": ["Get the hour of the day component of the Period.", "The hour as an integer, between 0 and 23.", "See also", "Get the second component of the Period.", "Get the minute component of the Period.", "Examples", "Period longer than a day"]}, {"name": "pandas.Period.is_leap_year", "path": "reference/api/pandas.period.is_leap_year", "type": "Input/output", "text": ["Return True if the period\u2019s year is in a leap year."]}, {"name": "pandas.Period.minute", "path": "reference/api/pandas.period.minute", "type": "Input/output", "text": ["Get minute of the hour component of the Period.", "The minute as an integer, between 0 and 59.", "See also", "Get the hour component of the Period.", "Get the second component of the Period.", "Examples"]}, {"name": "pandas.Period.month", "path": "reference/api/pandas.period.month", "type": "Input/output", "text": ["Return the month this Period falls on."]}, {"name": "pandas.Period.now", "path": "reference/api/pandas.period.now", "type": "Input/output", "text": ["Return the period of now\u2019s date."]}, {"name": "pandas.Period.ordinal", "path": "reference/api/pandas.period.ordinal", "type": "Input/output", "text": []}, {"name": "pandas.Period.quarter", "path": "reference/api/pandas.period.quarter", "type": "Input/output", "text": ["Return the quarter this Period falls on."]}, {"name": "pandas.Period.qyear", "path": "reference/api/pandas.period.qyear", "type": "Input/output", "text": ["Fiscal year the Period lies in according to its starting-quarter.", "The year and the qyear of the period will be the same if the fiscal and calendar years are the same. When they are not, the fiscal year can be different from the calendar year of the period.", "The fiscal year of the period.", "See also", "Return the calendar year of the period.", "Examples", "If the natural and fiscal year are the same, qyear and year will be the same.", "If the fiscal year starts in April (Q-MAR), the first quarter of 2018 will start in April 2017. year will then be 2018, but qyear will be the fiscal year, 2018."]}, {"name": "pandas.Period.second", "path": "reference/api/pandas.period.second", "type": "Input/output", "text": ["Get the second component of the Period.", "The second of the Period (ranges from 0 to 59).", "See also", "Get the hour component of the Period.", "Get the minute component of the Period.", "Examples"]}, {"name": "pandas.Period.start_time", "path": "reference/api/pandas.period.start_time", "type": "Input/output", "text": ["Get the Timestamp for the start of the period.", "See also", "Return the end Timestamp.", "Return the day of year.", "Return the days in that month.", "Return the day of the week.", "Examples"]}, {"name": "pandas.Period.strftime", "path": "reference/api/pandas.period.strftime", "type": "Input/output", "text": ["Returns the string representation of the Period, depending on the selected fmt. fmt must be a string containing one or several directives. The method recognizes the same directives as the time.strftime() function of the standard Python distribution, as well as the specific additional directives %f, %F, %q. (formatting & docs originally from scikits.timeries).", "Directive", "Meaning", "Notes", "%a", "Locale\u2019s abbreviated weekday name.", "%A", "Locale\u2019s full weekday name.", "%b", "Locale\u2019s abbreviated month name.", "%B", "Locale\u2019s full month name.", "%c", "Locale\u2019s appropriate date and time representation.", "%d", "Day of the month as a decimal number [01,31].", "%f", "\u2018Fiscal\u2019 year without a century as a decimal number [00,99]", "(1)", "%F", "\u2018Fiscal\u2019 year with a century as a decimal number", "(2)", "%H", "Hour (24-hour clock) as a decimal number [00,23].", "%I", "Hour (12-hour clock) as a decimal number [01,12].", "%j", "Day of the year as a decimal number [001,366].", "%m", "Month as a decimal number [01,12].", "%M", "Minute as a decimal number [00,59].", "%p", "Locale\u2019s equivalent of either AM or PM.", "(3)", "%q", "Quarter as a decimal number [01,04]", "%S", "Second as a decimal number [00,61].", "(4)", "%U", "Week number of the year (Sunday as the first day of the week) as a decimal number [00,53]. All days in a new year preceding the first Sunday are considered to be in week 0.", "(5)", "%w", "Weekday as a decimal number [0(Sunday),6].", "%W", "Week number of the year (Monday as the first day of the week) as a decimal number [00,53]. All days in a new year preceding the first Monday are considered to be in week 0.", "(5)", "%x", "Locale\u2019s appropriate date representation.", "%X", "Locale\u2019s appropriate time representation.", "%y", "Year without century as a decimal number [00,99].", "%Y", "Year with century as a decimal number.", "%Z", "Time zone name (no characters if no time zone exists).", "%%", "A literal '%' character.", "Notes", "The %f directive is the same as %y if the frequency is not quarterly. Otherwise, it corresponds to the \u2018fiscal\u2019 year, as defined by the qyear attribute.", "The %F directive is the same as %Y if the frequency is not quarterly. Otherwise, it corresponds to the \u2018fiscal\u2019 year, as defined by the qyear attribute.", "The %p directive only affects the output hour field if the %I directive is used to parse the hour.", "The range really is 0 to 61; this accounts for leap seconds and the (very rare) double leap seconds.", "The %U and %W directives are only used in calculations when the day of the week and the year are specified.", "Examples"]}, {"name": "pandas.Period.to_timestamp", "path": "reference/api/pandas.period.to_timestamp", "type": "Input/output", "text": ["Return the Timestamp representation of the Period.", "Uses the target frequency specified at the part of the period specified by how, which is either Start or Finish.", "Target frequency. Default is \u2018D\u2019 if self.freq is week or longer and \u2018S\u2019 otherwise.", "One of \u2018S\u2019, \u2018E\u2019. Can be aliased as case insensitive \u2018Start\u2019, \u2018Finish\u2019, \u2018Begin\u2019, \u2018End\u2019."]}, {"name": "pandas.Period.week", "path": "reference/api/pandas.period.week", "type": "Input/output", "text": ["Get the week of the year on the given Period.", "See also", "Get the day component of the Period.", "Get the day component of the Period.", "Examples"]}, {"name": "pandas.Period.weekday", "path": "reference/api/pandas.period.weekday", "type": "Input/output", "text": ["Day of the week the period lies in, with Monday=0 and Sunday=6.", "If the period frequency is lower than daily (e.g. hourly), and the period spans over multiple days, the day at the start of the period is used.", "If the frequency is higher than daily (e.g. monthly), the last day of the period is used.", "Day of the week.", "See also", "Day of the week the period lies in.", "Alias of Period.dayofweek.", "Day of the month.", "Day of the year.", "Examples", "For periods that span over multiple days, the day at the beginning of the period is returned.", "For periods with a frequency higher than days, the last day of the period is returned."]}, {"name": "pandas.Period.weekofyear", "path": "reference/api/pandas.period.weekofyear", "type": "Input/output", "text": ["Get the week of the year on the given Period.", "See also", "Get the day component of the Period.", "Get the day component of the Period.", "Examples"]}, {"name": "pandas.Period.year", "path": "reference/api/pandas.period.year", "type": "Input/output", "text": ["Return the year this Period falls on."]}, {"name": "pandas.period_range", "path": "reference/api/pandas.period_range", "type": "Input/output", "text": ["Return a fixed frequency PeriodIndex.", "The day (calendar) is the default frequency.", "Left bound for generating periods.", "Right bound for generating periods.", "Number of periods to generate.", "Frequency alias. By default the freq is taken from start or end if those are Period objects. Otherwise, the default is \"D\" for daily frequency.", "Name of the resulting PeriodIndex.", "Notes", "Of the three parameters: start, end, and periods, exactly two must be specified.", "To learn more about the frequency strings, please see this link.", "Examples", "If start or end are Period objects, they will be used as anchor endpoints for a PeriodIndex with frequency matching that of the period_range constructor."]}, {"name": "pandas.PeriodDtype", "path": "reference/api/pandas.perioddtype", "type": "Input/output", "text": ["An ExtensionDtype for Period data.", "This is not an actual numpy dtype, but a duck type.", "The frequency of this PeriodDtype.", "Examples", "Attributes", "freq", "The frequency object of this PeriodDtype.", "Methods", "None"]}, {"name": "pandas.PeriodDtype.freq", "path": "reference/api/pandas.perioddtype.freq", "type": "Input/output", "text": ["The frequency object of this PeriodDtype."]}, {"name": "pandas.PeriodIndex", "path": "reference/api/pandas.periodindex", "type": "Input/output", "text": ["Immutable ndarray holding ordinal values indicating regular periods in time.", "Index keys are boxed to Period objects which carries the metadata (eg, frequency information).", "Optional period-like data to construct index with.", "Make a copy of input ndarray.", "One of pandas period strings or corresponding objects.", "See also", "The base pandas Index type.", "Represents a period of time.", "Index with datetime64 data.", "Index of timedelta64 data.", "Create a fixed-frequency PeriodIndex.", "Examples", "Attributes", "day", "The days of the period.", "dayofweek", "The day of the week with Monday=0, Sunday=6.", "day_of_week", "The day of the week with Monday=0, Sunday=6.", "dayofyear", "The ordinal day of the year.", "day_of_year", "The ordinal day of the year.", "days_in_month", "The number of days in the month.", "daysinmonth", "The number of days in the month.", "freq", "Return the frequency object if it is set, otherwise None.", "freqstr", "Return the frequency object as a string if its set, otherwise None.", "hour", "The hour of the period.", "is_leap_year", "Logical indicating if the date belongs to a leap year.", "minute", "The minute of the period.", "month", "The month as January=1, December=12.", "quarter", "The quarter of the date.", "second", "The second of the period.", "week", "The week ordinal of the year.", "weekday", "The day of the week with Monday=0, Sunday=6.", "weekofyear", "The week ordinal of the year.", "year", "The year of the period.", "end_time", "qyear", "start_time", "Methods", "asfreq([freq, how])", "Convert the PeriodArray to the specified frequency freq.", "strftime(*args, **kwargs)", "Convert to Index using specified date_format.", "to_timestamp([freq, how])", "Cast to DatetimeArray/Index."]}, {"name": "pandas.PeriodIndex.asfreq", "path": "reference/api/pandas.periodindex.asfreq", "type": "Input/output", "text": ["Convert the PeriodArray to the specified frequency freq.", "Equivalent to applying pandas.Period.asfreq() with the given arguments to each Period in this PeriodArray.", "A frequency.", "Whether the elements should be aligned to the end or start within pa period.", "\u2018E\u2019, \u2018END\u2019, or \u2018FINISH\u2019 for end,", "\u2018S\u2019, \u2018START\u2019, or \u2018BEGIN\u2019 for start.", "January 31st (\u2018END\u2019) vs. January 1st (\u2018START\u2019) for example.", "The transformed PeriodArray with the new frequency.", "See also", "Convert each Period in a PeriodArray to the given frequency.", "Convert a Period object to the given frequency.", "Examples"]}, {"name": "pandas.PeriodIndex.day", "path": "reference/api/pandas.periodindex.day", "type": "Input/output", "text": ["The days of the period."]}, {"name": "pandas.PeriodIndex.day_of_week", "path": "reference/api/pandas.periodindex.day_of_week", "type": "Input/output", "text": ["The day of the week with Monday=0, Sunday=6."]}, {"name": "pandas.PeriodIndex.day_of_year", "path": "reference/api/pandas.periodindex.day_of_year", "type": "Input/output", "text": ["The ordinal day of the year."]}, {"name": "pandas.PeriodIndex.dayofweek", "path": "reference/api/pandas.periodindex.dayofweek", "type": "Input/output", "text": ["The day of the week with Monday=0, Sunday=6."]}, {"name": "pandas.PeriodIndex.dayofyear", "path": "reference/api/pandas.periodindex.dayofyear", "type": "Input/output", "text": ["The ordinal day of the year."]}, {"name": "pandas.PeriodIndex.days_in_month", "path": "reference/api/pandas.periodindex.days_in_month", "type": "Input/output", "text": ["The number of days in the month."]}, {"name": "pandas.PeriodIndex.daysinmonth", "path": "reference/api/pandas.periodindex.daysinmonth", "type": "Input/output", "text": ["The number of days in the month."]}, {"name": "pandas.PeriodIndex.end_time", "path": "reference/api/pandas.periodindex.end_time", "type": "Input/output", "text": []}, {"name": "pandas.PeriodIndex.freq", "path": "reference/api/pandas.periodindex.freq", "type": "Input/output", "text": ["Return the frequency object if it is set, otherwise None."]}, {"name": "pandas.PeriodIndex.freqstr", "path": "reference/api/pandas.periodindex.freqstr", "type": "Input/output", "text": ["Return the frequency object as a string if its set, otherwise None."]}, {"name": "pandas.PeriodIndex.hour", "path": "reference/api/pandas.periodindex.hour", "type": "Input/output", "text": ["The hour of the period."]}, {"name": "pandas.PeriodIndex.is_leap_year", "path": "reference/api/pandas.periodindex.is_leap_year", "type": "Input/output", "text": ["Logical indicating if the date belongs to a leap year."]}, {"name": "pandas.PeriodIndex.minute", "path": "reference/api/pandas.periodindex.minute", "type": "Input/output", "text": ["The minute of the period."]}, {"name": "pandas.PeriodIndex.month", "path": "reference/api/pandas.periodindex.month", "type": "Input/output", "text": ["The month as January=1, December=12."]}, {"name": "pandas.PeriodIndex.quarter", "path": "reference/api/pandas.periodindex.quarter", "type": "Input/output", "text": ["The quarter of the date."]}, {"name": "pandas.PeriodIndex.qyear", "path": "reference/api/pandas.periodindex.qyear", "type": "Input/output", "text": []}, {"name": "pandas.PeriodIndex.second", "path": "reference/api/pandas.periodindex.second", "type": "Input/output", "text": ["The second of the period."]}, {"name": "pandas.PeriodIndex.start_time", "path": "reference/api/pandas.periodindex.start_time", "type": "Input/output", "text": []}, {"name": "pandas.PeriodIndex.strftime", "path": "reference/api/pandas.periodindex.strftime", "type": "Input/output", "text": ["Convert to Index using specified date_format.", "Return an Index of formatted strings specified by date_format, which supports the same string format as the python standard library. Details of the string format can be found in python string format doc.", "Date format string (e.g. \u201c%Y-%m-%d\u201d).", "NumPy ndarray of formatted strings.", "See also", "Convert the given argument to datetime.", "Return DatetimeIndex with times to midnight.", "Round the DatetimeIndex to the specified freq.", "Floor the DatetimeIndex to the specified freq.", "Examples"]}, {"name": "pandas.PeriodIndex.to_timestamp", "path": "reference/api/pandas.periodindex.to_timestamp", "type": "Input/output", "text": ["Cast to DatetimeArray/Index.", "Target frequency. The default is \u2018D\u2019 for week or longer, \u2018S\u2019 otherwise.", "Whether to use the start or end of the time period being converted."]}, {"name": "pandas.PeriodIndex.week", "path": "reference/api/pandas.periodindex.week", "type": "Input/output", "text": ["The week ordinal of the year."]}, {"name": "pandas.PeriodIndex.weekday", "path": "reference/api/pandas.periodindex.weekday", "type": "Input/output", "text": ["The day of the week with Monday=0, Sunday=6."]}, {"name": "pandas.PeriodIndex.weekofyear", "path": "reference/api/pandas.periodindex.weekofyear", "type": "Input/output", "text": ["The week ordinal of the year."]}, {"name": "pandas.PeriodIndex.year", "path": "reference/api/pandas.periodindex.year", "type": "Input/output", "text": ["The year of the period."]}, {"name": "pandas.pivot", "path": "reference/api/pandas.pivot", "type": "General functions", "text": ["Return reshaped DataFrame organized by given index / column values.", "Reshape data (produce a \u201cpivot\u201d table) based on column values. Uses unique values from specified index / columns to form axes of the resulting DataFrame. This function does not support data aggregation, multiple values will result in a MultiIndex in the columns. See the User Guide for more on reshaping.", "Column to use to make new frame\u2019s index. If None, uses existing index.", "Changed in version 1.1.0: Also accept list of index names.", "Column to use to make new frame\u2019s columns.", "Changed in version 1.1.0: Also accept list of columns names.", "Column(s) to use for populating new frame\u2019s values. If not specified, all remaining columns will be used and the result will have hierarchically indexed columns.", "Returns reshaped DataFrame.", "When there are any index, columns combinations with multiple values. DataFrame.pivot_table when you need to aggregate.", "See also", "Generalization of pivot that can handle duplicate values for one index/column pair.", "Pivot based on the index values instead of a column.", "Wide panel to long format. Less flexible but more user-friendly than melt.", "Notes", "For finer-tuned control, see hierarchical indexing documentation along with the related stack/unstack methods.", "Examples", "You could also assign a list of column names or a list of index names.", "A ValueError is raised if there are any duplicates.", "Notice that the first two rows are the same for our index and columns arguments."]}, {"name": "pandas.pivot_table", "path": "reference/api/pandas.pivot_table", "type": "General functions", "text": ["Create a spreadsheet-style pivot table as a DataFrame.", "The levels in the pivot table will be stored in MultiIndex objects (hierarchical indexes) on the index and columns of the result DataFrame.", "If an array is passed, it must be the same length as the data. The list can contain any of the other types (except list). Keys to group by on the pivot table index. If an array is passed, it is being used as the same manner as column values.", "If an array is passed, it must be the same length as the data. The list can contain any of the other types (except list). Keys to group by on the pivot table column. If an array is passed, it is being used as the same manner as column values.", "If list of functions passed, the resulting pivot table will have hierarchical columns whose top level are the function names (inferred from the function objects themselves) If dict is passed, the key is column to aggregate and value is function or list of functions.", "Value to replace missing values with (in the resulting pivot table, after aggregation).", "Add all row / columns (e.g. for subtotal / grand totals).", "Do not include columns whose entries are all NaN.", "Name of the row / column that will contain the totals when margins is True.", "This only applies if any of the groupers are Categoricals. If True: only show observed values for categorical groupers. If False: show all values for categorical groupers.", "Changed in version 0.25.0.", "Specifies if the result should be sorted.", "New in version 1.3.0.", "An Excel style pivot table.", "See also", "Pivot without aggregation that can handle non-numeric data.", "Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.", "Wide panel to long format. Less flexible but more user-friendly than melt.", "Examples", "This first example aggregates values by taking the sum.", "We can also fill missing values using the fill_value parameter.", "The next example aggregates by taking the mean across multiple columns.", "We can also calculate multiple types of aggregations for any given value column."]}, {"name": "pandas.plotting.andrews_curves", "path": "reference/api/pandas.plotting.andrews_curves", "type": "Plotting", "text": ["Generate a matplotlib plot of Andrews curves, for visualising clusters of multivariate data.", "Andrews curves have the functional form:", "x_4 sin(2t) + x_5 cos(2t) + \u2026", "Where x coefficients correspond to the values of each dimension and t is linearly spaced between -pi and +pi. Each row of frame then corresponds to a single curve.", "Data to be plotted, preferably normalized to (0.0, 1.0).", "Colors to use for the different classes.", "Colormap to select colors from. If string, load colormap with that name from matplotlib.", "Options to pass to matplotlib plotting method.", "Examples"]}, {"name": "pandas.plotting.autocorrelation_plot", "path": "reference/api/pandas.plotting.autocorrelation_plot", "type": "Input/output", "text": ["Autocorrelation plot for time series.", "Options to pass to matplotlib plotting method.", "Examples", "The horizontal lines in the plot correspond to 95% and 99% confidence bands.", "The dashed line is 99% confidence band."]}, {"name": "pandas.plotting.bootstrap_plot", "path": "reference/api/pandas.plotting.bootstrap_plot", "type": "Plotting", "text": ["Bootstrap plot on mean, median and mid-range statistics.", "The bootstrap plot is used to estimate the uncertainty of a statistic by relaying on random sampling with replacement [1]. This function will generate bootstrapping plots for mean, median and mid-range statistics for the given number of samples of the given size.", "\u201cBootstrapping (statistics)\u201d in https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29", "Series from where to get the samplings for the bootstrapping.", "If given, it will use the fig reference for plotting instead of creating a new one with default parameters.", "Number of data points to consider during each sampling. It must be less than or equal to the length of the series.", "Number of times the bootstrap procedure is performed.", "Options to pass to matplotlib plotting method.", "Matplotlib figure.", "See also", "Basic plotting for DataFrame objects.", "Basic plotting for Series objects.", "Examples", "This example draws a basic bootstrap plot for a Series."]}, {"name": "pandas.plotting.boxplot", "path": "reference/api/pandas.plotting.boxplot", "type": "Plotting", "text": ["Make a box plot from DataFrame columns.", "Make a box-and-whisker plot from DataFrame columns, optionally grouped by some other columns. A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of box to show the range of the data. By default, they extend no more than 1.5 * IQR (IQR = Q3 - Q1) from the edges of the box, ending at the farthest data point within that interval. Outliers are plotted as separate dots.", "For further details see Wikipedia\u2019s entry for boxplot.", "Column name or list of names, or vector. Can be any valid input to pandas.DataFrame.groupby().", "Column in the DataFrame to pandas.DataFrame.groupby(). One box-plot will be done per value of columns in by.", "The matplotlib axes to be used by boxplot.", "Tick label font size in points or as a string (e.g., large).", "The rotation angle of labels (in degrees) with respect to the screen coordinate system.", "Setting this to True will show the grid.", "The size of the figure to create in matplotlib.", "For example, (3, 5) will display the subplots using 3 columns and 5 rows, starting from the top-left.", "The kind of object to return. The default is axes.", "\u2018axes\u2019 returns the matplotlib axes the boxplot is drawn on.", "\u2018dict\u2019 returns a dictionary whose values are the matplotlib Lines of the boxplot.", "\u2018both\u2019 returns a namedtuple with the axes and dict.", "when grouping with by, a Series mapping columns to return_type is returned.", "If return_type is None, a NumPy array of axes with the same shape as layout is returned.", "All other plotting keyword arguments to be passed to matplotlib.pyplot.boxplot().", "See Notes.", "See also", "Make a histogram.", "Matplotlib equivalent plot.", "Notes", "The return type depends on the return_type parameter:", "\u2018axes\u2019 : object of class matplotlib.axes.Axes", "\u2018dict\u2019 : dict of matplotlib.lines.Line2D objects", "\u2018both\u2019 : a namedtuple with structure (ax, lines)", "For data grouped with by, return a Series of the above or a numpy array:", "Series", "array (for return_type = None)", "Use return_type='dict' when you want to tweak the appearance of the lines after plotting. In this case a dict containing the Lines making up the boxes, caps, fliers, medians, and whiskers is returned.", "Examples", "Boxplots can be created for every column in the dataframe by df.boxplot() or indicating the columns to be used:", "Boxplots of variables distributions grouped by the values of a third variable can be created using the option by. For instance:", "A list of strings (i.e. ['X', 'Y']) can be passed to boxplot in order to group the data by combination of the variables in the x-axis:", "The layout of boxplot can be adjusted giving a tuple to layout:", "Additional formatting can be done to the boxplot, like suppressing the grid (grid=False), rotating the labels in the x-axis (i.e. rot=45) or changing the fontsize (i.e. fontsize=15):", "The parameter return_type can be used to select the type of element returned by boxplot. When return_type='axes' is selected, the matplotlib axes on which the boxplot is drawn are returned:", "When grouping with by, a Series mapping columns to return_type is returned:", "If return_type is None, a NumPy array of axes with the same shape as layout is returned:"]}, {"name": "pandas.plotting.deregister_matplotlib_converters", "path": "reference/api/pandas.plotting.deregister_matplotlib_converters", "type": "Plotting", "text": ["Remove pandas formatters and converters.", "Removes the custom converters added by register(). This attempts to set the state of the registry back to the state before pandas registered its own units. Converters for pandas\u2019 own types like Timestamp and Period are removed completely. Converters for types pandas overwrites, like datetime.datetime, are restored to their original value.", "See also", "Register pandas formatters and converters with matplotlib."]}, {"name": "pandas.plotting.lag_plot", "path": "reference/api/pandas.plotting.lag_plot", "type": "Plotting", "text": ["Lag plot for time series.", "Matplotlib scatter method keyword arguments.", "Examples", "Lag plots are most commonly used to look for patterns in time series data.", "Given the following time series", "A lag plot with lag=1 returns"]}, {"name": "pandas.plotting.parallel_coordinates", "path": "reference/api/pandas.plotting.parallel_coordinates", "type": "Plotting", "text": ["Parallel coordinates plotting.", "Column name containing class names.", "A list of column names to use.", "Matplotlib axis object.", "Colors to use for the different classes.", "If true, columns will be used as xticks.", "A list of values to use for xticks.", "Colormap to use for line colors.", "If true, vertical lines will be added at each xtick.", "Options to be passed to axvline method for vertical lines.", "Sort class_column labels, useful when assigning colors.", "Options to pass to matplotlib plotting method.", "Examples"]}, {"name": "pandas.plotting.plot_params", "path": "reference/api/pandas.plotting.plot_params", "type": "Plotting", "text": ["Stores pandas plotting options.", "Allows for parameter aliasing so you can just use parameter names that are the same as the plot function parameters, but is stored in a canonical format that makes it easy to breakdown into groups later."]}, {"name": "pandas.plotting.radviz", "path": "reference/api/pandas.plotting.radviz", "type": "Plotting", "text": ["Plot a multidimensional dataset in 2D.", "Each Series in the DataFrame is represented as a evenly distributed slice on a circle. Each data point is rendered in the circle according to the value on each Series. Highly correlated Series in the DataFrame are placed closer on the unit circle.", "RadViz allow to project a N-dimensional data set into a 2D space where the influence of each dimension can be interpreted as a balance between the influence of all dimensions.", "More info available at the original article describing RadViz.", "Object holding the data.", "Column name containing the name of the data point category.", "A plot instance to which to add the information.", "Assign a color to each category. Example: [\u2018blue\u2019, \u2018green\u2019].", "Colormap to select colors from. If string, load colormap with that name from matplotlib.", "Options to pass to matplotlib scatter plotting method.", "See also", "Plot clustering visualization.", "Examples"]}, {"name": "pandas.plotting.register_matplotlib_converters", "path": "reference/api/pandas.plotting.register_matplotlib_converters", "type": "Plotting", "text": ["Register pandas formatters and converters with matplotlib.", "This function modifies the global matplotlib.units.registry dictionary. pandas adds custom converters for", "pd.Timestamp", "pd.Period", "np.datetime64", "datetime.datetime", "datetime.date", "datetime.time", "See also", "Remove pandas formatters and converters."]}, {"name": "pandas.plotting.scatter_matrix", "path": "reference/api/pandas.plotting.scatter_matrix", "type": "Plotting", "text": ["Draw a matrix of scatter plots.", "Amount of transparency applied.", "A tuple (width, height) in inches.", "Setting this to True will show the grid.", "Pick between \u2018kde\u2019 and \u2018hist\u2019 for either Kernel Density Estimation or Histogram plot in the diagonal.", "Matplotlib marker type, default \u2018.\u2019.", "Keyword arguments to be passed to kernel density estimate plot.", "Keyword arguments to be passed to hist function.", "Relative extension of axis range in x and y with respect to (x_max - x_min) or (y_max - y_min).", "Keyword arguments to be passed to scatter function.", "A matrix of scatter plots.", "Examples"]}, {"name": "pandas.plotting.table", "path": "reference/api/pandas.plotting.table", "type": "Plotting", "text": ["Helper function to convert DataFrame and Series to matplotlib.table.", "Data for table contents.", "Keyword arguments to be passed to matplotlib.table.table. If rowLabels or colLabels is not specified, data index or column name will be used."]}, {"name": "pandas.qcut", "path": "reference/api/pandas.qcut", "type": "General functions", "text": ["Quantile-based discretization function.", "Discretize variable into equal-sized buckets based on rank or based on sample quantiles. For example 1000 values for 10 quantiles would produce a Categorical object indicating quantile membership for each data point.", "Number of quantiles. 10 for deciles, 4 for quartiles, etc. Alternately array of quantiles, e.g. [0, .25, .5, .75, 1.] for quartiles.", "Used as labels for the resulting bins. Must be of the same length as the resulting bins. If False, return only integer indicators of the bins. If True, raises an error.", "Whether to return the (bins, labels) or not. Can be useful if bins is given as a scalar.", "The precision at which to store and display the bins labels.", "If bin edges are not unique, raise ValueError or drop non-uniques.", "The return type (Categorical or Series) depends on the input: a Series of type category if input is a Series else Categorical. Bins are represented as categories when categorical data is returned.", "Returned only if retbins is True.", "Notes", "Out of bounds values will be NA in the resulting Categorical object", "Examples"]}, {"name": "pandas.RangeIndex", "path": "reference/api/pandas.rangeindex", "type": "Index Objects", "text": ["Immutable Index implementing a monotonic integer range.", "RangeIndex is a memory-saving special case of Int64Index limited to representing monotonic ranges. Using RangeIndex may in some instances improve computing speed.", "This is the default index type used by DataFrame and Series when no explicit index is provided by the user.", "If int and \u201cstop\u201d is not given, interpreted as \u201cstop\u201d instead.", "Unused, accepted for homogeneity with other index types.", "Unused, accepted for homogeneity with other index types.", "Name to be stored in the index.", "See also", "The base pandas Index type.", "Index of int64 data.", "Attributes", "start", "The value of the start parameter (0 if this was not supplied).", "stop", "The value of the stop parameter.", "step", "The value of the step parameter (1 if this was not supplied).", "Methods", "from_range(data[, name, dtype])", "Create RangeIndex from a range object."]}, {"name": "pandas.RangeIndex.from_range", "path": "reference/api/pandas.rangeindex.from_range", "type": "Index Objects", "text": ["Create RangeIndex from a range object."]}, {"name": "pandas.RangeIndex.start", "path": "reference/api/pandas.rangeindex.start", "type": "Index Objects", "text": ["The value of the start parameter (0 if this was not supplied)."]}, {"name": "pandas.RangeIndex.step", "path": "reference/api/pandas.rangeindex.step", "type": "Index Objects", "text": ["The value of the step parameter (1 if this was not supplied)."]}, {"name": "pandas.RangeIndex.stop", "path": "reference/api/pandas.rangeindex.stop", "type": "Index Objects", "text": ["The value of the stop parameter."]}, {"name": "pandas.read_clipboard", "path": "reference/api/pandas.read_clipboard", "type": "Input/output", "text": ["Read text from clipboard and pass to read_csv.", "A string or regex delimiter. The default of \u2018s+\u2019 denotes one or more whitespace characters.", "See read_csv for the full argument list.", "A parsed DataFrame object."]}, {"name": "pandas.read_csv", "path": "reference/api/pandas.read_csv", "type": "Input/output", "text": ["Read a comma-separated values (csv) file into DataFrame.", "Also supports optionally iterating or breaking of the file into chunks.", "Additional help can be found in the online docs for IO Tools.", "Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv.", "If you want to pass in a path object, pandas accepts any os.PathLike.", "By file-like object, we refer to objects with a read() method, such as a file handle (e.g. via builtin open function) or StringIO.", "Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python\u2019s builtin sniffer tool, csv.Sniffer. In addition, separators longer than 1 character and different from '\\s+' will be interpreted as regular expressions and will also force the use of the Python parsing engine. Note that regex delimiters are prone to ignoring quoted data. Regex example: '\\r\\t'.", "Alias for sep.", "Row number(s) to use as the column names, and the start of the data. Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first line of the file, if column names are passed explicitly then the behavior is identical to header=None. Explicitly pass header=0 to be able to replace existing names. The header can be a list of integers that specify row locations for a multi-index on the columns e.g. [0,1,3]. Intervening rows that are not specified will be skipped (e.g. 2 in this example is skipped). Note that this parameter ignores commented lines and empty lines if skip_blank_lines=True, so header=0 denotes the first line of data rather than the first line of the file.", "List of column names to use. If the file contains a header row, then you should explicitly pass header=0 to override the column names. Duplicates in this list are not allowed.", "Column(s) to use as the row labels of the DataFrame, either given as string name or column index. If a sequence of int / str is given, a MultiIndex is used.", "Note: index_col=False can be used to force pandas to not use the first column as the index, e.g. when you have a malformed file with delimiters at the end of each line.", "Return a subset of the columns. If list-like, all elements must either be positional (i.e. integer indices into the document columns) or strings that correspond to column names provided either by the user in names or inferred from the document header row(s). If names are given, the document header row(s) are not taken into account. For example, a valid list-like usecols parameter would be [0, 1, 2] or ['foo', 'bar', 'baz']. Element order is ignored, so usecols=[0, 1] is the same as [1, 0]. To instantiate a DataFrame from data with element order preserved use pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']] for columns in ['foo', 'bar'] order or pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']] for ['bar', 'foo'] order.", "If callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to True. An example of a valid callable argument would be lambda x: x.upper() in\n['AAA', 'BBB', 'DDD']. Using this parameter results in much faster parsing time and lower memory usage.", "If the parsed data only contains one column then return a Series.", "Deprecated since version 1.4.0: Append .squeeze(\"columns\") to the call to read_csv to squeeze the data.", "Prefix to add to column numbers when no header, e.g. \u2018X\u2019 for X0, X1, \u2026", "Deprecated since version 1.4.0: Use a list comprehension on the DataFrame\u2019s columns after calling read_csv.", "Duplicate columns will be specified as \u2018X\u2019, \u2018X.1\u2019, \u2026\u2019X.N\u2019, rather than \u2018X\u2019\u2026\u2019X\u2019. Passing in False will cause data to be overwritten if there are duplicate names in the columns.", "Data type for data or columns. E.g. {\u2018a\u2019: np.float64, \u2018b\u2019: np.int32, \u2018c\u2019: \u2018Int64\u2019} Use str or object together with suitable na_values settings to preserve and not interpret dtype. If converters are specified, they will be applied INSTEAD of dtype conversion.", "Parser engine to use. The C and pyarrow engines are faster, while the python engine is currently more feature-complete. Multithreading is currently only supported by the pyarrow engine.", "New in version 1.4.0: The \u201cpyarrow\u201d engine was added as an experimental engine, and some features are unsupported, or may not work correctly, with this engine.", "Dict of functions for converting values in certain columns. Keys can either be integers or column labels.", "Values to consider as True.", "Values to consider as False.", "Skip spaces after delimiter.", "Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.", "If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise. An example of a valid callable argument would be lambda x: x in [0, 2].", "Number of lines at bottom of file to skip (Unsupported with engine=\u2019c\u2019).", "Number of rows of file to read. Useful for reading pieces of large files.", "Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. By default the following values are interpreted as NaN: \u2018\u2019, \u2018#N/A\u2019, \u2018#N/A N/A\u2019, \u2018#NA\u2019, \u2018-1.#IND\u2019, \u2018-1.#QNAN\u2019, \u2018-NaN\u2019, \u2018-nan\u2019, \u20181.#IND\u2019, \u20181.#QNAN\u2019, \u2018<NA>\u2019, \u2018N/A\u2019, \u2018NA\u2019, \u2018NULL\u2019, \u2018NaN\u2019, \u2018n/a\u2019, \u2018nan\u2019, \u2018null\u2019.", "Whether or not to include the default NaN values when parsing the data. Depending on whether na_values is passed in, the behavior is as follows:", "If keep_default_na is True, and na_values are specified, na_values is appended to the default NaN values used for parsing.", "If keep_default_na is True, and na_values are not specified, only the default NaN values are used for parsing.", "If keep_default_na is False, and na_values are specified, only the NaN values specified na_values are used for parsing.", "If keep_default_na is False, and na_values are not specified, no strings will be parsed as NaN.", "Note that if na_filter is passed in as False, the keep_default_na and na_values parameters will be ignored.", "Detect missing value markers (empty strings and the value of na_values). In data without any NAs, passing na_filter=False can improve the performance of reading a large file.", "Indicate number of NA values placed in non-numeric columns.", "If True, skip over blank lines rather than interpreting as NaN values.", "The behavior is as follows:", "boolean. If True -> try parsing the index.", "list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column.", "list of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column.", "dict, e.g. {\u2018foo\u2019 : [1, 3]} -> parse columns 1, 3 as date and call result \u2018foo\u2019", "If a column or index cannot be represented as an array of datetimes, say because of an unparsable value or a mixture of timezones, the column or index will be returned unaltered as an object data type. For non-standard datetime parsing, use pd.to_datetime after pd.read_csv. To parse an index or column with a mixture of timezones, specify date_parser to be a partially-applied pandas.to_datetime() with utc=True. See Parsing a CSV with mixed timezones for more.", "Note: A fast-path exists for iso8601-formatted dates.", "If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the parsing speed by 5-10x.", "If True and parse_dates specifies combining multiple columns then keep the original columns.", "Function to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. Pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments.", "DD/MM format dates, international and European format.", "If True, use a cache of unique, converted dates to apply the datetime conversion. May produce significant speed-up when parsing duplicate date strings, especially ones with timezone offsets.", "New in version 0.25.0.", "Return TextFileReader object for iteration or getting chunks with get_chunk().", "Changed in version 1.2: TextFileReader is a context manager.", "Return TextFileReader object for iteration. See the IO Tools docs for more information on iterator and chunksize.", "Changed in version 1.2: TextFileReader is a context manager.", "For on-the-fly decompression of on-disk data. If \u2018infer\u2019 and \u2018%s\u2019 is path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). If using \u2018zip\u2019, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: compression={'method': 'zstd', 'dict_data': my_compression_dict}.", "Changed in version 1.4.0: Zstandard support.", "Thousands separator.", "Character to recognize as decimal point (e.g. use \u2018,\u2019 for European data).", "Character to break file into lines. Only valid with C parser.", "The character used to denote the start and end of a quoted item. Quoted items can include the delimiter and it will be ignored.", "Control field quoting behavior per csv.QUOTE_* constants. Use one of QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).", "When quotechar is specified and quoting is not QUOTE_NONE, indicate whether or not to interpret two consecutive quotechar elements INSIDE a field as a single quotechar element.", "One-character string used to escape other characters.", "Indicates remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines (as long as skip_blank_lines=True), fully commented lines are ignored by the parameter header but not by skiprows. For example, if comment='#', parsing #empty\\na,b,c\\n1,2,3 with header=0 will result in \u2018a,b,c\u2019 being treated as the header.", "Encoding to use for UTF when reading/writing (ex. \u2018utf-8\u2019). List of Python standard encodings .", "Changed in version 1.2: When encoding is None, errors=\"replace\" is passed to open(). Otherwise, errors=\"strict\" is passed to open(). This behavior was previously only the case for engine=\"python\".", "Changed in version 1.3.0: encoding_errors is a new argument. encoding has no longer an influence on how encoding errors are handled.", "How encoding errors are treated. List of possible values .", "New in version 1.3.0.", "If provided, this parameter will override values (default or not) for the following parameters: delimiter, doublequote, escapechar, skipinitialspace, quotechar, and quoting. If it is necessary to override values, a ParserWarning will be issued. See csv.Dialect documentation for more details.", "Lines with too many fields (e.g. a csv line with too many commas) will by default cause an exception to be raised, and no DataFrame will be returned. If False, then these \u201cbad lines\u201d will be dropped from the DataFrame that is returned.", "Deprecated since version 1.3.0: The on_bad_lines parameter should be used instead to specify behavior upon encountering a bad line instead.", "If error_bad_lines is False, and warn_bad_lines is True, a warning for each \u201cbad line\u201d will be output.", "Deprecated since version 1.3.0: The on_bad_lines parameter should be used instead to specify behavior upon encountering a bad line instead.", "Specifies what to do upon encountering a bad line (a line with too many fields). Allowed values are :", "\u2018error\u2019, raise an Exception when a bad line is encountered.", "\u2018warn\u2019, raise a warning when a bad line is encountered and skip that line.", "\u2018skip\u2019, skip bad lines without raising or warning when they are encountered.", "New in version 1.3.0: ", "callable, function with signature (bad_line: list[str]) -> list[str] | None that will process a single bad line. bad_line is a list of strings split by the sep. If the function returns None`, the bad line will be ignored.\nIf the function returns a new list of strings with more elements than\nexpected, a ``ParserWarning will be emitted while dropping extra elements. Only supported when engine=\"python\"", "New in version 1.4.0.", "Specifies whether or not whitespace (e.g. ' ' or '\u00a0\u00a0\u00a0 ') will be used as the sep. Equivalent to setting sep='\\s+'. If this option is set to True, nothing should be passed in for the delimiter parameter.", "Internally process the file in chunks, resulting in lower memory use while parsing, but possibly mixed type inference. To ensure no mixed types either set False, or specify the type with the dtype parameter. Note that the entire file is read into a single DataFrame regardless, use the chunksize or iterator parameter to return the data in chunks. (Only valid with C parser).", "If a filepath is provided for filepath_or_buffer, map the file object directly onto memory and access the data directly from there. Using this option can improve performance because there is no longer any I/O overhead.", "Specifies which converter the C engine should use for floating-point values. The options are None or \u2018high\u2019 for the ordinary converter, \u2018legacy\u2019 for the original lower precision pandas converter, and \u2018round_trip\u2019 for the round-trip converter.", "Changed in version 1.2.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.", "A comma-separated values (csv) file is returned as two-dimensional data structure with labeled axes.", "See also", "Write DataFrame to a comma-separated values (csv) file.", "Read a comma-separated values (csv) file into DataFrame.", "Read a table of fixed-width formatted lines into DataFrame.", "Examples"]}, {"name": "pandas.read_excel", "path": "reference/api/pandas.read_excel", "type": "Input/output", "text": ["Read an Excel file into a pandas DataFrame.", "Supports xls, xlsx, xlsm, xlsb, odf, ods and odt file extensions read from a local filesystem or URL. Supports an option to read a single sheet or a list of sheets.", "Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.xlsx.", "If you want to pass in a path object, pandas accepts any os.PathLike.", "By file-like object, we refer to objects with a read() method, such as a file handle (e.g. via builtin open function) or StringIO.", "Strings are used for sheet names. Integers are used in zero-indexed sheet positions (chart sheets do not count as a sheet position). Lists of strings/integers are used to request multiple sheets. Specify None to get all worksheets.", "Available cases:", "Defaults to 0: 1st sheet as a DataFrame", "1: 2nd sheet as a DataFrame", "\"Sheet1\": Load sheet with name \u201cSheet1\u201d", "[0, 1, \"Sheet5\"]: Load first, second and sheet named \u201cSheet5\u201d as a dict of DataFrame", "None: All worksheets.", "Row (0-indexed) to use for the column labels of the parsed DataFrame. If a list of integers is passed those row positions will be combined into a MultiIndex. Use None if there is no header.", "List of column names to use. If file contains no header row, then you should explicitly pass header=None.", "Column (0-indexed) to use as the row labels of the DataFrame. Pass None if there is no such column. If a list is passed, those columns will be combined into a MultiIndex. If a subset of data is selected with usecols, index_col is based on the subset.", "If None, then parse all columns.", "If str, then indicates comma separated list of Excel column letters and column ranges (e.g. \u201cA:E\u201d or \u201cA,C,E:F\u201d). Ranges are inclusive of both sides.", "If list of int, then indicates list of column numbers to be parsed.", "If list of string, then indicates list of column names to be parsed.", "If callable, then evaluate each column name against it and parse the column if the callable returns True.", "Returns a subset of the columns according to behavior above.", "If the parsed data only contains one column then return a Series.", "Deprecated since version 1.4.0: Append .squeeze(\"columns\") to the call to read_excel to squeeze the data.", "Data type for data or columns. E.g. {\u2018a\u2019: np.float64, \u2018b\u2019: np.int32} Use object to preserve data as stored in Excel and not interpret dtype. If converters are specified, they will be applied INSTEAD of dtype conversion.", "If io is not a buffer or path, this must be set to identify io. Supported engines: \u201cxlrd\u201d, \u201copenpyxl\u201d, \u201codf\u201d, \u201cpyxlsb\u201d. Engine compatibility :", "\u201cxlrd\u201d supports old-style Excel files (.xls).", "\u201copenpyxl\u201d supports newer Excel file formats.", "\u201codf\u201d supports OpenDocument file formats (.odf, .ods, .odt).", "\u201cpyxlsb\u201d supports Binary Excel files.", "Changed in version 1.2.0: The engine xlrd now only supports old-style .xls files. When engine=None, the following logic will be used to determine the engine:", "If path_or_buffer is an OpenDocument format (.odf, .ods, .odt), then odf will be used.", "Otherwise if path_or_buffer is an xls format, xlrd will be used.", "Otherwise if path_or_buffer is in xlsb format, pyxlsb will be used.", "New in version 1.3.0.", "Otherwise openpyxl will be used.", "Changed in version 1.3.0.", "Dict of functions for converting values in certain columns. Keys can either be integers or column labels, values are functions that take one input argument, the Excel cell content, and return the transformed content.", "Values to consider as True.", "Values to consider as False.", "Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file. If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise. An example of a valid callable argument would be lambda\nx: x in [0, 2].", "Number of rows to parse.", "Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. By default the following values are interpreted as NaN: \u2018\u2019, \u2018#N/A\u2019, \u2018#N/A N/A\u2019, \u2018#NA\u2019, \u2018-1.#IND\u2019, \u2018-1.#QNAN\u2019, \u2018-NaN\u2019, \u2018-nan\u2019, \u20181.#IND\u2019, \u20181.#QNAN\u2019, \u2018<NA>\u2019, \u2018N/A\u2019, \u2018NA\u2019, \u2018NULL\u2019, \u2018NaN\u2019, \u2018n/a\u2019, \u2018nan\u2019, \u2018null\u2019.", "Whether or not to include the default NaN values when parsing the data. Depending on whether na_values is passed in, the behavior is as follows:", "If keep_default_na is True, and na_values are specified, na_values is appended to the default NaN values used for parsing.", "If keep_default_na is True, and na_values are not specified, only the default NaN values are used for parsing.", "If keep_default_na is False, and na_values are specified, only the NaN values specified na_values are used for parsing.", "If keep_default_na is False, and na_values are not specified, no strings will be parsed as NaN.", "Note that if na_filter is passed in as False, the keep_default_na and na_values parameters will be ignored.", "Detect missing value markers (empty strings and the value of na_values). In data without any NAs, passing na_filter=False can improve the performance of reading a large file.", "Indicate number of NA values placed in non-numeric columns.", "The behavior is as follows:", "bool. If True -> try parsing the index.", "list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column.", "list of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column.", "dict, e.g. {\u2018foo\u2019 : [1, 3]} -> parse columns 1, 3 as date and call result \u2018foo\u2019", "If a column or index contains an unparsable date, the entire column or index will be returned unaltered as an object data type. If you don`t want to parse some cells as date just change their type in Excel to \u201cText\u201d. For non-standard datetime parsing, use pd.to_datetime after pd.read_excel.", "Note: A fast-path exists for iso8601-formatted dates.", "Function to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. Pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments.", "Thousands separator for parsing string columns to numeric. Note that this parameter is only necessary for columns stored as TEXT in Excel, any numeric columns will automatically be parsed, regardless of display format.", "Character to recognize as decimal point for parsing string columns to numeric. Note that this parameter is only necessary for columns stored as TEXT in Excel, any numeric columns will automatically be parsed, regardless of display format.(e.g. use \u2018,\u2019 for European data).", "New in version 1.4.0.", "Comments out remainder of line. Pass a character or characters to this argument to indicate comments in the input file. Any data between the comment string and the end of the current line is ignored.", "Rows at the end to skip (0-indexed).", "Convert integral floats to int (i.e., 1.0 \u2013> 1). If False, all numeric data will be read in as floats: Excel stores all numbers as floats internally.", "Deprecated since version 1.3.0: convert_float will be removed in a future version", "Duplicate columns will be specified as \u2018X\u2019, \u2018X.1\u2019, \u2026\u2019X.N\u2019, rather than \u2018X\u2019\u2026\u2019X\u2019. Passing in False will cause data to be overwritten if there are duplicate names in the columns.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc., if using a URL that will be parsed by fsspec, e.g., starting \u201cs3://\u201d, \u201cgcs://\u201d. An error will be raised if providing this argument with a local path or a file-like buffer. See the fsspec and backend storage implementation docs for the set of allowed keys and values.", "New in version 1.2.0.", "DataFrame from the passed in Excel file. See notes in sheet_name argument for more information on when a dict of DataFrames is returned.", "See also", "Write DataFrame to an Excel file.", "Write DataFrame to a comma-separated values (csv) file.", "Read a comma-separated values (csv) file into DataFrame.", "Read a table of fixed-width formatted lines into DataFrame.", "Examples", "The file can be read using the file name as string or an open file object:", "Index and header can be specified via the index_col and header arguments", "Column types are inferred but can be explicitly specified", "True, False, and NA values, and thousands separators have defaults, but can be explicitly specified, too. Supply the values you would like as strings or lists of strings!", "Comment lines in the excel input file can be skipped using the comment kwarg"]}, {"name": "pandas.read_feather", "path": "reference/api/pandas.read_feather", "type": "Input/output", "text": ["Load a feather-format object from the file path.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a binary read() function. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.feather.", "If not provided, all columns are read.", "Whether to parallelize reading using multiple threads.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.0."]}, {"name": "pandas.read_fwf", "path": "reference/api/pandas.read_fwf", "type": "Input/output", "text": ["Read a table of fixed-width formatted lines into DataFrame.", "Also supports optionally iterating or breaking of the file into chunks.", "Additional help can be found in the online docs for IO Tools.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a text read() function.The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv.", "A list of tuples giving the extents of the fixed-width fields of each line as half-open intervals (i.e., [from, to[ ). String value \u2018infer\u2019 can be used to instruct the parser to try detecting the column specifications from the first 100 rows of the data which are not being skipped via skiprows (default=\u2019infer\u2019).", "A list of field widths which can be used instead of \u2018colspecs\u2019 if the intervals are contiguous.", "The number of rows to consider when letting the parser determine the colspecs.", "Optional keyword arguments can be passed to TextFileReader.", "A comma-separated values (csv) file is returned as two-dimensional data structure with labeled axes.", "See also", "Write DataFrame to a comma-separated values (csv) file.", "Read a comma-separated values (csv) file into DataFrame.", "Examples"]}, {"name": "pandas.read_gbq", "path": "reference/api/pandas.read_gbq", "type": "Input/output", "text": ["Load data from Google BigQuery.", "This function requires the pandas-gbq package.", "See the How to authenticate with Google BigQuery guide for authentication instructions.", "SQL-Like Query to return data values.", "Google BigQuery Account project ID. Optional when available from the environment.", "Name of result column to use for index in results DataFrame.", "List of BigQuery column names in the desired order for results DataFrame.", "Force Google BigQuery to re-authenticate the user. This is useful if multiple accounts are used.", "Use the local webserver flow instead of the console flow when getting user credentials.", "New in version 0.2.0 of pandas-gbq.", "Note: The default value is changing to \u2018standard\u2019 in a future version.", "SQL syntax dialect to use. Value can be one of:", "Use BigQuery\u2019s legacy SQL dialect. For more information see BigQuery Legacy SQL Reference.", "Use BigQuery\u2019s standard SQL, which is compliant with the SQL 2011 standard. For more information see BigQuery Standard SQL Reference.", "Location where the query job should run. See the BigQuery locations documentation for a list of available locations. The location must match that of any datasets used in the query.", "New in version 0.5.0 of pandas-gbq.", "Query config parameters for job processing. For example:", "configuration = {\u2018query\u2019: {\u2018useQueryCache\u2019: False}}", "For more information see BigQuery REST API Reference.", "Credentials for accessing Google APIs. Use this parameter to override default credentials, such as to use Compute Engine google.auth.compute_engine.Credentials or Service Account google.oauth2.service_account.Credentials directly.", "New in version 0.8.0 of pandas-gbq.", "Use the BigQuery Storage API to download query results quickly, but at an increased cost. To use this API, first enable it in the Cloud Console. You must also have the bigquery.readsessions.create permission on the project you are billing queries to.", "This feature requires version 0.10.0 or later of the pandas-gbq package. It also requires the google-cloud-bigquery-storage and fastavro packages.", "New in version 0.25.0.", "If set, limit the maximum number of rows to fetch from the query results.", "New in version 0.12.0 of pandas-gbq.", "New in version 1.1.0.", "If set, use the tqdm library to display a progress bar while the data downloads. Install the tqdm package to use this feature.", "Possible values of progress_bar_type include:", "No progress bar.", "Use the tqdm.tqdm() function to print a progress bar to sys.stderr.", "Use the tqdm.tqdm_notebook() function to display a progress bar as a Jupyter notebook widget.", "Use the tqdm.tqdm_gui() function to display a progress bar as a graphical dialog box.", "Note that this feature requires version 0.12.0 or later of the pandas-gbq package. And it requires the tqdm package. Slightly different than pandas-gbq, here the default is None.", "New in version 1.0.0.", "DataFrame representing results of query.", "See also", "This function in the pandas-gbq library.", "Write a DataFrame to Google BigQuery."]}, {"name": "pandas.read_hdf", "path": "reference/api/pandas.read_hdf", "type": "Input/output", "text": ["Read from the store, close it if we opened it.", "Retrieve pandas object stored in file, optionally based on where criteria.", "Warning", "Pandas uses PyTables for reading and writing HDF5 files, which allows serializing object-dtype data with pickle when using the \u201cfixed\u201d format. Loading pickled data received from untrusted sources can be unsafe.", "See: https://docs.python.org/3/library/pickle.html for more.", "Any valid string path is acceptable. Only supports the local file system, remote URLs and file-like objects are not supported.", "If you want to pass in a path object, pandas accepts any os.PathLike.", "Alternatively, pandas accepts an open pandas.HDFStore object.", "The group identifier in the store. Can be omitted if the HDF file contains a single pandas object.", "Mode to use when opening the file. Ignored if path_or_buf is a pandas.HDFStore. Default is \u2018r\u2019.", "Specifies how encoding and decoding errors are to be handled. See the errors argument for open() for a full list of options.", "A list of Term (or convertible) objects.", "Row number to start selection.", "Row number to stop selection.", "A list of columns names to return.", "Return an iterator object.", "Number of rows to include in an iteration when using an iterator.", "Additional keyword arguments passed to HDFStore.", "The selected object. Return type depends on the object stored.", "See also", "Write a HDF file from a DataFrame.", "Low-level access to HDF files.", "Examples"]}, {"name": "pandas.read_html", "path": "reference/api/pandas.read_html", "type": "Input/output", "text": ["Read HTML tables into a list of DataFrame objects.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a string read() function. The string can represent a URL or the HTML itself. Note that lxml only accepts the http, ftp and file url protocols. If you have a URL that starts with 'https' you might try removing the 's'.", "The set of tables containing text matching this regex or string will be returned. Unless the HTML is extremely simple you will probably need to pass a non-empty string here. Defaults to \u2018.+\u2019 (match any non-empty string). The default value will return all tables contained on a page. This value is converted to a regular expression so that there is consistent behavior between Beautiful Soup and lxml.", "The parsing engine to use. \u2018bs4\u2019 and \u2018html5lib\u2019 are synonymous with each other, they are both there for backwards compatibility. The default of None tries to use lxml to parse and if that fails it falls back on bs4 + html5lib.", "The row (or list of rows for a MultiIndex) to use to make the columns headers.", "The column (or list of columns) to use to create the index.", "Number of rows to skip after parsing the column integer. 0-based. If a sequence of integers or a slice is given, will skip the rows indexed by that sequence. Note that a single element sequence means \u2018skip the nth row\u2019 whereas an integer means \u2018skip n rows\u2019.", "This is a dictionary of attributes that you can pass to use to identify the table in the HTML. These are not checked for validity before being passed to lxml or Beautiful Soup. However, these attributes must be valid HTML table attributes to work correctly. For example,", "is a valid attribute dictionary because the \u2018id\u2019 HTML tag attribute is a valid HTML attribute for any HTML tag as per this document.", "is not a valid attribute dictionary because \u2018asdf\u2019 is not a valid HTML attribute even if it is a valid XML attribute. Valid HTML 4.01 table attributes can be found here. A working draft of the HTML 5 spec can be found here. It contains the latest information on table attributes for the modern web.", "See read_csv() for more details.", "Separator to use to parse thousands. Defaults to ','.", "The encoding used to decode the web page. Defaults to None.``None`` preserves the previous encoding behavior, which depends on the underlying parser library (e.g., the parser library will try to use the encoding provided by the document).", "Character to recognize as decimal point (e.g. use \u2018,\u2019 for European data).", "Dict of functions for converting values in certain columns. Keys can either be integers or column labels, values are functions that take one input argument, the cell (not column) content, and return the transformed content.", "Custom NA values.", "If na_values are specified and keep_default_na is False the default NaN values are overridden, otherwise they\u2019re appended to.", "Whether elements with \u201cdisplay: none\u201d should be parsed.", "A list of DataFrames.", "See also", "Read a comma-separated values (csv) file into DataFrame.", "Notes", "Before using this function you should read the gotchas about the HTML parsing libraries.", "Expect to do some cleanup after you call this function. For example, you might need to manually assign column names if the column names are converted to NaN when you pass the header=0 argument. We try to assume as little as possible about the structure of the table and push the idiosyncrasies of the HTML contained in the table to the user.", "This function searches for <table> elements and only for <tr> and <th> rows and <td> elements within each <tr> or <th> element in the table. <td> stands for \u201ctable data\u201d. This function attempts to properly handle colspan and rowspan attributes. If the function has a <thead> argument, it is used to construct the header, otherwise the function attempts to find the header within the body (by putting rows with only <th> elements into the header).", "Similar to read_csv() the header argument is applied after skiprows is applied.", "This function will always return a list of DataFrame or it will fail, e.g., it will not return an empty list.", "Examples", "See the read_html documentation in the IO section of the docs for some examples of reading in HTML tables."]}, {"name": "pandas.read_json", "path": "reference/api/pandas.read_json", "type": "Input/output", "text": ["Convert a JSON string to pandas object.", "Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.json.", "If you want to pass in a path object, pandas accepts any os.PathLike.", "By file-like object, we refer to objects with a read() method, such as a file handle (e.g. via builtin open function) or StringIO.", "Indication of expected JSON string format. Compatible JSON strings can be produced by to_json() with a corresponding orient value. The set of possible orients is:", "'split' : dict like {index -> [index], columns -> [columns], data -> [values]}", "'records' : list like [{column -> value}, ... , {column -> value}]", "'index' : dict like {index -> {column -> value}}", "'columns' : dict like {column -> {index -> value}}", "'values' : just the values array", "The allowed and default values depend on the value of the typ parameter.", "when typ == 'series',", "allowed orients are {'split','records','index'}", "default is 'index'", "The Series index must be unique for orient 'index'.", "when typ == 'frame',", "allowed orients are {'split','records','index',\n'columns','values', 'table'}", "default is 'columns'", "The DataFrame index must be unique for orients 'index' and 'columns'.", "The DataFrame columns must be unique for orients 'index', 'columns', and 'records'.", "The type of object to recover.", "If True, infer dtypes; if a dict of column to dtype, then use those; if False, then don\u2019t infer dtypes at all, applies only to the data.", "For all orient values except 'table', default is True.", "Changed in version 0.25.0: Not applicable for orient='table'.", "Try to convert the axes to the proper dtypes.", "For all orient values except 'table', default is True.", "Changed in version 0.25.0: Not applicable for orient='table'.", "If True then default datelike columns may be converted (depending on keep_default_dates). If False, no dates will be converted. If a list of column names, then those columns will be converted and default datelike columns may also be converted (depending on keep_default_dates).", "If parsing dates (convert_dates is not False), then try to parse the default datelike columns. A column label is datelike if", "it ends with '_at',", "it ends with '_time',", "it begins with 'timestamp',", "it is 'modified', or", "it is 'date'.", "Direct decoding to numpy arrays. Supports numeric data only, but non-numeric column and index labels are supported. Note also that the JSON ordering MUST be the same for each term if numpy=True.", "Deprecated since version 1.0.0.", "Set to enable usage of higher precision (strtod) function when decoding string to double values. Default (False) is to use fast but less precise builtin functionality.", "The timestamp unit to detect if converting dates. The default behaviour is to try and detect the correct precision, but if this is not desired then pass one of \u2018s\u2019, \u2018ms\u2019, \u2018us\u2019 or \u2018ns\u2019 to force parsing only seconds, milliseconds, microseconds or nanoseconds respectively.", "The encoding to use to decode py3 bytes.", "How encoding errors are treated. List of possible values .", "New in version 1.3.0.", "Read the file as a json object per line.", "Return JsonReader object for iteration. See the line-delimited json docs for more information on chunksize. This can only be passed if lines=True. If this is None, the file will be read into memory all at once.", "Changed in version 1.2: JsonReader is a context manager.", "For on-the-fly decompression of on-disk data. If \u2018infer\u2019 and \u2018path_or_buf\u2019 is path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). If using \u2018zip\u2019, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: compression={'method': 'zstd', 'dict_data': my_compression_dict}.", "Changed in version 1.4.0: Zstandard support.", "The number of lines from the line-delimited jsonfile that has to be read. This can only be passed if lines=True. If this is None, all the rows will be returned.", "New in version 1.1.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.0.", "The type returned depends on the value of typ.", "See also", "Convert a DataFrame to a JSON string.", "Convert a Series to a JSON string.", "Normalize semi-structured JSON data into a flat table.", "Notes", "Specific to orient='table', if a DataFrame with a literal Index name of index gets written with to_json(), the subsequent read operation will incorrectly set the Index name to None. This is because index is also used by DataFrame.to_json() to denote a missing Index name, and the subsequent read_json() operation cannot distinguish between the two. The same limitation is encountered with a MultiIndex and any names beginning with 'level_'.", "Examples", "Encoding/decoding a Dataframe using 'split' formatted JSON:", "Encoding/decoding a Dataframe using 'index' formatted JSON:", "Encoding/decoding a Dataframe using 'records' formatted JSON. Note that index labels are not preserved with this encoding.", "Encoding with Table Schema"]}, {"name": "pandas.read_orc", "path": "reference/api/pandas.read_orc", "type": "Input/output", "text": ["Load an ORC object from the file path, returning a DataFrame.", "New in version 1.0.0.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a binary read() function. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.orc.", "If not None, only these columns will be read from the file.", "Any additional kwargs are passed to pyarrow.", "Notes", "Before using this function you should read the user guide about ORC and install optional dependencies."]}, {"name": "pandas.read_parquet", "path": "reference/api/pandas.read_parquet", "type": "Input/output", "text": ["Load a parquet object from the file path, returning a DataFrame.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a binary read() function. The string could be a URL. Valid URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.parquet. A file URL can also be a path to a directory that contains multiple partitioned parquet files. Both pyarrow and fastparquet support paths to directories as well as file URLs. A directory path could be: file://localhost/path/to/tables or s3://bucket/partition_dir.", "Parquet library to use. If \u2018auto\u2019, then the option io.parquet.engine is used. The default io.parquet.engine behavior is to try \u2018pyarrow\u2019, falling back to \u2018fastparquet\u2019 if \u2018pyarrow\u2019 is unavailable.", "If not None, only these columns will be read from the file.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.3.0.", "If True, use dtypes that use pd.NA as missing value indicator for the resulting DataFrame. (only applicable for the pyarrow engine) As new dtypes are added that support pd.NA in the future, the output with this option will change to use those dtypes. Note: this is an experimental option, and behaviour (e.g. additional support dtypes) may change without notice.", "New in version 1.2.0.", "Any additional kwargs are passed to the engine."]}, {"name": "pandas.read_pickle", "path": "reference/api/pandas.read_pickle", "type": "Input/output", "text": ["Load pickled pandas object (or any object) from file.", "Warning", "Loading pickled data received from untrusted sources can be unsafe. See here.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a binary readlines() function.", "Changed in version 1.0.0: Accept URL. URL is not limited to S3 and GCS.", "For on-the-fly decompression of on-disk data. If \u2018infer\u2019 and \u2018filepath_or_buffer\u2019 is path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). If using \u2018zip\u2019, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: compression={'method': 'zstd', 'dict_data': my_compression_dict}.", "Changed in version 1.4.0: Zstandard support.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.0.", "See also", "Pickle (serialize) DataFrame object to file.", "Pickle (serialize) Series object to file.", "Read HDF5 file into a DataFrame.", "Read SQL query or database table into a DataFrame.", "Load a parquet object, returning a DataFrame.", "Notes", "read_pickle is only guaranteed to be backwards compatible to pandas 0.20.3.", "Examples"]}, {"name": "pandas.read_sas", "path": "reference/api/pandas.read_sas", "type": "Input/output", "text": ["Read SAS files stored as either XPORT or SAS7BDAT format files.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a binary read() function. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.sas.", "If None, file format is inferred from file extension. If \u2018xport\u2019 or \u2018sas7bdat\u2019, uses the corresponding format.", "Identifier of column that should be used as index of the DataFrame.", "Encoding for text data. If None, text data are stored as raw bytes.", "Read file chunksize lines at a time, returns iterator.", "Changed in version 1.2: TextFileReader is a context manager.", "If True, returns an iterator for reading the file incrementally.", "Changed in version 1.2: TextFileReader is a context manager."]}, {"name": "pandas.read_spss", "path": "reference/api/pandas.read_spss", "type": "Input/output", "text": ["Load an SPSS file from the file path, returning a DataFrame.", "New in version 0.25.0.", "File path.", "Return a subset of the columns. If None, return all columns.", "Convert categorical columns into pd.Categorical."]}, {"name": "pandas.read_sql", "path": "reference/api/pandas.read_sql", "type": "Input/output", "text": ["Read SQL query or database table into a DataFrame.", "This function is a convenience wrapper around read_sql_table and read_sql_query (for backward compatibility). It will delegate to the specific function depending on the provided input. A SQL query will be routed to read_sql_query, while a database table name will be routed to read_sql_table. Note that the delegated function might have more specific notes about their functionality not listed here.", "SQL query to be executed or a table name.", "Using SQLAlchemy makes it possible to use any DB supported by that library. If a DBAPI2 object, only sqlite3 is supported. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable; str connections are closed automatically. See here.", "Column(s) to set as index(MultiIndex).", "Attempts to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point, useful for SQL result sets.", "List of parameters to pass to execute method. The syntax used to pass parameters is database driver dependent. Check your database driver documentation for which of the five syntax styles, described in PEP 249\u2019s paramstyle, is supported. Eg. for psycopg2, uses %(name)s so use params={\u2018name\u2019 : \u2018value\u2019}.", "List of column names to parse as dates.", "Dict of {column_name: format string} where format string is strftime compatible in case of parsing string times, or is one of (D, s, ns, ms, us) in case of parsing integer timestamps.", "Dict of {column_name: arg dict}, where the arg dict corresponds to the keyword arguments of pandas.to_datetime() Especially useful with databases without native Datetime support, such as SQLite.", "List of column names to select from SQL table (only used when reading a table).", "If specified, return an iterator where chunksize is the number of rows to include in each chunk.", "See also", "Read SQL database table into a DataFrame.", "Read SQL query into a DataFrame.", "Examples", "Read data from SQL via either a SQL query or a SQL tablename. When using a SQLite database only SQL queries are accepted, providing only the SQL tablename will result in an error.", "Apply date parsing to columns through the parse_dates argument", "The parse_dates argument calls pd.to_datetime on the provided columns. Custom argument values for applying pd.to_datetime on a column are specified via a dictionary format: 1. Ignore errors while parsing the values of \u201cdate_column\u201d", "Apply a dayfirst date parsing order on the values of \u201cdate_column\u201d", "Apply custom formatting when date parsing the values of \u201cdate_column\u201d"]}, {"name": "pandas.read_sql_query", "path": "reference/api/pandas.read_sql_query", "type": "Input/output", "text": ["Read SQL query into a DataFrame.", "Returns a DataFrame corresponding to the result set of the query string. Optionally provide an index_col parameter to use one of the columns as the index, otherwise default integer index will be used.", "SQL query to be executed.", "Using SQLAlchemy makes it possible to use any DB supported by that library. If a DBAPI2 object, only sqlite3 is supported.", "Column(s) to set as index(MultiIndex).", "Attempts to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point. Useful for SQL result sets.", "List of parameters to pass to execute method. The syntax used to pass parameters is database driver dependent. Check your database driver documentation for which of the five syntax styles, described in PEP 249\u2019s paramstyle, is supported. Eg. for psycopg2, uses %(name)s so use params={\u2018name\u2019 : \u2018value\u2019}.", "List of column names to parse as dates.", "Dict of {column_name: format string} where format string is strftime compatible in case of parsing string times, or is one of (D, s, ns, ms, us) in case of parsing integer timestamps.", "Dict of {column_name: arg dict}, where the arg dict corresponds to the keyword arguments of pandas.to_datetime() Especially useful with databases without native Datetime support, such as SQLite.", "If specified, return an iterator where chunksize is the number of rows to include in each chunk.", "Data type for data or columns. E.g. np.float64 or {\u2018a\u2019: np.float64, \u2018b\u2019: np.int32, \u2018c\u2019: \u2018Int64\u2019}.", "New in version 1.3.0.", "See also", "Read SQL database table into a DataFrame.", "Read SQL query or database table into a DataFrame.", "Notes", "Any datetime values with time zone information parsed via the parse_dates parameter will be converted to UTC."]}, {"name": "pandas.read_sql_table", "path": "reference/api/pandas.read_sql_table", "type": "Input/output", "text": ["Read SQL database table into a DataFrame.", "Given a table name and a SQLAlchemy connectable, returns a DataFrame. This function does not support DBAPI connections.", "Name of SQL table in database.", "A database URI could be provided as str. SQLite DBAPI connection mode not supported.", "Name of SQL schema in database to query (if database flavor supports this). Uses default schema if None (default).", "Column(s) to set as index(MultiIndex).", "Attempts to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point. Can result in loss of Precision.", "List of column names to parse as dates.", "Dict of {column_name: format string} where format string is strftime compatible in case of parsing string times or is one of (D, s, ns, ms, us) in case of parsing integer timestamps.", "Dict of {column_name: arg dict}, where the arg dict corresponds to the keyword arguments of pandas.to_datetime() Especially useful with databases without native Datetime support, such as SQLite.", "List of column names to select from SQL table.", "If specified, returns an iterator where chunksize is the number of rows to include in each chunk.", "A SQL table is returned as two-dimensional data structure with labeled axes.", "See also", "Read SQL query into a DataFrame.", "Read SQL query or database table into a DataFrame.", "Notes", "Any datetime values with time zone information will be converted to UTC.", "Examples"]}, {"name": "pandas.read_stata", "path": "reference/api/pandas.read_stata", "type": "Input/output", "text": ["Read Stata file into DataFrame.", "Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.dta.", "If you want to pass in a path object, pandas accepts any os.PathLike.", "By file-like object, we refer to objects with a read() method, such as a file handle (e.g. via builtin open function) or StringIO.", "Convert date variables to DataFrame time values.", "Read value labels and convert columns to Categorical/Factor variables.", "Column to set as index.", "Flag indicating whether to convert missing values to their Stata representations. If False, missing values are replaced with nan. If True, columns containing missing values are returned with object data types and missing values are represented by StataMissingValue objects.", "Preserve Stata datatypes. If False, numeric data are upcast to pandas default types for foreign data (float64 or int64).", "Columns to retain. Columns will be returned in the given order. None returns all columns.", "Flag indicating whether converted categorical data are ordered.", "Return StataReader object for iterations, returns chunks with given number of lines.", "Return StataReader object.", "For on-the-fly decompression of on-disk data. If \u2018infer\u2019 and \u2018%s\u2019 is path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). If using \u2018zip\u2019, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: compression={'method': 'zstd', 'dict_data': my_compression_dict}.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "See also", "Low-level reader for Stata data files.", "Export Stata data files.", "Notes", "Categorical variables read through an iterator may not have the same categories and dtype. This occurs when a variable stored in a DTA file is associated to an incomplete set of value labels that only label a strict subset of the values.", "Examples", "Creating a dummy stata for this example >>> df = pd.DataFrame({\u2018animal\u2019: [\u2018falcon\u2019, \u2018parrot\u2019, \u2018falcon\u2019, \u2026 \u2018parrot\u2019], \u2026 \u2018speed\u2019: [350, 18, 361, 15]}) # doctest: +SKIP >>> df.to_stata(\u2018animals.dta\u2019) # doctest: +SKIP", "Read a Stata dta file:", "Read a Stata dta file in 10,000 line chunks: >>> values = np.random.randint(0, 10, size=(20_000, 1), dtype=\u201duint8\u201d) # doctest: +SKIP >>> df = pd.DataFrame(values, columns=[\u201ci\u201d]) # doctest: +SKIP >>> df.to_stata(\u2018filename.dta\u2019) # doctest: +SKIP"]}, {"name": "pandas.read_table", "path": "reference/api/pandas.read_table", "type": "Input/output", "text": ["Read general delimited file into DataFrame.", "Also supports optionally iterating or breaking of the file into chunks.", "Additional help can be found in the online docs for IO Tools.", "Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv.", "If you want to pass in a path object, pandas accepts any os.PathLike.", "By file-like object, we refer to objects with a read() method, such as a file handle (e.g. via builtin open function) or StringIO.", "Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python\u2019s builtin sniffer tool, csv.Sniffer. In addition, separators longer than 1 character and different from '\\s+' will be interpreted as regular expressions and will also force the use of the Python parsing engine. Note that regex delimiters are prone to ignoring quoted data. Regex example: '\\r\\t'.", "Alias for sep.", "Row number(s) to use as the column names, and the start of the data. Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first line of the file, if column names are passed explicitly then the behavior is identical to header=None. Explicitly pass header=0 to be able to replace existing names. The header can be a list of integers that specify row locations for a multi-index on the columns e.g. [0,1,3]. Intervening rows that are not specified will be skipped (e.g. 2 in this example is skipped). Note that this parameter ignores commented lines and empty lines if skip_blank_lines=True, so header=0 denotes the first line of data rather than the first line of the file.", "List of column names to use. If the file contains a header row, then you should explicitly pass header=0 to override the column names. Duplicates in this list are not allowed.", "Column(s) to use as the row labels of the DataFrame, either given as string name or column index. If a sequence of int / str is given, a MultiIndex is used.", "Note: index_col=False can be used to force pandas to not use the first column as the index, e.g. when you have a malformed file with delimiters at the end of each line.", "Return a subset of the columns. If list-like, all elements must either be positional (i.e. integer indices into the document columns) or strings that correspond to column names provided either by the user in names or inferred from the document header row(s). If names are given, the document header row(s) are not taken into account. For example, a valid list-like usecols parameter would be [0, 1, 2] or ['foo', 'bar', 'baz']. Element order is ignored, so usecols=[0, 1] is the same as [1, 0]. To instantiate a DataFrame from data with element order preserved use pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']] for columns in ['foo', 'bar'] order or pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']] for ['bar', 'foo'] order.", "If callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to True. An example of a valid callable argument would be lambda x: x.upper() in\n['AAA', 'BBB', 'DDD']. Using this parameter results in much faster parsing time and lower memory usage.", "If the parsed data only contains one column then return a Series.", "Deprecated since version 1.4.0: Append .squeeze(\"columns\") to the call to read_table to squeeze the data.", "Prefix to add to column numbers when no header, e.g. \u2018X\u2019 for X0, X1, \u2026", "Deprecated since version 1.4.0: Use a list comprehension on the DataFrame\u2019s columns after calling read_csv.", "Duplicate columns will be specified as \u2018X\u2019, \u2018X.1\u2019, \u2026\u2019X.N\u2019, rather than \u2018X\u2019\u2026\u2019X\u2019. Passing in False will cause data to be overwritten if there are duplicate names in the columns.", "Data type for data or columns. E.g. {\u2018a\u2019: np.float64, \u2018b\u2019: np.int32, \u2018c\u2019: \u2018Int64\u2019} Use str or object together with suitable na_values settings to preserve and not interpret dtype. If converters are specified, they will be applied INSTEAD of dtype conversion.", "Parser engine to use. The C and pyarrow engines are faster, while the python engine is currently more feature-complete. Multithreading is currently only supported by the pyarrow engine.", "New in version 1.4.0: The \u201cpyarrow\u201d engine was added as an experimental engine, and some features are unsupported, or may not work correctly, with this engine.", "Dict of functions for converting values in certain columns. Keys can either be integers or column labels.", "Values to consider as True.", "Values to consider as False.", "Skip spaces after delimiter.", "Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.", "If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise. An example of a valid callable argument would be lambda x: x in [0, 2].", "Number of lines at bottom of file to skip (Unsupported with engine=\u2019c\u2019).", "Number of rows of file to read. Useful for reading pieces of large files.", "Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. By default the following values are interpreted as NaN: \u2018\u2019, \u2018#N/A\u2019, \u2018#N/A N/A\u2019, \u2018#NA\u2019, \u2018-1.#IND\u2019, \u2018-1.#QNAN\u2019, \u2018-NaN\u2019, \u2018-nan\u2019, \u20181.#IND\u2019, \u20181.#QNAN\u2019, \u2018<NA>\u2019, \u2018N/A\u2019, \u2018NA\u2019, \u2018NULL\u2019, \u2018NaN\u2019, \u2018n/a\u2019, \u2018nan\u2019, \u2018null\u2019.", "Whether or not to include the default NaN values when parsing the data. Depending on whether na_values is passed in, the behavior is as follows:", "If keep_default_na is True, and na_values are specified, na_values is appended to the default NaN values used for parsing.", "If keep_default_na is True, and na_values are not specified, only the default NaN values are used for parsing.", "If keep_default_na is False, and na_values are specified, only the NaN values specified na_values are used for parsing.", "If keep_default_na is False, and na_values are not specified, no strings will be parsed as NaN.", "Note that if na_filter is passed in as False, the keep_default_na and na_values parameters will be ignored.", "Detect missing value markers (empty strings and the value of na_values). In data without any NAs, passing na_filter=False can improve the performance of reading a large file.", "Indicate number of NA values placed in non-numeric columns.", "If True, skip over blank lines rather than interpreting as NaN values.", "The behavior is as follows:", "boolean. If True -> try parsing the index.", "list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column.", "list of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column.", "dict, e.g. {\u2018foo\u2019 : [1, 3]} -> parse columns 1, 3 as date and call result \u2018foo\u2019", "If a column or index cannot be represented as an array of datetimes, say because of an unparsable value or a mixture of timezones, the column or index will be returned unaltered as an object data type. For non-standard datetime parsing, use pd.to_datetime after pd.read_csv. To parse an index or column with a mixture of timezones, specify date_parser to be a partially-applied pandas.to_datetime() with utc=True. See Parsing a CSV with mixed timezones for more.", "Note: A fast-path exists for iso8601-formatted dates.", "If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the parsing speed by 5-10x.", "If True and parse_dates specifies combining multiple columns then keep the original columns.", "Function to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. Pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments.", "DD/MM format dates, international and European format.", "If True, use a cache of unique, converted dates to apply the datetime conversion. May produce significant speed-up when parsing duplicate date strings, especially ones with timezone offsets.", "New in version 0.25.0.", "Return TextFileReader object for iteration or getting chunks with get_chunk().", "Changed in version 1.2: TextFileReader is a context manager.", "Return TextFileReader object for iteration. See the IO Tools docs for more information on iterator and chunksize.", "Changed in version 1.2: TextFileReader is a context manager.", "For on-the-fly decompression of on-disk data. If \u2018infer\u2019 and \u2018%s\u2019 is path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). If using \u2018zip\u2019, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: compression={'method': 'zstd', 'dict_data': my_compression_dict}.", "Changed in version 1.4.0: Zstandard support.", "Thousands separator.", "Character to recognize as decimal point (e.g. use \u2018,\u2019 for European data).", "Character to break file into lines. Only valid with C parser.", "The character used to denote the start and end of a quoted item. Quoted items can include the delimiter and it will be ignored.", "Control field quoting behavior per csv.QUOTE_* constants. Use one of QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).", "When quotechar is specified and quoting is not QUOTE_NONE, indicate whether or not to interpret two consecutive quotechar elements INSIDE a field as a single quotechar element.", "One-character string used to escape other characters.", "Indicates remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines (as long as skip_blank_lines=True), fully commented lines are ignored by the parameter header but not by skiprows. For example, if comment='#', parsing #empty\\na,b,c\\n1,2,3 with header=0 will result in \u2018a,b,c\u2019 being treated as the header.", "Encoding to use for UTF when reading/writing (ex. \u2018utf-8\u2019). List of Python standard encodings .", "Changed in version 1.2: When encoding is None, errors=\"replace\" is passed to open(). Otherwise, errors=\"strict\" is passed to open(). This behavior was previously only the case for engine=\"python\".", "Changed in version 1.3.0: encoding_errors is a new argument. encoding has no longer an influence on how encoding errors are handled.", "How encoding errors are treated. List of possible values .", "New in version 1.3.0.", "If provided, this parameter will override values (default or not) for the following parameters: delimiter, doublequote, escapechar, skipinitialspace, quotechar, and quoting. If it is necessary to override values, a ParserWarning will be issued. See csv.Dialect documentation for more details.", "Lines with too many fields (e.g. a csv line with too many commas) will by default cause an exception to be raised, and no DataFrame will be returned. If False, then these \u201cbad lines\u201d will be dropped from the DataFrame that is returned.", "Deprecated since version 1.3.0: The on_bad_lines parameter should be used instead to specify behavior upon encountering a bad line instead.", "If error_bad_lines is False, and warn_bad_lines is True, a warning for each \u201cbad line\u201d will be output.", "Deprecated since version 1.3.0: The on_bad_lines parameter should be used instead to specify behavior upon encountering a bad line instead.", "Specifies what to do upon encountering a bad line (a line with too many fields). Allowed values are :", "\u2018error\u2019, raise an Exception when a bad line is encountered.", "\u2018warn\u2019, raise a warning when a bad line is encountered and skip that line.", "\u2018skip\u2019, skip bad lines without raising or warning when they are encountered.", "New in version 1.3.0: ", "callable, function with signature (bad_line: list[str]) -> list[str] | None that will process a single bad line. bad_line is a list of strings split by the sep. If the function returns None`, the bad line will be ignored.\nIf the function returns a new list of strings with more elements than\nexpected, a ``ParserWarning will be emitted while dropping extra elements. Only supported when engine=\"python\"", "New in version 1.4.0.", "Specifies whether or not whitespace (e.g. ' ' or '\u00a0\u00a0\u00a0 ') will be used as the sep. Equivalent to setting sep='\\s+'. If this option is set to True, nothing should be passed in for the delimiter parameter.", "Internally process the file in chunks, resulting in lower memory use while parsing, but possibly mixed type inference. To ensure no mixed types either set False, or specify the type with the dtype parameter. Note that the entire file is read into a single DataFrame regardless, use the chunksize or iterator parameter to return the data in chunks. (Only valid with C parser).", "If a filepath is provided for filepath_or_buffer, map the file object directly onto memory and access the data directly from there. Using this option can improve performance because there is no longer any I/O overhead.", "Specifies which converter the C engine should use for floating-point values. The options are None or \u2018high\u2019 for the ordinary converter, \u2018legacy\u2019 for the original lower precision pandas converter, and \u2018round_trip\u2019 for the round-trip converter.", "Changed in version 1.2.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.", "A comma-separated values (csv) file is returned as two-dimensional data structure with labeled axes.", "See also", "Write DataFrame to a comma-separated values (csv) file.", "Read a comma-separated values (csv) file into DataFrame.", "Read a table of fixed-width formatted lines into DataFrame.", "Examples"]}, {"name": "pandas.read_xml", "path": "reference/api/pandas.read_xml", "type": "Input/output", "text": ["Read XML document into a DataFrame object.", "New in version 1.3.0.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a read() function. The string can be any valid XML string or a path. The string can further be a URL. Valid URL schemes include http, ftp, s3, and file.", "The XPath to parse required set of nodes for migration to DataFrame. XPath should return a collection of elements and not a single element. Note: The etree parser supports limited XPath expressions. For more complex XPath, use lxml which requires installation.", "The namespaces defined in XML document as dicts with key being namespace prefix and value the URI. There is no need to include all namespaces in XML, only the ones used in xpath expression. Note: if XML document uses default namespace denoted as xmlns=\u2019<URI>\u2019 without a prefix, you must assign any temporary namespace prefix such as \u2018doc\u2019 to the URI in order to parse underlying nodes and/or attributes. For example,", "Parse only the child elements at the specified xpath. By default, all child elements and non-empty text nodes are returned.", "Parse only the attributes at the specified xpath. By default, all attributes are returned.", "Column names for DataFrame of parsed XML data. Use this parameter to rename original element names and distinguish same named elements.", "Encoding of XML document.", "Parser module to use for retrieval of data. Only \u2018lxml\u2019 and \u2018etree\u2019 are supported. With \u2018lxml\u2019 more complex XPath searches and ability to use XSLT stylesheet are supported.", "A URL, file-like object, or a raw string containing an XSLT script. This stylesheet should flatten complex, deeply nested XML documents for easier parsing. To use this feature you must have lxml module installed and specify \u2018lxml\u2019 as parser. The xpath must reference nodes of transformed XML document generated after XSLT transformation and not the original XML document. Only XSLT 1.0 scripts and not later versions is currently supported.", "For on-the-fly decompression of on-disk data. If \u2018infer\u2019 and \u2018path_or_buffer\u2019 is path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). If using \u2018zip\u2019, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: compression={'method': 'zstd', 'dict_data': my_compression_dict}.", "Changed in version 1.4.0: Zstandard support.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "A DataFrame.", "See also", "Convert a JSON string to pandas object.", "Read HTML tables into a list of DataFrame objects.", "Notes", "This method is best designed to import shallow XML documents in following format which is the ideal fit for the two-dimensions of a DataFrame (row by column).", "As a file format, XML documents can be designed any way including layout of elements and attributes as long as it conforms to W3C specifications. Therefore, this method is a convenience handler for a specific flatter design and not all possible XML structures.", "However, for more complex XML documents, stylesheet allows you to temporarily redesign original document with XSLT (a special purpose language) for a flatter version for migration to a DataFrame.", "This function will always return a single DataFrame or raise exceptions due to issues with XML document, xpath, or other parameters.", "Examples"]}, {"name": "pandas.reset_option", "path": "reference/api/pandas.reset_option", "type": "General utility functions", "text": ["Reset one or more options to their default value.", "Pass \u201call\u201d as argument to reset all options.", "Available options:", "compute.[use_bottleneck, use_numba, use_numexpr]", "display.[chop_threshold, colheader_justify, column_space, date_dayfirst, date_yearfirst, encoding, expand_frame_repr, float_format]", "display.html.[border, table_schema, use_mathjax]", "display.[large_repr]", "display.latex.[escape, longtable, multicolumn, multicolumn_format, multirow, repr]", "display.[max_categories, max_columns, max_colwidth, max_dir_items, max_info_columns, max_info_rows, max_rows, max_seq_items, memory_usage, min_rows, multi_sparse, notebook_repr_html, pprint_nest_depth, precision, show_dimensions]", "display.unicode.[ambiguous_as_wide, east_asian_width]", "display.[width]", "io.excel.ods.[reader, writer]", "io.excel.xls.[reader, writer]", "io.excel.xlsb.[reader]", "io.excel.xlsm.[reader, writer]", "io.excel.xlsx.[reader, writer]", "io.hdf.[default_format, dropna_table]", "io.parquet.[engine]", "io.sql.[engine]", "mode.[chained_assignment, data_manager, sim_interactive, string_storage, use_inf_as_na, use_inf_as_null]", "plotting.[backend]", "plotting.matplotlib.[register_converters]", "styler.format.[decimal, escape, formatter, na_rep, precision, thousands]", "styler.html.[mathjax]", "styler.latex.[environment, hrules, multicol_align, multirow_align]", "styler.render.[encoding, max_columns, max_elements, max_rows, repr]", "styler.sparse.[columns, index]", "If specified only options matching prefix* will be reset. Note: partial matches are supported for convenience, but unless you use the full option name (e.g. x.y.z.option_name), your code may break in future versions if new options with similar names are introduced.", "Notes", "The available options with its descriptions:", "Use the bottleneck library to accelerate if it is installed, the default is True Valid values: False,True [default: True] [currently: True]", "Use the numba engine option for select operations if it is installed, the default is False Valid values: False,True [default: False] [currently: False]", "Use the numexpr library to accelerate computation if it is installed, the default is True Valid values: False,True [default: True] [currently: True]", "if set to a float value, all float values smaller then the given threshold will be displayed as exactly 0 by repr and friends. [default: None] [currently: None]", "Controls the justification of column headers. used by DataFrameFormatter. [default: right] [currently: right]", "[default: 12] [currently: 12]", "When True, prints and parses dates with the day first, eg 20/01/2005 [default: False] [currently: False]", "When True, prints and parses dates with the year first, eg 2005/01/20 [default: False] [currently: False]", "Defaults to the detected encoding of the console. Specifies the encoding to be used for strings returned by to_string, these are generally strings meant to be displayed on the console. [default: utf-8] [currently: utf-8]", "Whether to print out the full DataFrame repr for wide DataFrames across multiple lines, max_columns is still respected, but the output will wrap-around across multiple \u201cpages\u201d if its width exceeds display.width. [default: True] [currently: True]", "The callable should accept a floating point number and return a string with the desired format of the number. This is used in some places like SeriesFormatter. See formats.format.EngFormatter for an example. [default: None] [currently: None]", "A border=value attribute is inserted in the <table> tag for the DataFrame HTML repr. [default: 1] [currently: 1]", "Whether to publish a Table Schema representation for frontends that support it. (default: False) [default: False] [currently: False]", "When True, Jupyter notebook will process table contents using MathJax, rendering mathematical expressions enclosed by the dollar symbol. (default: True) [default: True] [currently: True]", "For DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show a truncated table (the default from 0.13), or switch to the view from df.info() (the behaviour in earlier versions of pandas). [default: truncate] [currently: truncate]", "This specifies if the to_latex method of a Dataframe uses escapes special characters. Valid values: False,True [default: True] [currently: True]", "This specifies if the to_latex method of a Dataframe uses the longtable format. Valid values: False,True [default: False] [currently: False]", "This specifies if the to_latex method of a Dataframe uses multicolumns to pretty-print MultiIndex columns. Valid values: False,True [default: True] [currently: True]", "This specifies if the to_latex method of a Dataframe uses multicolumns to pretty-print MultiIndex columns. Valid values: False,True [default: l] [currently: l]", "This specifies if the to_latex method of a Dataframe uses multirows to pretty-print MultiIndex rows. Valid values: False,True [default: False] [currently: False]", "Whether to produce a latex DataFrame representation for jupyter environments that support it. (default: False) [default: False] [currently: False]", "This sets the maximum number of categories pandas should output when printing out a Categorical or a Series of dtype \u201ccategory\u201d. [default: 8] [currently: 8]", "If max_cols is exceeded, switch to truncate view. Depending on large_repr, objects are either centrally truncated or printed as a summary view. \u2018None\u2019 value means unlimited.", "In case python/IPython is running in a terminal and large_repr equals \u2018truncate\u2019 this can be set to 0 and pandas will auto-detect the width of the terminal and print a truncated object which fits the screen width. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection. [default: 0] [currently: 0]", "The maximum width in characters of a column in the repr of a pandas data structure. When the column overflows, a \u201c\u2026\u201d placeholder is embedded in the output. A \u2018None\u2019 value means unlimited. [default: 50] [currently: 50]", "The number of items that will be added to dir(\u2026). \u2018None\u2019 value means unlimited. Because dir is cached, changing this option will not immediately affect already existing dataframes until a column is deleted or added.", "This is for instance used to suggest columns from a dataframe to tab completion. [default: 100] [currently: 100]", "max_info_columns is used in DataFrame.info method to decide if per column information will be printed. [default: 100] [currently: 100]", "df.info() will usually show null-counts for each column. For large frames this can be quite slow. max_info_rows and max_info_cols limit this null check only to frames with smaller dimensions than specified. [default: 1690785] [currently: 1690785]", "If max_rows is exceeded, switch to truncate view. Depending on large_repr, objects are either centrally truncated or printed as a summary view. \u2018None\u2019 value means unlimited.", "In case python/IPython is running in a terminal and large_repr equals \u2018truncate\u2019 this can be set to 0 and pandas will auto-detect the height of the terminal and print a truncated object which fits the screen height. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection. [default: 60] [currently: 60]", "When pretty-printing a long sequence, no more then max_seq_items will be printed. If items are omitted, they will be denoted by the addition of \u201c\u2026\u201d to the resulting string.", "If set to None, the number of items to be printed is unlimited. [default: 100] [currently: 100]", "This specifies if the memory usage of a DataFrame should be displayed when df.info() is called. Valid values True,False,\u2019deep\u2019 [default: True] [currently: True]", "The numbers of rows to show in a truncated view (when max_rows is exceeded). Ignored when max_rows is set to None or 0. When set to None, follows the value of max_rows. [default: 10] [currently: 10]", "\u201csparsify\u201d MultiIndex display (don\u2019t display repeated elements in outer levels within groups) [default: True] [currently: True]", "When True, IPython notebook will use html representation for pandas objects (if it is available). [default: True] [currently: True]", "Controls the number of nested levels to process when pretty-printing [default: 3] [currently: 3]", "Floating point output precision in terms of number of places after the decimal, for regular formatting as well as scientific notation. Similar to precision in numpy.set_printoptions(). [default: 6] [currently: 6]", "Whether to print out dimensions at the end of DataFrame repr. If \u2018truncate\u2019 is specified, only print out the dimensions if the frame is truncated (e.g. not display all rows and/or columns) [default: truncate] [currently: truncate]", "Whether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False]", "Whether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False]", "Width of the display in characters. In case python/IPython is running in a terminal this can be set to None and pandas will correctly auto-detect the width. Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to correctly detect the width. [default: 80] [currently: 80]", "The default Excel reader engine for \u2018ods\u2019 files. Available options: auto, odf. [default: auto] [currently: auto]", "The default Excel writer engine for \u2018ods\u2019 files. Available options: auto, odf. [default: auto] [currently: auto]", "The default Excel reader engine for \u2018xls\u2019 files. Available options: auto, xlrd. [default: auto] [currently: auto]", "The default Excel writer engine for \u2018xls\u2019 files. Available options: auto, xlwt. [default: auto] [currently: auto] (Deprecated, use `` instead.)", "The default Excel reader engine for \u2018xlsb\u2019 files. Available options: auto, pyxlsb. [default: auto] [currently: auto]", "The default Excel reader engine for \u2018xlsm\u2019 files. Available options: auto, xlrd, openpyxl. [default: auto] [currently: auto]", "The default Excel writer engine for \u2018xlsm\u2019 files. Available options: auto, openpyxl. [default: auto] [currently: auto]", "The default Excel reader engine for \u2018xlsx\u2019 files. Available options: auto, xlrd, openpyxl. [default: auto] [currently: auto]", "The default Excel writer engine for \u2018xlsx\u2019 files. Available options: auto, openpyxl, xlsxwriter. [default: auto] [currently: auto]", "default format writing format, if None, then put will default to \u2018fixed\u2019 and append will default to \u2018table\u2019 [default: None] [currently: None]", "drop ALL nan rows when appending to a table [default: False] [currently: False]", "The default parquet reader/writer engine. Available options: \u2018auto\u2019, \u2018pyarrow\u2019, \u2018fastparquet\u2019, the default is \u2018auto\u2019 [default: auto] [currently: auto]", "The default sql reader/writer engine. Available options: \u2018auto\u2019, \u2018sqlalchemy\u2019, the default is \u2018auto\u2019 [default: auto] [currently: auto]", "Raise an exception, warn, or no action if trying to use chained assignment, The default is warn [default: warn] [currently: warn]", "Internal data manager type; can be \u201cblock\u201d or \u201carray\u201d. Defaults to \u201cblock\u201d, unless overridden by the \u2018PANDAS_DATA_MANAGER\u2019 environment variable (needs to be set before pandas is imported). [default: block] [currently: block]", "Whether to simulate interactive mode for purposes of testing [default: False] [currently: False]", "The default storage for StringDtype. [default: python] [currently: python]", "True means treat None, NaN, INF, -INF as NA (old way), False means None and NaN are null, but INF, -INF are not NA (new way). [default: False] [currently: False]", "use_inf_as_null had been deprecated and will be removed in a future version. Use use_inf_as_na instead. [default: False] [currently: False] (Deprecated, use mode.use_inf_as_na instead.)", "The plotting backend to use. The default value is \u201cmatplotlib\u201d, the backend provided with pandas. Other backends can be specified by providing the name of the module that implements the backend. [default: matplotlib] [currently: matplotlib]", "Whether to register converters with matplotlib\u2019s units registry for dates, times, datetimes, and Periods. Toggling to False will remove the converters, restoring any converters that pandas overwrote. [default: auto] [currently: auto]", "The character representation for the decimal separator for floats and complex. [default: .] [currently: .]", "Whether to escape certain characters according to the given context; html or latex. [default: None] [currently: None]", "A formatter object to be used as default within Styler.format. [default: None] [currently: None]", "The string representation for values identified as missing. [default: None] [currently: None]", "The precision for floats and complex numbers. [default: 6] [currently: 6]", "The character representation for thousands separator for floats, int and complex. [default: None] [currently: None]", "If False will render special CSS classes to table attributes that indicate Mathjax will not be used in Jupyter Notebook. [default: True] [currently: True]", "The environment to replace \\begin{table}. If \u201clongtable\u201d is used results in a specific longtable environment format. [default: None] [currently: None]", "Whether to add horizontal rules on top and bottom and below the headers. [default: False] [currently: False]", "The specifier for horizontal alignment of sparsified LaTeX multicolumns. Pipe decorators can also be added to non-naive values to draw vertical rules, e.g. \u201c|r\u201d will draw a rule on the left side of right aligned merged cells. [default: r] [currently: r]", "The specifier for vertical alignment of sparsified LaTeX multirows. [default: c] [currently: c]", "The encoding used for output HTML and LaTeX files. [default: utf-8] [currently: utf-8]", "The maximum number of columns that will be rendered. May still be reduced to satsify max_elements, which takes precedence. [default: None] [currently: None]", "The maximum number of data-cell (<td>) elements that will be rendered before trimming will occur over columns, rows or both if needed. [default: 262144] [currently: 262144]", "The maximum number of rows that will be rendered. May still be reduced to satsify max_elements, which takes precedence. [default: None] [currently: None]", "Determine which output to use in Jupyter Notebook in {\u201chtml\u201d, \u201clatex\u201d}. [default: html] [currently: html]", "Whether to sparsify the display of hierarchical columns. Setting to False will display each explicit level element in a hierarchical key for each column. [default: True] [currently: True]", "Whether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. [default: True] [currently: True]"]}, {"name": "pandas.Series", "path": "reference/api/pandas.series", "type": "Series", "text": ["One-dimensional ndarray with axis labels (including time series).", "Labels need not be unique but must be a hashable type. The object supports both integer- and label-based indexing and provides a host of methods for performing operations involving the index. Statistical methods from ndarray have been overridden to automatically exclude missing data (currently represented as NaN).", "Operations between Series (+, -, /, *, **) align values based on their associated index values\u2013 they need not be the same length. The result index will be the sorted union of the two indexes.", "Contains data stored in Series. If data is a dict, argument order is maintained.", "Values must be hashable and have the same length as data. Non-unique index values are allowed. Will default to RangeIndex (0, 1, 2, \u2026, n) if not provided. If data is dict-like and index is None, then the keys in the data are used as the index. If the index is not None, the resulting Series is reindexed with the index values.", "Data type for the output Series. If not specified, this will be inferred from data. See the user guide for more usages.", "The name to give to the Series.", "Copy input data. Only affects Series or 1d ndarray input. See examples.", "Examples", "Constructing Series from a dictionary with an Index specified", "The keys of the dictionary match with the Index values, hence the Index values have no effect.", "Note that the Index is first build with the keys from the dictionary. After this the Series is reindexed with the given Index values, hence we get all NaN as a result.", "Constructing Series from a list with copy=False.", "Due to input data type the Series has a copy of the original data even though copy=False, so the data is unchanged.", "Constructing Series from a 1d ndarray with copy=False.", "Due to input data type the Series has a view on the original data, so the data is changed as well.", "Attributes", "T", "Return the transpose, which is by definition self.", "array", "The ExtensionArray of the data backing this Series or Index.", "at", "Access a single value for a row/column label pair.", "attrs", "Dictionary of global attributes of this dataset.", "axes", "Return a list of the row axis labels.", "dtype", "Return the dtype object of the underlying data.", "dtypes", "Return the dtype object of the underlying data.", "flags", "Get the properties associated with this pandas object.", "hasnans", "Return True if there are any NaNs.", "iat", "Access a single value for a row/column pair by integer position.", "iloc", "Purely integer-location based indexing for selection by position.", "index", "The index (axis labels) of the Series.", "is_monotonic", "Return boolean if values in the object are monotonic_increasing.", "is_monotonic_decreasing", "Return boolean if values in the object are monotonic_decreasing.", "is_monotonic_increasing", "Alias for is_monotonic.", "is_unique", "Return boolean if values in the object are unique.", "loc", "Access a group of rows and columns by label(s) or a boolean array.", "name", "Return the name of the Series.", "nbytes", "Return the number of bytes in the underlying data.", "ndim", "Number of dimensions of the underlying data, by definition 1.", "shape", "Return a tuple of the shape of the underlying data.", "size", "Return the number of elements in the underlying data.", "values", "Return Series as ndarray or ndarray-like depending on the dtype.", "empty", "Methods", "abs()", "Return a Series/DataFrame with absolute numeric value of each element.", "add(other[, level, fill_value, axis])", "Return Addition of series and other, element-wise (binary operator add).", "add_prefix(prefix)", "Prefix labels with string prefix.", "add_suffix(suffix)", "Suffix labels with string suffix.", "agg([func, axis])", "Aggregate using one or more operations over the specified axis.", "aggregate([func, axis])", "Aggregate using one or more operations over the specified axis.", "align(other[, join, axis, level, copy, ...])", "Align two objects on their axes with the specified join method.", "all([axis, bool_only, skipna, level])", "Return whether all elements are True, potentially over an axis.", "any([axis, bool_only, skipna, level])", "Return whether any element is True, potentially over an axis.", "append(to_append[, ignore_index, ...])", "Concatenate two or more Series.", "apply(func[, convert_dtype, args])", "Invoke function on values of Series.", "argmax([axis, skipna])", "Return int position of the largest value in the Series.", "argmin([axis, skipna])", "Return int position of the smallest value in the Series.", "argsort([axis, kind, order])", "Return the integer indices that would sort the Series values.", "asfreq(freq[, method, how, normalize, ...])", "Convert time series to specified frequency.", "asof(where[, subset])", "Return the last row(s) without any NaNs before where.", "astype(dtype[, copy, errors])", "Cast a pandas object to a specified dtype dtype.", "at_time(time[, asof, axis])", "Select values at particular time of day (e.g., 9:30AM).", "autocorr([lag])", "Compute the lag-N autocorrelation.", "backfill([axis, inplace, limit, downcast])", "Synonym for DataFrame.fillna() with method='bfill'.", "between(left, right[, inclusive])", "Return boolean Series equivalent to left <= series <= right.", "between_time(start_time, end_time[, ...])", "Select values between particular times of the day (e.g., 9:00-9:30 AM).", "bfill([axis, inplace, limit, downcast])", "Synonym for DataFrame.fillna() with method='bfill'.", "bool()", "Return the bool of a single element Series or DataFrame.", "cat", "alias of pandas.core.arrays.categorical.CategoricalAccessor", "clip([lower, upper, axis, inplace])", "Trim values at input threshold(s).", "combine(other, func[, fill_value])", "Combine the Series with a Series or scalar according to func.", "combine_first(other)", "Update null elements with value in the same location in 'other'.", "compare(other[, align_axis, keep_shape, ...])", "Compare to another Series and show the differences.", "convert_dtypes([infer_objects, ...])", "Convert columns to best possible dtypes using dtypes supporting pd.NA.", "copy([deep])", "Make a copy of this object's indices and data.", "corr(other[, method, min_periods])", "Compute correlation with other Series, excluding missing values.", "count([level])", "Return number of non-NA/null observations in the Series.", "cov(other[, min_periods, ddof])", "Compute covariance with Series, excluding missing values.", "cummax([axis, skipna])", "Return cumulative maximum over a DataFrame or Series axis.", "cummin([axis, skipna])", "Return cumulative minimum over a DataFrame or Series axis.", "cumprod([axis, skipna])", "Return cumulative product over a DataFrame or Series axis.", "cumsum([axis, skipna])", "Return cumulative sum over a DataFrame or Series axis.", "describe([percentiles, include, exclude, ...])", "Generate descriptive statistics.", "diff([periods])", "First discrete difference of element.", "div(other[, level, fill_value, axis])", "Return Floating division of series and other, element-wise (binary operator truediv).", "divide(other[, level, fill_value, axis])", "Return Floating division of series and other, element-wise (binary operator truediv).", "divmod(other[, level, fill_value, axis])", "Return Integer division and modulo of series and other, element-wise (binary operator divmod).", "dot(other)", "Compute the dot product between the Series and the columns of other.", "drop([labels, axis, index, columns, level, ...])", "Return Series with specified index labels removed.", "drop_duplicates([keep, inplace])", "Return Series with duplicate values removed.", "droplevel(level[, axis])", "Return Series/DataFrame with requested index / column level(s) removed.", "dropna([axis, inplace, how])", "Return a new Series with missing values removed.", "dt", "alias of pandas.core.indexes.accessors.CombinedDatetimelikeProperties", "duplicated([keep])", "Indicate duplicate Series values.", "eq(other[, level, fill_value, axis])", "Return Equal to of series and other, element-wise (binary operator eq).", "equals(other)", "Test whether two objects contain the same elements.", "ewm([com, span, halflife, alpha, ...])", "Provide exponentially weighted (EW) calculations.", "expanding([min_periods, center, axis, method])", "Provide expanding window calculations.", "explode([ignore_index])", "Transform each element of a list-like to a row.", "factorize([sort, na_sentinel])", "Encode the object as an enumerated type or categorical variable.", "ffill([axis, inplace, limit, downcast])", "Synonym for DataFrame.fillna() with method='ffill'.", "fillna([value, method, axis, inplace, ...])", "Fill NA/NaN values using the specified method.", "filter([items, like, regex, axis])", "Subset the dataframe rows or columns according to the specified index labels.", "first(offset)", "Select initial periods of time series data based on a date offset.", "first_valid_index()", "Return index for first non-NA value or None, if no NA value is found.", "floordiv(other[, level, fill_value, axis])", "Return Integer division of series and other, element-wise (binary operator floordiv).", "ge(other[, level, fill_value, axis])", "Return Greater than or equal to of series and other, element-wise (binary operator ge).", "get(key[, default])", "Get item from object for given key (ex: DataFrame column).", "groupby([by, axis, level, as_index, sort, ...])", "Group Series using a mapper or by a Series of columns.", "gt(other[, level, fill_value, axis])", "Return Greater than of series and other, element-wise (binary operator gt).", "head([n])", "Return the first n rows.", "hist([by, ax, grid, xlabelsize, xrot, ...])", "Draw histogram of the input series using matplotlib.", "idxmax([axis, skipna])", "Return the row label of the maximum value.", "idxmin([axis, skipna])", "Return the row label of the minimum value.", "infer_objects()", "Attempt to infer better dtypes for object columns.", "info([verbose, buf, max_cols, memory_usage, ...])", "Print a concise summary of a Series.", "interpolate([method, axis, limit, inplace, ...])", "Fill NaN values using an interpolation method.", "isin(values)", "Whether elements in Series are contained in values.", "isna()", "Detect missing values.", "isnull()", "Series.isnull is an alias for Series.isna.", "item()", "Return the first element of the underlying data as a Python scalar.", "items()", "Lazily iterate over (index, value) tuples.", "iteritems()", "Lazily iterate over (index, value) tuples.", "keys()", "Return alias for index.", "kurt([axis, skipna, level, numeric_only])", "Return unbiased kurtosis over requested axis.", "kurtosis([axis, skipna, level, numeric_only])", "Return unbiased kurtosis over requested axis.", "last(offset)", "Select final periods of time series data based on a date offset.", "last_valid_index()", "Return index for last non-NA value or None, if no NA value is found.", "le(other[, level, fill_value, axis])", "Return Less than or equal to of series and other, element-wise (binary operator le).", "lt(other[, level, fill_value, axis])", "Return Less than of series and other, element-wise (binary operator lt).", "mad([axis, skipna, level])", "Return the mean absolute deviation of the values over the requested axis.", "map(arg[, na_action])", "Map values of Series according to an input mapping or function.", "mask(cond[, other, inplace, axis, level, ...])", "Replace values where the condition is True.", "max([axis, skipna, level, numeric_only])", "Return the maximum of the values over the requested axis.", "mean([axis, skipna, level, numeric_only])", "Return the mean of the values over the requested axis.", "median([axis, skipna, level, numeric_only])", "Return the median of the values over the requested axis.", "memory_usage([index, deep])", "Return the memory usage of the Series.", "min([axis, skipna, level, numeric_only])", "Return the minimum of the values over the requested axis.", "mod(other[, level, fill_value, axis])", "Return Modulo of series and other, element-wise (binary operator mod).", "mode([dropna])", "Return the mode(s) of the Series.", "mul(other[, level, fill_value, axis])", "Return Multiplication of series and other, element-wise (binary operator mul).", "multiply(other[, level, fill_value, axis])", "Return Multiplication of series and other, element-wise (binary operator mul).", "ne(other[, level, fill_value, axis])", "Return Not equal to of series and other, element-wise (binary operator ne).", "nlargest([n, keep])", "Return the largest n elements.", "notna()", "Detect existing (non-missing) values.", "notnull()", "Series.notnull is an alias for Series.notna.", "nsmallest([n, keep])", "Return the smallest n elements.", "nunique([dropna])", "Return number of unique elements in the object.", "pad([axis, inplace, limit, downcast])", "Synonym for DataFrame.fillna() with method='ffill'.", "pct_change([periods, fill_method, limit, freq])", "Percentage change between the current and a prior element.", "pipe(func, *args, **kwargs)", "Apply chainable functions that expect Series or DataFrames.", "plot", "alias of pandas.plotting._core.PlotAccessor", "pop(item)", "Return item and drops from series.", "pow(other[, level, fill_value, axis])", "Return Exponential power of series and other, element-wise (binary operator pow).", "prod([axis, skipna, level, numeric_only, ...])", "Return the product of the values over the requested axis.", "product([axis, skipna, level, numeric_only, ...])", "Return the product of the values over the requested axis.", "quantile([q, interpolation])", "Return value at the given quantile.", "radd(other[, level, fill_value, axis])", "Return Addition of series and other, element-wise (binary operator radd).", "rank([axis, method, numeric_only, ...])", "Compute numerical data ranks (1 through n) along axis.", "ravel([order])", "Return the flattened underlying data as an ndarray.", "rdiv(other[, level, fill_value, axis])", "Return Floating division of series and other, element-wise (binary operator rtruediv).", "rdivmod(other[, level, fill_value, axis])", "Return Integer division and modulo of series and other, element-wise (binary operator rdivmod).", "reindex(*args, **kwargs)", "Conform Series to new index with optional filling logic.", "reindex_like(other[, method, copy, limit, ...])", "Return an object with matching indices as other object.", "rename([index, axis, copy, inplace, level, ...])", "Alter Series index labels or name.", "rename_axis([mapper, index, columns, axis, ...])", "Set the name of the axis for the index or columns.", "reorder_levels(order)", "Rearrange index levels using input order.", "repeat(repeats[, axis])", "Repeat elements of a Series.", "replace([to_replace, value, inplace, limit, ...])", "Replace values given in to_replace with value.", "resample(rule[, axis, closed, label, ...])", "Resample time-series data.", "reset_index([level, drop, name, inplace])", "Generate a new DataFrame or Series with the index reset.", "rfloordiv(other[, level, fill_value, axis])", "Return Integer division of series and other, element-wise (binary operator rfloordiv).", "rmod(other[, level, fill_value, axis])", "Return Modulo of series and other, element-wise (binary operator rmod).", "rmul(other[, level, fill_value, axis])", "Return Multiplication of series and other, element-wise (binary operator rmul).", "rolling(window[, min_periods, center, ...])", "Provide rolling window calculations.", "round([decimals])", "Round each value in a Series to the given number of decimals.", "rpow(other[, level, fill_value, axis])", "Return Exponential power of series and other, element-wise (binary operator rpow).", "rsub(other[, level, fill_value, axis])", "Return Subtraction of series and other, element-wise (binary operator rsub).", "rtruediv(other[, level, fill_value, axis])", "Return Floating division of series and other, element-wise (binary operator rtruediv).", "sample([n, frac, replace, weights, ...])", "Return a random sample of items from an axis of object.", "searchsorted(value[, side, sorter])", "Find indices where elements should be inserted to maintain order.", "sem([axis, skipna, level, ddof, numeric_only])", "Return unbiased standard error of the mean over requested axis.", "set_axis(labels[, axis, inplace])", "Assign desired index to given axis.", "set_flags(*[, copy, allows_duplicate_labels])", "Return a new object with updated flags.", "shift([periods, freq, axis, fill_value])", "Shift index by desired number of periods with an optional time freq.", "skew([axis, skipna, level, numeric_only])", "Return unbiased skew over requested axis.", "slice_shift([periods, axis])", "(DEPRECATED) Equivalent to shift without copying data.", "sort_index([axis, level, ascending, ...])", "Sort Series by index labels.", "sort_values([axis, ascending, inplace, ...])", "Sort by the values.", "sparse", "alias of pandas.core.arrays.sparse.accessor.SparseAccessor", "squeeze([axis])", "Squeeze 1 dimensional axis objects into scalars.", "std([axis, skipna, level, ddof, numeric_only])", "Return sample standard deviation over requested axis.", "str", "alias of pandas.core.strings.accessor.StringMethods", "sub(other[, level, fill_value, axis])", "Return Subtraction of series and other, element-wise (binary operator sub).", "subtract(other[, level, fill_value, axis])", "Return Subtraction of series and other, element-wise (binary operator sub).", "sum([axis, skipna, level, numeric_only, ...])", "Return the sum of the values over the requested axis.", "swapaxes(axis1, axis2[, copy])", "Interchange axes and swap values axes appropriately.", "swaplevel([i, j, copy])", "Swap levels i and j in a MultiIndex.", "tail([n])", "Return the last n rows.", "take(indices[, axis, is_copy])", "Return the elements in the given positional indices along an axis.", "to_clipboard([excel, sep])", "Copy object to the system clipboard.", "to_csv([path_or_buf, sep, na_rep, ...])", "Write object to a comma-separated values (csv) file.", "to_dict([into])", "Convert Series to {label -> value} dict or dict-like object.", "to_excel(excel_writer[, sheet_name, na_rep, ...])", "Write object to an Excel sheet.", "to_frame([name])", "Convert Series to DataFrame.", "to_hdf(path_or_buf, key[, mode, complevel, ...])", "Write the contained data to an HDF5 file using HDFStore.", "to_json([path_or_buf, orient, date_format, ...])", "Convert the object to a JSON string.", "to_latex([buf, columns, col_space, header, ...])", "Render object to a LaTeX tabular, longtable, or nested table.", "to_list()", "Return a list of the values.", "to_markdown([buf, mode, index, storage_options])", "Print Series in Markdown-friendly format.", "to_numpy([dtype, copy, na_value])", "A NumPy ndarray representing the values in this Series or Index.", "to_period([freq, copy])", "Convert Series from DatetimeIndex to PeriodIndex.", "to_pickle(path[, compression, protocol, ...])", "Pickle (serialize) object to file.", "to_sql(name, con[, schema, if_exists, ...])", "Write records stored in a DataFrame to a SQL database.", "to_string([buf, na_rep, float_format, ...])", "Render a string representation of the Series.", "to_timestamp([freq, how, copy])", "Cast to DatetimeIndex of Timestamps, at beginning of period.", "to_xarray()", "Return an xarray object from the pandas object.", "tolist()", "Return a list of the values.", "transform(func[, axis])", "Call func on self producing a Series with the same axis shape as self.", "transpose(*args, **kwargs)", "Return the transpose, which is by definition self.", "truediv(other[, level, fill_value, axis])", "Return Floating division of series and other, element-wise (binary operator truediv).", "truncate([before, after, axis, copy])", "Truncate a Series or DataFrame before and after some index value.", "tshift([periods, freq, axis])", "(DEPRECATED) Shift the time index, using the index's frequency if available.", "tz_convert(tz[, axis, level, copy])", "Convert tz-aware axis to target time zone.", "tz_localize(tz[, axis, level, copy, ...])", "Localize tz-naive index of a Series or DataFrame to target time zone.", "unique()", "Return unique values of Series object.", "unstack([level, fill_value])", "Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.", "update(other)", "Modify Series in place using values from passed Series.", "value_counts([normalize, sort, ascending, ...])", "Return a Series containing counts of unique values.", "var([axis, skipna, level, ddof, numeric_only])", "Return unbiased variance over requested axis.", "view([dtype])", "Create a new view of the Series.", "where(cond[, other, inplace, axis, level, ...])", "Replace values where the condition is False.", "xs(key[, axis, level, drop_level])", "Return cross-section from the Series/DataFrame."]}, {"name": "pandas.Series.__array__", "path": "reference/api/pandas.series.__array__", "type": "Series", "text": ["Return the values as a NumPy array.", "Users should not call this directly. Rather, it is invoked by numpy.array() and numpy.asarray().", "The dtype to use for the resulting NumPy array. By default, the dtype is inferred from the data.", "The values in the series converted to a numpy.ndarray with the specified dtype.", "See also", "Create a new array from data.", "Zero-copy view to the array backing the Series.", "Series method for similar behavior.", "Examples", "For timezone-aware data, the timezones may be retained with dtype='object'", "Or the values may be localized to UTC and the tzinfo discarded with dtype='datetime64[ns]'"]}, {"name": "pandas.Series.__iter__", "path": "reference/api/pandas.series.__iter__", "type": "Series", "text": ["Return an iterator of the values.", "These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)"]}, {"name": "pandas.Series.abs", "path": "reference/api/pandas.series.abs", "type": "Series", "text": ["Return a Series/DataFrame with absolute numeric value of each element.", "This function only applies to elements that are all numeric.", "Series/DataFrame containing the absolute value of each element.", "See also", "Calculate the absolute value element-wise.", "Notes", "For complex inputs, 1.2 + 1j, the absolute value is \\(\\sqrt{ a^2 + b^2 }\\).", "Examples", "Absolute numeric values in a Series.", "Absolute numeric values in a Series with complex numbers.", "Absolute numeric values in a Series with a Timedelta element.", "Select rows with data closest to certain value using argsort (from StackOverflow)."]}, {"name": "pandas.Series.add", "path": "reference/api/pandas.series.add", "type": "Series", "text": ["Return Addition of series and other, element-wise (binary operator add).", "Equivalent to series + other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Reverse of the Addition operator, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.add_prefix", "path": "reference/api/pandas.series.add_prefix", "type": "Series", "text": ["Prefix labels with string prefix.", "For Series, the row labels are prefixed. For DataFrame, the column labels are prefixed.", "The string to add before each label.", "New Series or DataFrame with updated labels.", "See also", "Suffix row labels with string suffix.", "Suffix column labels with string suffix.", "Examples"]}, {"name": "pandas.Series.add_suffix", "path": "reference/api/pandas.series.add_suffix", "type": "Series", "text": ["Suffix labels with string suffix.", "For Series, the row labels are suffixed. For DataFrame, the column labels are suffixed.", "The string to add after each label.", "New Series or DataFrame with updated labels.", "See also", "Prefix row labels with string prefix.", "Prefix column labels with string prefix.", "Examples"]}, {"name": "pandas.Series.agg", "path": "reference/api/pandas.series.agg", "type": "Series", "text": ["Aggregate using one or more operations over the specified axis.", "Function to use for aggregating the data. If a function, must either work when passed a Series or when passed to Series.apply.", "Accepted combinations are:", "function", "string function name", "list of functions and/or function names, e.g. [np.sum, 'mean']", "dict of axis labels -> functions, function names or list of such.", "Parameter needed for compatibility with DataFrame.", "Positional arguments to pass to func.", "Keyword arguments to pass to func.", "The return can be:", "scalar : when Series.agg is called with single function", "Series : when DataFrame.agg is called with a single function", "DataFrame : when DataFrame.agg is called with several functions", "Return scalar, Series or DataFrame.", "See also", "Invoke function on a Series.", "Transform function producing a Series with like indexes.", "Notes", "agg is an alias for aggregate. Use the alias.", "Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.", "A passed user-defined-function will be passed a Series for evaluation.", "Examples"]}, {"name": "pandas.Series.aggregate", "path": "reference/api/pandas.series.aggregate", "type": "Series", "text": ["Aggregate using one or more operations over the specified axis.", "Function to use for aggregating the data. If a function, must either work when passed a Series or when passed to Series.apply.", "Accepted combinations are:", "function", "string function name", "list of functions and/or function names, e.g. [np.sum, 'mean']", "dict of axis labels -> functions, function names or list of such.", "Parameter needed for compatibility with DataFrame.", "Positional arguments to pass to func.", "Keyword arguments to pass to func.", "The return can be:", "scalar : when Series.agg is called with single function", "Series : when DataFrame.agg is called with a single function", "DataFrame : when DataFrame.agg is called with several functions", "Return scalar, Series or DataFrame.", "See also", "Invoke function on a Series.", "Transform function producing a Series with like indexes.", "Notes", "agg is an alias for aggregate. Use the alias.", "Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.", "A passed user-defined-function will be passed a Series for evaluation.", "Examples"]}, {"name": "pandas.Series.align", "path": "reference/api/pandas.series.align", "type": "Series", "text": ["Align two objects on their axes with the specified join method.", "Join method is specified for each axis Index.", "Align on index (0), columns (1), or both (None).", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Always returns new objects. If copy=False and no reindexing is required then original objects are returned.", "Value to use for missing values. Defaults to NaN, but can be any \u201ccompatible\u201d value.", "Method to use for filling holes in reindexed Series:", "pad / ffill: propagate last valid observation forward to next valid.", "backfill / bfill: use NEXT valid observation to fill gap.", "If method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None.", "Filling axis, method and limit.", "Broadcast values along this axis, if aligning two objects of different dimensions.", "Aligned objects.", "Examples", "Align on columns:", "We can also align on the index:", "Finally, the default axis=None will align on both index and columns:"]}, {"name": "pandas.Series.all", "path": "reference/api/pandas.series.all", "type": "Series", "text": ["Return whether all elements are True, potentially over an axis.", "Returns True unless there at least one element within a series or along a Dataframe axis that is False or equivalent (e.g. zero or empty).", "Indicate which axis or axes should be reduced.", "0 / \u2018index\u2019 : reduce the index, return a Series whose index is the original column labels.", "1 / \u2018columns\u2019 : reduce the columns, return a Series whose index is the original index.", "None : reduce all axes, return a scalar.", "Include only boolean columns. If None, will attempt to use everything, then use only boolean data. Not implemented for Series.", "Exclude NA/null values. If the entire row/column is NA and skipna is True, then the result will be True, as for an empty row/column. If skipna is False, then NA are treated as True, because these are not equal to zero.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar.", "Additional keywords have no effect but might be accepted for compatibility with NumPy.", "If level is specified, then, Series is returned; otherwise, scalar is returned.", "See also", "Return True if all elements are True.", "Return True if one (or more) elements are True.", "Examples", "Series", "DataFrames", "Create a dataframe from a dictionary.", "Default behaviour checks if column-wise values all return True.", "Specify axis='columns' to check if row-wise values all return True.", "Or axis=None for whether every value is True."]}, {"name": "pandas.Series.any", "path": "reference/api/pandas.series.any", "type": "Series", "text": ["Return whether any element is True, potentially over an axis.", "Returns False unless there is at least one element within a series or along a Dataframe axis that is True or equivalent (e.g. non-zero or non-empty).", "Indicate which axis or axes should be reduced.", "0 / \u2018index\u2019 : reduce the index, return a Series whose index is the original column labels.", "1 / \u2018columns\u2019 : reduce the columns, return a Series whose index is the original index.", "None : reduce all axes, return a scalar.", "Include only boolean columns. If None, will attempt to use everything, then use only boolean data. Not implemented for Series.", "Exclude NA/null values. If the entire row/column is NA and skipna is True, then the result will be False, as for an empty row/column. If skipna is False, then NA are treated as True, because these are not equal to zero.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar.", "Additional keywords have no effect but might be accepted for compatibility with NumPy.", "If level is specified, then, Series is returned; otherwise, scalar is returned.", "See also", "Numpy version of this method.", "Return whether any element is True.", "Return whether all elements are True.", "Return whether any element is True over requested axis.", "Return whether all elements are True over requested axis.", "Examples", "Series", "For Series input, the output is a scalar indicating whether any element is True.", "DataFrame", "Whether each column contains at least one True element (the default).", "Aggregating over the columns.", "Aggregating over the entire DataFrame with axis=None.", "any for an empty DataFrame is an empty Series."]}, {"name": "pandas.Series.append", "path": "reference/api/pandas.series.append", "type": "Series", "text": ["Concatenate two or more Series.", "Series to append with self.", "If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.", "If True, raise Exception on creating index with duplicates.", "Concatenated Series.", "See also", "General function to concatenate DataFrame or Series objects.", "Notes", "Iteratively appending to a Series can be more computationally intensive than a single concatenate. A better solution is to append values to a list and then concatenate the list with the original Series all at once.", "Examples", "With ignore_index set to True:", "With verify_integrity set to True:"]}, {"name": "pandas.Series.apply", "path": "reference/api/pandas.series.apply", "type": "Series", "text": ["Invoke function on values of Series.", "Can be ufunc (a NumPy function that applies to the entire Series) or a Python function that only works on single values.", "Python function or NumPy ufunc to apply.", "Try to find better dtype for elementwise function results. If False, leave as dtype=object. Note that the dtype is always preserved for some extension array dtypes, such as Categorical.", "Positional arguments passed to func after the series value.", "Additional keyword arguments passed to func.", "If func returns a Series object the result will be a DataFrame.", "See also", "For element-wise operations.", "Only perform aggregating type operations.", "Only perform transforming type operations.", "Notes", "Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.", "Examples", "Create a series with typical summer temperatures for each city.", "Square the values by defining a function and passing it as an argument to apply().", "Square the values by passing an anonymous function as an argument to apply().", "Define a custom function that needs additional positional arguments and pass these additional arguments using the args keyword.", "Define a custom function that takes keyword arguments and pass these arguments to apply.", "Use a function from the Numpy library."]}, {"name": "pandas.Series.argmax", "path": "reference/api/pandas.series.argmax", "type": "Series", "text": ["Return int position of the largest value in the Series.", "If the maximum is achieved in multiple locations, the first row position is returned.", "Dummy argument for consistency with Series.", "Exclude NA/null values when showing the result.", "Additional arguments and keywords for compatibility with NumPy.", "Row position of the maximum value.", "See also", "Return position of the maximum value.", "Return position of the minimum value.", "Equivalent method for numpy arrays.", "Return index label of the maximum values.", "Return index label of the minimum values.", "Examples", "Consider dataset containing cereal calories", "The maximum cereal calories is the third element and the minimum cereal calories is the first element, since series is zero-indexed."]}, {"name": "pandas.Series.argmin", "path": "reference/api/pandas.series.argmin", "type": "Series", "text": ["Return int position of the smallest value in the Series.", "If the minimum is achieved in multiple locations, the first row position is returned.", "Dummy argument for consistency with Series.", "Exclude NA/null values when showing the result.", "Additional arguments and keywords for compatibility with NumPy.", "Row position of the minimum value.", "See also", "Return position of the minimum value.", "Return position of the maximum value.", "Equivalent method for numpy arrays.", "Return index label of the maximum values.", "Return index label of the minimum values.", "Examples", "Consider dataset containing cereal calories", "The maximum cereal calories is the third element and the minimum cereal calories is the first element, since series is zero-indexed."]}, {"name": "pandas.Series.argsort", "path": "reference/api/pandas.series.argsort", "type": "Series", "text": ["Return the integer indices that would sort the Series values.", "Override ndarray.argsort. Argsorts the value, omitting NA/null values, and places the result in the same locations as the non-NA values.", "Has no effect but is accepted for compatibility with numpy.", "Choice of sorting algorithm. See numpy.sort() for more information. \u2018mergesort\u2019 and \u2018stable\u2019 are the only stable algorithms.", "Has no effect but is accepted for compatibility with numpy.", "Positions of values within the sort order with -1 indicating nan values.", "See also", "Returns the indices that would sort this array."]}, {"name": "pandas.Series.array", "path": "reference/api/pandas.series.array", "type": "Series", "text": ["The ExtensionArray of the data backing this Series or Index.", "An ExtensionArray of the values stored within. For extension types, this is the actual array. For NumPy native types, this is a thin (no copy) wrapper around numpy.ndarray.", ".array differs .values which may require converting the data to a different form.", "See also", "Similar method that always returns a NumPy array.", "Similar method that always returns a NumPy array.", "Notes", "This table lays out the different array types for each extension dtype within pandas.", "dtype", "array type", "category", "Categorical", "period", "PeriodArray", "interval", "IntervalArray", "IntegerNA", "IntegerArray", "string", "StringArray", "boolean", "BooleanArray", "datetime64[ns, tz]", "DatetimeArray", "For any 3rd-party extension types, the array type will be an ExtensionArray.", "For all remaining dtypes .array will be a arrays.NumpyExtensionArray wrapping the actual ndarray stored within. If you absolutely need a NumPy array (possibly with copying / coercing data), then use Series.to_numpy() instead.", "Examples", "For regular NumPy types like int, and float, a PandasArray is returned.", "For extension types, like Categorical, the actual ExtensionArray is returned"]}, {"name": "pandas.Series.asfreq", "path": "reference/api/pandas.series.asfreq", "type": "Series", "text": ["Convert time series to specified frequency.", "Returns the original data conformed to a new index with the specified frequency.", "If the index of this Series is a PeriodIndex, the new index is the result of transforming the original index with PeriodIndex.asfreq (so the original index will map one-to-one to the new index).", "Otherwise, the new index will be equivalent to pd.date_range(start, end,\nfreq=freq) where start and end are, respectively, the first and last entries in the original index (see pandas.date_range()). The values corresponding to any timesteps in the new index which were not present in the original index will be null (NaN), unless a method for filling such unknowns is provided (see the method parameter below).", "The resample() method is more appropriate if an operation on each group of timesteps (such as an aggregate) is necessary to represent the data at the new frequency.", "Frequency DateOffset or string.", "Method to use for filling holes in reindexed Series (note this does not fill NaNs that already were present):", "\u2018pad\u2019 / \u2018ffill\u2019: propagate last valid observation forward to next valid", "\u2018backfill\u2019 / \u2018bfill\u2019: use NEXT valid observation to fill.", "For PeriodIndex only (see PeriodIndex.asfreq).", "Whether to reset output index to midnight.", "Value to use for missing values, applied during upsampling (note this does not fill NaNs that already were present).", "Series object reindexed to the specified frequency.", "See also", "Conform DataFrame to new index with optional filling logic.", "Notes", "To learn more about the frequency strings, please see this link.", "Examples", "Start by creating a series with 4 one minute timestamps.", "Upsample the series into 30 second bins.", "Upsample again, providing a fill value.", "Upsample again, providing a method."]}, {"name": "pandas.Series.asof", "path": "reference/api/pandas.series.asof", "type": "Series", "text": ["Return the last row(s) without any NaNs before where.", "The last row (for each element in where, if list) without any NaN is taken. In case of a DataFrame, the last row without NaN considering only the subset of columns (if not None)", "If there is no good value, NaN is returned for a Series or a Series of NaN values for a DataFrame", "Date(s) before which the last row(s) are returned.", "For DataFrame, if not None, only use these columns to check for NaNs.", "The return can be:", "scalar : when self is a Series and where is a scalar", "Series: when self is a Series and where is an array-like, or when self is a DataFrame and where is a scalar", "DataFrame : when self is a DataFrame and where is an array-like", "Return scalar, Series, or DataFrame.", "See also", "Perform an asof merge. Similar to left join.", "Notes", "Dates are assumed to be sorted. Raises if this is not the case.", "Examples", "A Series and a scalar where.", "For a sequence where, a Series is returned. The first value is NaN, because the first element of where is before the first index value.", "Missing values are not considered. The following is 2.0, not NaN, even though NaN is at the index location for 30.", "Take all columns into consideration", "Take a single column into consideration"]}, {"name": "pandas.Series.astype", "path": "reference/api/pandas.series.astype", "type": "Series", "text": ["Cast a pandas object to a specified dtype dtype.", "Use a numpy.dtype or Python type to cast entire pandas object to the same type. Alternatively, use {col: dtype, \u2026}, where col is a column label and dtype is a numpy.dtype or Python type to cast one or more of the DataFrame\u2019s columns to column-specific types.", "Return a copy when copy=True (be very careful setting copy=False as changes to values then may propagate to other pandas objects).", "Control raising of exceptions on invalid data for provided dtype.", "raise : allow exceptions to be raised", "ignore : suppress exceptions. On error return original object.", "See also", "Convert argument to datetime.", "Convert argument to timedelta.", "Convert argument to a numeric type.", "Cast a numpy array to a specified type.", "Notes", "Deprecated since version 1.3.0: Using astype to convert from timezone-naive dtype to timezone-aware dtype is deprecated and will raise in a future version. Use Series.dt.tz_localize() instead.", "Examples", "Create a DataFrame:", "Cast all columns to int32:", "Cast col1 to int32 using a dictionary:", "Create a series:", "Convert to categorical type:", "Convert to ordered categorical type with custom ordering:", "Note that using copy=False and changing data on a new pandas object may propagate changes:", "Create a series of dates:"]}, {"name": "pandas.Series.at", "path": "reference/api/pandas.series.at", "type": "Series", "text": ["Access a single value for a row/column label pair.", "Similar to loc, in that both provide label-based lookups. Use at if you only need to get or set a single value in a DataFrame or Series.", "If \u2018label\u2019 does not exist in DataFrame.", "See also", "Access a single value for a row/column pair by integer position.", "Access a group of rows and columns by label(s).", "Access a single value using a label.", "Examples", "Get value at specified row/column pair", "Set value at specified row/column pair", "Get value within a Series"]}, {"name": "pandas.Series.at_time", "path": "reference/api/pandas.series.at_time", "type": "Series", "text": ["Select values at particular time of day (e.g., 9:30AM).", "If the index is not a DatetimeIndex", "See also", "Select values between particular times of the day.", "Select initial periods of time series based on a date offset.", "Select final periods of time series based on a date offset.", "Get just the index locations for values at particular time of the day.", "Examples"]}, {"name": "pandas.Series.attrs", "path": "reference/api/pandas.series.attrs", "type": "Series", "text": ["Dictionary of global attributes of this dataset.", "Warning", "attrs is experimental and may change without warning.", "See also", "Global flags applying to this object."]}, {"name": "pandas.Series.autocorr", "path": "reference/api/pandas.series.autocorr", "type": "Series", "text": ["Compute the lag-N autocorrelation.", "This method computes the Pearson correlation between the Series and its shifted self.", "Number of lags to apply before performing autocorrelation.", "The Pearson correlation between self and self.shift(lag).", "See also", "Compute the correlation between two Series.", "Shift index by desired number of periods.", "Compute pairwise correlation of columns.", "Compute pairwise correlation between rows or columns of two DataFrame objects.", "Notes", "If the Pearson correlation is not well defined return \u2018NaN\u2019.", "Examples", "If the Pearson correlation is not well defined, then \u2018NaN\u2019 is returned."]}, {"name": "pandas.Series.axes", "path": "reference/api/pandas.series.axes", "type": "Series", "text": ["Return a list of the row axis labels."]}, {"name": "pandas.Series.backfill", "path": "reference/api/pandas.series.backfill", "type": "Series", "text": ["Synonym for DataFrame.fillna() with method='bfill'.", "Object with missing values filled or None if inplace=True."]}, {"name": "pandas.Series.between", "path": "reference/api/pandas.series.between", "type": "Series", "text": ["Return boolean Series equivalent to left <= series <= right.", "This function returns a boolean vector containing True wherever the corresponding Series element is between the boundary values left and right. NA values are treated as False.", "Left boundary.", "Right boundary.", "Include boundaries. Whether to set each bound as closed or open.", "Changed in version 1.3.0.", "Series representing whether each element is between left and right (inclusive).", "See also", "Greater than of series and other.", "Less than of series and other.", "Notes", "This function is equivalent to (left <= ser) & (ser <= right)", "Examples", "Boundary values are included by default:", "With inclusive set to \"neither\" boundary values are excluded:", "left and right can be any scalar value:"]}, {"name": "pandas.Series.between_time", "path": "reference/api/pandas.series.between_time", "type": "Series", "text": ["Select values between particular times of the day (e.g., 9:00-9:30 AM).", "By setting start_time to be later than end_time, you can get the times that are not between the two times.", "Initial time as a time filter limit.", "End time as a time filter limit.", "Whether the start time needs to be included in the result.", "Deprecated since version 1.4.0: Arguments include_start and include_end have been deprecated to standardize boundary inputs. Use inclusive instead, to set each bound as closed or open.", "Whether the end time needs to be included in the result.", "Deprecated since version 1.4.0: Arguments include_start and include_end have been deprecated to standardize boundary inputs. Use inclusive instead, to set each bound as closed or open.", "Include boundaries; whether to set each bound as closed or open.", "Determine range time on index or columns value.", "Data from the original object filtered to the specified dates range.", "If the index is not a DatetimeIndex", "See also", "Select values at a particular time of the day.", "Select initial periods of time series based on a date offset.", "Select final periods of time series based on a date offset.", "Get just the index locations for values between particular times of the day.", "Examples", "You get the times that are not between two times by setting start_time later than end_time:"]}, {"name": "pandas.Series.bfill", "path": "reference/api/pandas.series.bfill", "type": "Series", "text": ["Synonym for DataFrame.fillna() with method='bfill'.", "Object with missing values filled or None if inplace=True."]}, {"name": "pandas.Series.bool", "path": "reference/api/pandas.series.bool", "type": "Series", "text": ["Return the bool of a single element Series or DataFrame.", "This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).", "The value in the Series or DataFrame.", "See also", "Change the data type of a Series, including to boolean.", "Change the data type of a DataFrame, including to boolean.", "NumPy boolean data type, used by pandas for boolean values.", "Examples", "The method will only work for single element objects with a boolean value:"]}, {"name": "pandas.Series.cat", "path": "reference/api/pandas.series.cat", "type": "Series", "text": ["Accessor object for categorical properties of the Series values.", "Be aware that assigning to categories is a inplace operation, while all methods return new categorical data per default (but can be called with inplace=True).", "Examples"]}, {"name": "pandas.Series.cat.add_categories", "path": "reference/api/pandas.series.cat.add_categories", "type": "Series", "text": ["Add new categories.", "new_categories will be included at the last/highest place in the categories and will be unused directly after this call.", "The new categories to be included.", "Whether or not to add the categories inplace or return a copy of this categorical with added categories.", "Deprecated since version 1.3.0.", "Categorical with new categories added or None if inplace=True.", "If the new categories include old categories or do not validate as categories", "See also", "Rename categories.", "Reorder categories.", "Remove the specified categories.", "Remove categories which are not used.", "Set the categories to the specified ones.", "Examples"]}, {"name": "pandas.Series.cat.as_ordered", "path": "reference/api/pandas.series.cat.as_ordered", "type": "Series", "text": ["Set the Categorical to be ordered.", "Whether or not to set the ordered attribute in-place or return a copy of this categorical with ordered set to True.", "Ordered Categorical or None if inplace=True."]}, {"name": "pandas.Series.cat.as_unordered", "path": "reference/api/pandas.series.cat.as_unordered", "type": "Series", "text": ["Set the Categorical to be unordered.", "Whether or not to set the ordered attribute in-place or return a copy of this categorical with ordered set to False.", "Unordered Categorical or None if inplace=True."]}, {"name": "pandas.Series.cat.categories", "path": "reference/api/pandas.series.cat.categories", "type": "Series", "text": ["The categories of this categorical.", "Setting assigns new values to each category (effectively a rename of each individual category).", "The assigned value has to be a list-like object. All items must be unique and the number of items in the new categories must be the same as the number of items in the old categories.", "Assigning to categories is a inplace operation!", "If the new categories do not validate as categories or if the number of new categories is unequal the number of old categories", "See also", "Rename categories.", "Reorder categories.", "Add new categories.", "Remove the specified categories.", "Remove categories which are not used.", "Set the categories to the specified ones."]}, {"name": "pandas.Series.cat.codes", "path": "reference/api/pandas.series.cat.codes", "type": "Series", "text": ["Return Series of codes as well as the index."]}, {"name": "pandas.Series.cat.ordered", "path": "reference/api/pandas.series.cat.ordered", "type": "Series", "text": ["Whether the categories have an ordered relationship."]}, {"name": "pandas.Series.cat.remove_categories", "path": "reference/api/pandas.series.cat.remove_categories", "type": "Series", "text": ["Remove the specified categories.", "removals must be included in the old categories. Values which were in the removed categories will be set to NaN", "The categories which should be removed.", "Whether or not to remove the categories inplace or return a copy of this categorical with removed categories.", "Deprecated since version 1.3.0.", "Categorical with removed categories or None if inplace=True.", "If the removals are not contained in the categories", "See also", "Rename categories.", "Reorder categories.", "Add new categories.", "Remove categories which are not used.", "Set the categories to the specified ones.", "Examples"]}, {"name": "pandas.Series.cat.remove_unused_categories", "path": "reference/api/pandas.series.cat.remove_unused_categories", "type": "Series", "text": ["Remove categories which are not used.", "Whether or not to drop unused categories inplace or return a copy of this categorical with unused categories dropped.", "Deprecated since version 1.2.0.", "Categorical with unused categories dropped or None if inplace=True.", "See also", "Rename categories.", "Reorder categories.", "Add new categories.", "Remove the specified categories.", "Set the categories to the specified ones.", "Examples"]}, {"name": "pandas.Series.cat.rename_categories", "path": "reference/api/pandas.series.cat.rename_categories", "type": "Series", "text": ["Rename categories.", "New categories which will replace old categories.", "list-like: all items must be unique and the number of items in the new categories must match the existing number of categories.", "dict-like: specifies a mapping from old categories to new. Categories not contained in the mapping are passed through and extra categories in the mapping are ignored.", "callable : a callable that is called on all items in the old categories and whose return values comprise the new categories.", "Whether or not to rename the categories inplace or return a copy of this categorical with renamed categories.", "Deprecated since version 1.3.0.", "Categorical with removed categories or None if inplace=True.", "If new categories are list-like and do not have the same number of items than the current categories or do not validate as categories", "See also", "Reorder categories.", "Add new categories.", "Remove the specified categories.", "Remove categories which are not used.", "Set the categories to the specified ones.", "Examples", "For dict-like new_categories, extra keys are ignored and categories not in the dictionary are passed through", "You may also provide a callable to create the new categories"]}, {"name": "pandas.Series.cat.reorder_categories", "path": "reference/api/pandas.series.cat.reorder_categories", "type": "Series", "text": ["Reorder categories as specified in new_categories.", "new_categories need to include all old categories and no new category items.", "The categories in new order.", "Whether or not the categorical is treated as a ordered categorical. If not given, do not change the ordered information.", "Whether or not to reorder the categories inplace or return a copy of this categorical with reordered categories.", "Deprecated since version 1.3.0.", "Categorical with removed categories or None if inplace=True.", "If the new categories do not contain all old category items or any new ones", "See also", "Rename categories.", "Add new categories.", "Remove the specified categories.", "Remove categories which are not used.", "Set the categories to the specified ones."]}, {"name": "pandas.Series.cat.set_categories", "path": "reference/api/pandas.series.cat.set_categories", "type": "Series", "text": ["Set the categories to the specified new_categories.", "new_categories can include new categories (which will result in unused categories) or remove old categories (which results in values set to NaN). If rename==True, the categories will simple be renamed (less or more items than in old categories will result in values set to NaN or in unused categories respectively).", "This method can be used to perform more than one action of adding, removing, and reordering simultaneously and is therefore faster than performing the individual steps via the more specialised methods.", "On the other hand this methods does not do checks (e.g., whether the old categories are included in the new categories on a reorder), which can result in surprising changes, for example when using special string dtypes, which does not considers a S1 string equal to a single char python string.", "The categories in new order.", "Whether or not the categorical is treated as a ordered categorical. If not given, do not change the ordered information.", "Whether or not the new_categories should be considered as a rename of the old categories or as reordered categories.", "Whether or not to reorder the categories in-place or return a copy of this categorical with reordered categories.", "Deprecated since version 1.3.0.", "If new_categories does not validate as categories", "See also", "Rename categories.", "Reorder categories.", "Add new categories.", "Remove the specified categories.", "Remove categories which are not used."]}, {"name": "pandas.Series.clip", "path": "reference/api/pandas.series.clip", "type": "Series", "text": ["Trim values at input threshold(s).", "Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis.", "Minimum threshold value. All values below this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.", "Maximum threshold value. All values above this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.", "Align object with lower and upper along the given axis.", "Whether to perform the operation in place on the data.", "Additional keywords have no effect but might be accepted for compatibility with numpy.", "Same type as calling object with the values outside the clip boundaries replaced or None if inplace=True.", "See also", "Trim values at input threshold in series.", "Trim values at input threshold in dataframe.", "Clip (limit) the values in an array.", "Examples", "Clips per column using lower and upper thresholds:", "Clips using specific lower and upper thresholds per column element:", "Clips using specific lower threshold per column element, with missing values:"]}, {"name": "pandas.Series.combine", "path": "reference/api/pandas.series.combine", "type": "Series", "text": ["Combine the Series with a Series or scalar according to func.", "Combine the Series and other using func to perform elementwise selection for combined Series. fill_value is assumed when value is missing at some index from one of the two objects being combined.", "The value(s) to be combined with the Series.", "Function that takes two scalars as inputs and returns an element.", "The value to assume when an index is missing from one Series or the other. The default specifies to use the appropriate NaN value for the underlying dtype of the Series.", "The result of combining the Series with the other object.", "See also", "Combine Series values, choosing the calling Series\u2019 values first.", "Examples", "Consider 2 Datasets s1 and s2 containing highest clocked speeds of different birds.", "Now, to combine the two datasets and view the highest speeds of the birds across the two datasets", "In the previous example, the resulting value for duck is missing, because the maximum of a NaN and a float is a NaN. So, in the example, we set fill_value=0, so the maximum value returned will be the value from some dataset."]}, {"name": "pandas.Series.combine_first", "path": "reference/api/pandas.series.combine_first", "type": "Series", "text": ["Update null elements with value in the same location in \u2018other\u2019.", "Combine two Series objects by filling null values in one Series with non-null values from the other Series. Result index will be the union of the two indexes.", "The value(s) to be used for filling null values.", "The result of combining the provided Series with the other object.", "See also", "Perform element-wise operation on two Series using a given function.", "Examples", "Null values still persist if the location of that null value does not exist in other"]}, {"name": "pandas.Series.compare", "path": "reference/api/pandas.series.compare", "type": "Series", "text": ["Compare to another Series and show the differences.", "New in version 1.1.0.", "Object to compare with.", "Determine which axis to align the comparison on.", "with rows drawn alternately from self and other.", "with columns drawn alternately from self and other.", "If true, all rows and columns are kept. Otherwise, only the ones with different values are kept.", "If true, the result keeps values that are equal. Otherwise, equal values are shown as NaNs.", "If axis is 0 or \u2018index\u2019 the result will be a Series. The resulting index will be a MultiIndex with \u2018self\u2019 and \u2018other\u2019 stacked alternately at the inner level.", "If axis is 1 or \u2018columns\u2019 the result will be a DataFrame. It will have two columns namely \u2018self\u2019 and \u2018other\u2019.", "See also", "Compare with another DataFrame and show differences.", "Notes", "Matching NaNs will not appear as a difference.", "Examples", "Align the differences on columns", "Stack the differences on indices", "Keep all original rows", "Keep all original rows and also all original values"]}, {"name": "pandas.Series.convert_dtypes", "path": "reference/api/pandas.series.convert_dtypes", "type": "General utility functions", "text": ["Convert columns to best possible dtypes using dtypes supporting pd.NA.", "New in version 1.0.0.", "Whether object dtypes should be converted to the best possible types.", "Whether object dtypes should be converted to StringDtype().", "Whether, if possible, conversion can be done to integer extension types.", "Whether object dtypes should be converted to BooleanDtypes().", "Whether, if possible, conversion can be done to floating extension types. If convert_integer is also True, preference will be give to integer dtypes if the floats can be faithfully casted to integers.", "New in version 1.2.0.", "Copy of input object with new dtype.", "See also", "Infer dtypes of objects.", "Convert argument to datetime.", "Convert argument to timedelta.", "Convert argument to a numeric type.", "Notes", "By default, convert_dtypes will attempt to convert a Series (or each Series in a DataFrame) to dtypes that support pd.NA. By using the options convert_string, convert_integer, convert_boolean and convert_boolean, it is possible to turn off individual conversions to StringDtype, the integer extension types, BooleanDtype or floating extension types, respectively.", "For object-dtyped columns, if infer_objects is True, use the inference rules as during normal Series/DataFrame construction. Then, if possible, convert to StringDtype, BooleanDtype or an appropriate integer or floating extension type, otherwise leave as object.", "If the dtype is integer, convert to an appropriate integer extension type.", "If the dtype is numeric, and consists of all integers, convert to an appropriate integer extension type. Otherwise, convert to an appropriate floating extension type.", "Changed in version 1.2: Starting with pandas 1.2, this method also converts float columns to the nullable floating extension type.", "In the future, as new dtypes are added that support pd.NA, the results of this method will change to support those new dtypes.", "Examples", "Start with a DataFrame with default dtypes.", "Convert the DataFrame to use best possible dtypes.", "Start with a Series of strings and missing data represented by np.nan.", "Obtain a Series with dtype StringDtype."]}, {"name": "pandas.Series.copy", "path": "reference/api/pandas.series.copy", "type": "Series", "text": ["Make a copy of this object\u2019s indices and data.", "When deep=True (default), a new object will be created with a copy of the calling object\u2019s data and indices. Modifications to the data or indices of the copy will not be reflected in the original object (see notes below).", "When deep=False, a new object will be created without copying the calling object\u2019s data or index (only references to the data and index are copied). Any changes to the data of the original will be reflected in the shallow copy (and vice versa).", "Make a deep copy, including a copy of the data and the indices. With deep=False neither the indices nor the data are copied.", "Object type matches caller.", "Notes", "When deep=True, data is copied but actual Python objects will not be copied recursively, only the reference to the object. This is in contrast to copy.deepcopy in the Standard Library, which recursively copies object data (see examples below).", "While Index objects are copied when deep=True, the underlying numpy array is not copied for performance reasons. Since Index is immutable, the underlying data can be safely shared and a copy is not needed.", "Examples", "Shallow copy versus default (deep) copy:", "Shallow copy shares data and index with original.", "Deep copy has own copy of data and index.", "Updates to the data shared by shallow copy and original is reflected in both; deep copy remains unchanged.", "Note that when copying an object containing Python objects, a deep copy will copy the data, but will not do so recursively. Updating a nested data object will be reflected in the deep copy."]}, {"name": "pandas.Series.corr", "path": "reference/api/pandas.series.corr", "type": "Series", "text": ["Compute correlation with other Series, excluding missing values.", "Series with which to compute the correlation.", "Method used to compute correlation:", "pearson : Standard correlation coefficient", "kendall : Kendall Tau correlation coefficient", "spearman : Spearman rank correlation", "callable: Callable with input two 1d ndarrays and returning a float.", "Warning", "Note that the returned matrix from corr will have 1 along the diagonals and will be symmetric regardless of the callable\u2019s behavior.", "Minimum number of observations needed to have a valid result.", "Correlation with other.", "See also", "Compute pairwise correlation between columns.", "Compute pairwise correlation with another DataFrame or Series.", "Examples"]}, {"name": "pandas.Series.count", "path": "reference/api/pandas.series.count", "type": "Series", "text": ["Return number of non-NA/null observations in the Series.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a smaller Series.", "Number of non-null values in the Series.", "See also", "Count non-NA cells for each column or row.", "Examples"]}, {"name": "pandas.Series.cov", "path": "reference/api/pandas.series.cov", "type": "Series", "text": ["Compute covariance with Series, excluding missing values.", "Series with which to compute the covariance.", "Minimum number of observations needed to have a valid result.", "Delta degrees of freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "New in version 1.1.0.", "Covariance between Series and other normalized by N-1 (unbiased estimator).", "See also", "Compute pairwise covariance of columns.", "Examples"]}, {"name": "pandas.Series.cummax", "path": "reference/api/pandas.series.cummax", "type": "Series", "text": ["Return cumulative maximum over a DataFrame or Series axis.", "Returns a DataFrame or Series of the same size containing the cumulative maximum.", "The index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "Additional keywords have no effect but might be accepted for compatibility with NumPy.", "Return cumulative maximum of scalar or Series.", "See also", "Similar functionality but ignores NaN values.", "Return the maximum over Series axis.", "Return cumulative maximum over Series axis.", "Return cumulative minimum over Series axis.", "Return cumulative sum over Series axis.", "Return cumulative product over Series axis.", "Examples", "Series", "By default, NA values are ignored.", "To include NA values in the operation, use skipna=False", "DataFrame", "By default, iterates over rows and finds the maximum in each column. This is equivalent to axis=None or axis='index'.", "To iterate over columns and find the maximum in each row, use axis=1"]}, {"name": "pandas.Series.cummin", "path": "reference/api/pandas.series.cummin", "type": "Series", "text": ["Return cumulative minimum over a DataFrame or Series axis.", "Returns a DataFrame or Series of the same size containing the cumulative minimum.", "The index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "Additional keywords have no effect but might be accepted for compatibility with NumPy.", "Return cumulative minimum of scalar or Series.", "See also", "Similar functionality but ignores NaN values.", "Return the minimum over Series axis.", "Return cumulative maximum over Series axis.", "Return cumulative minimum over Series axis.", "Return cumulative sum over Series axis.", "Return cumulative product over Series axis.", "Examples", "Series", "By default, NA values are ignored.", "To include NA values in the operation, use skipna=False", "DataFrame", "By default, iterates over rows and finds the minimum in each column. This is equivalent to axis=None or axis='index'.", "To iterate over columns and find the minimum in each row, use axis=1"]}, {"name": "pandas.Series.cumprod", "path": "reference/api/pandas.series.cumprod", "type": "Series", "text": ["Return cumulative product over a DataFrame or Series axis.", "Returns a DataFrame or Series of the same size containing the cumulative product.", "The index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "Additional keywords have no effect but might be accepted for compatibility with NumPy.", "Return cumulative product of scalar or Series.", "See also", "Similar functionality but ignores NaN values.", "Return the product over Series axis.", "Return cumulative maximum over Series axis.", "Return cumulative minimum over Series axis.", "Return cumulative sum over Series axis.", "Return cumulative product over Series axis.", "Examples", "Series", "By default, NA values are ignored.", "To include NA values in the operation, use skipna=False", "DataFrame", "By default, iterates over rows and finds the product in each column. This is equivalent to axis=None or axis='index'.", "To iterate over columns and find the product in each row, use axis=1"]}, {"name": "pandas.Series.cumsum", "path": "reference/api/pandas.series.cumsum", "type": "Series", "text": ["Return cumulative sum over a DataFrame or Series axis.", "Returns a DataFrame or Series of the same size containing the cumulative sum.", "The index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "Additional keywords have no effect but might be accepted for compatibility with NumPy.", "Return cumulative sum of scalar or Series.", "See also", "Similar functionality but ignores NaN values.", "Return the sum over Series axis.", "Return cumulative maximum over Series axis.", "Return cumulative minimum over Series axis.", "Return cumulative sum over Series axis.", "Return cumulative product over Series axis.", "Examples", "Series", "By default, NA values are ignored.", "To include NA values in the operation, use skipna=False", "DataFrame", "By default, iterates over rows and finds the sum in each column. This is equivalent to axis=None or axis='index'.", "To iterate over columns and find the sum in each row, use axis=1"]}, {"name": "pandas.Series.describe", "path": "reference/api/pandas.series.describe", "type": "Series", "text": ["Generate descriptive statistics.", "Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values.", "Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. Refer to the notes below for more detail.", "The percentiles to include in the output. All should fall between 0 and 1. The default is [.25, .5, .75], which returns the 25th, 50th, and 75th percentiles.", "A white list of data types to include in the result. Ignored for Series. Here are the options:", "\u2018all\u2019 : All columns of the input will be included in the output.", "A list-like of dtypes : Limits the results to the provided data types. To limit the result to numeric types submit numpy.number. To limit it instead to object columns submit the numpy.object data type. Strings can also be used in the style of select_dtypes (e.g. df.describe(include=['O'])). To select pandas categorical columns, use 'category'", "None (default) : The result will include all numeric columns.", "A black list of data types to omit from the result. Ignored for Series. Here are the options:", "A list-like of dtypes : Excludes the provided data types from the result. To exclude numeric types submit numpy.number. To exclude object columns submit the data type numpy.object. Strings can also be used in the style of select_dtypes (e.g. df.describe(exclude=['O'])). To exclude pandas categorical columns, use 'category'", "None (default) : The result will exclude nothing.", "Whether to treat datetime dtypes as numeric. This affects statistics calculated for the column. For DataFrame input, this also controls whether datetime columns are included by default.", "New in version 1.1.0.", "Summary statistics of the Series or Dataframe provided.", "See also", "Count number of non-NA/null observations.", "Maximum of the values in the object.", "Minimum of the values in the object.", "Mean of the values.", "Standard deviation of the observations.", "Subset of a DataFrame including/excluding columns based on their dtype.", "Notes", "For numeric data, the result\u2019s index will include count, mean, std, min, max as well as lower, 50 and upper percentiles. By default the lower percentile is 25 and the upper percentile is 75. The 50 percentile is the same as the median.", "For object data (e.g. strings or timestamps), the result\u2019s index will include count, unique, top, and freq. The top is the most common value. The freq is the most common value\u2019s frequency. Timestamps also include the first and last items.", "If multiple object values have the highest count, then the count and top results will be arbitrarily chosen from among those with the highest count.", "For mixed data types provided via a DataFrame, the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If include='all' is provided as an option, the result will include a union of attributes of each type.", "The include and exclude parameters can be used to limit which columns in a DataFrame are analyzed for the output. The parameters are ignored when analyzing a Series.", "Examples", "Describing a numeric Series.", "Describing a categorical Series.", "Describing a timestamp Series.", "Describing a DataFrame. By default only numeric fields are returned.", "Describing all columns of a DataFrame regardless of data type.", "Describing a column from a DataFrame by accessing it as an attribute.", "Including only numeric columns in a DataFrame description.", "Including only string columns in a DataFrame description.", "Including only categorical columns from a DataFrame description.", "Excluding numeric columns from a DataFrame description.", "Excluding object columns from a DataFrame description."]}, {"name": "pandas.Series.diff", "path": "reference/api/pandas.series.diff", "type": "Series", "text": ["First discrete difference of element.", "Calculates the difference of a Series element compared with another element in the Series (default is element in previous row).", "Periods to shift for calculating difference, accepts negative values.", "First differences of the Series.", "See also", "Percent change over given number of periods.", "Shift index by desired number of periods with an optional time freq.", "First discrete difference of object.", "Notes", "For boolean dtypes, this uses operator.xor() rather than operator.sub(). The result is calculated according to current dtype in Series, however dtype of the result is always float64.", "Examples", "Difference with previous row", "Difference with 3rd previous row", "Difference with following row", "Overflow in input dtype"]}, {"name": "pandas.Series.div", "path": "reference/api/pandas.series.div", "type": "Series", "text": ["Return Floating division of series and other, element-wise (binary operator truediv).", "Equivalent to series / other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Reverse of the Floating division operator, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.divide", "path": "reference/api/pandas.series.divide", "type": "Series", "text": ["Return Floating division of series and other, element-wise (binary operator truediv).", "Equivalent to series / other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Reverse of the Floating division operator, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.divmod", "path": "reference/api/pandas.series.divmod", "type": "Series", "text": ["Return Integer division and modulo of series and other, element-wise (binary operator divmod).", "Equivalent to divmod(series, other), but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Reverse of the Integer division and modulo operator, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.dot", "path": "reference/api/pandas.series.dot", "type": "Series", "text": ["Compute the dot product between the Series and the columns of other.", "This method computes the dot product between the Series and another one, or the Series and each columns of a DataFrame, or the Series and each columns of an array.", "It can also be called using self @ other in Python >= 3.5.", "The other object to compute the dot product with its columns.", "Return the dot product of the Series and other if other is a Series, the Series of the dot product of Series and each rows of other if other is a DataFrame or a numpy.ndarray between the Series and each columns of the numpy array.", "See also", "Compute the matrix product with the DataFrame.", "Multiplication of series and other, element-wise.", "Notes", "The Series and other has to share the same index if other is a Series or a DataFrame.", "Examples"]}, {"name": "pandas.Series.drop", "path": "reference/api/pandas.series.drop", "type": "Series", "text": ["Return Series with specified index labels removed.", "Remove elements of a Series based on specifying the index labels. When using a multi-index, labels on different levels can be removed by specifying the level.", "Index labels to drop.", "Redundant for application on Series.", "Redundant for application on Series, but \u2018index\u2019 can be used instead of \u2018labels\u2019.", "No change is made to the Series; use \u2018index\u2019 or \u2018labels\u2019 instead.", "For MultiIndex, level for which the labels will be removed.", "If True, do operation inplace and return None.", "If \u2018ignore\u2019, suppress error and only existing labels are dropped.", "Series with specified index labels removed or None if inplace=True.", "If none of the labels are found in the index.", "See also", "Return only specified index labels of Series.", "Return series without null values.", "Return Series with duplicate values removed.", "Drop specified labels from rows or columns.", "Examples", "Drop labels B en C", "Drop 2nd level label in MultiIndex Series"]}, {"name": "pandas.Series.drop_duplicates", "path": "reference/api/pandas.series.drop_duplicates", "type": "Series", "text": ["Return Series with duplicate values removed.", "Method to handle dropping duplicates:", "\u2018first\u2019 : Drop duplicates except for the first occurrence.", "\u2018last\u2019 : Drop duplicates except for the last occurrence.", "False : Drop all duplicates.", "If True, performs operation inplace and returns None.", "Series with duplicates dropped or None if inplace=True.", "See also", "Equivalent method on Index.", "Equivalent method on DataFrame.", "Related method on Series, indicating duplicate Series values.", "Examples", "Generate a Series with duplicated entries.", "With the \u2018keep\u2019 parameter, the selection behaviour of duplicated values can be changed. The value \u2018first\u2019 keeps the first occurrence for each set of duplicated entries. The default value of keep is \u2018first\u2019.", "The value \u2018last\u2019 for parameter \u2018keep\u2019 keeps the last occurrence for each set of duplicated entries.", "The value False for parameter \u2018keep\u2019 discards all sets of duplicated entries. Setting the value of \u2018inplace\u2019 to True performs the operation inplace and returns None."]}, {"name": "pandas.Series.droplevel", "path": "reference/api/pandas.series.droplevel", "type": "Series", "text": ["Return Series/DataFrame with requested index / column level(s) removed.", "If a string is given, must be the name of a level If list-like, elements must be names or positional indexes of levels.", "Axis along which the level(s) is removed:", "0 or \u2018index\u2019: remove level(s) in column.", "1 or \u2018columns\u2019: remove level(s) in row.", "Series/DataFrame with requested index / column level(s) removed.", "Examples"]}, {"name": "pandas.Series.dropna", "path": "reference/api/pandas.series.dropna", "type": "Series", "text": ["Return a new Series with missing values removed.", "See the User Guide for more on which values are considered missing, and how to work with missing data.", "There is only one axis to drop values from.", "If True, do operation inplace and return None.", "Not in use. Kept for compatibility.", "Series with NA entries dropped from it or None if inplace=True.", "See also", "Indicate missing values.", "Indicate existing (non-missing) values.", "Replace missing values.", "Drop rows or columns which contain NA values.", "Drop missing indices.", "Examples", "Drop NA values from a Series.", "Keep the Series with valid entries in the same variable.", "Empty strings are not considered NA values. None is considered an NA value."]}, {"name": "pandas.Series.dt", "path": "reference/api/pandas.series.dt", "type": "Series", "text": ["Accessor object for datetimelike properties of the Series values.", "Examples", "Returns a Series indexed like the original Series. Raises TypeError if the Series does not contain datetimelike values."]}, {"name": "pandas.Series.dt.ceil", "path": "reference/api/pandas.series.dt.ceil", "type": "Series", "text": ["Perform ceil operation on the data to the specified freq.", "The frequency level to ceil the index to. Must be a fixed frequency like \u2018S\u2019 (second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible freq values.", "Only relevant for DatetimeIndex:", "\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order", "bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times)", "\u2018NaT\u2019 will return NaT where there are ambiguous times", "\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST.", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time", "\u2018NaT\u2019 will return NaT where there are nonexistent times", "timedelta objects will shift nonexistent times by the timedelta", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "Index of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series.", "Notes", "If the timestamps have a timezone, ceiling will take place relative to the local (\u201cwall\u201d) time and re-localized to the same timezone. When ceiling near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.", "Examples", "DatetimeIndex", "Series", "When rounding near a daylight savings time transition, use ambiguous or nonexistent to control how the timestamp should be re-localized."]}, {"name": "pandas.Series.dt.components", "path": "reference/api/pandas.series.dt.components", "type": "Series", "text": ["Return a Dataframe of the components of the Timedeltas.", "Examples"]}, {"name": "pandas.Series.dt.date", "path": "reference/api/pandas.series.dt.date", "type": "Series", "text": ["Returns numpy array of python datetime.date objects.", "Namely, the date part of Timestamps without time and timezone information."]}, {"name": "pandas.Series.dt.day", "path": "reference/api/pandas.series.dt.day", "type": "Series", "text": ["The day of the datetime.", "Examples"]}, {"name": "pandas.Series.dt.day_name", "path": "reference/api/pandas.series.dt.day_name", "type": "Series", "text": ["Return the day names of the DateTimeIndex with specified locale.", "Locale determining the language in which to return the day name. Default is English locale.", "Index of day names.", "Examples"]}, {"name": "pandas.Series.dt.day_of_week", "path": "reference/api/pandas.series.dt.day_of_week", "type": "Series", "text": ["The day of the week with Monday=0, Sunday=6.", "Return the day of the week. It is assumed the week starts on Monday, which is denoted by 0 and ends on Sunday which is denoted by 6. This method is available on both Series with datetime values (using the dt accessor) or DatetimeIndex.", "Containing integers indicating the day number.", "See also", "Alias.", "Alias.", "Returns the name of the day of the week.", "Examples"]}, {"name": "pandas.Series.dt.day_of_year", "path": "reference/api/pandas.series.dt.day_of_year", "type": "Series", "text": ["The ordinal day of the year."]}, {"name": "pandas.Series.dt.dayofweek", "path": "reference/api/pandas.series.dt.dayofweek", "type": "Series", "text": ["The day of the week with Monday=0, Sunday=6.", "Return the day of the week. It is assumed the week starts on Monday, which is denoted by 0 and ends on Sunday which is denoted by 6. This method is available on both Series with datetime values (using the dt accessor) or DatetimeIndex.", "Containing integers indicating the day number.", "See also", "Alias.", "Alias.", "Returns the name of the day of the week.", "Examples"]}, {"name": "pandas.Series.dt.dayofyear", "path": "reference/api/pandas.series.dt.dayofyear", "type": "Series", "text": ["The ordinal day of the year."]}, {"name": "pandas.Series.dt.days", "path": "reference/api/pandas.series.dt.days", "type": "Series", "text": ["Number of days for each element."]}, {"name": "pandas.Series.dt.days_in_month", "path": "reference/api/pandas.series.dt.days_in_month", "type": "Series", "text": ["The number of days in the month."]}, {"name": "pandas.Series.dt.daysinmonth", "path": "reference/api/pandas.series.dt.daysinmonth", "type": "Series", "text": ["The number of days in the month."]}, {"name": "pandas.Series.dt.end_time", "path": "reference/api/pandas.series.dt.end_time", "type": "Series", "text": []}, {"name": "pandas.Series.dt.floor", "path": "reference/api/pandas.series.dt.floor", "type": "Series", "text": ["Perform floor operation on the data to the specified freq.", "The frequency level to floor the index to. Must be a fixed frequency like \u2018S\u2019 (second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible freq values.", "Only relevant for DatetimeIndex:", "\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order", "bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times)", "\u2018NaT\u2019 will return NaT where there are ambiguous times", "\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST.", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time", "\u2018NaT\u2019 will return NaT where there are nonexistent times", "timedelta objects will shift nonexistent times by the timedelta", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "Index of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series.", "Notes", "If the timestamps have a timezone, flooring will take place relative to the local (\u201cwall\u201d) time and re-localized to the same timezone. When flooring near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.", "Examples", "DatetimeIndex", "Series", "When rounding near a daylight savings time transition, use ambiguous or nonexistent to control how the timestamp should be re-localized."]}, {"name": "pandas.Series.dt.freq", "path": "reference/api/pandas.series.dt.freq", "type": "Series", "text": []}, {"name": "pandas.Series.dt.hour", "path": "reference/api/pandas.series.dt.hour", "type": "Series", "text": ["The hours of the datetime.", "Examples"]}, {"name": "pandas.Series.dt.is_leap_year", "path": "reference/api/pandas.series.dt.is_leap_year", "type": "Series", "text": ["Boolean indicator if the date belongs to a leap year.", "A leap year is a year, which has 366 days (instead of 365) including 29th of February as an intercalary day. Leap years are years which are multiples of four with the exception of years divisible by 100 but not by 400.", "Booleans indicating if dates belong to a leap year.", "Examples", "This method is available on Series with datetime values under the .dt accessor, and directly on DatetimeIndex."]}, {"name": "pandas.Series.dt.is_month_end", "path": "reference/api/pandas.series.dt.is_month_end", "type": "Series", "text": ["Indicates whether the date is the last day of the month.", "For Series, returns a Series with boolean values. For DatetimeIndex, returns a boolean array.", "See also", "Return a boolean indicating whether the date is the first day of the month.", "Return a boolean indicating whether the date is the last day of the month.", "Examples", "This method is available on Series with datetime values under the .dt accessor, and directly on DatetimeIndex."]}, {"name": "pandas.Series.dt.is_month_start", "path": "reference/api/pandas.series.dt.is_month_start", "type": "Series", "text": ["Indicates whether the date is the first day of the month.", "For Series, returns a Series with boolean values. For DatetimeIndex, returns a boolean array.", "See also", "Return a boolean indicating whether the date is the first day of the month.", "Return a boolean indicating whether the date is the last day of the month.", "Examples", "This method is available on Series with datetime values under the .dt accessor, and directly on DatetimeIndex."]}, {"name": "pandas.Series.dt.is_quarter_end", "path": "reference/api/pandas.series.dt.is_quarter_end", "type": "Series", "text": ["Indicator for whether the date is the last day of a quarter.", "The same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.", "See also", "Return the quarter of the date.", "Similar property indicating the quarter start.", "Examples", "This method is available on Series with datetime values under the .dt accessor, and directly on DatetimeIndex."]}, {"name": "pandas.Series.dt.is_quarter_start", "path": "reference/api/pandas.series.dt.is_quarter_start", "type": "Series", "text": ["Indicator for whether the date is the first day of a quarter.", "The same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.", "See also", "Return the quarter of the date.", "Similar property for indicating the quarter start.", "Examples", "This method is available on Series with datetime values under the .dt accessor, and directly on DatetimeIndex."]}, {"name": "pandas.Series.dt.is_year_end", "path": "reference/api/pandas.series.dt.is_year_end", "type": "Series", "text": ["Indicate whether the date is the last day of the year.", "The same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.", "See also", "Similar property indicating the start of the year.", "Examples", "This method is available on Series with datetime values under the .dt accessor, and directly on DatetimeIndex."]}, {"name": "pandas.Series.dt.is_year_start", "path": "reference/api/pandas.series.dt.is_year_start", "type": "Series", "text": ["Indicate whether the date is the first day of a year.", "The same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.", "See also", "Similar property indicating the last day of the year.", "Examples", "This method is available on Series with datetime values under the .dt accessor, and directly on DatetimeIndex."]}, {"name": "pandas.Series.dt.microsecond", "path": "reference/api/pandas.series.dt.microsecond", "type": "Series", "text": ["The microseconds of the datetime.", "Examples"]}, {"name": "pandas.Series.dt.microseconds", "path": "reference/api/pandas.series.dt.microseconds", "type": "Series", "text": ["Number of microseconds (>= 0 and less than 1 second) for each element."]}, {"name": "pandas.Series.dt.minute", "path": "reference/api/pandas.series.dt.minute", "type": "Series", "text": ["The minutes of the datetime.", "Examples"]}, {"name": "pandas.Series.dt.month", "path": "reference/api/pandas.series.dt.month", "type": "Series", "text": ["The month as January=1, December=12.", "Examples"]}, {"name": "pandas.Series.dt.month_name", "path": "reference/api/pandas.series.dt.month_name", "type": "Series", "text": ["Return the month names of the DateTimeIndex with specified locale.", "Locale determining the language in which to return the month name. Default is English locale.", "Index of month names.", "Examples"]}, {"name": "pandas.Series.dt.nanosecond", "path": "reference/api/pandas.series.dt.nanosecond", "type": "Series", "text": ["The nanoseconds of the datetime.", "Examples"]}, {"name": "pandas.Series.dt.nanoseconds", "path": "reference/api/pandas.series.dt.nanoseconds", "type": "Series", "text": ["Number of nanoseconds (>= 0 and less than 1 microsecond) for each element."]}, {"name": "pandas.Series.dt.normalize", "path": "reference/api/pandas.series.dt.normalize", "type": "Series", "text": ["Convert times to midnight.", "The time component of the date-time is converted to midnight i.e. 00:00:00. This is useful in cases, when the time does not matter. Length is unaltered. The timezones are unaffected.", "This method is available on Series with datetime values under the .dt accessor, and directly on Datetime Array/Index.", "The same type as the original data. Series will have the same name and index. DatetimeIndex will have the same name.", "See also", "Floor the datetimes to the specified freq.", "Ceil the datetimes to the specified freq.", "Round the datetimes to the specified freq.", "Examples"]}, {"name": "pandas.Series.dt.quarter", "path": "reference/api/pandas.series.dt.quarter", "type": "Series", "text": ["The quarter of the date."]}, {"name": "pandas.Series.dt.qyear", "path": "reference/api/pandas.series.dt.qyear", "type": "Series", "text": []}, {"name": "pandas.Series.dt.round", "path": "reference/api/pandas.series.dt.round", "type": "Series", "text": ["Perform round operation on the data to the specified freq.", "The frequency level to round the index to. Must be a fixed frequency like \u2018S\u2019 (second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible freq values.", "Only relevant for DatetimeIndex:", "\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order", "bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times)", "\u2018NaT\u2019 will return NaT where there are ambiguous times", "\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST.", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time", "\u2018NaT\u2019 will return NaT where there are nonexistent times", "timedelta objects will shift nonexistent times by the timedelta", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "Index of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series.", "Notes", "If the timestamps have a timezone, rounding will take place relative to the local (\u201cwall\u201d) time and re-localized to the same timezone. When rounding near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.", "Examples", "DatetimeIndex", "Series", "When rounding near a daylight savings time transition, use ambiguous or nonexistent to control how the timestamp should be re-localized."]}, {"name": "pandas.Series.dt.second", "path": "reference/api/pandas.series.dt.second", "type": "Series", "text": ["The seconds of the datetime.", "Examples"]}, {"name": "pandas.Series.dt.seconds", "path": "reference/api/pandas.series.dt.seconds", "type": "Series", "text": ["Number of seconds (>= 0 and less than 1 day) for each element."]}, {"name": "pandas.Series.dt.start_time", "path": "reference/api/pandas.series.dt.start_time", "type": "Series", "text": []}, {"name": "pandas.Series.dt.strftime", "path": "reference/api/pandas.series.dt.strftime", "type": "Series", "text": ["Convert to Index using specified date_format.", "Return an Index of formatted strings specified by date_format, which supports the same string format as the python standard library. Details of the string format can be found in python string format doc.", "Date format string (e.g. \u201c%Y-%m-%d\u201d).", "NumPy ndarray of formatted strings.", "See also", "Convert the given argument to datetime.", "Return DatetimeIndex with times to midnight.", "Round the DatetimeIndex to the specified freq.", "Floor the DatetimeIndex to the specified freq.", "Examples"]}, {"name": "pandas.Series.dt.time", "path": "reference/api/pandas.series.dt.time", "type": "Series", "text": ["Returns numpy array of datetime.time objects.", "The time part of the Timestamps."]}, {"name": "pandas.Series.dt.timetz", "path": "reference/api/pandas.series.dt.timetz", "type": "Series", "text": ["Returns numpy array of datetime.time objects with timezone information.", "The time part of the Timestamps."]}, {"name": "pandas.Series.dt.to_period", "path": "reference/api/pandas.series.dt.to_period", "type": "Input/output", "text": ["Cast to PeriodArray/Index at a particular frequency.", "Converts DatetimeArray/Index to PeriodArray/Index.", "One of pandas\u2019 offset strings or an Offset object. Will be inferred by default.", "When converting a DatetimeArray/Index with non-regular values, so that a frequency cannot be inferred.", "See also", "Immutable ndarray holding ordinal values.", "Return DatetimeIndex as object.", "Examples", "Infer the daily frequency"]}, {"name": "pandas.Series.dt.to_pydatetime", "path": "reference/api/pandas.series.dt.to_pydatetime", "type": "Series", "text": ["Return the data as an array of datetime.datetime objects.", "Timezone information is retained if present.", "Warning", "Python\u2019s datetime uses microsecond resolution, which is lower than pandas (nanosecond). The values are truncated.", "Object dtype array containing native Python datetime objects.", "See also", "Standard library value for a datetime.", "Examples", "pandas\u2019 nanosecond precision is truncated to microseconds."]}, {"name": "pandas.Series.dt.to_pytimedelta", "path": "reference/api/pandas.series.dt.to_pytimedelta", "type": "Series", "text": ["Return an array of native datetime.timedelta objects.", "Python\u2019s standard datetime library uses a different representation timedelta\u2019s. This method converts a Series of pandas Timedeltas to datetime.timedelta format with the same length as the original Series.", "Array of 1D containing data with datetime.timedelta type.", "See also", "A duration expressing the difference between two date, time, or datetime.", "Examples"]}, {"name": "pandas.Series.dt.total_seconds", "path": "reference/api/pandas.series.dt.total_seconds", "type": "Series", "text": ["Return total duration of each element expressed in seconds.", "This method is available directly on TimedeltaArray, TimedeltaIndex and on Series containing timedelta values under the .dt namespace.", "When the calling object is a TimedeltaArray, the return type is ndarray. When the calling object is a TimedeltaIndex, the return type is a Float64Index. When the calling object is a Series, the return type is Series of type float64 whose index is the same as the original.", "See also", "Standard library version of this method.", "Return a DataFrame with components of each Timedelta.", "Examples", "Series", "TimedeltaIndex"]}, {"name": "pandas.Series.dt.tz", "path": "reference/api/pandas.series.dt.tz", "type": "Series", "text": ["Return the timezone.", "Returns None when the array is tz-naive."]}, {"name": "pandas.Series.dt.tz_convert", "path": "reference/api/pandas.series.dt.tz_convert", "type": "Series", "text": ["Convert tz-aware Datetime Array/Index from one time zone to another.", "Time zone for time. Corresponding timestamps would be converted to this time zone of the Datetime Array/Index. A tz of None will convert to UTC and remove the timezone information.", "If Datetime Array/Index is tz-naive.", "See also", "A timezone that has a variable offset from UTC.", "Localize tz-naive DatetimeIndex to a given time zone, or remove timezone from a tz-aware DatetimeIndex.", "Examples", "With the tz parameter, we can change the DatetimeIndex to other time zones:", "With the tz=None, we can remove the timezone (after converting to UTC if necessary):"]}, {"name": "pandas.Series.dt.tz_localize", "path": "reference/api/pandas.series.dt.tz_localize", "type": "Series", "text": ["Localize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index.", "This method takes a time zone (tz) naive Datetime Array/Index object and makes this time zone aware. It does not move the time to another time zone.", "This method can also be used to do the inverse \u2013 to create a time zone unaware object from an aware object. To that end, pass tz=None.", "Time zone to convert timestamps to. Passing None will remove the time zone information preserving local time.", "When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled.", "\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order", "bool-ndarray where True signifies a DST time, False signifies a non-DST time (note that this flag is only applicable for ambiguous times)", "\u2018NaT\u2019 will return NaT where there are ambiguous times", "\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST.", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time", "\u2018NaT\u2019 will return NaT where there are nonexistent times", "timedelta objects will shift nonexistent times by the timedelta", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "Array/Index converted to the specified time zone.", "If the Datetime Array/Index is tz-aware and tz is not None.", "See also", "Convert tz-aware DatetimeIndex from one time zone to another.", "Examples", "Localize DatetimeIndex in US/Eastern time zone:", "With the tz=None, we can remove the time zone information while keeping the local time (not converted to UTC):", "Be careful with DST changes. When there is sequential data, pandas can infer the DST time:", "In some cases, inferring the DST is impossible. In such cases, you can pass an ndarray to the ambiguous parameter to set the DST explicitly", "If the DST transition causes nonexistent times, you can shift these dates forward or backwards with a timedelta object or \u2018shift_forward\u2019 or \u2018shift_backwards\u2019."]}, {"name": "pandas.Series.dt.week", "path": "reference/api/pandas.series.dt.week", "type": "Series", "text": ["The week ordinal of the year.", "Deprecated since version 1.1.0.", "Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead."]}, {"name": "pandas.Series.dt.weekday", "path": "reference/api/pandas.series.dt.weekday", "type": "Series", "text": ["The day of the week with Monday=0, Sunday=6.", "Return the day of the week. It is assumed the week starts on Monday, which is denoted by 0 and ends on Sunday which is denoted by 6. This method is available on both Series with datetime values (using the dt accessor) or DatetimeIndex.", "Containing integers indicating the day number.", "See also", "Alias.", "Alias.", "Returns the name of the day of the week.", "Examples"]}, {"name": "pandas.Series.dt.weekofyear", "path": "reference/api/pandas.series.dt.weekofyear", "type": "Series", "text": ["The week ordinal of the year.", "Deprecated since version 1.1.0.", "Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead."]}, {"name": "pandas.Series.dt.year", "path": "reference/api/pandas.series.dt.year", "type": "Series", "text": ["The year of the datetime.", "Examples"]}, {"name": "pandas.Series.dtype", "path": "reference/api/pandas.series.dtype", "type": "Series", "text": ["Return the dtype object of the underlying data."]}, {"name": "pandas.Series.dtypes", "path": "reference/api/pandas.series.dtypes", "type": "General utility functions", "text": ["Return the dtype object of the underlying data."]}, {"name": "pandas.Series.duplicated", "path": "reference/api/pandas.series.duplicated", "type": "Series", "text": ["Indicate duplicate Series values.", "Duplicated values are indicated as True values in the resulting Series. Either all duplicates, all except the first or all except the last occurrence of duplicates can be indicated.", "Method to handle dropping duplicates:", "\u2018first\u2019 : Mark duplicates as True except for the first occurrence.", "\u2018last\u2019 : Mark duplicates as True except for the last occurrence.", "False : Mark all duplicates as True.", "Series indicating whether each value has occurred in the preceding values.", "See also", "Equivalent method on pandas.Index.", "Equivalent method on pandas.DataFrame.", "Remove duplicate values from Series.", "Examples", "By default, for each set of duplicated values, the first occurrence is set on False and all others on True:", "which is equivalent to", "By using \u2018last\u2019, the last occurrence of each set of duplicated values is set on False and all others on True:", "By setting keep on False, all duplicates are True:"]}, {"name": "pandas.Series.empty", "path": "reference/api/pandas.series.empty", "type": "Series", "text": ["Indicator whether Series/DataFrame is empty.", "True if Series/DataFrame is entirely empty (no items), meaning any of the axes are of length 0.", "If Series/DataFrame is empty, return True, if not return False.", "See also", "Return series without null values.", "Return DataFrame with labels on given axis omitted where (all or any) data are missing.", "Notes", "If Series/DataFrame contains only NaNs, it is still not considered empty. See the example below.", "Examples", "An example of an actual empty DataFrame. Notice the index is empty:", "If we only have NaNs in our DataFrame, it is not considered empty! We will need to drop the NaNs to make the DataFrame empty:"]}, {"name": "pandas.Series.eq", "path": "reference/api/pandas.series.eq", "type": "Series", "text": ["Return Equal to of series and other, element-wise (binary operator eq).", "Equivalent to series == other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "Examples"]}, {"name": "pandas.Series.equals", "path": "reference/api/pandas.series.equals", "type": "Series", "text": ["Test whether two objects contain the same elements.", "This function allows two Series or DataFrames to be compared against each other to see if they have the same shape and elements. NaNs in the same location are considered equal.", "The row/column index do not need to have the same type, as long as the values are considered equal. Corresponding columns must be of the same dtype.", "The other Series or DataFrame to be compared with the first.", "True if all elements are the same in both objects, False otherwise.", "See also", "Compare two Series objects of the same length and return a Series where each element is True if the element in each Series is equal, False otherwise.", "Compare two DataFrame objects of the same shape and return a DataFrame where each element is True if the respective element in each DataFrame is equal, False otherwise.", "Raises an AssertionError if left and right are not equal. Provides an easy interface to ignore inequality in dtypes, indexes and precision among others.", "Like assert_series_equal, but targets DataFrames.", "Return True if two arrays have the same shape and elements, False otherwise.", "Examples", "DataFrames df and exactly_equal have the same types and values for their elements and column labels, which will return True.", "DataFrames df and different_column_type have the same element types and values, but have different types for the column labels, which will still return True.", "DataFrames df and different_data_type have different types for the same values for their elements, and will return False even though their column labels are the same values and types."]}, {"name": "pandas.Series.ewm", "path": "reference/api/pandas.series.ewm", "type": "Series", "text": ["Provide exponentially weighted (EW) calculations.", "Exactly one parameter: com, span, halflife, or alpha must be provided.", "Specify decay in terms of center of mass", "\\(\\alpha = 1 / (1 + com)\\), for \\(com \\geq 0\\).", "Specify decay in terms of span", "\\(\\alpha = 2 / (span + 1)\\), for \\(span \\geq 1\\).", "Specify decay in terms of half-life", "\\(\\alpha = 1 - \\exp\\left(-\\ln(2) / halflife\\right)\\), for \\(halflife > 0\\).", "If times is specified, the time unit (str or timedelta) over which an observation decays to half its value. Only applicable to mean(), and halflife value will not apply to the other functions.", "New in version 1.1.0.", "Specify smoothing factor \\(\\alpha\\) directly", "\\(0 < \\alpha \\leq 1\\).", "Minimum number of observations in window required to have a value; otherwise, result is np.nan.", "Divide by decaying adjustment factor in beginning periods to account for imbalance in relative weightings (viewing EWMA as a moving average).", "When adjust=True (default), the EW function is calculated using weights \\(w_i = (1 - \\alpha)^i\\). For example, the EW moving average of the series [\\(x_0, x_1, ..., x_t\\)] would be:", "When adjust=False, the exponentially weighted function is calculated recursively:", "Ignore missing values when calculating weights.", "When ignore_na=False (default), weights are based on absolute positions. For example, the weights of \\(x_0\\) and \\(x_2\\) used in calculating the final weighted average of [\\(x_0\\), None, \\(x_2\\)] are \\((1-\\alpha)^2\\) and \\(1\\) if adjust=True, and \\((1-\\alpha)^2\\) and \\(\\alpha\\) if adjust=False.", "When ignore_na=True, weights are based on relative positions. For example, the weights of \\(x_0\\) and \\(x_2\\) used in calculating the final weighted average of [\\(x_0\\), None, \\(x_2\\)] are \\(1-\\alpha\\) and \\(1\\) if adjust=True, and \\(1-\\alpha\\) and \\(\\alpha\\) if adjust=False.", "If 0 or 'index', calculate across the rows.", "If 1 or 'columns', calculate across the columns.", "New in version 1.1.0.", "Only applicable to mean().", "Times corresponding to the observations. Must be monotonically increasing and datetime64[ns] dtype.", "If 1-D array like, a sequence with the same shape as the observations.", "Deprecated since version 1.4.0: If str, the name of the column in the DataFrame representing the times.", "New in version 1.4.0.", "Execute the rolling operation per single column or row ('single') or over the entire object ('table').", "This argument is only implemented when specifying engine='numba' in the method call.", "Only applicable to mean()", "See also", "Provides rolling window calculations.", "Provides expanding transformations.", "Notes", "See Windowing Operations for further usage details and examples.", "Examples", "adjust", "ignore_na", "times", "Exponentially weighted mean with weights calculated with a timedelta halflife relative to times."]}, {"name": "pandas.Series.expanding", "path": "reference/api/pandas.series.expanding", "type": "Series", "text": ["Provide expanding window calculations.", "Minimum number of observations in window required to have a value; otherwise, result is np.nan.", "If False, set the window labels as the right edge of the window index.", "If True, set the window labels as the center of the window index.", "Deprecated since version 1.1.0.", "If 0 or 'index', roll across the rows.", "If 1 or 'columns', roll across the columns.", "Execute the rolling operation per single column or row ('single') or over the entire object ('table').", "This argument is only implemented when specifying engine='numba' in the method call.", "New in version 1.3.0.", "See also", "Provides rolling window calculations.", "Provides exponential weighted functions.", "Notes", "See Windowing Operations for further usage details and examples.", "Examples", "min_periods", "Expanding sum with 1 vs 3 observations needed to calculate a value."]}, {"name": "pandas.Series.explode", "path": "reference/api/pandas.series.explode", "type": "Series", "text": ["Transform each element of a list-like to a row.", "New in version 0.25.0.", "If True, the resulting index will be labeled 0, 1, \u2026, n - 1.", "New in version 1.1.0.", "Exploded lists to rows; index will be duplicated for these rows.", "See also", "Split string values on specified separator.", "Unstack, a.k.a. pivot, Series with MultiIndex to produce DataFrame.", "Unpivot a DataFrame from wide format to long format.", "Explode a DataFrame from list-like columns to long format.", "Notes", "This routine will explode list-likes including lists, tuples, sets, Series, and np.ndarray. The result dtype of the subset rows will be object. Scalars will be returned unchanged, and empty list-likes will result in a np.nan for that row. In addition, the ordering of elements in the output will be non-deterministic when exploding sets.", "Examples"]}, {"name": "pandas.Series.factorize", "path": "reference/api/pandas.series.factorize", "type": "Series", "text": ["Encode the object as an enumerated type or categorical variable.", "This method is useful for obtaining a numeric representation of an array when all that matters is identifying distinct values. factorize is available as both a top-level function pandas.factorize(), and as a method Series.factorize() and Index.factorize().", "Sort uniques and shuffle codes to maintain the relationship.", "Value to mark \u201cnot found\u201d. If None, will not drop the NaN from the uniques of the values.", "Changed in version 1.1.2.", "An integer ndarray that\u2019s an indexer into uniques. uniques.take(codes) will have the same values as values.", "The unique valid values. When values is Categorical, uniques is a Categorical. When values is some other pandas object, an Index is returned. Otherwise, a 1-D ndarray is returned.", "Note", "Even if there\u2019s a missing value in values, uniques will not contain an entry for it.", "See also", "Discretize continuous-valued array.", "Find the unique value in an array.", "Examples", "These examples all show factorize as a top-level method like pd.factorize(values). The results are identical for methods like Series.factorize().", "With sort=True, the uniques will be sorted, and codes will be shuffled so that the relationship is the maintained.", "Missing values are indicated in codes with na_sentinel (-1 by default). Note that missing values are never included in uniques.", "Thus far, we\u2019ve only factorized lists (which are internally coerced to NumPy arrays). When factorizing pandas objects, the type of uniques will differ. For Categoricals, a Categorical is returned.", "Notice that 'b' is in uniques.categories, despite not being present in cat.values.", "For all other pandas objects, an Index of the appropriate type is returned.", "If NaN is in the values, and we want to include NaN in the uniques of the values, it can be achieved by setting na_sentinel=None."]}, {"name": "pandas.Series.ffill", "path": "reference/api/pandas.series.ffill", "type": "Series", "text": ["Synonym for DataFrame.fillna() with method='ffill'.", "Object with missing values filled or None if inplace=True."]}, {"name": "pandas.Series.fillna", "path": "reference/api/pandas.series.fillna", "type": "Series", "text": ["Fill NA/NaN values using the specified method.", "Value to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of values specifying which value to use for each index (for a Series) or column (for a DataFrame). Values not in the dict/Series/DataFrame will not be filled. This value cannot be a list.", "Method to use for filling holes in reindexed Series pad / ffill: propagate last valid observation forward to next valid backfill / bfill: use next valid observation to fill gap.", "Axis along which to fill missing values.", "If True, fill in-place. Note: this will modify any other views on this object (e.g., a no-copy slice for a column in a DataFrame).", "If method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None.", "A dict of item->dtype of what to downcast if possible, or the string \u2018infer\u2019 which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible).", "Object with missing values filled or None if inplace=True.", "See also", "Fill NaN values using interpolation.", "Conform object to new index.", "Convert TimeSeries to specified frequency.", "Examples", "Replace all NaN elements with 0s.", "We can also propagate non-null values forward or backward.", "Replace all NaN elements in column \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019, with 0, 1, 2, and 3 respectively.", "Only replace the first NaN element.", "When filling using a DataFrame, replacement happens along the same column names and same indices", "Note that column D is not affected since it is not present in df2."]}, {"name": "pandas.Series.filter", "path": "reference/api/pandas.series.filter", "type": "Series", "text": ["Subset the dataframe rows or columns according to the specified index labels.", "Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.", "Keep labels from axis which are in items.", "Keep labels from axis for which \u201clike in label == True\u201d.", "Keep labels from axis for which re.search(regex, label) == True.", "The axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, \u2018index\u2019 for Series, \u2018columns\u2019 for DataFrame.", "See also", "Access a group of rows and columns by label(s) or a boolean array.", "Notes", "The items, like, and regex parameters are enforced to be mutually exclusive.", "axis defaults to the info axis that is used when indexing with [].", "Examples"]}, {"name": "pandas.Series.first", "path": "reference/api/pandas.series.first", "type": "Series", "text": ["Select initial periods of time series data based on a date offset.", "When having a DataFrame with dates as index, this function can select the first few rows based on a date offset.", "The offset length of the data that will be selected. For instance, \u20181M\u2019 will display all the rows having their index within the first month.", "A subset of the caller.", "If the index is not a DatetimeIndex", "See also", "Select final periods of time series based on a date offset.", "Select values at a particular time of the day.", "Select values between particular times of the day.", "Examples", "Get the rows for the first 3 days:", "Notice the data for 3 first calendar days were returned, not the first 3 days observed in the dataset, and therefore data for 2018-04-13 was not returned."]}, {"name": "pandas.Series.first_valid_index", "path": "reference/api/pandas.series.first_valid_index", "type": "Series", "text": ["Return index for first non-NA value or None, if no NA value is found.", "Notes", "If all elements are non-NA/null, returns None. Also returns None for empty Series/DataFrame."]}, {"name": "pandas.Series.flags", "path": "reference/api/pandas.series.flags", "type": "Series", "text": ["Get the properties associated with this pandas object.", "The available flags are", "Flags.allows_duplicate_labels", "See also", "Flags that apply to pandas objects.", "Global metadata applying to this dataset.", "Notes", "\u201cFlags\u201d differ from \u201cmetadata\u201d. Flags reflect properties of the pandas object (the Series or DataFrame). Metadata refer to properties of the dataset, and should be stored in DataFrame.attrs.", "Examples", "Flags can be get or set using .", "Or by slicing with a key"]}, {"name": "pandas.Series.floordiv", "path": "reference/api/pandas.series.floordiv", "type": "Series", "text": ["Return Integer division of series and other, element-wise (binary operator floordiv).", "Equivalent to series // other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Reverse of the Integer division operator, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.ge", "path": "reference/api/pandas.series.ge", "type": "Series", "text": ["Return Greater than or equal to of series and other, element-wise (binary operator ge).", "Equivalent to series >= other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "Examples"]}, {"name": "pandas.Series.get", "path": "reference/api/pandas.series.get", "type": "Series", "text": ["Get item from object for given key (ex: DataFrame column).", "Returns default value if not found.", "Examples", "If the key isn\u2019t found, the default value will be used."]}, {"name": "pandas.Series.groupby", "path": "reference/api/pandas.series.groupby", "type": "Series", "text": ["Group Series using a mapper or by a Series of columns.", "A groupby operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups.", "Used to determine the groups for the groupby. If by is a function, it\u2019s called on each value of the object\u2019s index. If a dict or Series is passed, the Series or dict VALUES will be used to determine the groups (the Series\u2019 values are first aligned; see .align() method). If a list or ndarray of length equal to the selected axis is passed (see the groupby user guide), the values are used as-is to determine the groups. A label or list of labels may be passed to group by the columns in self. Notice that a tuple is interpreted as a (single) key.", "Split along rows (0) or columns (1).", "If the axis is a MultiIndex (hierarchical), group by a particular level or levels.", "For aggregated output, return object with group labels as the index. Only relevant for DataFrame input. as_index=False is effectively \u201cSQL-style\u201d grouped output.", "Sort group keys. Get better performance by turning this off. Note this does not influence the order of observations within each group. Groupby preserves the order of rows within each group.", "When calling apply, add group keys to index to identify pieces.", "Reduce the dimensionality of the return type if possible, otherwise return a consistent type.", "Deprecated since version 1.1.0.", "This only applies if any of the groupers are Categoricals. If True: only show observed values for categorical groupers. If False: show all values for categorical groupers.", "If True, and if group keys contain NA values, NA values together with row/column will be dropped. If False, NA values will also be treated as the key in groups.", "New in version 1.1.0.", "Returns a groupby object that contains information about the groups.", "See also", "Convenience method for frequency conversion and resampling of time series.", "Notes", "See the user guide for more detailed usage and examples, including splitting an object into groups, iterating through groups, selecting a group, aggregation, and more.", "Examples", "Grouping by Indexes", "We can groupby different levels of a hierarchical index using the level parameter:", "We can also choose to include NA in group keys or not by defining dropna parameter, the default setting is True."]}, {"name": "pandas.Series.gt", "path": "reference/api/pandas.series.gt", "type": "Series", "text": ["Return Greater than of series and other, element-wise (binary operator gt).", "Equivalent to series > other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "Examples"]}, {"name": "pandas.Series.hasnans", "path": "reference/api/pandas.series.hasnans", "type": "Series", "text": ["Return True if there are any NaNs.", "Enables various performance speedups."]}, {"name": "pandas.Series.head", "path": "reference/api/pandas.series.head", "type": "Series", "text": ["Return the first n rows.", "This function returns the first n rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it.", "For negative values of n, this function returns all rows except the last n rows, equivalent to df[:-n].", "Number of rows to select.", "The first n rows of the caller object.", "See also", "Returns the last n rows.", "Examples", "Viewing the first 5 lines", "Viewing the first n lines (three in this case)", "For negative values of n"]}, {"name": "pandas.Series.hist", "path": "reference/api/pandas.series.hist", "type": "Series", "text": ["Draw histogram of the input series using matplotlib.", "If passed, then used to form histograms for separate groups.", "If not passed, uses gca().", "Whether to show axis grid lines.", "If specified changes the x-axis label size.", "Rotation of x axis labels.", "If specified changes the y-axis label size.", "Rotation of y axis labels.", "Figure size in inches by default.", "Number of histogram bins to be used. If an integer is given, bins + 1 bin edges are calculated and returned. If bins is a sequence, gives bin edges, including left edge of first bin and right edge of last bin. In this case, bins is returned unmodified.", "Backend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.", "New in version 1.0.0.", "Whether to show the legend.", "New in version 1.1.0.", "To be passed to the actual plotting function.", "A histogram plot.", "See also", "Plot a histogram using matplotlib."]}, {"name": "pandas.Series.iat", "path": "reference/api/pandas.series.iat", "type": "Series", "text": ["Access a single value for a row/column pair by integer position.", "Similar to iloc, in that both provide integer-based lookups. Use iat if you only need to get or set a single value in a DataFrame or Series.", "When integer position is out of bounds.", "See also", "Access a single value for a row/column label pair.", "Access a group of rows and columns by label(s).", "Access a group of rows and columns by integer position(s).", "Examples", "Get value at specified row/column pair", "Set value at specified row/column pair", "Get value within a series"]}, {"name": "pandas.Series.idxmax", "path": "reference/api/pandas.series.idxmax", "type": "Series", "text": ["Return the row label of the maximum value.", "If multiple values equal the maximum, the first row label with that value is returned.", "For compatibility with DataFrame.idxmax. Redundant for application on Series.", "Exclude NA/null values. If the entire Series is NA, the result will be NA.", "Additional arguments and keywords have no effect but might be accepted for compatibility with NumPy.", "Label of the maximum value.", "If the Series is empty.", "See also", "Return indices of the maximum values along the given axis.", "Return index of first occurrence of maximum over requested axis.", "Return index label of the first occurrence of minimum of values.", "Notes", "This method is the Series version of ndarray.argmax. This method returns the label of the maximum, while ndarray.argmax returns the position. To get the position, use series.values.argmax().", "Examples", "If skipna is False and there is an NA value in the data, the function returns nan."]}, {"name": "pandas.Series.idxmin", "path": "reference/api/pandas.series.idxmin", "type": "Series", "text": ["Return the row label of the minimum value.", "If multiple values equal the minimum, the first row label with that value is returned.", "For compatibility with DataFrame.idxmin. Redundant for application on Series.", "Exclude NA/null values. If the entire Series is NA, the result will be NA.", "Additional arguments and keywords have no effect but might be accepted for compatibility with NumPy.", "Label of the minimum value.", "If the Series is empty.", "See also", "Return indices of the minimum values along the given axis.", "Return index of first occurrence of minimum over requested axis.", "Return index label of the first occurrence of maximum of values.", "Notes", "This method is the Series version of ndarray.argmin. This method returns the label of the minimum, while ndarray.argmin returns the position. To get the position, use series.values.argmin().", "Examples", "If skipna is False and there is an NA value in the data, the function returns nan."]}, {"name": "pandas.Series.iloc", "path": "reference/api/pandas.series.iloc", "type": "Series", "text": ["Purely integer-location based indexing for selection by position.", ".iloc[] is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array.", "Allowed inputs are:", "An integer, e.g. 5.", "A list or array of integers, e.g. [4, 3, 0].", "A slice object with ints, e.g. 1:7.", "A boolean array.", "A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above). This is useful in method chains, when you don\u2019t have a reference to the calling object, but would like to base your selection on some value.", ".iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing (this conforms with python/numpy slice semantics).", "See more at Selection by Position.", "See also", "Fast integer location scalar accessor.", "Purely label-location based indexer for selection by label.", "Purely integer-location based indexing for selection by position.", "Examples", "Indexing just the rows", "With a scalar integer.", "With a list of integers.", "With a slice object.", "With a boolean mask the same length as the index.", "With a callable, useful in method chains. The x passed to the lambda is the DataFrame being sliced. This selects the rows whose index label even.", "Indexing both axes", "You can mix the indexer types for the index and columns. Use : to select the entire axis.", "With scalar integers.", "With lists of integers.", "With slice objects.", "With a boolean array whose length matches the columns.", "With a callable function that expects the Series or DataFrame."]}, {"name": "pandas.Series.index", "path": "reference/api/pandas.series.index", "type": "Series", "text": ["The index (axis labels) of the Series."]}, {"name": "pandas.Series.infer_objects", "path": "reference/api/pandas.series.infer_objects", "type": "Series", "text": ["Attempt to infer better dtypes for object columns.", "Attempts soft conversion of object-dtyped columns, leaving non-object and unconvertible columns unchanged. The inference rules are the same as during normal Series/DataFrame construction.", "See also", "Convert argument to datetime.", "Convert argument to timedelta.", "Convert argument to numeric type.", "Convert argument to best possible dtype.", "Examples"]}, {"name": "pandas.Series.info", "path": "reference/api/pandas.series.info", "type": "Series", "text": ["Print a concise summary of a Series.", "This method prints information about a Series including the index dtype, non-null values and memory usage.", "New in version 1.4.0.", "Series to print information about.", "Whether to print the full summary. By default, the setting in pandas.options.display.max_info_columns is followed.", "Where to send the output. By default, the output is printed to sys.stdout. Pass a writable buffer if you need to further process the output.", "Specifies whether total memory usage of the Series elements (including the index) should be displayed. By default, this follows the pandas.options.display.memory_usage setting.", "True always show memory usage. False never shows memory usage. A value of \u2018deep\u2019 is equivalent to \u201cTrue with deep introspection\u201d. Memory usage is shown in human-readable units (base-2 representation). Without deep introspection a memory estimation is made based in column dtype and number of rows assuming values consume the same memory amount for corresponding dtypes. With deep memory introspection, a real memory usage calculation is performed at the cost of computational resources.", "Whether to show the non-null counts. By default, this is shown only if the DataFrame is smaller than pandas.options.display.max_info_rows and pandas.options.display.max_info_columns. A value of True always shows the counts, and False never shows the counts.", "This method prints a summary of a Series and returns None.", "See also", "Generate descriptive statistics of Series.", "Memory usage of Series.", "Examples", "Prints a summary excluding information about its values:", "Pipe output of Series.info to buffer instead of sys.stdout, get buffer content and writes to a text file:", "The memory_usage parameter allows deep introspection mode, specially useful for big Series and fine-tune memory optimization:"]}, {"name": "pandas.Series.interpolate", "path": "reference/api/pandas.series.interpolate", "type": "Series", "text": ["Fill NaN values using an interpolation method.", "Please note that only method='linear' is supported for DataFrame/Series with a MultiIndex.", "Interpolation technique to use. One of:", "\u2018linear\u2019: Ignore the index and treat the values as equally spaced. This is the only method supported on MultiIndexes.", "\u2018time\u2019: Works on daily and higher resolution data to interpolate given length of interval.", "\u2018index\u2019, \u2018values\u2019: use the actual numerical values of the index.", "\u2018pad\u2019: Fill in NaNs using existing values.", "\u2018nearest\u2019, \u2018zero\u2019, \u2018slinear\u2019, \u2018quadratic\u2019, \u2018cubic\u2019, \u2018spline\u2019, \u2018barycentric\u2019, \u2018polynomial\u2019: Passed to scipy.interpolate.interp1d. These methods use the numerical values of the index. Both \u2018polynomial\u2019 and \u2018spline\u2019 require that you also specify an order (int), e.g. df.interpolate(method='polynomial', order=5).", "\u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019, \u2018akima\u2019, \u2018cubicspline\u2019: Wrappers around the SciPy interpolation methods of similar names. See Notes.", "\u2018from_derivatives\u2019: Refers to scipy.interpolate.BPoly.from_derivatives which replaces \u2018piecewise_polynomial\u2019 interpolation method in scipy 0.18.", "Axis to interpolate along.", "Maximum number of consecutive NaNs to fill. Must be greater than 0.", "Update the data in place if possible.", "Consecutive NaNs will be filled in this direction.", "If \u2018method\u2019 is \u2018pad\u2019 or \u2018ffill\u2019, \u2018limit_direction\u2019 must be \u2018forward\u2019.", "If \u2018method\u2019 is \u2018backfill\u2019 or \u2018bfill\u2019, \u2018limit_direction\u2019 must be \u2018backwards\u2019.", "If \u2018method\u2019 is \u2018backfill\u2019 or \u2018bfill\u2019, the default is \u2018backward\u2019", "else the default is \u2018forward\u2019", "Changed in version 1.1.0: raises ValueError if limit_direction is \u2018forward\u2019 or \u2018both\u2019 and method is \u2018backfill\u2019 or \u2018bfill\u2019. raises ValueError if limit_direction is \u2018backward\u2019 or \u2018both\u2019 and method is \u2018pad\u2019 or \u2018ffill\u2019.", "If limit is specified, consecutive NaNs will be filled with this restriction.", "None: No fill restriction.", "\u2018inside\u2019: Only fill NaNs surrounded by valid values (interpolate).", "\u2018outside\u2019: Only fill NaNs outside valid values (extrapolate).", "Downcast dtypes if possible.", "Keyword arguments to pass on to the interpolating function.", "Returns the same object type as the caller, interpolated at some or all NaN values or None if inplace=True.", "See also", "Fill missing values using different methods.", "Piecewise cubic polynomials (Akima interpolator).", "Piecewise polynomial in the Bernstein basis.", "Interpolate a 1-D function.", "Interpolate polynomial (Krogh interpolator).", "PCHIP 1-d monotonic cubic interpolation.", "Cubic spline data interpolator.", "Notes", "The \u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019 and \u2018akima\u2019 methods are wrappers around the respective SciPy implementations of similar names. These use the actual numerical values of the index. For more information on their behavior, see the SciPy documentation and SciPy tutorial.", "Examples", "Filling in NaN in a Series via linear interpolation.", "Filling in NaN in a Series by padding, but filling at most two consecutive NaN at a time.", "Filling in NaN in a Series via polynomial interpolation or splines: Both \u2018polynomial\u2019 and \u2018spline\u2019 methods require that you also specify an order (int).", "Fill the DataFrame forward (that is, going down) along each column using linear interpolation.", "Note how the last entry in column \u2018a\u2019 is interpolated differently, because there is no entry after it to use for interpolation. Note how the first entry in column \u2018b\u2019 remains NaN, because there is no entry before it to use for interpolation.", "Using polynomial interpolation."]}, {"name": "pandas.Series.is_monotonic", "path": "reference/api/pandas.series.is_monotonic", "type": "Series", "text": ["Return boolean if values in the object are monotonic_increasing."]}, {"name": "pandas.Series.is_monotonic_decreasing", "path": "reference/api/pandas.series.is_monotonic_decreasing", "type": "Series", "text": ["Return boolean if values in the object are monotonic_decreasing."]}, {"name": "pandas.Series.is_monotonic_increasing", "path": "reference/api/pandas.series.is_monotonic_increasing", "type": "Series", "text": ["Alias for is_monotonic."]}, {"name": "pandas.Series.is_unique", "path": "reference/api/pandas.series.is_unique", "type": "Series", "text": ["Return boolean if values in the object are unique."]}, {"name": "pandas.Series.isin", "path": "reference/api/pandas.series.isin", "type": "Series", "text": ["Whether elements in Series are contained in values.", "Return a boolean Series showing whether each element in the Series matches an element in the passed sequence of values exactly.", "The sequence of values to test. Passing in a single string will raise a TypeError. Instead, turn a single string into a list of one element.", "Series of booleans indicating if each element is in values.", "If values is a string", "See also", "Equivalent method on DataFrame.", "Examples", "To invert the boolean values, use the ~ operator:", "Passing a single string as s.isin('lama') will raise an error. Use a list of one element instead:", "Strings and integers are distinct and are therefore not comparable:"]}, {"name": "pandas.Series.isna", "path": "reference/api/pandas.series.isna", "type": "Series", "text": ["Detect missing values.", "Return a boolean same-sized object indicating if the values are NA. NA values, such as None or numpy.NaN, gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True).", "Mask of bool values for each element in Series that indicates whether an element is an NA value.", "See also", "Alias of isna.", "Boolean inverse of isna.", "Omit axes labels with missing values.", "Top-level isna.", "Examples", "Show which entries in a DataFrame are NA.", "Show which entries in a Series are NA."]}, {"name": "pandas.Series.isnull", "path": "reference/api/pandas.series.isnull", "type": "Series", "text": ["Series.isnull is an alias for Series.isna.", "Detect missing values.", "Return a boolean same-sized object indicating if the values are NA. NA values, such as None or numpy.NaN, gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True).", "Mask of bool values for each element in Series that indicates whether an element is an NA value.", "See also", "Alias of isna.", "Boolean inverse of isna.", "Omit axes labels with missing values.", "Top-level isna.", "Examples", "Show which entries in a DataFrame are NA.", "Show which entries in a Series are NA."]}, {"name": "pandas.Series.item", "path": "reference/api/pandas.series.item", "type": "Series", "text": ["Return the first element of the underlying data as a Python scalar.", "The first element of %(klass)s.", "If the data is not length-1."]}, {"name": "pandas.Series.items", "path": "reference/api/pandas.series.items", "type": "Series", "text": ["Lazily iterate over (index, value) tuples.", "This method returns an iterable tuple (index, value). This is convenient if you want to create a lazy iterator.", "Iterable of tuples containing the (index, value) pairs from a Series.", "See also", "Iterate over (column name, Series) pairs.", "Iterate over DataFrame rows as (index, Series) pairs.", "Examples"]}, {"name": "pandas.Series.iteritems", "path": "reference/api/pandas.series.iteritems", "type": "Series", "text": ["Lazily iterate over (index, value) tuples.", "This method returns an iterable tuple (index, value). This is convenient if you want to create a lazy iterator.", "Iterable of tuples containing the (index, value) pairs from a Series.", "See also", "Iterate over (column name, Series) pairs.", "Iterate over DataFrame rows as (index, Series) pairs.", "Examples"]}, {"name": "pandas.Series.keys", "path": "reference/api/pandas.series.keys", "type": "Series", "text": ["Return alias for index.", "Index of the Series."]}, {"name": "pandas.Series.kurt", "path": "reference/api/pandas.series.kurt", "type": "Series", "text": ["Return unbiased kurtosis over requested axis.", "Kurtosis obtained using Fisher\u2019s definition of kurtosis (kurtosis of normal == 0.0). Normalized by N-1.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Additional keyword arguments to be passed to the function."]}, {"name": "pandas.Series.kurtosis", "path": "reference/api/pandas.series.kurtosis", "type": "Series", "text": ["Return unbiased kurtosis over requested axis.", "Kurtosis obtained using Fisher\u2019s definition of kurtosis (kurtosis of normal == 0.0). Normalized by N-1.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Additional keyword arguments to be passed to the function."]}, {"name": "pandas.Series.last", "path": "reference/api/pandas.series.last", "type": "Series", "text": ["Select final periods of time series data based on a date offset.", "For a DataFrame with a sorted DatetimeIndex, this function selects the last few rows based on a date offset.", "The offset length of the data that will be selected. For instance, \u20183D\u2019 will display all the rows having their index within the last 3 days.", "A subset of the caller.", "If the index is not a DatetimeIndex", "See also", "Select initial periods of time series based on a date offset.", "Select values at a particular time of the day.", "Select values between particular times of the day.", "Examples", "Get the rows for the last 3 days:", "Notice the data for 3 last calendar days were returned, not the last 3 observed days in the dataset, and therefore data for 2018-04-11 was not returned."]}, {"name": "pandas.Series.last_valid_index", "path": "reference/api/pandas.series.last_valid_index", "type": "Series", "text": ["Return index for last non-NA value or None, if no NA value is found.", "Notes", "If all elements are non-NA/null, returns None. Also returns None for empty Series/DataFrame."]}, {"name": "pandas.Series.le", "path": "reference/api/pandas.series.le", "type": "Series", "text": ["Return Less than or equal to of series and other, element-wise (binary operator le).", "Equivalent to series <= other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "Examples"]}, {"name": "pandas.Series.loc", "path": "reference/api/pandas.series.loc", "type": "Series", "text": ["Access a group of rows and columns by label(s) or a boolean array.", ".loc[] is primarily label based, but may also be used with a boolean array.", "Allowed inputs are:", "A single label, e.g. 5 or 'a', (note that 5 is interpreted as a label of the index, and never as an integer position along the index).", "A list or array of labels, e.g. ['a', 'b', 'c'].", "A slice object with labels, e.g. 'a':'f'.", "Warning", "Note that contrary to usual python slices, both the start and the stop are included", "A boolean array of the same length as the axis being sliced, e.g. [True, False, True].", "An alignable boolean Series. The index of the key will be aligned before masking.", "An alignable Index. The Index of the returned selection will be the input.", "A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above)", "See more at Selection by Label.", "If any items are not found.", "If an indexed key is passed and its index is unalignable to the frame index.", "See also", "Access a single value for a row/column label pair.", "Access group of rows and columns by integer position(s).", "Returns a cross-section (row(s) or column(s)) from the Series/DataFrame.", "Access group of values using labels.", "Examples", "Getting values", "Single label. Note this returns the row as a Series.", "List of labels. Note using [[]] returns a DataFrame.", "Single label for row and column", "Slice with labels for row and single label for column. As mentioned above, note that both the start and stop of the slice are included.", "Boolean list with the same length as the row axis", "Alignable boolean Series:", "Index (same behavior as df.reindex)", "Conditional that returns a boolean Series", "Conditional that returns a boolean Series with column labels specified", "Callable that returns a boolean Series", "Setting values", "Set value for all items matching the list of labels", "Set value for an entire row", "Set value for an entire column", "Set value for rows matching callable condition", "Getting values on a DataFrame with an index that has integer labels", "Another example using integers for the index", "Slice with integer labels for rows. As mentioned above, note that both the start and stop of the slice are included.", "Getting values with a MultiIndex", "A number of examples using a DataFrame with a MultiIndex", "Single label. Note this returns a DataFrame with a single index.", "Single index tuple. Note this returns a Series.", "Single label for row and column. Similar to passing in a tuple, this returns a Series.", "Single tuple. Note using [[]] returns a DataFrame.", "Single tuple for the index with a single label for the column", "Slice from index tuple to single label", "Slice from index tuple to index tuple"]}, {"name": "pandas.Series.lt", "path": "reference/api/pandas.series.lt", "type": "Series", "text": ["Return Less than of series and other, element-wise (binary operator lt).", "Equivalent to series < other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "Examples"]}, {"name": "pandas.Series.mad", "path": "reference/api/pandas.series.mad", "type": "Series", "text": ["Return the mean absolute deviation of the values over the requested axis.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar."]}, {"name": "pandas.Series.map", "path": "reference/api/pandas.series.map", "type": "Series", "text": ["Map values of Series according to an input mapping or function.", "Used for substituting each value in a Series with another value, that may be derived from a function, a dict or a Series.", "Mapping correspondence.", "If \u2018ignore\u2019, propagate NaN values, without passing them to the mapping correspondence.", "Same index as caller.", "See also", "For applying more complex functions on a Series.", "Apply a function row-/column-wise.", "Apply a function elementwise on a whole DataFrame.", "Notes", "When arg is a dictionary, values in Series that are not in the dictionary (as keys) are converted to NaN. However, if the dictionary is a dict subclass that defines __missing__ (i.e. provides a method for default values), then this default is used rather than NaN.", "Examples", "map accepts a dict or a Series. Values that are not found in the dict are converted to NaN, unless the dict has a default value (e.g. defaultdict):", "It also accepts a function:", "To avoid applying the function to missing values (and keep them as NaN) na_action='ignore' can be used:"]}, {"name": "pandas.Series.mask", "path": "reference/api/pandas.series.mask", "type": "Series", "text": ["Replace values where the condition is True.", "Where cond is False, keep the original value. Where True, replace with corresponding value from other. If cond is callable, it is computed on the Series/DataFrame and should return boolean Series/DataFrame or array. The callable must not change input Series/DataFrame (though pandas doesn\u2019t check it).", "Entries where cond is True are replaced with corresponding value from other. If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn\u2019t check it).", "Whether to perform the operation in place on the data.", "Alignment axis if needed.", "Alignment level if needed.", "Note that currently this parameter won\u2019t affect the results and will always coerce to a suitable dtype.", "\u2018raise\u2019 : allow exceptions to be raised.", "\u2018ignore\u2019 : suppress exceptions. On error return original object.", "Try to cast the result back to the input type (if possible).", "Deprecated since version 1.3.0: Manually cast back if necessary.", "See also", "Return an object of same shape as self.", "Notes", "The mask method is an application of the if-then idiom. For each element in the calling DataFrame, if cond is False the element is used; otherwise the corresponding element from the DataFrame other is used.", "The signature for DataFrame.where() differs from numpy.where(). Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2).", "For further details and examples see the mask documentation in indexing.", "Examples"]}, {"name": "pandas.Series.max", "path": "reference/api/pandas.series.max", "type": "Series", "text": ["Return the maximum of the values over the requested axis.", "If you want the index of the maximum, use idxmax. This is the equivalent of the numpy.ndarray method argmax.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Additional keyword arguments to be passed to the function.", "See also", "Return the sum.", "Return the minimum.", "Return the maximum.", "Return the index of the minimum.", "Return the index of the maximum.", "Return the sum over the requested axis.", "Return the minimum over the requested axis.", "Return the maximum over the requested axis.", "Return the index of the minimum over the requested axis.", "Return the index of the maximum over the requested axis.", "Examples"]}, {"name": "pandas.Series.mean", "path": "reference/api/pandas.series.mean", "type": "Series", "text": ["Return the mean of the values over the requested axis.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Additional keyword arguments to be passed to the function."]}, {"name": "pandas.Series.median", "path": "reference/api/pandas.series.median", "type": "Series", "text": ["Return the median of the values over the requested axis.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Additional keyword arguments to be passed to the function."]}, {"name": "pandas.Series.memory_usage", "path": "reference/api/pandas.series.memory_usage", "type": "Series", "text": ["Return the memory usage of the Series.", "The memory usage can optionally include the contribution of the index and of elements of object dtype.", "Specifies whether to include the memory usage of the Series index.", "If True, introspect the data deeply by interrogating object dtypes for system-level memory consumption, and include it in the returned value.", "Bytes of memory consumed.", "See also", "Total bytes consumed by the elements of the array.", "Bytes consumed by a DataFrame.", "Examples", "Not including the index gives the size of the rest of the data, which is necessarily smaller:", "The memory footprint of object values is ignored by default:"]}, {"name": "pandas.Series.min", "path": "reference/api/pandas.series.min", "type": "Series", "text": ["Return the minimum of the values over the requested axis.", "If you want the index of the minimum, use idxmin. This is the equivalent of the numpy.ndarray method argmin.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Additional keyword arguments to be passed to the function.", "See also", "Return the sum.", "Return the minimum.", "Return the maximum.", "Return the index of the minimum.", "Return the index of the maximum.", "Return the sum over the requested axis.", "Return the minimum over the requested axis.", "Return the maximum over the requested axis.", "Return the index of the minimum over the requested axis.", "Return the index of the maximum over the requested axis.", "Examples"]}, {"name": "pandas.Series.mod", "path": "reference/api/pandas.series.mod", "type": "Series", "text": ["Return Modulo of series and other, element-wise (binary operator mod).", "Equivalent to series % other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Reverse of the Modulo operator, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.mode", "path": "reference/api/pandas.series.mode", "type": "Series", "text": ["Return the mode(s) of the Series.", "The mode is the value that appears most often. There can be multiple modes.", "Always returns Series even if only one value is returned.", "Don\u2019t consider counts of NaN/NaT.", "Modes of the Series in sorted order."]}, {"name": "pandas.Series.mul", "path": "reference/api/pandas.series.mul", "type": "Series", "text": ["Return Multiplication of series and other, element-wise (binary operator mul).", "Equivalent to series * other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Reverse of the Multiplication operator, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.multiply", "path": "reference/api/pandas.series.multiply", "type": "Series", "text": ["Return Multiplication of series and other, element-wise (binary operator mul).", "Equivalent to series * other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Reverse of the Multiplication operator, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.name", "path": "reference/api/pandas.series.name", "type": "Series", "text": ["Return the name of the Series.", "The name of a Series becomes its index or column name if it is used to form a DataFrame. It is also used whenever displaying the Series using the interpreter.", "The name of the Series, also the column name if part of a DataFrame.", "See also", "Sets the Series name when given a scalar input.", "Corresponding Index property.", "Examples", "The Series name can be set initially when calling the constructor.", "The name of a Series within a DataFrame is its column name."]}, {"name": "pandas.Series.nbytes", "path": "reference/api/pandas.series.nbytes", "type": "Series", "text": ["Return the number of bytes in the underlying data."]}, {"name": "pandas.Series.ndim", "path": "reference/api/pandas.series.ndim", "type": "Series", "text": ["Number of dimensions of the underlying data, by definition 1."]}, {"name": "pandas.Series.ne", "path": "reference/api/pandas.series.ne", "type": "Series", "text": ["Return Not equal to of series and other, element-wise (binary operator ne).", "Equivalent to series != other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "Examples"]}, {"name": "pandas.Series.nlargest", "path": "reference/api/pandas.series.nlargest", "type": "Series", "text": ["Return the largest n elements.", "Return this many descending sorted values.", "When there are duplicate values that cannot all fit in a Series of n elements:", "first : return the first n occurrences in order of appearance.", "last : return the last n occurrences in reverse order of appearance.", "all : keep all occurrences. This can result in a Series of size larger than n.", "The n largest values in the Series, sorted in decreasing order.", "See also", "Get the n smallest elements.", "Sort Series by values.", "Return the first n rows.", "Notes", "Faster than .sort_values(ascending=False).head(n) for small n relative to the size of the Series object.", "Examples", "The n largest elements where n=5 by default.", "The n largest elements where n=3. Default keep value is \u2018first\u2019 so Malta will be kept.", "The n largest elements where n=3 and keeping the last duplicates. Brunei will be kept since it is the last with value 434000 based on the index order.", "The n largest elements where n=3 with all duplicates kept. Note that the returned Series has five elements due to the three duplicates."]}, {"name": "pandas.Series.notna", "path": "reference/api/pandas.series.notna", "type": "Series", "text": ["Detect existing (non-missing) values.", "Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True). NA values, such as None or numpy.NaN, get mapped to False values.", "Mask of bool values for each element in Series that indicates whether an element is not an NA value.", "See also", "Alias of notna.", "Boolean inverse of notna.", "Omit axes labels with missing values.", "Top-level notna.", "Examples", "Show which entries in a DataFrame are not NA.", "Show which entries in a Series are not NA."]}, {"name": "pandas.Series.notnull", "path": "reference/api/pandas.series.notnull", "type": "Series", "text": ["Series.notnull is an alias for Series.notna.", "Detect existing (non-missing) values.", "Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True). NA values, such as None or numpy.NaN, get mapped to False values.", "Mask of bool values for each element in Series that indicates whether an element is not an NA value.", "See also", "Alias of notna.", "Boolean inverse of notna.", "Omit axes labels with missing values.", "Top-level notna.", "Examples", "Show which entries in a DataFrame are not NA.", "Show which entries in a Series are not NA."]}, {"name": "pandas.Series.nsmallest", "path": "reference/api/pandas.series.nsmallest", "type": "Series", "text": ["Return the smallest n elements.", "Return this many ascending sorted values.", "When there are duplicate values that cannot all fit in a Series of n elements:", "first : return the first n occurrences in order of appearance.", "last : return the last n occurrences in reverse order of appearance.", "all : keep all occurrences. This can result in a Series of size larger than n.", "The n smallest values in the Series, sorted in increasing order.", "See also", "Get the n largest elements.", "Sort Series by values.", "Return the first n rows.", "Notes", "Faster than .sort_values().head(n) for small n relative to the size of the Series object.", "Examples", "The n smallest elements where n=5 by default.", "The n smallest elements where n=3. Default keep value is \u2018first\u2019 so Nauru and Tuvalu will be kept.", "The n smallest elements where n=3 and keeping the last duplicates. Anguilla and Tuvalu will be kept since they are the last with value 11300 based on the index order.", "The n smallest elements where n=3 with all duplicates kept. Note that the returned Series has four elements due to the three duplicates."]}, {"name": "pandas.Series.nunique", "path": "reference/api/pandas.series.nunique", "type": "Series", "text": ["Return number of unique elements in the object.", "Excludes NA values by default.", "Don\u2019t include NaN in the count.", "See also", "Method nunique for DataFrame.", "Count non-NA/null observations in the Series.", "Examples"]}, {"name": "pandas.Series.pad", "path": "reference/api/pandas.series.pad", "type": "Series", "text": ["Synonym for DataFrame.fillna() with method='ffill'.", "Object with missing values filled or None if inplace=True."]}, {"name": "pandas.Series.pct_change", "path": "reference/api/pandas.series.pct_change", "type": "Series", "text": ["Percentage change between the current and a prior element.", "Computes the percentage change from the immediately previous row by default. This is useful in comparing the percentage of change in a time series of elements.", "Periods to shift for forming percent change.", "How to handle NAs before computing percent changes.", "The number of consecutive NAs to fill before stopping.", "Increment to use from time series API (e.g. \u2018M\u2019 or BDay()).", "Additional keyword arguments are passed into DataFrame.shift or Series.shift.", "The same type as the calling object.", "See also", "Compute the difference of two elements in a Series.", "Compute the difference of two elements in a DataFrame.", "Shift the index by some number of periods.", "Shift the index by some number of periods.", "Examples", "Series", "See the percentage change in a Series where filling NAs with last valid observation forward to next valid.", "DataFrame", "Percentage change in French franc, Deutsche Mark, and Italian lira from 1980-01-01 to 1980-03-01.", "Percentage of change in GOOG and APPL stock volume. Shows computing the percentage change between columns."]}, {"name": "pandas.Series.pipe", "path": "reference/api/pandas.series.pipe", "type": "Series", "text": ["Apply chainable functions that expect Series or DataFrames.", "Function to apply to the Series/DataFrame. args, and kwargs are passed into func. Alternatively a (callable, data_keyword) tuple where data_keyword is a string indicating the keyword of callable that expects the Series/DataFrame.", "Positional arguments passed into func.", "A dictionary of keyword arguments passed into func.", "See also", "Apply a function along input axis of DataFrame.", "Apply a function elementwise on a whole DataFrame.", "Apply a mapping correspondence on a Series.", "Notes", "Use .pipe when chaining together functions that expect Series, DataFrames or GroupBy objects. Instead of writing", "You can write", "If you have a function that takes the data as (say) the second argument, pass a tuple indicating which keyword expects the data. For example, suppose f takes its data as arg2:"]}, {"name": "pandas.Series.plot", "path": "reference/api/pandas.series.plot", "type": "Series", "text": ["Make plots of Series or DataFrame.", "Uses the backend specified by the option plotting.backend. By default, matplotlib is used.", "The object for which the method is called.", "Only used if data is a DataFrame.", "Allows plotting of one column versus another. Only used if data is a DataFrame.", "The kind of plot to produce:", "\u2018line\u2019 : line plot (default)", "\u2018bar\u2019 : vertical bar plot", "\u2018barh\u2019 : horizontal bar plot", "\u2018hist\u2019 : histogram", "\u2018box\u2019 : boxplot", "\u2018kde\u2019 : Kernel Density Estimation plot", "\u2018density\u2019 : same as \u2018kde\u2019", "\u2018area\u2019 : area plot", "\u2018pie\u2019 : pie plot", "\u2018scatter\u2019 : scatter plot (DataFrame only)", "\u2018hexbin\u2019 : hexbin plot (DataFrame only)", "An axes of the current figure.", "Make separate subplots for each column.", "In case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in; Be aware, that passing in both an ax and sharex=True will alter all x axis labels for all axis in a figure.", "In case subplots=True, share y axis and set some y axis labels to invisible.", "(rows, columns) for the layout of subplots.", "Size of a figure object.", "Use index as ticks for x axis.", "Title to use for the plot. If a string is passed, print the string at the top of the figure. If a list is passed and subplots is True, print each item in the list above the corresponding subplot.", "Axis grid lines.", "Place legend on axis subplots.", "The matplotlib line style per column.", "Use log scaling or symlog scaling on x axis. .. versionchanged:: 0.25.0", "Use log scaling or symlog scaling on y axis. .. versionchanged:: 0.25.0", "Use log scaling or symlog scaling on both x and y axes. .. versionchanged:: 0.25.0", "Values to use for the xticks.", "Values to use for the yticks.", "Set the x limits of the current axes.", "Set the y limits of the current axes.", "Name to use for the xlabel on x-axis. Default uses index name as xlabel, or the x-column name for planar plots.", "New in version 1.1.0.", "Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).", "Name to use for the ylabel on y-axis. Default will show no ylabel, or the y-column name for planar plots.", "New in version 1.1.0.", "Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).", "Rotation for ticks (xticks for vertical, yticks for horizontal plots).", "Font size for xticks and yticks.", "Colormap to select colors from. If string, load colormap with that name from matplotlib.", "If True, plot colorbar (only relevant for \u2018scatter\u2019 and \u2018hexbin\u2019 plots).", "Specify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center).", "If True, draw a table using the data in the DataFrame and the data will be transposed to meet matplotlib\u2019s default layout. If a Series or DataFrame is passed, use passed data to draw a table.", "See Plotting with Error Bars for detail.", "Equivalent to yerr.", "If True, create stacked plot.", "Sort column names to determine plot ordering.", "Whether to plot on the secondary y-axis if a list/tuple, which columns to plot on secondary y-axis.", "When using a secondary_y axis, automatically mark the column labels with \u201c(right)\u201d in the legend.", "If True, boolean values can be plotted.", "Backend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.", "New in version 1.0.0.", "Options to pass to matplotlib plotting method.", "If the backend is not the default matplotlib one, the return value will be the object returned by the backend.", "Notes", "See matplotlib documentation online for more on this subject", "If kind = \u2018bar\u2019 or \u2018barh\u2019, you can specify relative alignments for bar plot layout by position keyword. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center)"]}, {"name": "pandas.Series.plot.area", "path": "reference/api/pandas.series.plot.area", "type": "Series", "text": ["Draw a stacked area plot.", "An area plot displays quantitative data visually. This function wraps the matplotlib area function.", "Coordinates for the X axis. By default uses the index.", "Column to plot. By default uses all columns.", "Area plots are stacked by default. Set to False to create a unstacked plot.", "Additional keyword arguments are documented in DataFrame.plot().", "Area plot, or array of area plots if subplots is True.", "See also", "Make plots of DataFrame using matplotlib / pylab.", "Examples", "Draw an area plot based on basic business metrics:", "Area plots are stacked by default. To produce an unstacked plot, pass stacked=False:", "Draw an area plot for a single column:", "Draw with a different x:"]}, {"name": "pandas.Series.plot.bar", "path": "reference/api/pandas.series.plot.bar", "type": "Series", "text": ["Vertical bar plot.", "A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.", "Allows plotting of one column versus another. If not specified, the index of the DataFrame is used.", "Allows plotting of one column versus another. If not specified, all numerical columns are used.", "The color for each of the DataFrame\u2019s columns. Possible values are:", "for instance \u2018red\u2019 or \u2018#a98d19\u2019.", "code, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.", "colored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.", "New in version 1.1.0.", "Additional keyword arguments are documented in DataFrame.plot().", "An ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.", "See also", "Horizontal bar plot.", "Make plots of a DataFrame.", "Make a bar plot with matplotlib.", "Examples", "Basic plot.", "Plot a whole dataframe to a bar plot. Each column is assigned a distinct color, and each row is nested in a group along the horizontal axis.", "Plot stacked bar charts for the DataFrame", "Instead of nesting, the figure can be split by column with subplots=True. In this case, a numpy.ndarray of matplotlib.axes.Axes are returned.", "If you don\u2019t like the default colours, you can specify how you\u2019d like each column to be colored.", "Plot a single column.", "Plot only selected categories for the DataFrame."]}, {"name": "pandas.Series.plot.barh", "path": "reference/api/pandas.series.plot.barh", "type": "Series", "text": ["Make a horizontal bar plot.", "A horizontal bar plot is a plot that presents quantitative data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.", "Allows plotting of one column versus another. If not specified, the index of the DataFrame is used.", "Allows plotting of one column versus another. If not specified, all numerical columns are used.", "The color for each of the DataFrame\u2019s columns. Possible values are:", "for instance \u2018red\u2019 or \u2018#a98d19\u2019.", "code, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.", "colored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.", "New in version 1.1.0.", "Additional keyword arguments are documented in DataFrame.plot().", "An ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.", "See also", "Vertical bar plot.", "Make plots of DataFrame using matplotlib.", "Plot a vertical bar plot using matplotlib.", "Examples", "Basic example", "Plot a whole DataFrame to a horizontal bar plot", "Plot stacked barh charts for the DataFrame", "We can specify colors for each column", "Plot a column of the DataFrame to a horizontal bar plot", "Plot DataFrame versus the desired column"]}, {"name": "pandas.Series.plot.box", "path": "reference/api/pandas.series.plot.box", "type": "Series", "text": ["Make a box plot of the DataFrame columns.", "A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of box to show the range of the data. The position of the whiskers is set by default to 1.5*IQR (IQR = Q3 - Q1) from the edges of the box. Outlier points are those past the end of the whiskers.", "For further details see Wikipedia\u2019s entry for boxplot.", "A consideration when using this chart is that the box and the whiskers can overlap, which is very common when plotting small sets of data.", "Column in the DataFrame to group by.", "Changed in version 1.4.0: Previously, by is silently ignore and makes no groupings", "Additional keywords are documented in DataFrame.plot().", "See also", "Another method to draw a box plot.", "Draw a box plot from a Series object.", "Draw a box plot in matplotlib.", "Examples", "Draw a box plot from a DataFrame with four columns of randomly generated data.", "You can also generate groupings if you specify the by parameter (which can take a column name, or a list or tuple of column names):", "Changed in version 1.4.0."]}, {"name": "pandas.Series.plot.density", "path": "reference/api/pandas.series.plot.density", "type": "Series", "text": ["Generate Kernel Density Estimate plot using Gaussian kernels.", "In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function (PDF) of a random variable. This function uses Gaussian kernels and includes automatic bandwidth determination.", "The method used to calculate the estimator bandwidth. This can be \u2018scott\u2019, \u2018silverman\u2019, a scalar constant or a callable. If None (default), \u2018scott\u2019 is used. See scipy.stats.gaussian_kde for more information.", "Evaluation points for the estimated PDF. If None (default), 1000 equally spaced points are used. If ind is a NumPy array, the KDE is evaluated at the points passed. If ind is an integer, ind number of equally spaced points are used.", "Additional keyword arguments are documented in pandas.%(this-datatype)s.plot().", "See also", "Representation of a kernel-density estimate using Gaussian kernels. This is the function used internally to estimate the PDF.", "Examples", "Given a Series of points randomly sampled from an unknown distribution, estimate its PDF using KDE with automatic bandwidth determination and plot the results, evaluating them at 1000 equally spaced points (default):", "A scalar bandwidth can be specified. Using a small bandwidth value can lead to over-fitting, while using a large bandwidth value may result in under-fitting:", "Finally, the ind parameter determines the evaluation points for the plot of the estimated PDF:", "For DataFrame, it works in the same way:", "A scalar bandwidth can be specified. Using a small bandwidth value can lead to over-fitting, while using a large bandwidth value may result in under-fitting:", "Finally, the ind parameter determines the evaluation points for the plot of the estimated PDF:"]}, {"name": "pandas.Series.plot.hist", "path": "reference/api/pandas.series.plot.hist", "type": "Series", "text": ["Draw one histogram of the DataFrame\u2019s columns.", "A histogram is a representation of the distribution of data. This function groups the values of all given Series in the DataFrame into bins and draws all bins in one matplotlib.axes.Axes. This is useful when the DataFrame\u2019s Series are in a similar scale.", "Column in the DataFrame to group by.", "Changed in version 1.4.0: Previously, by is silently ignore and makes no groupings", "Number of histogram bins to be used.", "Additional keyword arguments are documented in DataFrame.plot().", "Return a histogram plot.", "See also", "Draw histograms per DataFrame\u2019s Series.", "Draw a histogram with Series\u2019 data.", "Examples", "When we roll a die 6000 times, we expect to get each value around 1000 times. But when we roll two dice and sum the result, the distribution is going to be quite different. A histogram illustrates those distributions.", "A grouped histogram can be generated by providing the parameter by (which can be a column name, or a list of column names):"]}, {"name": "pandas.Series.plot.kde", "path": "reference/api/pandas.series.plot.kde", "type": "Series", "text": ["Generate Kernel Density Estimate plot using Gaussian kernels.", "In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function (PDF) of a random variable. This function uses Gaussian kernels and includes automatic bandwidth determination.", "The method used to calculate the estimator bandwidth. This can be \u2018scott\u2019, \u2018silverman\u2019, a scalar constant or a callable. If None (default), \u2018scott\u2019 is used. See scipy.stats.gaussian_kde for more information.", "Evaluation points for the estimated PDF. If None (default), 1000 equally spaced points are used. If ind is a NumPy array, the KDE is evaluated at the points passed. If ind is an integer, ind number of equally spaced points are used.", "Additional keyword arguments are documented in pandas.%(this-datatype)s.plot().", "See also", "Representation of a kernel-density estimate using Gaussian kernels. This is the function used internally to estimate the PDF.", "Examples", "Given a Series of points randomly sampled from an unknown distribution, estimate its PDF using KDE with automatic bandwidth determination and plot the results, evaluating them at 1000 equally spaced points (default):", "A scalar bandwidth can be specified. Using a small bandwidth value can lead to over-fitting, while using a large bandwidth value may result in under-fitting:", "Finally, the ind parameter determines the evaluation points for the plot of the estimated PDF:", "For DataFrame, it works in the same way:", "A scalar bandwidth can be specified. Using a small bandwidth value can lead to over-fitting, while using a large bandwidth value may result in under-fitting:", "Finally, the ind parameter determines the evaluation points for the plot of the estimated PDF:"]}, {"name": "pandas.Series.plot.line", "path": "reference/api/pandas.series.plot.line", "type": "Series", "text": ["Plot Series or DataFrame as lines.", "This function is useful to plot lines using DataFrame\u2019s values as coordinates.", "Allows plotting of one column versus another. If not specified, the index of the DataFrame is used.", "Allows plotting of one column versus another. If not specified, all numerical columns are used.", "The color for each of the DataFrame\u2019s columns. Possible values are:", "for instance \u2018red\u2019 or \u2018#a98d19\u2019.", "code, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s line will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.", "colored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color lines for column a in green and lines for column b in red.", "New in version 1.1.0.", "Additional keyword arguments are documented in DataFrame.plot().", "An ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.", "See also", "Plot y versus x as lines and/or markers.", "Examples", "The following example shows the populations for some animals over the years.", "An example with subplots, so an array of axes is returned.", "Let\u2019s repeat the same example, but specifying colors for each column (in this case, for each animal).", "The following example shows the relationship between both populations."]}, {"name": "pandas.Series.plot.pie", "path": "reference/api/pandas.series.plot.pie", "type": "Series", "text": ["Generate a pie plot.", "A pie plot is a proportional representation of the numerical data in a column. This function wraps matplotlib.pyplot.pie() for the specified column. If no column reference is passed and subplots=True a pie plot is drawn for each numerical column independently.", "Label or position of the column to plot. If not provided, subplots=True argument must be passed.", "Keyword arguments to pass on to DataFrame.plot().", "A NumPy array is returned when subplots is True.", "See also", "Generate a pie plot for a Series.", "Make plots of a DataFrame.", "Examples", "In the example below we have a DataFrame with the information about planet\u2019s mass and radius. We pass the \u2018mass\u2019 column to the pie function to get a pie plot."]}, {"name": "pandas.Series.pop", "path": "reference/api/pandas.series.pop", "type": "Series", "text": ["Return item and drops from series. Raise KeyError if not found.", "Index of the element that needs to be removed.", "Examples"]}, {"name": "pandas.Series.pow", "path": "reference/api/pandas.series.pow", "type": "Series", "text": ["Return Exponential power of series and other, element-wise (binary operator pow).", "Equivalent to series ** other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Reverse of the Exponential power operator, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.prod", "path": "reference/api/pandas.series.prod", "type": "Series", "text": ["Return the product of the values over the requested axis.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Additional keyword arguments to be passed to the function.", "See also", "Return the sum.", "Return the minimum.", "Return the maximum.", "Return the index of the minimum.", "Return the index of the maximum.", "Return the sum over the requested axis.", "Return the minimum over the requested axis.", "Return the maximum over the requested axis.", "Return the index of the minimum over the requested axis.", "Return the index of the maximum over the requested axis.", "Examples", "By default, the product of an empty or all-NA Series is 1", "This can be controlled with the min_count parameter", "Thanks to the skipna parameter, min_count handles all-NA and empty series identically."]}, {"name": "pandas.Series.product", "path": "reference/api/pandas.series.product", "type": "Series", "text": ["Return the product of the values over the requested axis.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Additional keyword arguments to be passed to the function.", "See also", "Return the sum.", "Return the minimum.", "Return the maximum.", "Return the index of the minimum.", "Return the index of the maximum.", "Return the sum over the requested axis.", "Return the minimum over the requested axis.", "Return the maximum over the requested axis.", "Return the index of the minimum over the requested axis.", "Return the index of the maximum over the requested axis.", "Examples", "By default, the product of an empty or all-NA Series is 1", "This can be controlled with the min_count parameter", "Thanks to the skipna parameter, min_count handles all-NA and empty series identically."]}, {"name": "pandas.Series.quantile", "path": "reference/api/pandas.series.quantile", "type": "Series", "text": ["Return value at the given quantile.", "The quantile(s) to compute, which can lie in range: 0 <= q <= 1.", "This optional parameter specifies the interpolation method to use, when the desired quantile lies between two data points i and j:", "linear: i + (j - i) * fraction, where fraction is the fractional part of the index surrounded by i and j.", "lower: i.", "higher: j.", "nearest: i or j whichever is nearest.", "midpoint: (i + j) / 2.", "If q is an array, a Series will be returned where the index is q and the values are the quantiles, otherwise a float will be returned.", "See also", "Calculate the rolling quantile.", "Returns the q-th percentile(s) of the array elements.", "Examples"]}, {"name": "pandas.Series.radd", "path": "reference/api/pandas.series.radd", "type": "Series", "text": ["Return Addition of series and other, element-wise (binary operator radd).", "Equivalent to other + series, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Element-wise Addition, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.rank", "path": "reference/api/pandas.series.rank", "type": "Series", "text": ["Compute numerical data ranks (1 through n) along axis.", "By default, equal values are assigned a rank that is the average of the ranks of those values.", "Index to direct ranking.", "How to rank the group of records that have the same value (i.e. ties):", "average: average rank of the group", "min: lowest rank in the group", "max: highest rank in the group", "first: ranks assigned in order they appear in the array", "dense: like \u2018min\u2019, but rank always increases by 1 between groups.", "For DataFrame objects, rank only numeric columns if set to True.", "How to rank NaN values:", "keep: assign NaN rank to NaN values", "top: assign lowest rank to NaN values", "bottom: assign highest rank to NaN values", "Whether or not the elements should be ranked in ascending order.", "Whether or not to display the returned rankings in percentile form.", "Return a Series or DataFrame with data ranks as values.", "See also", "Rank of values within each group.", "Examples", "The following example shows how the method behaves with the above parameters:", "default_rank: this is the default behaviour obtained without using any parameter.", "max_rank: setting method = 'max' the records that have the same values are ranked using the highest rank (e.g.: since \u2018cat\u2019 and \u2018dog\u2019 are both in the 2nd and 3rd position, rank 3 is assigned.)", "NA_bottom: choosing na_option = 'bottom', if there are records with NaN values they are placed at the bottom of the ranking.", "pct_rank: when setting pct = True, the ranking is expressed as percentile rank."]}, {"name": "pandas.Series.ravel", "path": "reference/api/pandas.series.ravel", "type": "Series", "text": ["Return the flattened underlying data as an ndarray.", "Flattened data of the Series.", "See also", "Return a flattened array."]}, {"name": "pandas.Series.rdiv", "path": "reference/api/pandas.series.rdiv", "type": "Series", "text": ["Return Floating division of series and other, element-wise (binary operator rtruediv).", "Equivalent to other / series, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Element-wise Floating division, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.rdivmod", "path": "reference/api/pandas.series.rdivmod", "type": "Series", "text": ["Return Integer division and modulo of series and other, element-wise (binary operator rdivmod).", "Equivalent to other divmod series, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Element-wise Integer division and modulo, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.reindex", "path": "reference/api/pandas.series.reindex", "type": "Series", "text": ["Conform Series to new index with optional filling logic.", "Places NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.", "New labels / index to conform to, should be specified using keywords. Preferably an Index object to avoid duplicating data.", "Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index.", "None (default): don\u2019t fill gaps", "pad / ffill: Propagate last valid observation forward to next valid.", "backfill / bfill: Use next valid observation to fill gap.", "nearest: Use nearest valid observations to fill gap.", "Return a new object, even if the passed indexes are the same.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "Value to use for missing values. Defaults to NaN, but can be any \u201ccompatible\u201d value.", "Maximum number of consecutive elements to forward or backward fill.", "Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations most satisfy the equation abs(index[indexer] - target) <= tolerance.", "Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index\u2019s type.", "See also", "Set row labels.", "Remove row labels or move them to new columns.", "Change to same indices as other DataFrame.", "Examples", "DataFrame.reindex supports two calling conventions", "(index=index_labels, columns=column_labels, ...)", "(labels, axis={'index', 'columns'}, ...)", "We highly recommend using keyword arguments to clarify your intent.", "Create a dataframe with some fictional data.", "Create a new index and reindex the dataframe. By default values in the new index that do not have corresponding records in the dataframe are assigned NaN.", "We can fill in the missing values by passing a value to the keyword fill_value. Because the index is not monotonically increasing or decreasing, we cannot use arguments to the keyword method to fill the NaN values.", "We can also reindex the columns.", "Or we can use \u201caxis-style\u201d keyword arguments", "To further illustrate the filling functionality in reindex, we will create a dataframe with a monotonically increasing index (for example, a sequence of dates).", "Suppose we decide to expand the dataframe to cover a wider date range.", "The index entries that did not have a value in the original data frame (for example, \u20182009-12-29\u2019) are by default filled with NaN. If desired, we can fill in the missing values using one of several options.", "For example, to back-propagate the last valid value to fill the NaN values, pass bfill as an argument to the method keyword.", "Please note that the NaN value present in the original dataframe (at index value 2010-01-03) will not be filled by any of the value propagation schemes. This is because filling while reindexing does not look at dataframe values, but only compares the original and desired indexes. If you do want to fill in the NaN values present in the original dataframe, use the fillna() method.", "See the user guide for more."]}, {"name": "pandas.Series.reindex_like", "path": "reference/api/pandas.series.reindex_like", "type": "Series", "text": ["Return an object with matching indices as other object.", "Conform the object to the same index on all axes. Optional filling logic, placing NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.", "Its row and column indices are used to define the new indices of this object.", "Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index.", "None (default): don\u2019t fill gaps", "pad / ffill: propagate last valid observation forward to next valid", "backfill / bfill: use next valid observation to fill gap", "nearest: use nearest valid observations to fill gap.", "Return a new object, even if the passed indexes are the same.", "Maximum number of consecutive labels to fill for inexact matches.", "Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance.", "Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index\u2019s type.", "Same type as caller, but with changed indices on each axis.", "See also", "Set row labels.", "Remove row labels or move them to new columns.", "Change to new indices or expand indices.", "Notes", "Same as calling .reindex(index=other.index, columns=other.columns,...).", "Examples"]}, {"name": "pandas.Series.rename", "path": "reference/api/pandas.series.rename", "type": "Series", "text": ["Alter Series index labels or name.", "Function / dict values must be unique (1-to-1). Labels not contained in a dict / Series will be left as-is. Extra labels listed don\u2019t throw an error.", "Alternatively, change Series.name with a scalar value.", "See the user guide for more.", "Unused. Accepted for compatibility with DataFrame method only.", "Functions or dict-like are transformations to apply to the index. Scalar or hashable sequence-like will alter the Series.name attribute.", "Additional keyword arguments passed to the function. Only the \u201cinplace\u201d keyword is used.", "Series with index labels or name altered or None if inplace=True.", "See also", "Corresponding DataFrame method.", "Set the name of the axis.", "Examples"]}, {"name": "pandas.Series.rename_axis", "path": "reference/api/pandas.series.rename_axis", "type": "Series", "text": ["Set the name of the axis for the index or columns.", "Value to set the axis name attribute.", "A scalar, list-like, dict-like or functions transformations to apply to that axis\u2019 values. Note that the columns parameter is not allowed if the object is a Series. This parameter only apply for DataFrame type objects.", "Use either mapper and axis to specify the axis to target with mapper, or index and/or columns.", "The axis to rename.", "Also copy underlying data.", "Modifies the object directly, instead of creating a new Series or DataFrame.", "The same type as the caller or None if inplace=True.", "See also", "Alter Series index labels or name.", "Alter DataFrame index labels or name.", "Set new names on index.", "Notes", "DataFrame.rename_axis supports two calling conventions", "(index=index_mapper, columns=columns_mapper, ...)", "(mapper, axis={'index', 'columns'}, ...)", "The first calling convention will only modify the names of the index and/or the names of the Index object that is the columns. In this case, the parameter copy is ignored.", "The second calling convention will modify the names of the corresponding index if mapper is a list or a scalar. However, if mapper is dict-like or a function, it will use the deprecated behavior of modifying the axis labels.", "We highly recommend using keyword arguments to clarify your intent.", "Examples", "Series", "DataFrame", "MultiIndex"]}, {"name": "pandas.Series.reorder_levels", "path": "reference/api/pandas.series.reorder_levels", "type": "Series", "text": ["Rearrange index levels using input order.", "May not drop or duplicate levels.", "Reference level by number or key."]}, {"name": "pandas.Series.repeat", "path": "reference/api/pandas.series.repeat", "type": "Series", "text": ["Repeat elements of a Series.", "Returns a new Series where each element of the current Series is repeated consecutively a given number of times.", "The number of repetitions for each element. This should be a non-negative integer. Repeating 0 times will return an empty Series.", "Must be None. Has no effect but is accepted for compatibility with numpy.", "Newly created Series with repeated elements.", "See also", "Equivalent function for Index.", "Similar method for numpy.ndarray.", "Examples"]}, {"name": "pandas.Series.replace", "path": "reference/api/pandas.series.replace", "type": "Series", "text": ["Replace values given in to_replace with value.", "Values of the Series are replaced with other values dynamically.", "This differs from updating with .loc or .iloc, which require you to specify a location to update with some value.", "How to find the values that will be replaced.", "numeric, str or regex:", "numeric: numeric values equal to to_replace will be replaced with value", "str: string exactly matching to_replace will be replaced with value", "regex: regexs matching to_replace will be replaced with value", "list of str, regex, or numeric:", "First, if to_replace and value are both lists, they must be the same length.", "Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. This doesn\u2019t matter much for value since there are only a few possible substitution regexes you can use.", "str, regex and numeric rules apply as above.", "dict:", "Dicts can be used to specify different replacement values for different existing values. For example, {'a': 'b', 'y': 'z'} replaces the value \u2018a\u2019 with \u2018b\u2019 and \u2018y\u2019 with \u2018z\u2019. To use a dict in this way the value parameter should be None.", "For a DataFrame a dict can specify that different values should be replaced in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1 in column \u2018a\u2019 and the value \u2018z\u2019 in column \u2018b\u2019 and replaces these values with whatever is specified in value. The value parameter should not be None in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in.", "For a DataFrame nested dictionaries, e.g., {'a': {'b': np.nan}}, are read as follows: look in column \u2018a\u2019 for the value \u2018b\u2019 and replace it with NaN. The value parameter should be None to use a nested dict in this way. You can nest regular expressions as well. Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions.", "None:", "This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series.", "See the examples section for examples of each of these.", "Value to replace any values matching to_replace with. For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed.", "If True, performs operation inplace and returns None.", "Maximum size gap to forward or backward fill.", "Whether to interpret to_replace and/or value as regular expressions. If this is True then to_replace must be a string. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None.", "The method to use when for replacement, when to_replace is a scalar, list or tuple and value is None.", "Changed in version 0.23.0: Added to DataFrame.", "Object after replacement.", "If regex is not a bool and to_replace is not None.", "If to_replace is not a scalar, array-like, dict, or None", "If to_replace is a dict and value is not a list, dict, ndarray, or Series", "If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series.", "When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced", "If a list or an ndarray is passed to to_replace and value but they are not the same length.", "See also", "Fill NA values.", "Replace values based on boolean condition.", "Simple string replacement.", "Notes", "Regex substitution is performed under the hood with re.sub. The rules for substitution for re.sub are the same.", "Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this.", "This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works.", "When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter.", "Examples", "Scalar `to_replace` and `value`", "List-like `to_replace`", "dict-like `to_replace`", "Regular expression `to_replace`", "Compare the behavior of s.replace({'a': None}) and s.replace('a', None) to understand the peculiarities of the to_replace parameter:", "When one uses a dict as the to_replace value, it is like the value(s) in the dict are equal to the value parameter. s.replace({'a': None}) is equivalent to s.replace(to_replace={'a': None}, value=None, method=None):", "When value is not explicitly passed and to_replace is a scalar, list or tuple, replace uses the method parameter (default \u2018pad\u2019) to do the replacement. So this is why the \u2018a\u2019 values are being replaced by 10 in rows 1 and 2 and \u2018b\u2019 in row 4 in this case.", "On the other hand, if None is explicitly passed for value, it will be respected:", "Changed in version 1.4.0: Previously the explicit None was silently ignored."]}, {"name": "pandas.Series.resample", "path": "reference/api/pandas.series.resample", "type": "Series", "text": ["Resample time-series data.", "Convenience method for frequency conversion and resampling of time series. The object must have a datetime-like index (DatetimeIndex, PeriodIndex, or TimedeltaIndex), or the caller must pass the label of a datetime-like series/index to the on/level keyword parameter.", "The offset string or object representing target conversion.", "Which axis to use for up- or down-sampling. For Series this will default to 0, i.e. along the rows. Must be DatetimeIndex, TimedeltaIndex or PeriodIndex.", "Which side of bin interval is closed. The default is \u2018left\u2019 for all frequency offsets except for \u2018M\u2019, \u2018A\u2019, \u2018Q\u2019, \u2018BM\u2019, \u2018BA\u2019, \u2018BQ\u2019, and \u2018W\u2019 which all have a default of \u2018right\u2019.", "Which bin edge label to label bucket with. The default is \u2018left\u2019 for all frequency offsets except for \u2018M\u2019, \u2018A\u2019, \u2018Q\u2019, \u2018BM\u2019, \u2018BA\u2019, \u2018BQ\u2019, and \u2018W\u2019 which all have a default of \u2018right\u2019.", "For PeriodIndex only, controls whether to use the start or end of rule.", "Pass \u2018timestamp\u2019 to convert the resulting index to a DateTimeIndex or \u2018period\u2019 to convert it to a PeriodIndex. By default the input representation is retained.", "Adjust the resampled time labels.", "Deprecated since version 1.1.0: You should add the loffset to the df.index after the resample. See below.", "For frequencies that evenly subdivide 1 day, the \u201corigin\u201d of the aggregated intervals. For example, for \u20185min\u2019 frequency, base could range from 0 through 4. Defaults to 0.", "Deprecated since version 1.1.0: The new arguments that you should use are \u2018offset\u2019 or \u2018origin\u2019.", "For a DataFrame, column to use instead of index for resampling. Column must be datetime-like.", "For a MultiIndex, level (name or number) to use for resampling. level must be datetime-like.", "The timestamp on which to adjust the grouping. The timezone of origin must match the timezone of the index. If string, must be one of the following:", "\u2018epoch\u2019: origin is 1970-01-01", "\u2018start\u2019: origin is the first value of the timeseries", "\u2018start_day\u2019: origin is the first day at midnight of the timeseries", "New in version 1.1.0.", "\u2018end\u2019: origin is the last value of the timeseries", "\u2018end_day\u2019: origin is the ceiling midnight of the last day", "New in version 1.3.0.", "An offset timedelta added to the origin.", "New in version 1.1.0.", "Resampler object.", "See also", "Resample a Series.", "Resample a DataFrame.", "Group Series by mapping, function, label, or list of labels.", "Reindex a Series with the given frequency without grouping.", "Notes", "See the user guide for more.", "To learn more about the offset strings, please see this link.", "Examples", "Start by creating a series with 9 one minute timestamps.", "Downsample the series into 3 minute bins and sum the values of the timestamps falling into a bin.", "Downsample the series into 3 minute bins as above, but label each bin using the right edge instead of the left. Please note that the value in the bucket used as the label is not included in the bucket, which it labels. For example, in the original series the bucket 2000-01-01 00:03:00 contains the value 3, but the summed value in the resampled bucket with the label 2000-01-01 00:03:00 does not include 3 (if it did, the summed value would be 6, not 3). To include this value close the right side of the bin interval as illustrated in the example below this one.", "Downsample the series into 3 minute bins as above, but close the right side of the bin interval.", "Upsample the series into 30 second bins.", "Upsample the series into 30 second bins and fill the NaN values using the pad method.", "Upsample the series into 30 second bins and fill the NaN values using the bfill method.", "Pass a custom function via apply", "For a Series with a PeriodIndex, the keyword convention can be used to control whether to use the start or end of rule.", "Resample a year by quarter using \u2018start\u2019 convention. Values are assigned to the first quarter of the period.", "Resample quarters by month using \u2018end\u2019 convention. Values are assigned to the last month of the period.", "For DataFrame objects, the keyword on can be used to specify the column instead of the index for resampling.", "For a DataFrame with MultiIndex, the keyword level can be used to specify on which level the resampling needs to take place.", "If you want to adjust the start of the bins based on a fixed timestamp:", "If you want to adjust the start of the bins with an offset Timedelta, the two following lines are equivalent:", "If you want to take the largest Timestamp as the end of the bins:", "In contrast with the start_day, you can use end_day to take the ceiling midnight of the largest Timestamp as the end of the bins and drop the bins not containing data:", "To replace the use of the deprecated base argument, you can now use offset, in this example it is equivalent to have base=2:", "To replace the use of the deprecated loffset argument:"]}, {"name": "pandas.Series.reset_index", "path": "reference/api/pandas.series.reset_index", "type": "Series", "text": ["Generate a new DataFrame or Series with the index reset.", "This is useful when the index needs to be treated as a column, or when the index is meaningless and needs to be reset to the default before another operation.", "For a Series with a MultiIndex, only remove the specified levels from the index. Removes all levels by default.", "Just reset the index, without inserting it as a column in the new DataFrame.", "The name to use for the column containing the original Series values. Uses self.name by default. This argument is ignored when drop is True.", "Modify the Series in place (do not create a new object).", "When drop is False (the default), a DataFrame is returned. The newly created columns will come first in the DataFrame, followed by the original Series values. When drop is True, a Series is returned. In either case, if inplace=True, no value is returned.", "See also", "Analogous function for DataFrame.", "Examples", "Generate a DataFrame with default index.", "To specify the name of the new column use name.", "To generate a new Series with the default set drop to True.", "To update the Series in place, without generating a new one set inplace to True. Note that it also requires drop=True.", "The level parameter is interesting for Series with a multi-level index.", "To remove a specific level from the Index, use level.", "If level is not set, all levels are removed from the Index."]}, {"name": "pandas.Series.rfloordiv", "path": "reference/api/pandas.series.rfloordiv", "type": "Series", "text": ["Return Integer division of series and other, element-wise (binary operator rfloordiv).", "Equivalent to other // series, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Element-wise Integer division, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.rmod", "path": "reference/api/pandas.series.rmod", "type": "Series", "text": ["Return Modulo of series and other, element-wise (binary operator rmod).", "Equivalent to other % series, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Element-wise Modulo, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.rmul", "path": "reference/api/pandas.series.rmul", "type": "Series", "text": ["Return Multiplication of series and other, element-wise (binary operator rmul).", "Equivalent to other * series, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Element-wise Multiplication, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.rolling", "path": "reference/api/pandas.series.rolling", "type": "Series", "text": ["Provide rolling window calculations.", "Size of the moving window.", "If an integer, the fixed number of observations used for each window.", "If an offset, the time period of each window. Each window will be a variable sized based on the observations included in the time-period. This is only valid for datetimelike indexes. To learn more about the offsets & frequency strings, please see this link.", "If a BaseIndexer subclass, the window boundaries based on the defined get_window_bounds method. Additional rolling keyword arguments, namely min_periods, center, and closed will be passed to get_window_bounds.", "Minimum number of observations in window required to have a value; otherwise, result is np.nan.", "For a window that is specified by an offset, min_periods will default to 1.", "For a window that is specified by an integer, min_periods will default to the size of the window.", "If False, set the window labels as the right edge of the window index.", "If True, set the window labels as the center of the window index.", "If None, all points are evenly weighted.", "If a string, it must be a valid scipy.signal window function.", "Certain Scipy window types require additional parameters to be passed in the aggregation function. The additional parameters must match the keywords specified in the Scipy window type method signature.", "For a DataFrame, a column label or Index level on which to calculate the rolling window, rather than the DataFrame\u2019s index.", "Provided integer column is ignored and excluded from result since an integer index is not used to calculate the rolling window.", "If 0 or 'index', roll across the rows.", "If 1 or 'columns', roll across the columns.", "If 'right', the first point in the window is excluded from calculations.", "If 'left', the last point in the window is excluded from calculations.", "If 'both', the no points in the window are excluded from calculations.", "If 'neither', the first and last points in the window are excluded from calculations.", "Default None ('right').", "Changed in version 1.2.0: The closed parameter with fixed windows is now supported.", "New in version 1.3.0.", "Execute the rolling operation per single column or row ('single') or over the entire object ('table').", "This argument is only implemented when specifying engine='numba' in the method call.", "See also", "Provides expanding transformations.", "Provides exponential weighted functions.", "Notes", "See Windowing Operations for further usage details and examples.", "Examples", "window", "Rolling sum with a window length of 2 observations.", "Rolling sum with a window span of 2 seconds.", "Rolling sum with forward looking windows with 2 observations.", "min_periods", "Rolling sum with a window length of 2 observations, but only needs a minimum of 1 observation to calculate a value.", "center", "Rolling sum with the result assigned to the center of the window index.", "win_type", "Rolling sum with a window length of 2, using the Scipy 'gaussian' window type. std is required in the aggregation function."]}, {"name": "pandas.Series.round", "path": "reference/api/pandas.series.round", "type": "Series", "text": ["Round each value in a Series to the given number of decimals.", "Number of decimal places to round to. If decimals is negative, it specifies the number of positions to the left of the decimal point.", "Additional arguments and keywords have no effect but might be accepted for compatibility with NumPy.", "Rounded values of the Series.", "See also", "Round values of an np.array.", "Round values of a DataFrame.", "Examples"]}, {"name": "pandas.Series.rpow", "path": "reference/api/pandas.series.rpow", "type": "Series", "text": ["Return Exponential power of series and other, element-wise (binary operator rpow).", "Equivalent to other ** series, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Element-wise Exponential power, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.rsub", "path": "reference/api/pandas.series.rsub", "type": "Series", "text": ["Return Subtraction of series and other, element-wise (binary operator rsub).", "Equivalent to other - series, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Element-wise Subtraction, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.rtruediv", "path": "reference/api/pandas.series.rtruediv", "type": "Series", "text": ["Return Floating division of series and other, element-wise (binary operator rtruediv).", "Equivalent to other / series, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Element-wise Floating division, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.sample", "path": "reference/api/pandas.series.sample", "type": "Series", "text": ["Return a random sample of items from an axis of object.", "You can use random_state for reproducibility.", "Number of items from axis to return. Cannot be used with frac. Default = 1 if frac = None.", "Fraction of axis items to return. Cannot be used with n.", "Allow or disallow sampling of the same row more than once.", "Default \u2018None\u2019 results in equal probability weighting. If passed a Series, will align with target object on index. Index values in weights not found in sampled object will be ignored and index values in sampled object not in weights will be assigned weights of zero. If called on a DataFrame, will accept the name of a column when axis = 0. Unless weights are a Series, weights must be same length as axis being sampled. If weights do not sum to 1, they will be normalized to sum to 1. Missing values in the weights column will be treated as zero. Infinite values not allowed.", "If int, array-like, or BitGenerator, seed for random number generator. If np.random.RandomState or np.random.Generator, use as given.", "Changed in version 1.1.0: array-like and BitGenerator object now passed to np.random.RandomState() as seed", "Changed in version 1.4.0: np.random.Generator objects now accepted", "Axis to sample. Accepts axis number or name. Default is stat axis for given data type (0 for Series and DataFrames).", "If True, the resulting index will be labeled 0, 1, \u2026, n - 1.", "New in version 1.3.0.", "A new object of same type as caller containing n items randomly sampled from the caller object.", "See also", "Generates random samples from each group of a DataFrame object.", "Generates random samples from each group of a Series object.", "Generates a random sample from a given 1-D numpy array.", "Notes", "If frac > 1, replacement should be set to True.", "Examples", "Extract 3 random elements from the Series df['num_legs']: Note that we use random_state to ensure the reproducibility of the examples.", "A random 50% sample of the DataFrame with replacement:", "An upsample sample of the DataFrame with replacement: Note that replace parameter has to be True for frac parameter > 1.", "Using a DataFrame column as weights. Rows with larger value in the num_specimen_seen column are more likely to be sampled."]}, {"name": "pandas.Series.searchsorted", "path": "reference/api/pandas.series.searchsorted", "type": "Series", "text": ["Find indices where elements should be inserted to maintain order.", "Find the indices into a sorted Series self such that, if the corresponding elements in value were inserted before the indices, the order of self would be preserved.", "Note", "The Series must be monotonically sorted, otherwise wrong locations will likely be returned. Pandas does not check this for you.", "Values to insert into self.", "If \u2018left\u2019, the index of the first suitable location found is given. If \u2018right\u2019, return the last such index. If there is no suitable index, return either 0 or N (where N is the length of self).", "Optional array of integer indices that sort self into ascending order. They are typically the result of np.argsort.", "A scalar or array of insertion points with the same shape as value.", "See also", "Sort by the values along either axis.", "Similar method from NumPy.", "Notes", "Binary search is used to find the required insertion points.", "Examples", "If the values are not monotonically sorted, wrong locations may be returned:"]}, {"name": "pandas.Series.sem", "path": "reference/api/pandas.series.sem", "type": "Series", "text": ["Return unbiased standard error of the mean over requested axis.", "Normalized by N-1 by default. This can be changed using the ddof argument", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar.", "Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series."]}, {"name": "pandas.Series.set_axis", "path": "reference/api/pandas.series.set_axis", "type": "Series", "text": ["Assign desired index to given axis.", "Indexes for row labels can be changed by assigning a list-like or Index.", "The values for the new index.", "The axis to update. The value 0 identifies the rows.", "Whether to return a new Series instance.", "An object of type Series or None if inplace=True.", "See also", "Alter the name of the index.", "Examples"]}, {"name": "pandas.Series.set_flags", "path": "reference/api/pandas.series.set_flags", "type": "Series", "text": ["Return a new object with updated flags.", "Whether the returned object allows duplicate labels.", "The same type as the caller.", "See also", "Global metadata applying to this dataset.", "Global flags applying to this object.", "Notes", "This method returns a new object that\u2019s a view on the same data as the input. Mutating the input or the output values will be reflected in the other.", "This method is intended to be used in method chains.", "\u201cFlags\u201d differ from \u201cmetadata\u201d. Flags reflect properties of the pandas object (the Series or DataFrame). Metadata refer to properties of the dataset, and should be stored in DataFrame.attrs.", "Examples"]}, {"name": "pandas.Series.shape", "path": "reference/api/pandas.series.shape", "type": "Series", "text": ["Return a tuple of the shape of the underlying data."]}, {"name": "pandas.Series.shift", "path": "reference/api/pandas.series.shift", "type": "Series", "text": ["Shift index by desired number of periods with an optional time freq.", "When freq is not passed, shift the index without realigning the data. If freq is passed (in this case, the index must be date or datetime, or it will raise a NotImplementedError), the index will be increased using the periods and the freq. freq can be inferred when specified as \u201cinfer\u201d as long as either freq or inferred_freq attribute is set in the index.", "Number of periods to shift. Can be positive or negative.", "Offset to use from the tseries module or time rule (e.g. \u2018EOM\u2019). If freq is specified then the index values are shifted but the data is not realigned. That is, use freq if you would like to extend the index when shifting and preserve the original data. If freq is specified as \u201cinfer\u201d then it will be inferred from the freq or inferred_freq attributes of the index. If neither of those attributes exist, a ValueError is thrown.", "Shift direction.", "The scalar value to use for newly introduced missing values. the default depends on the dtype of self. For numeric data, np.nan is used. For datetime, timedelta, or period data, etc. NaT is used. For extension dtypes, self.dtype.na_value is used.", "Changed in version 1.1.0.", "Copy of input object, shifted.", "See also", "Shift values of Index.", "Shift values of DatetimeIndex.", "Shift values of PeriodIndex.", "Shift the time index, using the index\u2019s frequency if available.", "Examples"]}, {"name": "pandas.Series.size", "path": "reference/api/pandas.series.size", "type": "Series", "text": ["Return the number of elements in the underlying data."]}, {"name": "pandas.Series.skew", "path": "reference/api/pandas.series.skew", "type": "Series", "text": ["Return unbiased skew over requested axis.", "Normalized by N-1.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Additional keyword arguments to be passed to the function."]}, {"name": "pandas.Series.slice_shift", "path": "reference/api/pandas.series.slice_shift", "type": "Series", "text": ["Equivalent to shift without copying data. The shifted data will not include the dropped periods and the shifted axis will be smaller than the original.", "Deprecated since version 1.2.0: slice_shift is deprecated, use DataFrame/Series.shift instead.", "Number of periods to move, can be positive or negative.", "Notes", "While the slice_shift is faster than shift, you may pay for it later during alignment."]}, {"name": "pandas.Series.sort_index", "path": "reference/api/pandas.series.sort_index", "type": "Series", "text": ["Sort Series by index labels.", "Returns a new Series sorted by label if inplace argument is False, otherwise updates the original series and returns None.", "Axis to direct sorting. This can only be 0 for Series.", "If not None, sort on values in specified index level(s).", "Sort ascending vs. descending. When the index is a MultiIndex the sort direction can be controlled for each level individually.", "If True, perform operation in-place.", "Choice of sorting algorithm. See also numpy.sort() for more information. \u2018mergesort\u2019 and \u2018stable\u2019 are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label.", "If \u2018first\u2019 puts NaNs at the beginning, \u2018last\u2019 puts NaNs at the end. Not implemented for MultiIndex.", "If True and sorting by level and index is multilevel, sort by other levels too (in order) after sorting by specified level.", "If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.", "New in version 1.0.0.", "If not None, apply the key function to the index values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect an Index and return an Index of the same shape.", "New in version 1.1.0.", "The original Series sorted by the labels or None if inplace=True.", "See also", "Sort DataFrame by the index.", "Sort DataFrame by the value.", "Sort Series by the value.", "Examples", "Sort Descending", "Sort Inplace", "By default NaNs are put at the end, but use na_position to place them at the beginning", "Specify index level to sort", "Does not sort by remaining levels when sorting by levels", "Apply a key function before sorting"]}, {"name": "pandas.Series.sort_values", "path": "reference/api/pandas.series.sort_values", "type": "Series", "text": ["Sort by the values.", "Sort a Series in ascending or descending order by some criterion.", "Axis to direct sorting. The value \u2018index\u2019 is accepted for compatibility with DataFrame.sort_values.", "If True, sort values in ascending order, otherwise descending.", "If True, perform operation in-place.", "Choice of sorting algorithm. See also numpy.sort() for more information. \u2018mergesort\u2019 and \u2018stable\u2019 are the only stable algorithms.", "Argument \u2018first\u2019 puts NaNs at the beginning, \u2018last\u2019 puts NaNs at the end.", "If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.", "New in version 1.0.0.", "If not None, apply the key function to the series values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect a Series and return an array-like.", "New in version 1.1.0.", "Series ordered by values or None if inplace=True.", "See also", "Sort by the Series indices.", "Sort DataFrame by the values along either axis.", "Sort DataFrame by indices.", "Examples", "Sort values ascending order (default behaviour)", "Sort values descending order", "Sort values inplace", "Sort values putting NAs first", "Sort a series of strings", "Sort using a key function. Your key function will be given the Series of values and should return an array-like.", "NumPy ufuncs work well here. For example, we can sort by the sin of the value", "More complicated user-defined functions can be used, as long as they expect a Series and return an array-like"]}, {"name": "pandas.Series.sparse", "path": "reference/api/pandas.series.sparse", "type": "Series", "text": ["Accessor for SparseSparse from other sparse matrix data types."]}, {"name": "pandas.Series.sparse.density", "path": "reference/api/pandas.series.sparse.density", "type": "Series", "text": ["The percent of non- fill_value points, as decimal.", "Examples"]}, {"name": "pandas.Series.sparse.fill_value", "path": "reference/api/pandas.series.sparse.fill_value", "type": "Series", "text": ["Elements in data that are fill_value are not stored.", "For memory savings, this should be the most common value in the array."]}, {"name": "pandas.Series.sparse.from_coo", "path": "reference/api/pandas.series.sparse.from_coo", "type": "Series", "text": ["Create a Series with sparse values from a scipy.sparse.coo_matrix.", "If False (default), the SparseSeries index consists of only the coords of the non-null entries of the original coo_matrix. If True, the SparseSeries index consists of the full sorted (row, col) coordinates of the coo_matrix.", "A Series with sparse values.", "Examples"]}, {"name": "pandas.Series.sparse.npoints", "path": "reference/api/pandas.series.sparse.npoints", "type": "Series", "text": ["The number of non- fill_value points.", "Examples"]}, {"name": "pandas.Series.sparse.sp_values", "path": "reference/api/pandas.series.sparse.sp_values", "type": "Series", "text": ["An ndarray containing the non- fill_value values.", "Examples"]}, {"name": "pandas.Series.sparse.to_coo", "path": "reference/api/pandas.series.sparse.to_coo", "type": "Series", "text": ["Create a scipy.sparse.coo_matrix from a Series with MultiIndex.", "Use row_levels and column_levels to determine the row and column coordinates respectively. row_levels and column_levels are the names (labels) or numbers of the levels. {row_levels, column_levels} must be a partition of the MultiIndex level names (or numbers).", "Sort the row and column labels before forming the sparse matrix. When row_levels and/or column_levels refer to a single level, set to True for a faster execution.", "Examples"]}, {"name": "pandas.Series.squeeze", "path": "reference/api/pandas.series.squeeze", "type": "Series", "text": ["Squeeze 1 dimensional axis objects into scalars.", "Series or DataFrames with a single element are squeezed to a scalar. DataFrames with a single column or a single row are squeezed to a Series. Otherwise the object is unchanged.", "This method is most useful when you don\u2019t know if your object is a Series or DataFrame, but you do know it has just a single column. In that case you can safely call squeeze to ensure you have a Series.", "A specific axis to squeeze. By default, all length-1 axes are squeezed.", "The projection after squeezing axis or all the axes.", "See also", "Integer-location based indexing for selecting scalars.", "Integer-location based indexing for selecting Series.", "Inverse of DataFrame.squeeze for a single-column DataFrame.", "Examples", "Slicing might produce a Series with a single value:", "Squeezing objects with more than one value in every axis does nothing:", "Squeezing is even more effective when used with DataFrames.", "Slicing a single column will produce a DataFrame with the columns having only one value:", "So the columns can be squeezed down, resulting in a Series:", "Slicing a single row from a single column will produce a single scalar DataFrame:", "Squeezing the rows produces a single scalar Series:", "Squeezing all axes will project directly into a scalar:"]}, {"name": "pandas.Series.std", "path": "reference/api/pandas.series.std", "type": "Series", "text": ["Return sample standard deviation over requested axis.", "Normalized by N-1 by default. This can be changed using the ddof argument.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar.", "Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Notes", "To have the same behaviour as numpy.std, use ddof=0 (instead of the default ddof=1)", "Examples", "The standard deviation of the columns can be found as follows:", "Alternatively, ddof=0 can be set to normalize by N instead of N-1:"]}, {"name": "pandas.Series.str", "path": "reference/api/pandas.series.str", "type": "Series", "text": ["Vectorized string functions for Series and Index.", "NAs stay NA unless handled otherwise by a particular method. Patterned after Python\u2019s string methods, with some inspiration from R\u2019s stringr package.", "Examples"]}, {"name": "pandas.Series.str.capitalize", "path": "reference/api/pandas.series.str.capitalize", "type": "Series", "text": ["Convert strings in the Series/Index to be capitalized.", "Equivalent to str.capitalize().", "See also", "Converts all characters to lowercase.", "Converts all characters to uppercase.", "Converts first character of each word to uppercase and remaining to lowercase.", "Converts first character to uppercase and remaining to lowercase.", "Converts uppercase to lowercase and lowercase to uppercase.", "Removes all case distinctions in the string.", "Examples"]}, {"name": "pandas.Series.str.casefold", "path": "reference/api/pandas.series.str.casefold", "type": "Series", "text": ["Convert strings in the Series/Index to be casefolded.", "New in version 0.25.0.", "Equivalent to str.casefold().", "See also", "Converts all characters to lowercase.", "Converts all characters to uppercase.", "Converts first character of each word to uppercase and remaining to lowercase.", "Converts first character to uppercase and remaining to lowercase.", "Converts uppercase to lowercase and lowercase to uppercase.", "Removes all case distinctions in the string.", "Examples"]}, {"name": "pandas.Series.str.cat", "path": "reference/api/pandas.series.str.cat", "type": "Series", "text": ["Concatenate strings in the Series/Index with given separator.", "If others is specified, this function concatenates the Series/Index and elements of others element-wise. If others is not passed, then all values in the Series/Index are concatenated into a single string with a given sep.", "Series, Index, DataFrame, np.ndarray (one- or two-dimensional) and other list-likes of strings must have the same length as the calling Series/Index, with the exception of indexed objects (i.e. Series/Index/DataFrame) if join is not None.", "If others is a list-like that contains a combination of Series, Index or np.ndarray (1-dim), then all elements will be unpacked and must satisfy the above criteria individually.", "If others is None, the method returns the concatenation of all strings in the calling Series/Index.", "The separator between the different elements/columns. By default the empty string \u2018\u2019 is used.", "Representation that is inserted for all missing values:", "If na_rep is None, and others is None, missing values in the Series/Index are omitted from the result.", "If na_rep is None, and others is not None, a row containing a missing value in any of the columns (before concatenation) will have a missing value in the result.", "Determines the join-style between the calling Series/Index and any Series/Index/DataFrame in others (objects without an index need to match the length of the calling Series/Index). To disable alignment, use .values on any Series/Index/DataFrame in others.", "New in version 0.23.0.", "Changed in version 1.0.0: Changed default of join from None to \u2018left\u2019.", "If others is None, str is returned, otherwise a Series/Index (same type as caller) of objects is returned.", "See also", "Split each string in the Series/Index.", "Join lists contained as elements in the Series/Index.", "Examples", "When not passing others, all values are concatenated into a single string:", "By default, NA values in the Series are ignored. Using na_rep, they can be given a representation:", "If others is specified, corresponding values are concatenated with the separator. Result will be a Series of strings.", "Missing values will remain missing in the result, but can again be represented using na_rep", "If sep is not specified, the values are concatenated without separation.", "Series with different indexes can be aligned before concatenation. The join-keyword works as in other methods.", "For more examples, see here."]}, {"name": "pandas.Series.str.center", "path": "reference/api/pandas.series.str.center", "type": "Series", "text": ["Pad left and right side of strings in the Series/Index.", "Equivalent to str.center().", "Minimum width of resulting string; additional characters will be filled with fillchar.", "Additional character for filling, default is whitespace."]}, {"name": "pandas.Series.str.contains", "path": "reference/api/pandas.series.str.contains", "type": "Series", "text": ["Test if pattern or regex is contained within a string of a Series or Index.", "Return boolean Series or Index based on whether a given pattern or regex is contained within a string of a Series or Index.", "Character sequence or regular expression.", "If True, case sensitive.", "Flags to pass through to the re module, e.g. re.IGNORECASE.", "Fill value for missing values. The default depends on dtype of the array. For object-dtype, numpy.nan is used. For StringDtype, pandas.NA is used.", "If True, assumes the pat is a regular expression.", "If False, treats the pat as a literal string.", "A Series or Index of boolean values indicating whether the given pattern is contained within the string of each element of the Series or Index.", "See also", "Analogous, but stricter, relying on re.match instead of re.search.", "Test if the start of each string element matches a pattern.", "Same as startswith, but tests the end of string.", "Examples", "Returning a Series of booleans using only a literal pattern.", "Returning an Index of booleans using only a literal pattern.", "Specifying case sensitivity using case.", "Specifying na to be False instead of NaN replaces NaN values with False. If Series or Index does not contain NaN values the resultant dtype will be bool, otherwise, an object dtype.", "Returning \u2018house\u2019 or \u2018dog\u2019 when either expression occurs in a string.", "Ignoring case sensitivity using flags with regex.", "Returning any digit using regular expression.", "Ensure pat is a not a literal pattern when regex is set to True. Note in the following example one might expect only s2[1] and s2[3] to return True. However, \u2018.0\u2019 as a regex matches any character followed by a 0."]}, {"name": "pandas.Series.str.count", "path": "reference/api/pandas.series.str.count", "type": "Series", "text": ["Count occurrences of pattern in each string of the Series/Index.", "This function is used to count the number of times a particular regex pattern is repeated in each of the string elements of the Series.", "Valid regular expression.", "Flags for the re module. For a complete list, see here.", "For compatibility with other string methods. Not used.", "Same type as the calling object containing the integer counts.", "See also", "Standard library module for regular expressions.", "Standard library version, without regular expression support.", "Notes", "Some characters need to be escaped when passing in pat. eg. '$' has a special meaning in regex and must be escaped when finding this literal character.", "Examples", "Escape '$' to find the literal dollar sign.", "This is also available on Index"]}, {"name": "pandas.Series.str.decode", "path": "reference/api/pandas.series.str.decode", "type": "Series", "text": ["Decode character string in the Series/Index using indicated encoding.", "Equivalent to str.decode() in python2 and bytes.decode() in python3."]}, {"name": "pandas.Series.str.encode", "path": "reference/api/pandas.series.str.encode", "type": "Series", "text": ["Encode character string in the Series/Index using indicated encoding.", "Equivalent to str.encode()."]}, {"name": "pandas.Series.str.endswith", "path": "reference/api/pandas.series.str.endswith", "type": "Series", "text": ["Test if the end of each string element matches a pattern.", "Equivalent to str.endswith().", "Character sequence. Regular expressions are not accepted.", "Object shown if element tested is not a string. The default depends on dtype of the array. For object-dtype, numpy.nan is used. For StringDtype, pandas.NA is used.", "A Series of booleans indicating whether the given pattern matches the end of each string element.", "See also", "Python standard library string method.", "Same as endswith, but tests the start of string.", "Tests if string element contains a pattern.", "Examples", "Specifying na to be False instead of NaN."]}, {"name": "pandas.Series.str.extract", "path": "reference/api/pandas.series.str.extract", "type": "Series", "text": ["Extract capture groups in the regex pat as columns in a DataFrame.", "For each subject string in the Series, extract groups from the first match of regular expression pat.", "Regular expression pattern with capturing groups.", "Flags from the re module, e.g. re.IGNORECASE, that modify regular expression matching for things like case, spaces, etc. For more details, see re.", "If True, return DataFrame with one column per capture group. If False, return a Series/Index if there is one capture group or DataFrame if there are multiple capture groups.", "A DataFrame with one row for each subject string, and one column for each group. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used. The dtype of each result column is always object, even when no match is found. If expand=False and pat has only one capture group, then return a Series (if subject is a Series) or Index (if subject is an Index).", "See also", "Returns all matches (not just the first match).", "Examples", "A pattern with two groups will return a DataFrame with two columns. Non-matches will be NaN.", "A pattern may contain optional groups.", "Named groups will become column names in the result.", "A pattern with one group will return a DataFrame with one column if expand=True.", "A pattern with one group will return a Series if expand=False."]}, {"name": "pandas.Series.str.extractall", "path": "reference/api/pandas.series.str.extractall", "type": "Series", "text": ["Extract capture groups in the regex pat as columns in DataFrame.", "For each subject string in the Series, extract groups from all matches of regular expression pat. When each subject string in the Series has exactly one match, extractall(pat).xs(0, level=\u2019match\u2019) is the same as extract(pat).", "Regular expression pattern with capturing groups.", "A re module flag, for example re.IGNORECASE. These allow to modify regular expression matching for things like case, spaces, etc. Multiple flags can be combined with the bitwise OR operator, for example re.IGNORECASE | re.MULTILINE.", "A DataFrame with one row for each match, and one column for each group. Its rows have a MultiIndex with first levels that come from the subject Series. The last level is named \u2018match\u2019 and indexes the matches in each item of the Series. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used.", "See also", "Returns first match only (not all matches).", "Examples", "A pattern with one group will return a DataFrame with one column. Indices with no matches will not appear in the result.", "Capture group names are used for column names of the result.", "A pattern with two groups will return a DataFrame with two columns.", "Optional groups that do not match are NaN in the result."]}, {"name": "pandas.Series.str.find", "path": "reference/api/pandas.series.str.find", "type": "Series", "text": ["Return lowest indexes in each strings in the Series/Index.", "Each of returned indexes corresponds to the position where the substring is fully contained between [start:end]. Return -1 on failure. Equivalent to standard str.find().", "Substring being searched.", "Left edge index.", "Right edge index.", "See also", "Return highest indexes in each strings."]}, {"name": "pandas.Series.str.findall", "path": "reference/api/pandas.series.str.findall", "type": "Series", "text": ["Find all occurrences of pattern or regular expression in the Series/Index.", "Equivalent to applying re.findall() to all the elements in the Series/Index.", "Pattern or regular expression.", "Flags from re module, e.g. re.IGNORECASE (default is 0, which means no flags).", "All non-overlapping matches of pattern or regular expression in each string of this Series/Index.", "See also", "Count occurrences of pattern or regular expression in each string of the Series/Index.", "For each string in the Series, extract groups from all matches of regular expression and return a DataFrame with one row for each match and one column for each group.", "The equivalent re function to all non-overlapping matches of pattern or regular expression in string, as a list of strings.", "Examples", "The search for the pattern \u2018Monkey\u2019 returns one match:", "On the other hand, the search for the pattern \u2018MONKEY\u2019 doesn\u2019t return any match:", "Flags can be added to the pattern or regular expression. For instance, to find the pattern \u2018MONKEY\u2019 ignoring the case:", "When the pattern matches more than one string in the Series, all matches are returned:", "Regular expressions are supported too. For instance, the search for all the strings ending with the word \u2018on\u2019 is shown next:", "If the pattern is found more than once in the same string, then a list of multiple strings is returned:"]}, {"name": "pandas.Series.str.fullmatch", "path": "reference/api/pandas.series.str.fullmatch", "type": "Series", "text": ["Determine if each string entirely matches a regular expression.", "New in version 1.1.0.", "Character sequence or regular expression.", "If True, case sensitive.", "Regex module flags, e.g. re.IGNORECASE.", "Fill value for missing values. The default depends on dtype of the array. For object-dtype, numpy.nan is used. For StringDtype, pandas.NA is used.", "See also", "Similar, but also returns True when only a prefix of the string matches the regular expression.", "Extract matched groups."]}, {"name": "pandas.Series.str.get", "path": "reference/api/pandas.series.str.get", "type": "Series", "text": ["Extract element from each component at specified position.", "Extract element from lists, tuples, or strings in each element in the Series/Index.", "Position of element to extract.", "Examples"]}, {"name": "pandas.Series.str.get_dummies", "path": "reference/api/pandas.series.str.get_dummies", "type": "Series", "text": ["Return DataFrame of dummy/indicator variables for Series.", "Each string in Series is split by sep and returned as a DataFrame of dummy/indicator variables.", "String to split on.", "Dummy variables corresponding to values of the Series.", "See also", "Convert categorical variable into dummy/indicator variables.", "Examples"]}, {"name": "pandas.Series.str.index", "path": "reference/api/pandas.series.str.index", "type": "Series", "text": ["Return lowest indexes in each string in Series/Index.", "Each of the returned indexes corresponds to the position where the substring is fully contained between [start:end]. This is the same as str.find except instead of returning -1, it raises a ValueError when the substring is not found. Equivalent to standard str.index.", "Substring being searched.", "Left edge index.", "Right edge index.", "See also", "Return highest indexes in each strings."]}, {"name": "pandas.Series.str.isalnum", "path": "reference/api/pandas.series.str.isalnum", "type": "Series", "text": ["Check whether all characters in each string are alphanumeric.", "This is equivalent to running the Python string method str.isalnum() for each element of the Series/Index. If a string has zero characters, False is returned for that check.", "Series or Index of boolean values with the same length as the original Series/Index.", "See also", "Check whether all characters are alphabetic.", "Check whether all characters are numeric.", "Check whether all characters are alphanumeric.", "Check whether all characters are digits.", "Check whether all characters are decimal.", "Check whether all characters are whitespace.", "Check whether all characters are lowercase.", "Check whether all characters are uppercase.", "Check whether all characters are titlecase.", "Examples", "Checks for Alphabetic and Numeric Characters", "Note that checks against characters mixed with any additional punctuation or whitespace will evaluate to false for an alphanumeric check.", "More Detailed Checks for Numeric Characters", "There are several different but overlapping sets of numeric characters that can be checked for.", "The s3.str.isdecimal method checks for characters used to form numbers in base 10.", "The s.str.isdigit method is the same as s3.str.isdecimal but also includes special digits, like superscripted and subscripted digits in unicode.", "The s.str.isnumeric method is the same as s3.str.isdigit but also includes other characters that can represent quantities such as unicode fractions.", "Checks for Whitespace", "Checks for Character Case", "The s5.str.istitle method checks for whether all words are in title case (whether only the first letter of each word is capitalized). Words are assumed to be as any sequence of non-numeric characters separated by whitespace characters."]}, {"name": "pandas.Series.str.isalpha", "path": "reference/api/pandas.series.str.isalpha", "type": "Series", "text": ["Check whether all characters in each string are alphabetic.", "This is equivalent to running the Python string method str.isalpha() for each element of the Series/Index. If a string has zero characters, False is returned for that check.", "Series or Index of boolean values with the same length as the original Series/Index.", "See also", "Check whether all characters are alphabetic.", "Check whether all characters are numeric.", "Check whether all characters are alphanumeric.", "Check whether all characters are digits.", "Check whether all characters are decimal.", "Check whether all characters are whitespace.", "Check whether all characters are lowercase.", "Check whether all characters are uppercase.", "Check whether all characters are titlecase.", "Examples", "Checks for Alphabetic and Numeric Characters", "Note that checks against characters mixed with any additional punctuation or whitespace will evaluate to false for an alphanumeric check.", "More Detailed Checks for Numeric Characters", "There are several different but overlapping sets of numeric characters that can be checked for.", "The s3.str.isdecimal method checks for characters used to form numbers in base 10.", "The s.str.isdigit method is the same as s3.str.isdecimal but also includes special digits, like superscripted and subscripted digits in unicode.", "The s.str.isnumeric method is the same as s3.str.isdigit but also includes other characters that can represent quantities such as unicode fractions.", "Checks for Whitespace", "Checks for Character Case", "The s5.str.istitle method checks for whether all words are in title case (whether only the first letter of each word is capitalized). Words are assumed to be as any sequence of non-numeric characters separated by whitespace characters."]}, {"name": "pandas.Series.str.isdecimal", "path": "reference/api/pandas.series.str.isdecimal", "type": "Series", "text": ["Check whether all characters in each string are decimal.", "This is equivalent to running the Python string method str.isdecimal() for each element of the Series/Index. If a string has zero characters, False is returned for that check.", "Series or Index of boolean values with the same length as the original Series/Index.", "See also", "Check whether all characters are alphabetic.", "Check whether all characters are numeric.", "Check whether all characters are alphanumeric.", "Check whether all characters are digits.", "Check whether all characters are decimal.", "Check whether all characters are whitespace.", "Check whether all characters are lowercase.", "Check whether all characters are uppercase.", "Check whether all characters are titlecase.", "Examples", "Checks for Alphabetic and Numeric Characters", "Note that checks against characters mixed with any additional punctuation or whitespace will evaluate to false for an alphanumeric check.", "More Detailed Checks for Numeric Characters", "There are several different but overlapping sets of numeric characters that can be checked for.", "The s3.str.isdecimal method checks for characters used to form numbers in base 10.", "The s.str.isdigit method is the same as s3.str.isdecimal but also includes special digits, like superscripted and subscripted digits in unicode.", "The s.str.isnumeric method is the same as s3.str.isdigit but also includes other characters that can represent quantities such as unicode fractions.", "Checks for Whitespace", "Checks for Character Case", "The s5.str.istitle method checks for whether all words are in title case (whether only the first letter of each word is capitalized). Words are assumed to be as any sequence of non-numeric characters separated by whitespace characters."]}, {"name": "pandas.Series.str.isdigit", "path": "reference/api/pandas.series.str.isdigit", "type": "Series", "text": ["Check whether all characters in each string are digits.", "This is equivalent to running the Python string method str.isdigit() for each element of the Series/Index. If a string has zero characters, False is returned for that check.", "Series or Index of boolean values with the same length as the original Series/Index.", "See also", "Check whether all characters are alphabetic.", "Check whether all characters are numeric.", "Check whether all characters are alphanumeric.", "Check whether all characters are digits.", "Check whether all characters are decimal.", "Check whether all characters are whitespace.", "Check whether all characters are lowercase.", "Check whether all characters are uppercase.", "Check whether all characters are titlecase.", "Examples", "Checks for Alphabetic and Numeric Characters", "Note that checks against characters mixed with any additional punctuation or whitespace will evaluate to false for an alphanumeric check.", "More Detailed Checks for Numeric Characters", "There are several different but overlapping sets of numeric characters that can be checked for.", "The s3.str.isdecimal method checks for characters used to form numbers in base 10.", "The s.str.isdigit method is the same as s3.str.isdecimal but also includes special digits, like superscripted and subscripted digits in unicode.", "The s.str.isnumeric method is the same as s3.str.isdigit but also includes other characters that can represent quantities such as unicode fractions.", "Checks for Whitespace", "Checks for Character Case", "The s5.str.istitle method checks for whether all words are in title case (whether only the first letter of each word is capitalized). Words are assumed to be as any sequence of non-numeric characters separated by whitespace characters."]}, {"name": "pandas.Series.str.islower", "path": "reference/api/pandas.series.str.islower", "type": "Series", "text": ["Check whether all characters in each string are lowercase.", "This is equivalent to running the Python string method str.islower() for each element of the Series/Index. If a string has zero characters, False is returned for that check.", "Series or Index of boolean values with the same length as the original Series/Index.", "See also", "Check whether all characters are alphabetic.", "Check whether all characters are numeric.", "Check whether all characters are alphanumeric.", "Check whether all characters are digits.", "Check whether all characters are decimal.", "Check whether all characters are whitespace.", "Check whether all characters are lowercase.", "Check whether all characters are uppercase.", "Check whether all characters are titlecase.", "Examples", "Checks for Alphabetic and Numeric Characters", "Note that checks against characters mixed with any additional punctuation or whitespace will evaluate to false for an alphanumeric check.", "More Detailed Checks for Numeric Characters", "There are several different but overlapping sets of numeric characters that can be checked for.", "The s3.str.isdecimal method checks for characters used to form numbers in base 10.", "The s.str.isdigit method is the same as s3.str.isdecimal but also includes special digits, like superscripted and subscripted digits in unicode.", "The s.str.isnumeric method is the same as s3.str.isdigit but also includes other characters that can represent quantities such as unicode fractions.", "Checks for Whitespace", "Checks for Character Case", "The s5.str.istitle method checks for whether all words are in title case (whether only the first letter of each word is capitalized). Words are assumed to be as any sequence of non-numeric characters separated by whitespace characters."]}, {"name": "pandas.Series.str.isnumeric", "path": "reference/api/pandas.series.str.isnumeric", "type": "Series", "text": ["Check whether all characters in each string are numeric.", "This is equivalent to running the Python string method str.isnumeric() for each element of the Series/Index. If a string has zero characters, False is returned for that check.", "Series or Index of boolean values with the same length as the original Series/Index.", "See also", "Check whether all characters are alphabetic.", "Check whether all characters are numeric.", "Check whether all characters are alphanumeric.", "Check whether all characters are digits.", "Check whether all characters are decimal.", "Check whether all characters are whitespace.", "Check whether all characters are lowercase.", "Check whether all characters are uppercase.", "Check whether all characters are titlecase.", "Examples", "Checks for Alphabetic and Numeric Characters", "Note that checks against characters mixed with any additional punctuation or whitespace will evaluate to false for an alphanumeric check.", "More Detailed Checks for Numeric Characters", "There are several different but overlapping sets of numeric characters that can be checked for.", "The s3.str.isdecimal method checks for characters used to form numbers in base 10.", "The s.str.isdigit method is the same as s3.str.isdecimal but also includes special digits, like superscripted and subscripted digits in unicode.", "The s.str.isnumeric method is the same as s3.str.isdigit but also includes other characters that can represent quantities such as unicode fractions.", "Checks for Whitespace", "Checks for Character Case", "The s5.str.istitle method checks for whether all words are in title case (whether only the first letter of each word is capitalized). Words are assumed to be as any sequence of non-numeric characters separated by whitespace characters."]}, {"name": "pandas.Series.str.isspace", "path": "reference/api/pandas.series.str.isspace", "type": "Series", "text": ["Check whether all characters in each string are whitespace.", "This is equivalent to running the Python string method str.isspace() for each element of the Series/Index. If a string has zero characters, False is returned for that check.", "Series or Index of boolean values with the same length as the original Series/Index.", "See also", "Check whether all characters are alphabetic.", "Check whether all characters are numeric.", "Check whether all characters are alphanumeric.", "Check whether all characters are digits.", "Check whether all characters are decimal.", "Check whether all characters are whitespace.", "Check whether all characters are lowercase.", "Check whether all characters are uppercase.", "Check whether all characters are titlecase.", "Examples", "Checks for Alphabetic and Numeric Characters", "Note that checks against characters mixed with any additional punctuation or whitespace will evaluate to false for an alphanumeric check.", "More Detailed Checks for Numeric Characters", "There are several different but overlapping sets of numeric characters that can be checked for.", "The s3.str.isdecimal method checks for characters used to form numbers in base 10.", "The s.str.isdigit method is the same as s3.str.isdecimal but also includes special digits, like superscripted and subscripted digits in unicode.", "The s.str.isnumeric method is the same as s3.str.isdigit but also includes other characters that can represent quantities such as unicode fractions.", "Checks for Whitespace", "Checks for Character Case", "The s5.str.istitle method checks for whether all words are in title case (whether only the first letter of each word is capitalized). Words are assumed to be as any sequence of non-numeric characters separated by whitespace characters."]}, {"name": "pandas.Series.str.istitle", "path": "reference/api/pandas.series.str.istitle", "type": "Series", "text": ["Check whether all characters in each string are titlecase.", "This is equivalent to running the Python string method str.istitle() for each element of the Series/Index. If a string has zero characters, False is returned for that check.", "Series or Index of boolean values with the same length as the original Series/Index.", "See also", "Check whether all characters are alphabetic.", "Check whether all characters are numeric.", "Check whether all characters are alphanumeric.", "Check whether all characters are digits.", "Check whether all characters are decimal.", "Check whether all characters are whitespace.", "Check whether all characters are lowercase.", "Check whether all characters are uppercase.", "Check whether all characters are titlecase.", "Examples", "Checks for Alphabetic and Numeric Characters", "Note that checks against characters mixed with any additional punctuation or whitespace will evaluate to false for an alphanumeric check.", "More Detailed Checks for Numeric Characters", "There are several different but overlapping sets of numeric characters that can be checked for.", "The s3.str.isdecimal method checks for characters used to form numbers in base 10.", "The s.str.isdigit method is the same as s3.str.isdecimal but also includes special digits, like superscripted and subscripted digits in unicode.", "The s.str.isnumeric method is the same as s3.str.isdigit but also includes other characters that can represent quantities such as unicode fractions.", "Checks for Whitespace", "Checks for Character Case", "The s5.str.istitle method checks for whether all words are in title case (whether only the first letter of each word is capitalized). Words are assumed to be as any sequence of non-numeric characters separated by whitespace characters."]}, {"name": "pandas.Series.str.isupper", "path": "reference/api/pandas.series.str.isupper", "type": "Series", "text": ["Check whether all characters in each string are uppercase.", "This is equivalent to running the Python string method str.isupper() for each element of the Series/Index. If a string has zero characters, False is returned for that check.", "Series or Index of boolean values with the same length as the original Series/Index.", "See also", "Check whether all characters are alphabetic.", "Check whether all characters are numeric.", "Check whether all characters are alphanumeric.", "Check whether all characters are digits.", "Check whether all characters are decimal.", "Check whether all characters are whitespace.", "Check whether all characters are lowercase.", "Check whether all characters are uppercase.", "Check whether all characters are titlecase.", "Examples", "Checks for Alphabetic and Numeric Characters", "Note that checks against characters mixed with any additional punctuation or whitespace will evaluate to false for an alphanumeric check.", "More Detailed Checks for Numeric Characters", "There are several different but overlapping sets of numeric characters that can be checked for.", "The s3.str.isdecimal method checks for characters used to form numbers in base 10.", "The s.str.isdigit method is the same as s3.str.isdecimal but also includes special digits, like superscripted and subscripted digits in unicode.", "The s.str.isnumeric method is the same as s3.str.isdigit but also includes other characters that can represent quantities such as unicode fractions.", "Checks for Whitespace", "Checks for Character Case", "The s5.str.istitle method checks for whether all words are in title case (whether only the first letter of each word is capitalized). Words are assumed to be as any sequence of non-numeric characters separated by whitespace characters."]}, {"name": "pandas.Series.str.join", "path": "reference/api/pandas.series.str.join", "type": "Series", "text": ["Join lists contained as elements in the Series/Index with passed delimiter.", "If the elements of a Series are lists themselves, join the content of these lists using the delimiter passed to the function. This function is an equivalent to str.join().", "Delimiter to use between list entries.", "The list entries concatenated by intervening occurrences of the delimiter.", "If the supplied Series contains neither strings nor lists.", "See also", "Standard library version of this method.", "Split strings around given separator/delimiter.", "Notes", "If any of the list items is not a string object, the result of the join will be NaN.", "Examples", "Example with a list that contains non-string elements.", "Join all lists using a \u2018-\u2019. The lists containing object(s) of types other than str will produce a NaN."]}, {"name": "pandas.Series.str.len", "path": "reference/api/pandas.series.str.len", "type": "Series", "text": ["Compute the length of each element in the Series/Index.", "The element may be a sequence (such as a string, tuple or list) or a collection (such as a dictionary).", "A Series or Index of integer values indicating the length of each element in the Series or Index.", "See also", "Python built-in function returning the length of an object.", "Returns the length of the Series.", "Examples", "Returns the length (number of characters) in a string. Returns the number of entries for dictionaries, lists or tuples."]}, {"name": "pandas.Series.str.ljust", "path": "reference/api/pandas.series.str.ljust", "type": "Series", "text": ["Pad right side of strings in the Series/Index.", "Equivalent to str.ljust().", "Minimum width of resulting string; additional characters will be filled with fillchar.", "Additional character for filling, default is whitespace."]}, {"name": "pandas.Series.str.lower", "path": "reference/api/pandas.series.str.lower", "type": "Series", "text": ["Convert strings in the Series/Index to lowercase.", "Equivalent to str.lower().", "See also", "Converts all characters to lowercase.", "Converts all characters to uppercase.", "Converts first character of each word to uppercase and remaining to lowercase.", "Converts first character to uppercase and remaining to lowercase.", "Converts uppercase to lowercase and lowercase to uppercase.", "Removes all case distinctions in the string.", "Examples"]}, {"name": "pandas.Series.str.lstrip", "path": "reference/api/pandas.series.str.lstrip", "type": "Series", "text": ["Remove leading characters.", "Strip whitespaces (including newlines) or a set of specified characters from each string in the Series/Index from left side. Equivalent to str.lstrip().", "Specifying the set of characters to be removed. All combinations of this set of characters will be stripped. If None then whitespaces are removed.", "See also", "Remove leading and trailing characters in Series/Index.", "Remove leading characters in Series/Index.", "Remove trailing characters in Series/Index.", "Examples"]}, {"name": "pandas.Series.str.match", "path": "reference/api/pandas.series.str.match", "type": "Series", "text": ["Determine if each string starts with a match of a regular expression.", "Character sequence or regular expression.", "If True, case sensitive.", "Regex module flags, e.g. re.IGNORECASE.", "Fill value for missing values. The default depends on dtype of the array. For object-dtype, numpy.nan is used. For StringDtype, pandas.NA is used.", "See also", "Stricter matching that requires the entire string to match.", "Analogous, but less strict, relying on re.search instead of re.match.", "Extract matched groups."]}, {"name": "pandas.Series.str.normalize", "path": "reference/api/pandas.series.str.normalize", "type": "Series", "text": ["Return the Unicode normal form for the strings in the Series/Index.", "For more information on the forms, see the unicodedata.normalize().", "Unicode form."]}, {"name": "pandas.Series.str.pad", "path": "reference/api/pandas.series.str.pad", "type": "Series", "text": ["Pad strings in the Series/Index up to width.", "Minimum width of resulting string; additional characters will be filled with character defined in fillchar.", "Side from which to fill resulting string.", "Additional character for filling, default is whitespace.", "Returns Series or Index with minimum number of char in object.", "See also", "Fills the left side of strings with an arbitrary character. Equivalent to Series.str.pad(side='left').", "Fills the right side of strings with an arbitrary character. Equivalent to Series.str.pad(side='right').", "Fills both sides of strings with an arbitrary character. Equivalent to Series.str.pad(side='both').", "Pad strings in the Series/Index by prepending \u20180\u2019 character. Equivalent to Series.str.pad(side='left', fillchar='0').", "Examples"]}, {"name": "pandas.Series.str.partition", "path": "reference/api/pandas.series.str.partition", "type": "Input/output", "text": ["Split the string at the first occurrence of sep.", "This method splits the string at the first occurrence of sep, and returns 3 elements containing the part before the separator, the separator itself, and the part after the separator. If the separator is not found, return 3 elements containing the string itself, followed by two empty strings.", "String to split on.", "If True, return DataFrame/MultiIndex expanding dimensionality. If False, return Series/Index.", "See also", "Split the string at the last occurrence of sep.", "Split strings around given separators.", "Standard library version.", "Examples", "To partition by the last space instead of the first one:", "To partition by something different than a space:", "To return a Series containing tuples instead of a DataFrame:", "Also available on indices:", "Which will create a MultiIndex:", "Or an index with tuples with expand=False:"]}, {"name": "pandas.Series.str.removeprefix", "path": "reference/api/pandas.series.str.removeprefix", "type": "Series", "text": ["Remove a prefix from an object series. If the prefix is not present, the original string will be returned.", "Remove the prefix of the string.", "The Series or Index with given prefix removed.", "See also", "Remove a suffix from an object series.", "Examples"]}, {"name": "pandas.Series.str.removesuffix", "path": "reference/api/pandas.series.str.removesuffix", "type": "Series", "text": ["Remove a suffix from an object series. If the suffix is not present, the original string will be returned.", "Remove the suffix of the string.", "The Series or Index with given suffix removed.", "See also", "Remove a prefix from an object series.", "Examples"]}, {"name": "pandas.Series.str.repeat", "path": "reference/api/pandas.series.str.repeat", "type": "Series", "text": ["Duplicate each string in the Series or Index.", "Same value for all (int) or different value per (sequence).", "Series or Index of repeated string objects specified by input parameter repeats.", "Examples", "Single int repeats string in Series", "Sequence of int repeats corresponding string in Series"]}, {"name": "pandas.Series.str.replace", "path": "reference/api/pandas.series.str.replace", "type": "Series", "text": ["Replace each occurrence of pattern/regex in the Series/Index.", "Equivalent to str.replace() or re.sub(), depending on the regex value.", "String can be a character sequence or regular expression.", "Replacement string or a callable. The callable is passed the regex match object and must return a replacement string to be used. See re.sub().", "Number of replacements to make from start.", "Determines if replace is case sensitive:", "If True, case sensitive (the default if pat is a string)", "Set to False for case insensitive", "Cannot be set if pat is a compiled regex.", "Regex module flags, e.g. re.IGNORECASE. Cannot be set if pat is a compiled regex.", "Determines if the passed-in pattern is a regular expression:", "If True, assumes the passed-in pattern is a regular expression.", "If False, treats the pattern as a literal string", "Cannot be set to False if pat is a compiled regex or repl is a callable.", "New in version 0.23.0.", "A copy of the object with all matching occurrences of pat replaced by repl.", "if regex is False and repl is a callable or pat is a compiled regex", "if pat is a compiled regex and case or flags is set", "Notes", "When pat is a compiled regex, all flags should be included in the compiled regex. Use of case, flags, or regex=False with a compiled regex will raise an error.", "Examples", "When pat is a string and regex is True (the default), the given pat is compiled as a regex. When repl is a string, it replaces matching regex patterns as with re.sub(). NaN value(s) in the Series are left as is:", "When pat is a string and regex is False, every pat is replaced with repl as with str.replace():", "When repl is a callable, it is called on every pat using re.sub(). The callable should expect one positional argument (a regex object) and return a string.", "To get the idea:", "Reverse every lowercase alphabetic word:", "Using regex groups (extract second group and swap case):", "Using a compiled regex with flags"]}, {"name": "pandas.Series.str.rfind", "path": "reference/api/pandas.series.str.rfind", "type": "Series", "text": ["Return highest indexes in each strings in the Series/Index.", "Each of returned indexes corresponds to the position where the substring is fully contained between [start:end]. Return -1 on failure. Equivalent to standard str.rfind().", "Substring being searched.", "Left edge index.", "Right edge index.", "See also", "Return lowest indexes in each strings."]}, {"name": "pandas.Series.str.rindex", "path": "reference/api/pandas.series.str.rindex", "type": "Series", "text": ["Return highest indexes in each string in Series/Index.", "Each of the returned indexes corresponds to the position where the substring is fully contained between [start:end]. This is the same as str.rfind except instead of returning -1, it raises a ValueError when the substring is not found. Equivalent to standard str.rindex.", "Substring being searched.", "Left edge index.", "Right edge index.", "See also", "Return lowest indexes in each strings."]}, {"name": "pandas.Series.str.rjust", "path": "reference/api/pandas.series.str.rjust", "type": "Series", "text": ["Pad left side of strings in the Series/Index.", "Equivalent to str.rjust().", "Minimum width of resulting string; additional characters will be filled with fillchar.", "Additional character for filling, default is whitespace."]}, {"name": "pandas.Series.str.rpartition", "path": "reference/api/pandas.series.str.rpartition", "type": "Input/output", "text": ["Split the string at the last occurrence of sep.", "This method splits the string at the last occurrence of sep, and returns 3 elements containing the part before the separator, the separator itself, and the part after the separator. If the separator is not found, return 3 elements containing two empty strings, followed by the string itself.", "String to split on.", "If True, return DataFrame/MultiIndex expanding dimensionality. If False, return Series/Index.", "See also", "Split the string at the first occurrence of sep.", "Split strings around given separators.", "Standard library version.", "Examples", "To partition by the last space instead of the first one:", "To partition by something different than a space:", "To return a Series containing tuples instead of a DataFrame:", "Also available on indices:", "Which will create a MultiIndex:", "Or an index with tuples with expand=False:"]}, {"name": "pandas.Series.str.rsplit", "path": "reference/api/pandas.series.str.rsplit", "type": "Series", "text": ["Split strings around given separator/delimiter.", "Splits the string in the Series/Index from the end, at the specified delimiter string.", "String or regular expression to split on. If not specified, split on whitespace.", "Limit number of splits in output. None, 0 and -1 will be interpreted as return all splits.", "Expand the split strings into separate columns.", "If True, return DataFrame/MultiIndex expanding dimensionality.", "If False, return Series/Index, containing lists of strings.", "Determines if the passed-in pattern is a regular expression:", "If True, assumes the passed-in pattern is a regular expression", "If False, treats the pattern as a literal string.", "If None and pat length is 1, treats pat as a literal string.", "If None and pat length is not 1, treats pat as a regular expression.", "Cannot be set to False if pat is a compiled regex", "New in version 1.4.0.", "Type matches caller unless expand=True (see Notes).", "if regex is False and pat is a compiled regex", "See also", "Split strings around given separator/delimiter.", "Splits string around given separator/delimiter, starting from the right.", "Join lists contained as elements in the Series/Index with passed delimiter.", "Standard library version for split.", "Standard library version for rsplit.", "Notes", "The handling of the n keyword depends on the number of found splits:", "If found splits > n, make first n splits only", "If found splits <= n, make all splits", "If for a certain row the number of found splits < n, append None for padding up to n if expand=True", "If using expand=True, Series and Index callers return DataFrame and MultiIndex objects, respectively.", "Use of regex=False with a pat as a compiled regex will raise an error.", "Examples", "In the default setting, the string is split by whitespace.", "Without the n parameter, the outputs of rsplit and split are identical.", "The n parameter can be used to limit the number of splits on the delimiter. The outputs of split and rsplit are different.", "The pat parameter can be used to split by other characters.", "When using expand=True, the split elements will expand out into separate columns. If NaN is present, it is propagated throughout the columns during the split.", "For slightly more complex use cases like splitting the html document name from a url, a combination of parameter settings can be used.", "Remember to escape special characters when explicitly using regular expressions.", "Regular expressions can be used to handle urls or file names. When pat is a string and regex=None (the default), the given pat is compiled as a regex only if len(pat) != 1.", "When regex=True, pat is interpreted as a regex", "A compiled regex can be passed as pat", "When regex=False, pat is interpreted as the string itself"]}, {"name": "pandas.Series.str.rstrip", "path": "reference/api/pandas.series.str.rstrip", "type": "Series", "text": ["Remove trailing characters.", "Strip whitespaces (including newlines) or a set of specified characters from each string in the Series/Index from right side. Equivalent to str.rstrip().", "Specifying the set of characters to be removed. All combinations of this set of characters will be stripped. If None then whitespaces are removed.", "See also", "Remove leading and trailing characters in Series/Index.", "Remove leading characters in Series/Index.", "Remove trailing characters in Series/Index.", "Examples"]}, {"name": "pandas.Series.str.slice", "path": "reference/api/pandas.series.str.slice", "type": "Series", "text": ["Slice substrings from each element in the Series or Index.", "Start position for slice operation.", "Stop position for slice operation.", "Step size for slice operation.", "Series or Index from sliced substring from original string object.", "See also", "Replace a slice with a string.", "Return element at position. Equivalent to Series.str.slice(start=i, stop=i+1) with i being the position.", "Examples", "Equivalent behaviour to:"]}, {"name": "pandas.Series.str.slice_replace", "path": "reference/api/pandas.series.str.slice_replace", "type": "Series", "text": ["Replace a positional slice of a string with another value.", "Left index position to use for the slice. If not specified (None), the slice is unbounded on the left, i.e. slice from the start of the string.", "Right index position to use for the slice. If not specified (None), the slice is unbounded on the right, i.e. slice until the end of the string.", "String for replacement. If not specified (None), the sliced region is replaced with an empty string.", "Same type as the original object.", "See also", "Just slicing without replacement.", "Examples", "Specify just start, meaning replace start until the end of the string with repl.", "Specify just stop, meaning the start of the string to stop is replaced with repl, and the rest of the string is included.", "Specify start and stop, meaning the slice from start to stop is replaced with repl. Everything before or after start and stop is included as is."]}, {"name": "pandas.Series.str.split", "path": "reference/api/pandas.series.str.split", "type": "Series", "text": ["Split strings around given separator/delimiter.", "Splits the string in the Series/Index from the beginning, at the specified delimiter string.", "String or regular expression to split on. If not specified, split on whitespace.", "Limit number of splits in output. None, 0 and -1 will be interpreted as return all splits.", "Expand the split strings into separate columns.", "If True, return DataFrame/MultiIndex expanding dimensionality.", "If False, return Series/Index, containing lists of strings.", "Determines if the passed-in pattern is a regular expression:", "If True, assumes the passed-in pattern is a regular expression", "If False, treats the pattern as a literal string.", "If None and pat length is 1, treats pat as a literal string.", "If None and pat length is not 1, treats pat as a regular expression.", "Cannot be set to False if pat is a compiled regex", "New in version 1.4.0.", "Type matches caller unless expand=True (see Notes).", "if regex is False and pat is a compiled regex", "See also", "Split strings around given separator/delimiter.", "Splits string around given separator/delimiter, starting from the right.", "Join lists contained as elements in the Series/Index with passed delimiter.", "Standard library version for split.", "Standard library version for rsplit.", "Notes", "The handling of the n keyword depends on the number of found splits:", "If found splits > n, make first n splits only", "If found splits <= n, make all splits", "If for a certain row the number of found splits < n, append None for padding up to n if expand=True", "If using expand=True, Series and Index callers return DataFrame and MultiIndex objects, respectively.", "Use of regex=False with a pat as a compiled regex will raise an error.", "Examples", "In the default setting, the string is split by whitespace.", "Without the n parameter, the outputs of rsplit and split are identical.", "The n parameter can be used to limit the number of splits on the delimiter. The outputs of split and rsplit are different.", "The pat parameter can be used to split by other characters.", "When using expand=True, the split elements will expand out into separate columns. If NaN is present, it is propagated throughout the columns during the split.", "For slightly more complex use cases like splitting the html document name from a url, a combination of parameter settings can be used.", "Remember to escape special characters when explicitly using regular expressions.", "Regular expressions can be used to handle urls or file names. When pat is a string and regex=None (the default), the given pat is compiled as a regex only if len(pat) != 1.", "When regex=True, pat is interpreted as a regex", "A compiled regex can be passed as pat", "When regex=False, pat is interpreted as the string itself"]}, {"name": "pandas.Series.str.startswith", "path": "reference/api/pandas.series.str.startswith", "type": "Series", "text": ["Test if the start of each string element matches a pattern.", "Equivalent to str.startswith().", "Character sequence. Regular expressions are not accepted.", "Object shown if element tested is not a string. The default depends on dtype of the array. For object-dtype, numpy.nan is used. For StringDtype, pandas.NA is used.", "A Series of booleans indicating whether the given pattern matches the start of each string element.", "See also", "Python standard library string method.", "Same as startswith, but tests the end of string.", "Tests if string element contains a pattern.", "Examples", "Specifying na to be False instead of NaN."]}, {"name": "pandas.Series.str.strip", "path": "reference/api/pandas.series.str.strip", "type": "Series", "text": ["Remove leading and trailing characters.", "Strip whitespaces (including newlines) or a set of specified characters from each string in the Series/Index from left and right sides. Equivalent to str.strip().", "Specifying the set of characters to be removed. All combinations of this set of characters will be stripped. If None then whitespaces are removed.", "See also", "Remove leading and trailing characters in Series/Index.", "Remove leading characters in Series/Index.", "Remove trailing characters in Series/Index.", "Examples"]}, {"name": "pandas.Series.str.swapcase", "path": "reference/api/pandas.series.str.swapcase", "type": "Series", "text": ["Convert strings in the Series/Index to be swapcased.", "Equivalent to str.swapcase().", "See also", "Converts all characters to lowercase.", "Converts all characters to uppercase.", "Converts first character of each word to uppercase and remaining to lowercase.", "Converts first character to uppercase and remaining to lowercase.", "Converts uppercase to lowercase and lowercase to uppercase.", "Removes all case distinctions in the string.", "Examples"]}, {"name": "pandas.Series.str.title", "path": "reference/api/pandas.series.str.title", "type": "Series", "text": ["Convert strings in the Series/Index to titlecase.", "Equivalent to str.title().", "See also", "Converts all characters to lowercase.", "Converts all characters to uppercase.", "Converts first character of each word to uppercase and remaining to lowercase.", "Converts first character to uppercase and remaining to lowercase.", "Converts uppercase to lowercase and lowercase to uppercase.", "Removes all case distinctions in the string.", "Examples"]}, {"name": "pandas.Series.str.translate", "path": "reference/api/pandas.series.str.translate", "type": "Series", "text": ["Map all characters in the string through the given mapping table.", "Equivalent to standard str.translate().", "Table is a mapping of Unicode ordinals to Unicode ordinals, strings, or None. Unmapped characters are left untouched. Characters mapped to None are deleted. str.maketrans() is a helper function for making translation tables."]}, {"name": "pandas.Series.str.upper", "path": "reference/api/pandas.series.str.upper", "type": "Series", "text": ["Convert strings in the Series/Index to uppercase.", "Equivalent to str.upper().", "See also", "Converts all characters to lowercase.", "Converts all characters to uppercase.", "Converts first character of each word to uppercase and remaining to lowercase.", "Converts first character to uppercase and remaining to lowercase.", "Converts uppercase to lowercase and lowercase to uppercase.", "Removes all case distinctions in the string.", "Examples"]}, {"name": "pandas.Series.str.wrap", "path": "reference/api/pandas.series.str.wrap", "type": "Series", "text": ["Wrap strings in Series/Index at specified line width.", "This method has the same keyword parameters and defaults as textwrap.TextWrapper.", "Maximum line width.", "If True, tab characters will be expanded to spaces (default: True).", "If True, each whitespace character (as defined by string.whitespace) remaining after tab expansion will be replaced by a single space (default: True).", "If True, whitespace that, after wrapping, happens to end up at the beginning or end of a line is dropped (default: True).", "If True, then words longer than width will be broken in order to ensure that no lines are longer than width. If it is false, long words will not be broken, and some lines may be longer than width (default: True).", "If True, wrapping will occur preferably on whitespace and right after hyphens in compound words, as it is customary in English. If false, only whitespaces will be considered as potentially good places for line breaks, but you need to set break_long_words to false if you want truly insecable words (default: True).", "Notes", "Internally, this method uses a textwrap.TextWrapper instance with default settings. To achieve behavior matching R\u2019s stringr library str_wrap function, use the arguments:", "expand_tabs = False", "replace_whitespace = True", "drop_whitespace = True", "break_long_words = False", "break_on_hyphens = False", "Examples"]}, {"name": "pandas.Series.str.zfill", "path": "reference/api/pandas.series.str.zfill", "type": "Series", "text": ["Pad strings in the Series/Index by prepending \u20180\u2019 characters.", "Strings in the Series/Index are padded with \u20180\u2019 characters on the left of the string to reach a total string length width. Strings in the Series/Index with length greater or equal to width are unchanged.", "Minimum length of resulting string; strings with length less than width be prepended with \u20180\u2019 characters.", "See also", "Fills the left side of strings with an arbitrary character.", "Fills the right side of strings with an arbitrary character.", "Fills the specified sides of strings with an arbitrary character.", "Fills both sides of strings with an arbitrary character.", "Notes", "Differs from str.zfill() which has special handling for \u2018+\u2019/\u2019-\u2019 in the string.", "Examples", "Note that 10 and NaN are not strings, therefore they are converted to NaN. The minus sign in '-1' is treated as a regular character and the zero is added to the left of it (str.zfill() would have moved it to the left). 1000 remains unchanged as it is longer than width."]}, {"name": "pandas.Series.sub", "path": "reference/api/pandas.series.sub", "type": "Series", "text": ["Return Subtraction of series and other, element-wise (binary operator sub).", "Equivalent to series - other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Reverse of the Subtraction operator, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.subtract", "path": "reference/api/pandas.series.subtract", "type": "Series", "text": ["Return Subtraction of series and other, element-wise (binary operator sub).", "Equivalent to series - other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Reverse of the Subtraction operator, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.sum", "path": "reference/api/pandas.series.sum", "type": "Series", "text": ["Return the sum of the values over the requested axis.", "This is equivalent to the method numpy.sum.", "Axis for the function to be applied on.", "Exclude NA/null values when computing the result.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.", "Additional keyword arguments to be passed to the function.", "See also", "Return the sum.", "Return the minimum.", "Return the maximum.", "Return the index of the minimum.", "Return the index of the maximum.", "Return the sum over the requested axis.", "Return the minimum over the requested axis.", "Return the maximum over the requested axis.", "Return the index of the minimum over the requested axis.", "Return the index of the maximum over the requested axis.", "Examples", "By default, the sum of an empty or all-NA Series is 0.", "This can be controlled with the min_count parameter. For example, if you\u2019d like the sum of an empty series to be NaN, pass min_count=1.", "Thanks to the skipna parameter, min_count handles all-NA and empty series identically."]}, {"name": "pandas.Series.swapaxes", "path": "reference/api/pandas.series.swapaxes", "type": "Series", "text": ["Interchange axes and swap values axes appropriately."]}, {"name": "pandas.Series.swaplevel", "path": "reference/api/pandas.series.swaplevel", "type": "Series", "text": ["Swap levels i and j in a MultiIndex.", "Default is to swap the two innermost levels of the index.", "Levels of the indices to be swapped. Can pass level name as string.", "Whether to copy underlying data.", "Series with levels swapped in MultiIndex.", "Examples", "In the following example, we will swap the levels of the indices. Here, we will swap the levels column-wise, but levels can be swapped row-wise in a similar manner. Note that column-wise is the default behaviour. By not supplying any arguments for i and j, we swap the last and second to last indices.", "By supplying one argument, we can choose which index to swap the last index with. We can for example swap the first index with the last one as follows.", "We can also define explicitly which indices we want to swap by supplying values for both i and j. Here, we for example swap the first and second indices."]}, {"name": "pandas.Series.T", "path": "reference/api/pandas.series.t", "type": "Series", "text": ["Return the transpose, which is by definition self."]}, {"name": "pandas.Series.tail", "path": "reference/api/pandas.series.tail", "type": "Series", "text": ["Return the last n rows.", "This function returns last n rows from the object based on position. It is useful for quickly verifying data, for example, after sorting or appending rows.", "For negative values of n, this function returns all rows except the first n rows, equivalent to df[n:].", "Number of rows to select.", "The last n rows of the caller object.", "See also", "The first n rows of the caller object.", "Examples", "Viewing the last 5 lines", "Viewing the last n lines (three in this case)", "For negative values of n"]}, {"name": "pandas.Series.take", "path": "reference/api/pandas.series.take", "type": "Series", "text": ["Return the elements in the given positional indices along an axis.", "This means that we are not indexing according to actual values in the index attribute of the object. We are indexing according to the actual position of the element in the object.", "An array of ints indicating which positions to take.", "The axis on which to select elements. 0 means that we are selecting rows, 1 means that we are selecting columns.", "Before pandas 1.0, is_copy=False can be specified to ensure that the return value is an actual copy. Starting with pandas 1.0, take always returns a copy, and the keyword is therefore deprecated.", "Deprecated since version 1.0.0.", "For compatibility with numpy.take(). Has no effect on the output.", "An array-like containing the elements taken from the object.", "See also", "Select a subset of a DataFrame by labels.", "Select a subset of a DataFrame by positions.", "Take elements from an array along an axis.", "Examples", "Take elements at positions 0 and 3 along the axis 0 (default).", "Note how the actual indices selected (0 and 1) do not correspond to our selected indices 0 and 3. That\u2019s because we are selecting the 0th and 3rd rows, not rows whose indices equal 0 and 3.", "Take elements at indices 1 and 2 along the axis 1 (column selection).", "We may take elements using negative integers for positive indices, starting from the end of the object, just like with Python lists."]}, {"name": "pandas.Series.to_clipboard", "path": "reference/api/pandas.series.to_clipboard", "type": "Series", "text": ["Copy object to the system clipboard.", "Write a text representation of object to the system clipboard. This can be pasted into Excel, for example.", "Produce output in a csv format for easy pasting into excel.", "True, use the provided separator for csv pasting.", "False, write a string representation of the object to the clipboard.", "Field delimiter.", "These parameters will be passed to DataFrame.to_csv.", "See also", "Write a DataFrame to a comma-separated values (csv) file.", "Read text from clipboard and pass to read_csv.", "Notes", "Requirements for your platform.", "Linux : xclip, or xsel (with PyQt4 modules)", "Windows : none", "macOS : none", "Examples", "Copy the contents of a DataFrame to the clipboard.", "We can omit the index by passing the keyword index and setting it to false."]}, {"name": "pandas.Series.to_csv", "path": "reference/api/pandas.series.to_csv", "type": "Series", "text": ["Write object to a comma-separated values (csv) file.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a write() function. If None, the result is returned as a string. If a non-binary file object is passed, it should be opened with newline=\u2019\u2019, disabling universal newlines. If a binary file object is passed, mode might need to contain a \u2018b\u2019.", "Changed in version 1.2.0: Support for binary file objects was introduced.", "String of length 1. Field delimiter for the output file.", "Missing data representation.", "Format string for floating point numbers.", "Columns to write.", "Write out the column names. If a list of strings is given it is assumed to be aliases for the column names.", "Write row names (index).", "Column label for index column(s) if desired. If None is given, and header and index are True, then the index names are used. A sequence should be given if the object uses MultiIndex. If False do not print fields for index names. Use index_label=False for easier importing in R.", "Python write mode, default \u2018w\u2019.", "A string representing the encoding to use in the output file, defaults to \u2018utf-8\u2019. encoding is not supported if path_or_buf is a non-binary file object.", "For on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018%s\u2019 path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.", "Changed in version 1.0.0: May now be a dict with key \u2018method\u2019 as compression mode and other entries as additional compression options if compression mode is \u2018zip\u2019.", "Changed in version 1.1.0: Passing compression options as keys in dict is supported for compression modes \u2018gzip\u2019, \u2018bz2\u2019, \u2018zstd\u2019, and \u2018zip\u2019.", "Changed in version 1.2.0: Compression is supported for binary file objects.", "Changed in version 1.2.0: Previous versions forwarded dict entries for \u2018gzip\u2019 to gzip.open instead of gzip.GzipFile which prevented setting mtime.", "Defaults to csv.QUOTE_MINIMAL. If you have set a float_format then floats are converted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-numeric.", "String of length 1. Character used to quote fields.", "The newline character or character sequence to use in the output file. Defaults to os.linesep, which depends on the OS in which this method is called (\u2019\\n\u2019 for linux, \u2018\\r\\n\u2019 for Windows, i.e.).", "Rows to write at a time.", "Format string for datetime objects.", "Control quoting of quotechar inside a field.", "String of length 1. Character used to escape sep and quotechar when appropriate.", "Character recognized as decimal separator. E.g. use \u2018,\u2019 for European data.", "Specifies how encoding and decoding errors are to be handled. See the errors argument for open() for a full list of options.", "New in version 1.1.0.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.0.", "If path_or_buf is None, returns the resulting csv format as a string. Otherwise returns None.", "See also", "Load a CSV file into a DataFrame.", "Write DataFrame to an Excel file.", "Examples", "Create \u2018out.zip\u2019 containing \u2018out.csv\u2019", "To write a csv file to a new folder or nested folder you will first need to create it using either Pathlib or os:"]}, {"name": "pandas.Series.to_dict", "path": "reference/api/pandas.series.to_dict", "type": "Series", "text": ["Convert Series to {label -> value} dict or dict-like object.", "The collections.abc.Mapping subclass to use as the return object. Can be the actual class or an empty instance of the mapping type you want. If you want a collections.defaultdict, you must pass it initialized.", "Key-value representation of Series.", "Examples"]}, {"name": "pandas.Series.to_excel", "path": "reference/api/pandas.series.to_excel", "type": "Series", "text": ["Write object to an Excel sheet.", "To write a single object to an Excel .xlsx file it is only necessary to specify a target file name. To write to multiple sheets it is necessary to create an ExcelWriter object with a target file name, and specify a sheet in the file to write to.", "Multiple sheets may be written to by specifying unique sheet_name. With all data written to the file it is necessary to save the changes. Note that creating an ExcelWriter object with a file name that already exists will result in the contents of the existing file being erased.", "File path or existing ExcelWriter.", "Name of sheet which will contain DataFrame.", "Missing data representation.", "Format string for floating point numbers. For example float_format=\"%.2f\" will format 0.1234 to 0.12.", "Columns to write.", "Write out the column names. If a list of string is given it is assumed to be aliases for the column names.", "Write row names (index).", "Column label for index column(s) if desired. If not specified, and header and index are True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex.", "Upper left cell row to dump data frame.", "Upper left cell column to dump data frame.", "Write engine to use, \u2018openpyxl\u2019 or \u2018xlsxwriter\u2019. You can also set this via the options io.excel.xlsx.writer, io.excel.xls.writer, and io.excel.xlsm.writer.", "Deprecated since version 1.2.0: As the xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas.", "Write MultiIndex and Hierarchical Rows as merged cells.", "Encoding of the resulting excel file. Only necessary for xlwt, other writers support unicode natively.", "Representation for infinity (there is no native representation for infinity in Excel).", "Display more information in the error logs.", "Specifies the one-based bottommost row and rightmost column that is to be frozen.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.0.", "See also", "Write DataFrame to a comma-separated values (csv) file.", "Class for writing DataFrame objects into excel sheets.", "Read an Excel file into a pandas DataFrame.", "Read a comma-separated values (csv) file into DataFrame.", "Notes", "For compatibility with to_csv(), to_excel serializes lists and dicts to strings before writing.", "Once a workbook has been saved it is not possible to write further data without rewriting the whole workbook.", "Examples", "Create, write to and save a workbook:", "To specify the sheet name:", "If you wish to write to more than one sheet in the workbook, it is necessary to specify an ExcelWriter object:", "ExcelWriter can also be used to append to an existing Excel file:", "To set the library that is used to write the Excel file, you can pass the engine keyword (the default engine is automatically chosen depending on the file extension):"]}, {"name": "pandas.Series.to_frame", "path": "reference/api/pandas.series.to_frame", "type": "Series", "text": ["Convert Series to DataFrame.", "The passed name should substitute for the series name (if it has one).", "DataFrame representation of Series.", "Examples"]}, {"name": "pandas.Series.to_hdf", "path": "reference/api/pandas.series.to_hdf", "type": "Series", "text": ["Write the contained data to an HDF5 file using HDFStore.", "Hierarchical Data Format (HDF) is self-describing, allowing an application to interpret the structure and contents of a file with no outside information. One HDF file can hold a mix of related objects which can be accessed as a group or as individual objects.", "In order to add another DataFrame or Series to an existing HDF file please use append mode and a different a key.", "Warning", "One can store a subclass of DataFrame or Series to HDF5, but the type of the subclass is lost upon storing.", "For more information see the user guide.", "File path or HDFStore object.", "Identifier for the group in the store.", "Mode to open file:", "\u2018w\u2019: write, a new file is created (an existing file with the same name would be deleted).", "\u2018a\u2019: append, an existing file is opened for reading and writing, and if the file does not exist it is created.", "\u2018r+\u2019: similar to \u2018a\u2019, but the file must already exist.", "Specifies a compression level for data. A value of 0 or None disables compression.", "Specifies the compression library to be used. As of v0.20.2 these additional compressors for Blosc are supported (default if no compressor specified: \u2018blosc:blosclz\u2019): {\u2018blosc:blosclz\u2019, \u2018blosc:lz4\u2019, \u2018blosc:lz4hc\u2019, \u2018blosc:snappy\u2019, \u2018blosc:zlib\u2019, \u2018blosc:zstd\u2019}. Specifying a compression library which is not available issues a ValueError.", "For Table formats, append the input data to the existing.", "Possible values:", "\u2018fixed\u2019: Fixed format. Fast writing/reading. Not-appendable, nor searchable.", "\u2018table\u2019: Table format. Write as a PyTables Table structure which may perform worse but allow more flexible operations like searching / selecting subsets of the data.", "If None, pd.get_option(\u2018io.hdf.default_format\u2019) is checked, followed by fallback to \u201cfixed\u201d.", "Specifies how encoding and decoding errors are to be handled. See the errors argument for open() for a full list of options.", "Map column names to minimum string sizes for columns.", "How to represent null values as str. Not allowed with append=True.", "List of columns to create as indexed data columns for on-disk queries, or True to use all columns. By default only the axes of the object are indexed. See Query via data columns. Applicable only to format=\u2019table\u2019.", "See also", "Read from HDF file.", "Write a DataFrame to the binary parquet format.", "Write to a SQL table.", "Write out feather-format for DataFrames.", "Write out to a csv file.", "Examples", "We can add another object to the same file:", "Reading from HDF file:"]}, {"name": "pandas.Series.to_json", "path": "reference/api/pandas.series.to_json", "type": "Series", "text": ["Convert the object to a JSON string.", "Note NaN\u2019s and None will be converted to null and datetime objects will be converted to UNIX timestamps.", "String, path object (implementing os.PathLike[str]), or file-like object implementing a write() function. If None, the result is returned as a string.", "Indication of expected JSON string format.", "Series:", "default is \u2018index\u2019", "allowed values are: {\u2018split\u2019, \u2018records\u2019, \u2018index\u2019, \u2018table\u2019}.", "DataFrame:", "default is \u2018columns\u2019", "allowed values are: {\u2018split\u2019, \u2018records\u2019, \u2018index\u2019, \u2018columns\u2019, \u2018values\u2019, \u2018table\u2019}.", "The format of the JSON string:", "\u2018split\u2019 : dict like {\u2018index\u2019 -> [index], \u2018columns\u2019 -> [columns], \u2018data\u2019 -> [values]}", "\u2018records\u2019 : list like [{column -> value}, \u2026 , {column -> value}]", "\u2018index\u2019 : dict like {index -> {column -> value}}", "\u2018columns\u2019 : dict like {column -> {index -> value}}", "\u2018values\u2019 : just the values array", "\u2018table\u2019 : dict like {\u2018schema\u2019: {schema}, \u2018data\u2019: {data}}", "Describing the data, where data component is like orient='records'.", "Type of date conversion. \u2018epoch\u2019 = epoch milliseconds, \u2018iso\u2019 = ISO8601. The default depends on the orient. For orient='table', the default is \u2018iso\u2019. For all other orients, the default is \u2018epoch\u2019.", "The number of decimal places to use when encoding floating point values.", "Force encoded string to be ASCII.", "The time unit to encode to, governs timestamp and ISO8601 precision. One of \u2018s\u2019, \u2018ms\u2019, \u2018us\u2019, \u2018ns\u2019 for second, millisecond, microsecond, and nanosecond respectively.", "Handler to call if object cannot otherwise be converted to a suitable format for JSON. Should receive a single argument which is the object to convert and return a serialisable object.", "If \u2018orient\u2019 is \u2018records\u2019 write out line-delimited json format. Will throw ValueError if incorrect \u2018orient\u2019 since others are not list-like.", "For on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018path_or_buf\u2019 path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.", "Changed in version 1.4.0: Zstandard support.", "Whether to include the index values in the JSON string. Not including the index (index=False) is only supported when orient is \u2018split\u2019 or \u2018table\u2019.", "Length of whitespace used to indent each record.", "New in version 1.0.0.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.0.", "If path_or_buf is None, returns the resulting json format as a string. Otherwise returns None.", "See also", "Convert a JSON string to pandas object.", "Notes", "The behavior of indent=0 varies from the stdlib, which does not indent the output but does insert newlines. Currently, indent=0 and the default indent=None are equivalent in pandas, though this may change in a future release.", "orient='table' contains a \u2018pandas_version\u2019 field under \u2018schema\u2019. This stores the version of pandas used in the latest revision of the schema.", "Examples", "Encoding/decoding a Dataframe using 'records' formatted JSON. Note that index labels are not preserved with this encoding.", "Encoding/decoding a Dataframe using 'index' formatted JSON:", "Encoding/decoding a Dataframe using 'columns' formatted JSON:", "Encoding/decoding a Dataframe using 'values' formatted JSON:", "Encoding with Table Schema:"]}, {"name": "pandas.Series.to_latex", "path": "reference/api/pandas.series.to_latex", "type": "Series", "text": ["Render object to a LaTeX tabular, longtable, or nested table.", "Requires \\usepackage{booktabs}. The output can be copy/pasted into a main LaTeX document or read from an external file with \\input{table.tex}.", "Changed in version 1.0.0: Added caption and label arguments.", "Changed in version 1.2.0: Added position argument, changed meaning of caption argument.", "Buffer to write to. If None, the output is returned as a string.", "The subset of columns to write. Writes all columns by default.", "The minimum width of each column.", "Write out the column names. If a list of strings is given, it is assumed to be aliases for the column names.", "Write row names (index).", "Missing data representation.", "Formatter functions to apply to columns\u2019 elements by position or name. The result of each function must be a unicode string. List must be of length equal to the number of columns.", "Formatter for floating point numbers. For example float_format=\"%.2f\" and float_format=\"{:0.2f}\".format will both result in 0.1234 being formatted as 0.12.", "Set to False for a DataFrame with a hierarchical index to print every multiindex key at each row. By default, the value will be read from the config module.", "Prints the names of the indexes.", "Make the row labels bold in the output.", "The columns format as specified in LaTeX table format e.g. \u2018rcl\u2019 for 3 columns. By default, \u2018l\u2019 will be used for all columns except columns of numbers, which default to \u2018r\u2019.", "By default, the value will be read from the pandas config module. Use a longtable environment instead of tabular. Requires adding a usepackage{longtable} to your LaTeX preamble.", "By default, the value will be read from the pandas config module. When set to False prevents from escaping latex special characters in column names.", "A string representing the encoding to use in the output file, defaults to \u2018utf-8\u2019.", "Character recognized as decimal separator, e.g. \u2018,\u2019 in Europe.", "Use multicolumn to enhance MultiIndex columns. The default will be read from the config module.", "The alignment for multicolumns, similar to column_format The default will be read from the config module.", "Use multirow to enhance MultiIndex rows. Requires adding a usepackage{multirow} to your LaTeX preamble. Will print centered labels (instead of top-aligned) across the contained rows, separating groups via clines. The default will be read from the pandas config module.", "Tuple (full_caption, short_caption), which results in \\caption[short_caption]{full_caption}; if a single string is passed, no short caption will be set.", "New in version 1.0.0.", "Changed in version 1.2.0: Optionally allow caption to be a tuple (full_caption, short_caption).", "The LaTeX label to be placed inside \\label{} in the output. This is used with \\ref{} in the main .tex file.", "New in version 1.0.0.", "The LaTeX positional argument for tables, to be placed after \\begin{} in the output.", "New in version 1.2.0.", "If buf is None, returns the result as a string. Otherwise returns None.", "See also", "Render a DataFrame to LaTeX with conditional formatting.", "Render a DataFrame to a console-friendly tabular output.", "Render a DataFrame as an HTML table.", "Examples"]}, {"name": "pandas.Series.to_list", "path": "reference/api/pandas.series.to_list", "type": "Series", "text": ["Return a list of the values.", "These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)", "See also", "Return the array as an a.ndim-levels deep nested list of Python scalars."]}, {"name": "pandas.Series.to_markdown", "path": "reference/api/pandas.series.to_markdown", "type": "Series", "text": ["Print Series in Markdown-friendly format.", "New in version 1.0.0.", "Buffer to write to. If None, the output is returned as a string.", "Mode in which file is opened, \u201cwt\u201d by default.", "Add index (row) labels.", "New in version 1.1.0.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.0.", "These parameters will be passed to tabulate.", "Series in Markdown-friendly format.", "Notes", "Requires the tabulate package.", "Examples", "Output markdown with a tabulate option."]}, {"name": "pandas.Series.to_numpy", "path": "reference/api/pandas.series.to_numpy", "type": "Series", "text": ["A NumPy ndarray representing the values in this Series or Index.", "The dtype to pass to numpy.asarray().", "Whether to ensure that the returned value is not a view on another array. Note that copy=False does not ensure that to_numpy() is no-copy. Rather, copy=True ensure that a copy is made, even if not strictly necessary.", "The value to use for missing values. The default value depends on dtype and the type of the array.", "New in version 1.0.0.", "Additional keywords passed through to the to_numpy method of the underlying array (for extension arrays).", "New in version 1.0.0.", "See also", "Get the actual data stored within.", "Get the actual data stored within.", "Similar method for DataFrame.", "Notes", "The returned array will be the same up to equality (values equal in self will be equal in the returned array; likewise for values that are not equal). When self contains an ExtensionArray, the dtype may be different. For example, for a category-dtype Series, to_numpy() will return a NumPy array and the categorical dtype will be lost.", "For NumPy dtypes, this will be a reference to the actual data stored in this Series or Index (assuming copy=False). Modifying the result in place will modify the data stored in the Series or Index (not that we recommend doing that).", "For extension types, to_numpy() may require copying data and coercing the result to a NumPy type (possibly object), which may be expensive. When you need a no-copy reference to the underlying data, Series.array should be used instead.", "This table lays out the different dtypes and default return types of to_numpy() for various dtypes within pandas.", "dtype", "array type", "category[T]", "ndarray[T] (same dtype as input)", "period", "ndarray[object] (Periods)", "interval", "ndarray[object] (Intervals)", "IntegerNA", "ndarray[object]", "datetime64[ns]", "datetime64[ns]", "datetime64[ns, tz]", "ndarray[object] (Timestamps)", "Examples", "Specify the dtype to control how datetime-aware data is represented. Use dtype=object to return an ndarray of pandas Timestamp objects, each with the correct tz.", "Or dtype='datetime64[ns]' to return an ndarray of native datetime64 values. The values are converted to UTC and the timezone info is dropped."]}, {"name": "pandas.Series.to_period", "path": "reference/api/pandas.series.to_period", "type": "Input/output", "text": ["Convert Series from DatetimeIndex to PeriodIndex.", "Frequency associated with the PeriodIndex.", "Whether or not to return a copy.", "Series with index converted to PeriodIndex."]}, {"name": "pandas.Series.to_pickle", "path": "reference/api/pandas.series.to_pickle", "type": "Series", "text": ["Pickle (serialize) object to file.", "File path where the pickled object will be stored.", "For on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018path\u2019 path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.", "Int which indicates which protocol should be used by the pickler, default HIGHEST_PROTOCOL (see [1] paragraph 12.1.2). The possible values are 0, 1, 2, 3, 4, 5. A negative value for the protocol parameter is equivalent to setting its value to HIGHEST_PROTOCOL.", "https://docs.python.org/3/library/pickle.html.", "Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.", "New in version 1.2.0.", "See also", "Load pickled pandas object (or any object) from file.", "Write DataFrame to an HDF5 file.", "Write DataFrame to a SQL database.", "Write a DataFrame to the binary parquet format.", "Examples"]}, {"name": "pandas.Series.to_sql", "path": "reference/api/pandas.series.to_sql", "type": "Series", "text": ["Write records stored in a DataFrame to a SQL database.", "Databases supported by SQLAlchemy [1] are supported. Tables can be newly created, appended to, or overwritten.", "Name of SQL table.", "Using SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable See here.", "Specify the schema (if database flavor supports this). If None, use default schema.", "How to behave if the table already exists.", "fail: Raise a ValueError.", "replace: Drop the table before inserting new values.", "append: Insert new values to the existing table.", "Write DataFrame index as a column. Uses index_label as the column name in the table.", "Column label for index column(s). If None is given (default) and index is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex.", "Specify the number of rows in each batch to be written at a time. By default, all rows will be written at once.", "Specifying the datatype for columns. If a dictionary is used, the keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. If a scalar is provided, it will be applied to all columns.", "Controls the SQL insertion clause used:", "None : Uses standard SQL INSERT clause (one per row).", "\u2018multi\u2019: Pass multiple values in a single INSERT clause.", "callable with signature (pd_table, conn, keys, data_iter).", "Details and a sample callable implementation can be found in the section insert method.", "Number of rows affected by to_sql. None is returned if the callable passed into method does not return the number of rows.", "The number of returned rows affected is the sum of the rowcount attribute of sqlite3.Cursor or SQLAlchemy connectable which may not reflect the exact number of written rows as stipulated in the sqlite3 or SQLAlchemy.", "New in version 1.4.0.", "When the table already exists and if_exists is \u2018fail\u2019 (the default).", "See also", "Read a DataFrame from a table.", "Notes", "Timezone aware datetime columns will be written as Timestamp with timezone type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone.", "References", "https://docs.sqlalchemy.org", "https://www.python.org/dev/peps/pep-0249/", "Examples", "Create an in-memory SQLite database.", "Create a table from scratch with 3 rows.", "An sqlalchemy.engine.Connection can also be passed to con:", "This is allowed to support operations that require that the same DBAPI connection is used for the entire operation.", "Overwrite the table with just df2.", "Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars."]}, {"name": "pandas.Series.to_string", "path": "reference/api/pandas.series.to_string", "type": "Series", "text": ["Render a string representation of the Series.", "Buffer to write to.", "String representation of NaN to use, default \u2018NaN\u2019.", "Formatter function to apply to columns\u2019 elements if they are floats, default None.", "Add the Series header (index name).", "Add index (row) labels, default True.", "Add the Series length.", "Add the Series dtype.", "Add the Series name if not None.", "Maximum number of rows to show before truncating. If None, show all.", "The number of rows to display in a truncated repr (when number of rows is above max_rows).", "String representation of Series if buf=None, otherwise None."]}, {"name": "pandas.Series.to_timestamp", "path": "reference/api/pandas.series.to_timestamp", "type": "Series", "text": ["Cast to DatetimeIndex of Timestamps, at beginning of period.", "Desired frequency.", "Convention for converting period to timestamp; start of period vs. end.", "Whether or not to return a copy."]}, {"name": "pandas.Series.to_xarray", "path": "reference/api/pandas.series.to_xarray", "type": "Series", "text": ["Return an xarray object from the pandas object.", "Data in the pandas structure converted to Dataset if the object is a DataFrame, or a DataArray if the object is a Series.", "See also", "Write DataFrame to an HDF5 file.", "Write a DataFrame to the binary parquet format.", "Notes", "See the xarray docs", "Examples"]}, {"name": "pandas.Series.tolist", "path": "reference/api/pandas.series.tolist", "type": "Series", "text": ["Return a list of the values.", "These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)", "See also", "Return the array as an a.ndim-levels deep nested list of Python scalars."]}, {"name": "pandas.Series.transform", "path": "reference/api/pandas.series.transform", "type": "Series", "text": ["Call func on self producing a Series with the same axis shape as self.", "Function to use for transforming the data. If a function, must either work when passed a Series or when passed to Series.apply. If func is both list-like and dict-like, dict-like behavior takes precedence.", "Accepted combinations are:", "function", "string function name", "list-like of functions and/or function names, e.g. [np.exp, 'sqrt']", "dict-like of axis labels -> functions, function names or list-like of such.", "Parameter needed for compatibility with DataFrame.", "Positional arguments to pass to func.", "Keyword arguments to pass to func.", "A Series that must have the same length as self.", "See also", "Only perform aggregating type operations.", "Invoke function on a Series.", "Notes", "Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.", "Examples", "Even though the resulting Series must have the same length as the input Series, it is possible to provide several input functions:", "You can call transform on a GroupBy object:"]}, {"name": "pandas.Series.transpose", "path": "reference/api/pandas.series.transpose", "type": "Series", "text": ["Return the transpose, which is by definition self."]}, {"name": "pandas.Series.truediv", "path": "reference/api/pandas.series.truediv", "type": "Series", "text": ["Return Floating division of series and other, element-wise (binary operator truediv).", "Equivalent to series / other, but with support to substitute a fill_value for missing data in either one of the inputs.", "Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.", "Broadcast across a level, matching Index values on the passed MultiIndex level.", "The result of the operation.", "See also", "Reverse of the Floating division operator, see Python documentation for more details.", "Examples"]}, {"name": "pandas.Series.truncate", "path": "reference/api/pandas.series.truncate", "type": "Series", "text": ["Truncate a Series or DataFrame before and after some index value.", "This is a useful shorthand for boolean indexing based on index values above or below certain thresholds.", "Truncate all rows before this index value.", "Truncate all rows after this index value.", "Axis to truncate. Truncates the index (rows) by default.", "Return a copy of the truncated section.", "The truncated Series or DataFrame.", "See also", "Select a subset of a DataFrame by label.", "Select a subset of a DataFrame by position.", "Notes", "If the index being truncated contains only datetime values, before and after may be specified as strings instead of Timestamps.", "Examples", "The columns of a DataFrame can be truncated.", "For Series, only rows can be truncated.", "The index values in truncate can be datetimes or string dates.", "Because the index is a DatetimeIndex containing only dates, we can specify before and after as strings. They will be coerced to Timestamps before truncation.", "Note that truncate assumes a 0 value for any unspecified time component (midnight). This differs from partial string slicing, which returns any partially matching dates."]}, {"name": "pandas.Series.tshift", "path": "reference/api/pandas.series.tshift", "type": "Series", "text": ["Shift the time index, using the index\u2019s frequency if available.", "Deprecated since version 1.1.0: Use shift instead.", "Number of periods to move, can be positive or negative.", "Increment to use from the tseries module or time rule expressed as a string (e.g. \u2018EOM\u2019).", "Corresponds to the axis that contains the Index.", "Notes", "If freq is not specified then tries to use the freq or inferred_freq attributes of the index. If neither of those attributes exist, a ValueError is thrown"]}, {"name": "pandas.Series.tz_convert", "path": "reference/api/pandas.series.tz_convert", "type": "Series", "text": ["Convert tz-aware axis to target time zone.", "If axis is a MultiIndex, convert a specific level. Otherwise must be None.", "Also make a copy of the underlying data.", "Object with time zone converted axis.", "If the axis is tz-naive."]}, {"name": "pandas.Series.tz_localize", "path": "reference/api/pandas.series.tz_localize", "type": "Series", "text": ["Localize tz-naive index of a Series or DataFrame to target time zone.", "This operation localizes the Index. To localize the values in a timezone-naive Series, use Series.dt.tz_localize().", "If axis ia a MultiIndex, localize a specific level. Otherwise must be None.", "Also make a copy of the underlying data.", "When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled.", "\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order", "bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times)", "\u2018NaT\u2019 will return NaT where there are ambiguous times", "\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. Valid values are:", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time", "\u2018NaT\u2019 will return NaT where there are nonexistent times", "timedelta objects will shift nonexistent times by the timedelta", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "Same type as the input.", "If the TimeSeries is tz-aware and tz is not None.", "Examples", "Localize local times:", "Be careful with DST changes. When there is sequential data, pandas can infer the DST time:", "In some cases, inferring the DST is impossible. In such cases, you can pass an ndarray to the ambiguous parameter to set the DST explicitly", "If the DST transition causes nonexistent times, you can shift these dates forward or backward with a timedelta object or \u2018shift_forward\u2019 or \u2018shift_backward\u2019."]}, {"name": "pandas.Series.unique", "path": "reference/api/pandas.series.unique", "type": "Series", "text": ["Return unique values of Series object.", "Uniques are returned in order of appearance. Hash table-based unique, therefore does NOT sort.", "The unique values returned as a NumPy array. See Notes.", "See also", "Top-level unique method for any 1-d array-like object.", "Return Index with unique values from an Index object.", "Notes", "Returns the unique values as a NumPy array. In case of an extension-array backed Series, a new ExtensionArray of that type with just the unique values is returned. This includes", "Categorical", "Period", "Datetime with Timezone", "Interval", "Sparse", "IntegerNA", "See Examples section.", "Examples", "An Categorical will return categories in the order of appearance and with the same dtype."]}, {"name": "pandas.Series.unstack", "path": "reference/api/pandas.series.unstack", "type": "Series", "text": ["Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.", "Level(s) to unstack, can pass level name.", "Value to use when replacing NaN values.", "Unstacked Series.", "Examples"]}, {"name": "pandas.Series.update", "path": "reference/api/pandas.series.update", "type": "Series", "text": ["Modify Series in place using values from passed Series.", "Uses non-NA values from passed Series to make updates. Aligns on index.", "Examples", "If other contains NaNs the corresponding values are not updated in the original Series.", "other can also be a non-Series object type that is coercible into a Series"]}, {"name": "pandas.Series.value_counts", "path": "reference/api/pandas.series.value_counts", "type": "Series", "text": ["Return a Series containing counts of unique values.", "The resulting object will be in descending order so that the first element is the most frequently-occurring element. Excludes NA values by default.", "If True then the object returned will contain the relative frequencies of the unique values.", "Sort by frequencies.", "Sort in ascending order.", "Rather than count values, group them into half-open bins, a convenience for pd.cut, only works with numeric data.", "Don\u2019t include counts of NaN.", "See also", "Number of non-NA elements in a Series.", "Number of non-NA elements in a DataFrame.", "Equivalent method on DataFrames.", "Examples", "With normalize set to True, returns the relative frequency by dividing all values by the sum of values.", "bins", "Bins can be useful for going from a continuous variable to a categorical variable; instead of counting unique apparitions of values, divide the index in the specified number of half-open bins.", "dropna", "With dropna set to False we can also see NaN index values."]}, {"name": "pandas.Series.values", "path": "reference/api/pandas.series.values", "type": "Series", "text": ["Return Series as ndarray or ndarray-like depending on the dtype.", "Warning", "We recommend using Series.array or Series.to_numpy(), depending on whether you need a reference to the underlying data or a NumPy array.", "See also", "Reference to the underlying data.", "A NumPy array representing the underlying data.", "Examples", "Timezone aware datetime data is converted to UTC:"]}, {"name": "pandas.Series.var", "path": "reference/api/pandas.series.var", "type": "Series", "text": ["Return unbiased variance over requested axis.", "Normalized by N-1 by default. This can be changed using the ddof argument.", "Exclude NA/null values. If an entire row/column is NA, the result will be NA.", "If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar.", "Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.", "Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.", "Examples", "Alternatively, ddof=0 can be set to normalize by N instead of N-1:"]}, {"name": "pandas.Series.view", "path": "reference/api/pandas.series.view", "type": "Series", "text": ["Create a new view of the Series.", "This function will return a new Series with a view of the same underlying values in memory, optionally reinterpreted with a new data type. The new data type must preserve the same size in bytes as to not cause index misalignment.", "Data type object or one of their string representations.", "A new Series object as a view of the same data in memory.", "See also", "Equivalent numpy function to create a new view of the same data in memory.", "Notes", "Series are instantiated with dtype=float64 by default. While numpy.ndarray.view() will return a view with the same data type as the original array, Series.view() (without specified dtype) will try using float64 and may fail if the original data type size in bytes is not the same.", "Examples", "The 8 bit signed integer representation of -1 is 0b11111111, but the same bytes represent 255 if read as an 8 bit unsigned integer:", "The views share the same underlying values:"]}, {"name": "pandas.Series.where", "path": "reference/api/pandas.series.where", "type": "Series", "text": ["Replace values where the condition is False.", "Where cond is True, keep the original value. Where False, replace with corresponding value from other. If cond is callable, it is computed on the Series/DataFrame and should return boolean Series/DataFrame or array. The callable must not change input Series/DataFrame (though pandas doesn\u2019t check it).", "Entries where cond is False are replaced with corresponding value from other. If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn\u2019t check it).", "Whether to perform the operation in place on the data.", "Alignment axis if needed.", "Alignment level if needed.", "Note that currently this parameter won\u2019t affect the results and will always coerce to a suitable dtype.", "\u2018raise\u2019 : allow exceptions to be raised.", "\u2018ignore\u2019 : suppress exceptions. On error return original object.", "Try to cast the result back to the input type (if possible).", "Deprecated since version 1.3.0: Manually cast back if necessary.", "See also", "Return an object of same shape as self.", "Notes", "The where method is an application of the if-then idiom. For each element in the calling DataFrame, if cond is True the element is used; otherwise the corresponding element from the DataFrame other is used.", "The signature for DataFrame.where() differs from numpy.where(). Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2).", "For further details and examples see the where documentation in indexing.", "Examples"]}, {"name": "pandas.Series.xs", "path": "reference/api/pandas.series.xs", "type": "Series", "text": ["Return cross-section from the Series/DataFrame.", "This method takes a key argument to select data at a particular level of a MultiIndex.", "Label contained in the index, or partially in a MultiIndex.", "Axis to retrieve cross-section on.", "In case of a key partially contained in a MultiIndex, indicate which levels are used. Levels can be referred by label or position.", "If False, returns object with same levels as self.", "Cross-section from the original Series or DataFrame corresponding to the selected index levels.", "See also", "Access a group of rows and columns by label(s) or a boolean array.", "Purely integer-location based indexing for selection by position.", "Notes", "xs can not be used to set values.", "MultiIndex Slicers is a generic way to get/set values on any level or levels. It is a superset of xs functionality, see MultiIndex Slicers.", "Examples", "Get values at specified index", "Get values at several indexes", "Get values at specified index and level", "Get values at several indexes and levels", "Get values at specified column and axis"]}, {"name": "pandas.set_option", "path": "reference/api/pandas.set_option", "type": "General utility functions", "text": ["Sets the value of the specified option.", "Available options:", "compute.[use_bottleneck, use_numba, use_numexpr]", "display.[chop_threshold, colheader_justify, column_space, date_dayfirst, date_yearfirst, encoding, expand_frame_repr, float_format]", "display.html.[border, table_schema, use_mathjax]", "display.[large_repr]", "display.latex.[escape, longtable, multicolumn, multicolumn_format, multirow, repr]", "display.[max_categories, max_columns, max_colwidth, max_dir_items, max_info_columns, max_info_rows, max_rows, max_seq_items, memory_usage, min_rows, multi_sparse, notebook_repr_html, pprint_nest_depth, precision, show_dimensions]", "display.unicode.[ambiguous_as_wide, east_asian_width]", "display.[width]", "io.excel.ods.[reader, writer]", "io.excel.xls.[reader, writer]", "io.excel.xlsb.[reader]", "io.excel.xlsm.[reader, writer]", "io.excel.xlsx.[reader, writer]", "io.hdf.[default_format, dropna_table]", "io.parquet.[engine]", "io.sql.[engine]", "mode.[chained_assignment, data_manager, sim_interactive, string_storage, use_inf_as_na, use_inf_as_null]", "plotting.[backend]", "plotting.matplotlib.[register_converters]", "styler.format.[decimal, escape, formatter, na_rep, precision, thousands]", "styler.html.[mathjax]", "styler.latex.[environment, hrules, multicol_align, multirow_align]", "styler.render.[encoding, max_columns, max_elements, max_rows, repr]", "styler.sparse.[columns, index]", "Regexp which should match a single option. Note: partial matches are supported for convenience, but unless you use the full option name (e.g. x.y.z.option_name), your code may break in future versions if new options with similar names are introduced.", "New value of option.", "Notes", "The available options with its descriptions:", "Use the bottleneck library to accelerate if it is installed, the default is True Valid values: False,True [default: True] [currently: True]", "Use the numba engine option for select operations if it is installed, the default is False Valid values: False,True [default: False] [currently: False]", "Use the numexpr library to accelerate computation if it is installed, the default is True Valid values: False,True [default: True] [currently: True]", "if set to a float value, all float values smaller then the given threshold will be displayed as exactly 0 by repr and friends. [default: None] [currently: None]", "Controls the justification of column headers. used by DataFrameFormatter. [default: right] [currently: right]", "[default: 12] [currently: 12]", "When True, prints and parses dates with the day first, eg 20/01/2005 [default: False] [currently: False]", "When True, prints and parses dates with the year first, eg 2005/01/20 [default: False] [currently: False]", "Defaults to the detected encoding of the console. Specifies the encoding to be used for strings returned by to_string, these are generally strings meant to be displayed on the console. [default: utf-8] [currently: utf-8]", "Whether to print out the full DataFrame repr for wide DataFrames across multiple lines, max_columns is still respected, but the output will wrap-around across multiple \u201cpages\u201d if its width exceeds display.width. [default: True] [currently: True]", "The callable should accept a floating point number and return a string with the desired format of the number. This is used in some places like SeriesFormatter. See formats.format.EngFormatter for an example. [default: None] [currently: None]", "A border=value attribute is inserted in the <table> tag for the DataFrame HTML repr. [default: 1] [currently: 1]", "Whether to publish a Table Schema representation for frontends that support it. (default: False) [default: False] [currently: False]", "When True, Jupyter notebook will process table contents using MathJax, rendering mathematical expressions enclosed by the dollar symbol. (default: True) [default: True] [currently: True]", "For DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show a truncated table (the default from 0.13), or switch to the view from df.info() (the behaviour in earlier versions of pandas). [default: truncate] [currently: truncate]", "This specifies if the to_latex method of a Dataframe uses escapes special characters. Valid values: False,True [default: True] [currently: True]", "This specifies if the to_latex method of a Dataframe uses the longtable format. Valid values: False,True [default: False] [currently: False]", "This specifies if the to_latex method of a Dataframe uses multicolumns to pretty-print MultiIndex columns. Valid values: False,True [default: True] [currently: True]", "This specifies if the to_latex method of a Dataframe uses multicolumns to pretty-print MultiIndex columns. Valid values: False,True [default: l] [currently: l]", "This specifies if the to_latex method of a Dataframe uses multirows to pretty-print MultiIndex rows. Valid values: False,True [default: False] [currently: False]", "Whether to produce a latex DataFrame representation for jupyter environments that support it. (default: False) [default: False] [currently: False]", "This sets the maximum number of categories pandas should output when printing out a Categorical or a Series of dtype \u201ccategory\u201d. [default: 8] [currently: 8]", "If max_cols is exceeded, switch to truncate view. Depending on large_repr, objects are either centrally truncated or printed as a summary view. \u2018None\u2019 value means unlimited.", "In case python/IPython is running in a terminal and large_repr equals \u2018truncate\u2019 this can be set to 0 and pandas will auto-detect the width of the terminal and print a truncated object which fits the screen width. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection. [default: 0] [currently: 0]", "The maximum width in characters of a column in the repr of a pandas data structure. When the column overflows, a \u201c\u2026\u201d placeholder is embedded in the output. A \u2018None\u2019 value means unlimited. [default: 50] [currently: 50]", "The number of items that will be added to dir(\u2026). \u2018None\u2019 value means unlimited. Because dir is cached, changing this option will not immediately affect already existing dataframes until a column is deleted or added.", "This is for instance used to suggest columns from a dataframe to tab completion. [default: 100] [currently: 100]", "max_info_columns is used in DataFrame.info method to decide if per column information will be printed. [default: 100] [currently: 100]", "df.info() will usually show null-counts for each column. For large frames this can be quite slow. max_info_rows and max_info_cols limit this null check only to frames with smaller dimensions than specified. [default: 1690785] [currently: 1690785]", "If max_rows is exceeded, switch to truncate view. Depending on large_repr, objects are either centrally truncated or printed as a summary view. \u2018None\u2019 value means unlimited.", "In case python/IPython is running in a terminal and large_repr equals \u2018truncate\u2019 this can be set to 0 and pandas will auto-detect the height of the terminal and print a truncated object which fits the screen height. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection. [default: 60] [currently: 60]", "When pretty-printing a long sequence, no more then max_seq_items will be printed. If items are omitted, they will be denoted by the addition of \u201c\u2026\u201d to the resulting string.", "If set to None, the number of items to be printed is unlimited. [default: 100] [currently: 100]", "This specifies if the memory usage of a DataFrame should be displayed when df.info() is called. Valid values True,False,\u2019deep\u2019 [default: True] [currently: True]", "The numbers of rows to show in a truncated view (when max_rows is exceeded). Ignored when max_rows is set to None or 0. When set to None, follows the value of max_rows. [default: 10] [currently: 10]", "\u201csparsify\u201d MultiIndex display (don\u2019t display repeated elements in outer levels within groups) [default: True] [currently: True]", "When True, IPython notebook will use html representation for pandas objects (if it is available). [default: True] [currently: True]", "Controls the number of nested levels to process when pretty-printing [default: 3] [currently: 3]", "Floating point output precision in terms of number of places after the decimal, for regular formatting as well as scientific notation. Similar to precision in numpy.set_printoptions(). [default: 6] [currently: 6]", "Whether to print out dimensions at the end of DataFrame repr. If \u2018truncate\u2019 is specified, only print out the dimensions if the frame is truncated (e.g. not display all rows and/or columns) [default: truncate] [currently: truncate]", "Whether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False]", "Whether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False]", "Width of the display in characters. In case python/IPython is running in a terminal this can be set to None and pandas will correctly auto-detect the width. Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to correctly detect the width. [default: 80] [currently: 80]", "The default Excel reader engine for \u2018ods\u2019 files. Available options: auto, odf. [default: auto] [currently: auto]", "The default Excel writer engine for \u2018ods\u2019 files. Available options: auto, odf. [default: auto] [currently: auto]", "The default Excel reader engine for \u2018xls\u2019 files. Available options: auto, xlrd. [default: auto] [currently: auto]", "The default Excel writer engine for \u2018xls\u2019 files. Available options: auto, xlwt. [default: auto] [currently: auto] (Deprecated, use `` instead.)", "The default Excel reader engine for \u2018xlsb\u2019 files. Available options: auto, pyxlsb. [default: auto] [currently: auto]", "The default Excel reader engine for \u2018xlsm\u2019 files. Available options: auto, xlrd, openpyxl. [default: auto] [currently: auto]", "The default Excel writer engine for \u2018xlsm\u2019 files. Available options: auto, openpyxl. [default: auto] [currently: auto]", "The default Excel reader engine for \u2018xlsx\u2019 files. Available options: auto, xlrd, openpyxl. [default: auto] [currently: auto]", "The default Excel writer engine for \u2018xlsx\u2019 files. Available options: auto, openpyxl, xlsxwriter. [default: auto] [currently: auto]", "default format writing format, if None, then put will default to \u2018fixed\u2019 and append will default to \u2018table\u2019 [default: None] [currently: None]", "drop ALL nan rows when appending to a table [default: False] [currently: False]", "The default parquet reader/writer engine. Available options: \u2018auto\u2019, \u2018pyarrow\u2019, \u2018fastparquet\u2019, the default is \u2018auto\u2019 [default: auto] [currently: auto]", "The default sql reader/writer engine. Available options: \u2018auto\u2019, \u2018sqlalchemy\u2019, the default is \u2018auto\u2019 [default: auto] [currently: auto]", "Raise an exception, warn, or no action if trying to use chained assignment, The default is warn [default: warn] [currently: warn]", "Internal data manager type; can be \u201cblock\u201d or \u201carray\u201d. Defaults to \u201cblock\u201d, unless overridden by the \u2018PANDAS_DATA_MANAGER\u2019 environment variable (needs to be set before pandas is imported). [default: block] [currently: block]", "Whether to simulate interactive mode for purposes of testing [default: False] [currently: False]", "The default storage for StringDtype. [default: python] [currently: python]", "True means treat None, NaN, INF, -INF as NA (old way), False means None and NaN are null, but INF, -INF are not NA (new way). [default: False] [currently: False]", "use_inf_as_null had been deprecated and will be removed in a future version. Use use_inf_as_na instead. [default: False] [currently: False] (Deprecated, use mode.use_inf_as_na instead.)", "The plotting backend to use. The default value is \u201cmatplotlib\u201d, the backend provided with pandas. Other backends can be specified by providing the name of the module that implements the backend. [default: matplotlib] [currently: matplotlib]", "Whether to register converters with matplotlib\u2019s units registry for dates, times, datetimes, and Periods. Toggling to False will remove the converters, restoring any converters that pandas overwrote. [default: auto] [currently: auto]", "The character representation for the decimal separator for floats and complex. [default: .] [currently: .]", "Whether to escape certain characters according to the given context; html or latex. [default: None] [currently: None]", "A formatter object to be used as default within Styler.format. [default: None] [currently: None]", "The string representation for values identified as missing. [default: None] [currently: None]", "The precision for floats and complex numbers. [default: 6] [currently: 6]", "The character representation for thousands separator for floats, int and complex. [default: None] [currently: None]", "If False will render special CSS classes to table attributes that indicate Mathjax will not be used in Jupyter Notebook. [default: True] [currently: True]", "The environment to replace \\begin{table}. If \u201clongtable\u201d is used results in a specific longtable environment format. [default: None] [currently: None]", "Whether to add horizontal rules on top and bottom and below the headers. [default: False] [currently: False]", "The specifier for horizontal alignment of sparsified LaTeX multicolumns. Pipe decorators can also be added to non-naive values to draw vertical rules, e.g. \u201c|r\u201d will draw a rule on the left side of right aligned merged cells. [default: r] [currently: r]", "The specifier for vertical alignment of sparsified LaTeX multirows. [default: c] [currently: c]", "The encoding used for output HTML and LaTeX files. [default: utf-8] [currently: utf-8]", "The maximum number of columns that will be rendered. May still be reduced to satsify max_elements, which takes precedence. [default: None] [currently: None]", "The maximum number of data-cell (<td>) elements that will be rendered before trimming will occur over columns, rows or both if needed. [default: 262144] [currently: 262144]", "The maximum number of rows that will be rendered. May still be reduced to satsify max_elements, which takes precedence. [default: None] [currently: None]", "Determine which output to use in Jupyter Notebook in {\u201chtml\u201d, \u201clatex\u201d}. [default: html] [currently: html]", "Whether to sparsify the display of hierarchical columns. Setting to False will display each explicit level element in a hierarchical key for each column. [default: True] [currently: True]", "Whether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. [default: True] [currently: True]"]}, {"name": "pandas.show_versions", "path": "reference/api/pandas.show_versions", "type": "General utility functions", "text": ["Provide useful information, important for bug reports.", "It comprises info about hosting operation system, pandas version, and versions of other installed relative packages.", "If False, outputs info in a human readable form to the console.", "If str, it will be considered as a path to a file. Info will be written to that file in JSON format.", "If True, outputs info in JSON format to the console."]}, {"name": "pandas.SparseDtype", "path": "reference/api/pandas.sparsedtype", "type": "Pandas arrays", "text": ["Dtype for data stored in SparseArray.", "This dtype implements the pandas ExtensionDtype interface.", "The dtype of the underlying array storing the non-fill value values.", "The scalar value not stored in the SparseArray. By default, this depends on dtype.", "dtype", "na_value", "float", "np.nan", "int", "0", "bool", "False", "datetime64", "pd.NaT", "timedelta64", "pd.NaT", "The default value may be overridden by specifying a fill_value.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.StringDtype", "path": "reference/api/pandas.stringdtype", "type": "Pandas arrays", "text": ["Extension dtype for string data.", "New in version 1.0.0.", "Warning", "StringDtype is considered experimental. The implementation and parts of the API may change without warning.", "In particular, StringDtype.na_value may change to no longer be numpy.nan.", "If not given, the value of pd.options.mode.string_storage.", "Examples", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.test", "path": "reference/api/pandas.test", "type": "General functions", "text": ["Run the pandas test suite using pytest."]}, {"name": "pandas.testing.assert_extension_array_equal", "path": "reference/api/pandas.testing.assert_extension_array_equal", "type": "General utility functions", "text": ["Check that left and right ExtensionArrays are equal.", "The two arrays to compare.", "Whether to check if the ExtensionArray dtypes are identical.", "Optional index (shared by both left and right), used in output.", "Specify comparison precision. Only used when check_exact is False. 5 digits (False) or 3 digits (True) after decimal points are compared. If int, then specify the digits to compare.", "Deprecated since version 1.1.0: Use rtol and atol instead to define relative/absolute tolerance, respectively. Similar to math.isclose().", "Whether to compare number exactly.", "Relative tolerance. Only used when check_exact is False.", "New in version 1.1.0.", "Absolute tolerance. Only used when check_exact is False.", "New in version 1.1.0.", "Notes", "Missing values are checked separately from valid values. A mask of missing values is computed for each and checked to match. The remaining all-valid values are cast to object dtype and checked.", "Examples"]}, {"name": "pandas.testing.assert_frame_equal", "path": "reference/api/pandas.testing.assert_frame_equal", "type": "General utility functions", "text": ["Check that left and right DataFrame are equal.", "This function is intended to compare two DataFrames and output any differences. Is is mostly intended for use in unit tests. Additional parameters allow varying the strictness of the equality checks performed.", "First DataFrame to compare.", "Second DataFrame to compare.", "Whether to check the DataFrame dtype is identical.", "Whether to check the Index class, dtype and inferred_type are identical.", "Whether to check the columns class, dtype and inferred_type are identical. Is passed as the exact argument of assert_index_equal().", "Whether to check the DataFrame class is identical.", "Specify comparison precision. Only used when check_exact is False. 5 digits (False) or 3 digits (True) after decimal points are compared. If int, then specify the digits to compare.", "When comparing two numbers, if the first number has magnitude less than 1e-5, we compare the two numbers directly and check whether they are equivalent within the specified precision. Otherwise, we compare the ratio of the second number to the first number and check whether it is equivalent to 1 within the specified precision.", "Deprecated since version 1.1.0: Use rtol and atol instead to define relative/absolute tolerance, respectively. Similar to math.isclose().", "Whether to check that the names attribute for both the index and column attributes of the DataFrame is identical.", "Specify how to compare internal data. If False, compare by columns. If True, compare by blocks.", "Whether to compare number exactly.", "Compare datetime-like which is comparable ignoring dtype.", "Whether to compare internal Categorical exactly.", "If True, ignore the order of index & columns. Note: index labels must match their respective rows (same as in columns) - same labels must be with the same data.", "Whether to check the freq attribute on a DatetimeIndex or TimedeltaIndex.", "New in version 1.1.0.", "Whether to check the flags attribute.", "Relative tolerance. Only used when check_exact is False.", "New in version 1.1.0.", "Absolute tolerance. Only used when check_exact is False.", "New in version 1.1.0.", "Specify object name being compared, internally used to show appropriate assertion message.", "See also", "Equivalent method for asserting Series equality.", "Check DataFrame equality.", "Examples", "This example shows comparing two DataFrames that are equal but with columns of differing dtypes.", "df1 equals itself.", "df1 differs from df2 as column \u2018b\u2019 is of a different type.", "Attribute \u201cdtype\u201d are different [left]: int64 [right]: float64", "Ignore differing dtypes in columns with check_dtype."]}, {"name": "pandas.testing.assert_index_equal", "path": "reference/api/pandas.testing.assert_index_equal", "type": "General utility functions", "text": ["Check that left and right Index are equal.", "Whether to check the Index class, dtype and inferred_type are identical. If \u2018equiv\u2019, then RangeIndex can be substituted for Int64Index as well.", "Whether to check the names attribute.", "Specify comparison precision. Only used when check_exact is False. 5 digits (False) or 3 digits (True) after decimal points are compared. If int, then specify the digits to compare.", "Deprecated since version 1.1.0: Use rtol and atol instead to define relative/absolute tolerance, respectively. Similar to math.isclose().", "Whether to compare number exactly.", "Whether to compare internal Categorical exactly.", "Whether to compare the order of index entries as well as their values. If True, both indexes must contain the same elements, in the same order. If False, both indexes must contain the same elements, but in any order.", "New in version 1.2.0.", "Relative tolerance. Only used when check_exact is False.", "New in version 1.1.0.", "Absolute tolerance. Only used when check_exact is False.", "New in version 1.1.0.", "Specify object name being compared, internally used to show appropriate assertion message.", "Examples"]}, {"name": "pandas.testing.assert_series_equal", "path": "reference/api/pandas.testing.assert_series_equal", "type": "General utility functions", "text": ["Check that left and right Series are equal.", "Whether to check the Series dtype is identical.", "Whether to check the Index class, dtype and inferred_type are identical.", "Whether to check the Series class is identical.", "Specify comparison precision. Only used when check_exact is False. 5 digits (False) or 3 digits (True) after decimal points are compared. If int, then specify the digits to compare.", "When comparing two numbers, if the first number has magnitude less than 1e-5, we compare the two numbers directly and check whether they are equivalent within the specified precision. Otherwise, we compare the ratio of the second number to the first number and check whether it is equivalent to 1 within the specified precision.", "Deprecated since version 1.1.0: Use rtol and atol instead to define relative/absolute tolerance, respectively. Similar to math.isclose().", "Whether to check the Series and Index names attribute.", "Whether to compare number exactly.", "Compare datetime-like which is comparable ignoring dtype.", "Whether to compare internal Categorical exactly.", "Whether to compare category order of internal Categoricals.", "New in version 1.0.2.", "Whether to check the freq attribute on a DatetimeIndex or TimedeltaIndex.", "New in version 1.1.0.", "Whether to check the flags attribute.", "New in version 1.2.0.", "Relative tolerance. Only used when check_exact is False.", "New in version 1.1.0.", "Absolute tolerance. Only used when check_exact is False.", "New in version 1.1.0.", "Specify object name being compared, internally used to show appropriate assertion message.", "Whether to check index equivalence. If False, then compare only values.", "New in version 1.3.0.", "Examples"]}, {"name": "pandas.Timedelta", "path": "reference/api/pandas.timedelta", "type": "Pandas arrays", "text": ["Represents a duration, the difference between two dates or times.", "Timedelta is the pandas equivalent of python\u2019s datetime.timedelta and is interchangeable with it in most cases.", "Denote the unit of the input, if input is an integer.", "Possible values:", "\u2018W\u2019, \u2018D\u2019, \u2018T\u2019, \u2018S\u2019, \u2018L\u2019, \u2018U\u2019, or \u2018N\u2019", "\u2018days\u2019 or \u2018day\u2019", "\u2018hours\u2019, \u2018hour\u2019, \u2018hr\u2019, or \u2018h\u2019", "\u2018minutes\u2019, \u2018minute\u2019, \u2018min\u2019, or \u2018m\u2019", "\u2018seconds\u2019, \u2018second\u2019, or \u2018sec\u2019", "\u2018milliseconds\u2019, \u2018millisecond\u2019, \u2018millis\u2019, or \u2018milli\u2019", "\u2018microseconds\u2019, \u2018microsecond\u2019, \u2018micros\u2019, or \u2018micro\u2019", "\u2018nanoseconds\u2019, \u2018nanosecond\u2019, \u2018nanos\u2019, \u2018nano\u2019, or \u2018ns\u2019.", "Available kwargs: {days, seconds, microseconds, milliseconds, minutes, hours, weeks}. Values for construction in compat with datetime.timedelta. Numpy ints and floats will be coerced to python ints and floats.", "Notes", "The constructor may take in either both values of value and unit or kwargs as above. Either one of them must be used during initialization", "The .value attribute is always in ns.", "If the precision is higher than nanoseconds, the precision of the duration is truncated to nanoseconds.", "Examples", "Here we initialize Timedelta object with both value and unit", "Here we initialize the Timedelta object with kwargs", "We see that either way we get the same result", "Attributes", "asm8", "Return a numpy timedelta64 array scalar view.", "components", "Return a components namedtuple-like.", "days", "Number of days.", "delta", "Return the timedelta in nanoseconds (ns), for internal compatibility.", "microseconds", "Number of microseconds (>= 0 and less than 1 second).", "nanoseconds", "Return the number of nanoseconds (n), where 0 <= n < 1 microsecond.", "resolution_string", "Return a string representing the lowest timedelta resolution.", "seconds", "Number of seconds (>= 0 and less than 1 day).", "freq", "is_populated", "value", "Methods", "ceil(freq)", "Return a new Timedelta ceiled to this resolution.", "floor(freq)", "Return a new Timedelta floored to this resolution.", "isoformat", "Format Timedelta as ISO 8601 Duration like P[n]Y[n]M[n]DT[n]H[n]M[n]S, where the [n] s are replaced by the values.", "round(freq)", "Round the Timedelta to the specified resolution.", "to_numpy", "Convert the Timedelta to a NumPy timedelta64.", "to_pytimedelta", "Convert a pandas Timedelta object into a python datetime.timedelta object.", "to_timedelta64", "Return a numpy.timedelta64 object with 'ns' precision.", "total_seconds", "Total seconds in the duration.", "view", "Array view compatibility."]}, {"name": "pandas.Timedelta.asm8", "path": "reference/api/pandas.timedelta.asm8", "type": "Pandas arrays", "text": ["Return a numpy timedelta64 array scalar view.", "Provides access to the array scalar view (i.e. a combination of the value and the units) associated with the numpy.timedelta64().view(), including a 64-bit integer representation of the timedelta in nanoseconds (Python int compatible).", "Array scalar view of the timedelta in nanoseconds.", "Examples"]}, {"name": "pandas.Timedelta.ceil", "path": "reference/api/pandas.timedelta.ceil", "type": "Pandas arrays", "text": ["Return a new Timedelta ceiled to this resolution.", "Frequency string indicating the ceiling resolution."]}, {"name": "pandas.Timedelta.components", "path": "reference/api/pandas.timedelta.components", "type": "Pandas arrays", "text": ["Return a components namedtuple-like."]}, {"name": "pandas.Timedelta.days", "path": "reference/api/pandas.timedelta.days", "type": "Pandas arrays", "text": ["Number of days."]}, {"name": "pandas.Timedelta.delta", "path": "reference/api/pandas.timedelta.delta", "type": "Pandas arrays", "text": ["Return the timedelta in nanoseconds (ns), for internal compatibility.", "Timedelta in nanoseconds.", "Examples"]}, {"name": "pandas.Timedelta.floor", "path": "reference/api/pandas.timedelta.floor", "type": "Pandas arrays", "text": ["Return a new Timedelta floored to this resolution.", "Frequency string indicating the flooring resolution."]}, {"name": "pandas.Timedelta.freq", "path": "reference/api/pandas.timedelta.freq", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timedelta.is_populated", "path": "reference/api/pandas.timedelta.is_populated", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timedelta.isoformat", "path": "reference/api/pandas.timedelta.isoformat", "type": "Pandas arrays", "text": ["Format Timedelta as ISO 8601 Duration like P[n]Y[n]M[n]DT[n]H[n]M[n]S, where the [n] s are replaced by the values. See https://en.wikipedia.org/wiki/ISO_8601#Durations.", "See also", "Function is used to convert the given Timestamp object into the ISO format.", "Notes", "The longest component is days, whose value may be larger than 365. Every component is always included, even if its value is 0. Pandas uses nanosecond precision, so up to 9 decimal places may be included in the seconds component. Trailing 0\u2019s are removed from the seconds component after the decimal. We do not 0 pad components, so it\u2019s \u2026T5H\u2026, not \u2026T05H\u2026", "Examples"]}, {"name": "pandas.Timedelta.max", "path": "reference/api/pandas.timedelta.max", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timedelta.microseconds", "path": "reference/api/pandas.timedelta.microseconds", "type": "Pandas arrays", "text": ["Number of microseconds (>= 0 and less than 1 second)."]}, {"name": "pandas.Timedelta.min", "path": "reference/api/pandas.timedelta.min", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timedelta.nanoseconds", "path": "reference/api/pandas.timedelta.nanoseconds", "type": "Pandas arrays", "text": ["Return the number of nanoseconds (n), where 0 <= n < 1 microsecond.", "Number of nanoseconds.", "See also", "Return all attributes with assigned values (i.e. days, hours, minutes, seconds, milliseconds, microseconds, nanoseconds).", "Examples", "Using string input", "Using integer input"]}, {"name": "pandas.Timedelta.resolution", "path": "reference/api/pandas.timedelta.resolution", "type": "Input/output", "text": []}, {"name": "pandas.Timedelta.resolution_string", "path": "reference/api/pandas.timedelta.resolution_string", "type": "Input/output", "text": ["Return a string representing the lowest timedelta resolution.", "Each timedelta has a defined resolution that represents the lowest OR most granular level of precision. Each level of resolution is represented by a short string as defined below:", "Resolution: Return value", "Days: \u2018D\u2019", "Hours: \u2018H\u2019", "Minutes: \u2018T\u2019", "Seconds: \u2018S\u2019", "Milliseconds: \u2018L\u2019", "Microseconds: \u2018U\u2019", "Nanoseconds: \u2018N\u2019", "Timedelta resolution.", "Examples"]}, {"name": "pandas.Timedelta.round", "path": "reference/api/pandas.timedelta.round", "type": "Pandas arrays", "text": ["Round the Timedelta to the specified resolution.", "Frequency string indicating the rounding resolution."]}, {"name": "pandas.Timedelta.seconds", "path": "reference/api/pandas.timedelta.seconds", "type": "Pandas arrays", "text": ["Number of seconds (>= 0 and less than 1 day)."]}, {"name": "pandas.Timedelta.to_numpy", "path": "reference/api/pandas.timedelta.to_numpy", "type": "Pandas arrays", "text": ["Convert the Timedelta to a NumPy timedelta64.", "New in version 0.25.0.", "This is an alias method for Timedelta.to_timedelta64(). The dtype and copy parameters are available here only for compatibility. Their values will not affect the return value.", "See also", "Similar method for Series."]}, {"name": "pandas.Timedelta.to_pytimedelta", "path": "reference/api/pandas.timedelta.to_pytimedelta", "type": "Pandas arrays", "text": ["Convert a pandas Timedelta object into a python datetime.timedelta object.", "Timedelta objects are internally saved as numpy datetime64[ns] dtype. Use to_pytimedelta() to convert to object dtype.", "See also", "Convert argument to Timedelta type.", "Notes", "Any nanosecond resolution will be lost."]}, {"name": "pandas.Timedelta.to_timedelta64", "path": "reference/api/pandas.timedelta.to_timedelta64", "type": "Pandas arrays", "text": ["Return a numpy.timedelta64 object with \u2018ns\u2019 precision."]}, {"name": "pandas.Timedelta.total_seconds", "path": "reference/api/pandas.timedelta.total_seconds", "type": "Pandas arrays", "text": ["Total seconds in the duration."]}, {"name": "pandas.Timedelta.value", "path": "reference/api/pandas.timedelta.value", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timedelta.view", "path": "reference/api/pandas.timedelta.view", "type": "Pandas arrays", "text": ["Array view compatibility."]}, {"name": "pandas.timedelta_range", "path": "reference/api/pandas.timedelta_range", "type": "General functions", "text": ["Return a fixed frequency TimedeltaIndex, with day as the default frequency.", "Left bound for generating timedeltas.", "Right bound for generating timedeltas.", "Number of periods to generate.", "Frequency strings can have multiples, e.g. \u20185H\u2019.", "Name of the resulting TimedeltaIndex.", "Make the interval closed with respect to the given frequency to the \u2018left\u2019, \u2018right\u2019, or both sides (None).", "Notes", "Of the four parameters start, end, periods, and freq, exactly three must be specified. If freq is omitted, the resulting TimedeltaIndex will have periods linearly spaced elements between start and end (closed on both sides).", "To learn more about the frequency strings, please see this link.", "Examples", "The closed parameter specifies which endpoint is included. The default behavior is to include both endpoints.", "The freq parameter specifies the frequency of the TimedeltaIndex. Only fixed frequencies can be passed, non-fixed frequencies such as \u2018M\u2019 (month end) will raise.", "Specify start, end, and periods; the frequency is generated automatically (linearly spaced)."]}, {"name": "pandas.TimedeltaIndex", "path": "reference/api/pandas.timedeltaindex", "type": "Index Objects", "text": ["Immutable ndarray of timedelta64 data, represented internally as int64, and which can be boxed to timedelta objects.", "Optional timedelta-like data to construct index with.", "Which is an integer/float number.", "One of pandas date offset strings or corresponding objects. The string \u2018infer\u2019 can be passed in order to set the frequency of the index as the inferred frequency upon creation.", "Make a copy of input ndarray.", "Name to be stored in the index.", "See also", "The base pandas Index type.", "Represents a duration between two dates or times.", "Index of datetime64 data.", "Index of Period data.", "Create a fixed-frequency TimedeltaIndex.", "Notes", "To learn more about the frequency strings, please see this link.", "Attributes", "days", "Number of days for each element.", "seconds", "Number of seconds (>= 0 and less than 1 day) for each element.", "microseconds", "Number of microseconds (>= 0 and less than 1 second) for each element.", "nanoseconds", "Number of nanoseconds (>= 0 and less than 1 microsecond) for each element.", "components", "Return a dataframe of the components (days, hours, minutes, seconds, milliseconds, microseconds, nanoseconds) of the Timedeltas.", "inferred_freq", "Tries to return a string representing a frequency guess, generated by infer_freq.", "Methods", "to_pytimedelta(*args, **kwargs)", "Return Timedelta Array/Index as object ndarray of datetime.timedelta objects.", "to_series([index, name])", "Create a Series with both index and values equal to the index keys.", "round(*args, **kwargs)", "Perform round operation on the data to the specified freq.", "floor(*args, **kwargs)", "Perform floor operation on the data to the specified freq.", "ceil(*args, **kwargs)", "Perform ceil operation on the data to the specified freq.", "to_frame([index, name])", "Create a DataFrame with a column containing the Index.", "mean(*args, **kwargs)", "Return the mean value of the Array."]}, {"name": "pandas.TimedeltaIndex.ceil", "path": "reference/api/pandas.timedeltaindex.ceil", "type": "Index Objects", "text": ["Perform ceil operation on the data to the specified freq.", "The frequency level to ceil the index to. Must be a fixed frequency like \u2018S\u2019 (second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible freq values.", "Only relevant for DatetimeIndex:", "\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order", "bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times)", "\u2018NaT\u2019 will return NaT where there are ambiguous times", "\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST.", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time", "\u2018NaT\u2019 will return NaT where there are nonexistent times", "timedelta objects will shift nonexistent times by the timedelta", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "Index of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series.", "Notes", "If the timestamps have a timezone, ceiling will take place relative to the local (\u201cwall\u201d) time and re-localized to the same timezone. When ceiling near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.", "Examples", "DatetimeIndex", "Series", "When rounding near a daylight savings time transition, use ambiguous or nonexistent to control how the timestamp should be re-localized."]}, {"name": "pandas.TimedeltaIndex.components", "path": "reference/api/pandas.timedeltaindex.components", "type": "Index Objects", "text": ["Return a dataframe of the components (days, hours, minutes, seconds, milliseconds, microseconds, nanoseconds) of the Timedeltas."]}, {"name": "pandas.TimedeltaIndex.days", "path": "reference/api/pandas.timedeltaindex.days", "type": "Index Objects", "text": ["Number of days for each element."]}, {"name": "pandas.TimedeltaIndex.floor", "path": "reference/api/pandas.timedeltaindex.floor", "type": "Index Objects", "text": ["Perform floor operation on the data to the specified freq.", "The frequency level to floor the index to. Must be a fixed frequency like \u2018S\u2019 (second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible freq values.", "Only relevant for DatetimeIndex:", "\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order", "bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times)", "\u2018NaT\u2019 will return NaT where there are ambiguous times", "\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST.", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time", "\u2018NaT\u2019 will return NaT where there are nonexistent times", "timedelta objects will shift nonexistent times by the timedelta", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "Index of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series.", "Notes", "If the timestamps have a timezone, flooring will take place relative to the local (\u201cwall\u201d) time and re-localized to the same timezone. When flooring near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.", "Examples", "DatetimeIndex", "Series", "When rounding near a daylight savings time transition, use ambiguous or nonexistent to control how the timestamp should be re-localized."]}, {"name": "pandas.TimedeltaIndex.inferred_freq", "path": "reference/api/pandas.timedeltaindex.inferred_freq", "type": "Index Objects", "text": ["Tries to return a string representing a frequency guess, generated by infer_freq. Returns None if it can\u2019t autodetect the frequency."]}, {"name": "pandas.TimedeltaIndex.mean", "path": "reference/api/pandas.timedeltaindex.mean", "type": "Index Objects", "text": ["Return the mean value of the Array.", "New in version 0.25.0.", "Whether to ignore any NaT elements.", "Timestamp or Timedelta.", "See also", "Returns the average of array elements along a given axis.", "Return the mean value in a Series.", "Notes", "mean is only defined for Datetime and Timedelta dtypes, not for Period."]}, {"name": "pandas.TimedeltaIndex.microseconds", "path": "reference/api/pandas.timedeltaindex.microseconds", "type": "Index Objects", "text": ["Number of microseconds (>= 0 and less than 1 second) for each element."]}, {"name": "pandas.TimedeltaIndex.nanoseconds", "path": "reference/api/pandas.timedeltaindex.nanoseconds", "type": "Index Objects", "text": ["Number of nanoseconds (>= 0 and less than 1 microsecond) for each element."]}, {"name": "pandas.TimedeltaIndex.round", "path": "reference/api/pandas.timedeltaindex.round", "type": "Index Objects", "text": ["Perform round operation on the data to the specified freq.", "The frequency level to round the index to. Must be a fixed frequency like \u2018S\u2019 (second) not \u2018ME\u2019 (month end). See frequency aliases for a list of possible freq values.", "Only relevant for DatetimeIndex:", "\u2018infer\u2019 will attempt to infer fall dst-transition hours based on order", "bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times)", "\u2018NaT\u2019 will return NaT where there are ambiguous times", "\u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST.", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time", "\u2018NaT\u2019 will return NaT where there are nonexistent times", "timedelta objects will shift nonexistent times by the timedelta", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "Index of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series.", "Notes", "If the timestamps have a timezone, rounding will take place relative to the local (\u201cwall\u201d) time and re-localized to the same timezone. When rounding near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.", "Examples", "DatetimeIndex", "Series", "When rounding near a daylight savings time transition, use ambiguous or nonexistent to control how the timestamp should be re-localized."]}, {"name": "pandas.TimedeltaIndex.seconds", "path": "reference/api/pandas.timedeltaindex.seconds", "type": "Index Objects", "text": ["Number of seconds (>= 0 and less than 1 day) for each element."]}, {"name": "pandas.TimedeltaIndex.to_frame", "path": "reference/api/pandas.timedeltaindex.to_frame", "type": "DataFrame", "text": ["Create a DataFrame with a column containing the Index.", "Set the index of the returned DataFrame as the original Index.", "The passed name should substitute for the index name (if it has one).", "DataFrame containing the original Index data.", "See also", "Convert an Index to a Series.", "Convert Series to DataFrame.", "Examples", "By default, the original Index is reused. To enforce a new Index:", "To override the name of the resulting column, specify name:"]}, {"name": "pandas.TimedeltaIndex.to_pytimedelta", "path": "reference/api/pandas.timedeltaindex.to_pytimedelta", "type": "Index Objects", "text": ["Return Timedelta Array/Index as object ndarray of datetime.timedelta objects."]}, {"name": "pandas.TimedeltaIndex.to_series", "path": "reference/api/pandas.timedeltaindex.to_series", "type": "Index Objects", "text": ["Create a Series with both index and values equal to the index keys.", "Useful with map for returning an indexer based on an index.", "Index of resulting Series. If None, defaults to original index.", "Name of resulting Series. If None, defaults to name of original index.", "The dtype will be based on the type of the Index values.", "See also", "Convert an Index to a DataFrame.", "Convert Series to DataFrame.", "Examples", "By default, the original Index and original name is reused.", "To enforce a new Index, specify new labels to index:", "To override the name of the resulting column, specify name:"]}, {"name": "pandas.Timestamp", "path": "reference/api/pandas.timestamp", "type": "Pandas arrays", "text": ["Pandas replacement for python datetime.datetime object.", "Timestamp is the pandas equivalent of python\u2019s Datetime and is interchangeable with it in most cases. It\u2019s the type used for the entries that make up a DatetimeIndex, and other timeseries oriented data structures in pandas.", "Value to be converted to Timestamp.", "Offset which Timestamp will have.", "Time zone for time which Timestamp will have.", "Unit used for conversion if ts_input is of type int or float. The valid values are \u2018D\u2019, \u2018h\u2019, \u2018m\u2019, \u2018s\u2019, \u2018ms\u2019, \u2018us\u2019, and \u2018ns\u2019. For example, \u2018s\u2019 means seconds and \u2018ms\u2019 means milliseconds.", "Due to daylight saving time, one wall clock time can occur twice when shifting from summer to winter time; fold describes whether the datetime-like corresponds to the first (0) or the second time (1) the wall clock hits the ambiguous time.", "New in version 1.1.0.", "Notes", "There are essentially three calling conventions for the constructor. The primary form accepts four parameters. They can be passed by position or keyword.", "The other two forms mimic the parameters from datetime.datetime. They can be passed by either position or keyword, but not both mixed together.", "Examples", "Using the primary calling convention:", "This converts a datetime-like string", "This converts a float representing a Unix epoch in units of seconds", "This converts an int representing a Unix-epoch in units of seconds and for a particular timezone", "Using the other two forms that mimic the API for datetime.datetime:", "Attributes", "asm8", "Return numpy datetime64 format in nanoseconds.", "day_of_week", "Return day of the week.", "day_of_year", "Return the day of the year.", "dayofweek", "Return day of the week.", "dayofyear", "Return the day of the year.", "days_in_month", "Return the number of days in the month.", "daysinmonth", "Return the number of days in the month.", "freqstr", "Return the total number of days in the month.", "is_leap_year", "Return True if year is a leap year.", "is_month_end", "Return True if date is last day of month.", "is_month_start", "Return True if date is first day of month.", "is_quarter_end", "Return True if date is last day of the quarter.", "is_quarter_start", "Return True if date is first day of the quarter.", "is_year_end", "Return True if date is last day of the year.", "is_year_start", "Return True if date is first day of the year.", "quarter", "Return the quarter of the year.", "tz", "Alias for tzinfo.", "week", "Return the week number of the year.", "weekofyear", "Return the week number of the year.", "day", "fold", "freq", "hour", "microsecond", "minute", "month", "nanosecond", "second", "tzinfo", "value", "year", "Methods", "astimezone(tz)", "Convert timezone-aware Timestamp to another time zone.", "ceil(freq[, ambiguous, nonexistent])", "Return a new Timestamp ceiled to this resolution.", "combine(date, time)", "Combine date, time into datetime with same date and time fields.", "ctime", "Return ctime() style string.", "date", "Return date object with same year, month and day.", "day_name", "Return the day name of the Timestamp with specified locale.", "dst", "Return self.tzinfo.dst(self).", "floor(freq[, ambiguous, nonexistent])", "Return a new Timestamp floored to this resolution.", "fromisocalendar", "int, int, int -> Construct a date from the ISO year, week number and weekday.", "fromisoformat", "string -> datetime from datetime.isoformat() output", "fromordinal(ordinal[, freq, tz])", "Passed an ordinal, translate and convert to a ts.", "fromtimestamp(ts)", "Transform timestamp[, tz] to tz's local time from POSIX timestamp.", "isocalendar", "Return a 3-tuple containing ISO year, week number, and weekday.", "isoformat", "Return the time formatted according to ISO 8610.", "isoweekday()", "Return the day of the week represented by the date.", "month_name", "Return the month name of the Timestamp with specified locale.", "normalize", "Normalize Timestamp to midnight, preserving tz information.", "now([tz])", "Return new Timestamp object representing current time local to tz.", "replace([year, month, day, hour, minute, ...])", "Implements datetime.replace, handles nanoseconds.", "round(freq[, ambiguous, nonexistent])", "Round the Timestamp to the specified resolution.", "strftime(format)", "Return a string representing the given POSIX timestamp controlled by an explicit format string.", "strptime(string, format)", "Function is not implemented.", "time", "Return time object with same time but with tzinfo=None.", "timestamp", "Return POSIX timestamp as float.", "timetuple", "Return time tuple, compatible with time.localtime().", "timetz", "Return time object with same time and tzinfo.", "to_datetime64", "Return a numpy.datetime64 object with 'ns' precision.", "to_julian_date()", "Convert TimeStamp to a Julian Date.", "to_numpy", "Convert the Timestamp to a NumPy datetime64.", "to_period", "Return an period of which this timestamp is an observation.", "to_pydatetime", "Convert a Timestamp object to a native Python datetime object.", "today(cls[, tz])", "Return the current time in the local timezone.", "toordinal", "Return proleptic Gregorian ordinal.", "tz_convert(tz)", "Convert timezone-aware Timestamp to another time zone.", "tz_localize(tz[, ambiguous, nonexistent])", "Convert naive Timestamp to local time zone, or remove timezone from timezone-aware Timestamp.", "tzname", "Return self.tzinfo.tzname(self).", "utcfromtimestamp(ts)", "Construct a naive UTC datetime from a POSIX timestamp.", "utcnow()", "Return a new Timestamp representing UTC day and time.", "utcoffset", "Return self.tzinfo.utcoffset(self).", "utctimetuple", "Return UTC time tuple, compatible with time.localtime().", "weekday()", "Return the day of the week represented by the date."]}, {"name": "pandas.Timestamp.asm8", "path": "reference/api/pandas.timestamp.asm8", "type": "Pandas arrays", "text": ["Return numpy datetime64 format in nanoseconds.", "Examples"]}, {"name": "pandas.Timestamp.astimezone", "path": "reference/api/pandas.timestamp.astimezone", "type": "Pandas arrays", "text": ["Convert timezone-aware Timestamp to another time zone.", "Time zone for time which Timestamp will be converted to. None will remove timezone holding UTC time.", "If Timestamp is tz-naive.", "Examples", "Create a timestamp object with UTC timezone:", "Change to Tokyo timezone:", "Can also use astimezone:", "Analogous for pd.NaT:"]}, {"name": "pandas.Timestamp.ceil", "path": "reference/api/pandas.timestamp.ceil", "type": "Pandas arrays", "text": ["Return a new Timestamp ceiled to this resolution.", "Frequency string indicating the ceiling resolution.", "The behavior is as follows:", "bool contains flags to determine if time is dst or not (note that this flag is only applicable for ambiguous fall dst dates).", "\u2018NaT\u2019 will return NaT for an ambiguous time.", "\u2018raise\u2019 will raise an AmbiguousTimeError for an ambiguous time.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST.", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time.", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time.", "\u2018NaT\u2019 will return NaT where there are nonexistent times.", "timedelta objects will shift nonexistent times by the timedelta.", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "Notes", "If the Timestamp has a timezone, ceiling will take place relative to the local (\u201cwall\u201d) time and re-localized to the same timezone. When ceiling near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.", "Examples", "Create a timestamp object:", "A timestamp can be ceiled using multiple frequency units:", "freq can also be a multiple of a single unit, like \u20185T\u2019 (i.e. 5 minutes):", "or a combination of multiple units, like \u20181H30T\u2019 (i.e. 1 hour and 30 minutes):", "Analogous for pd.NaT:", "When rounding near a daylight savings time transition, use ambiguous or nonexistent to control how the timestamp should be re-localized."]}, {"name": "pandas.Timestamp.combine", "path": "reference/api/pandas.timestamp.combine", "type": "Pandas arrays", "text": ["Combine date, time into datetime with same date and time fields.", "Examples"]}, {"name": "pandas.Timestamp.ctime", "path": "reference/api/pandas.timestamp.ctime", "type": "Pandas arrays", "text": ["Return ctime() style string."]}, {"name": "pandas.Timestamp.date", "path": "reference/api/pandas.timestamp.date", "type": "Pandas arrays", "text": ["Return date object with same year, month and day."]}, {"name": "pandas.Timestamp.day", "path": "reference/api/pandas.timestamp.day", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timestamp.day_name", "path": "reference/api/pandas.timestamp.day_name", "type": "Pandas arrays", "text": ["Return the day name of the Timestamp with specified locale.", "Locale determining the language in which to return the day name.", "Examples", "Analogous for pd.NaT:"]}, {"name": "pandas.Timestamp.day_of_week", "path": "reference/api/pandas.timestamp.day_of_week", "type": "Pandas arrays", "text": ["Return day of the week.", "Examples"]}, {"name": "pandas.Timestamp.day_of_year", "path": "reference/api/pandas.timestamp.day_of_year", "type": "Pandas arrays", "text": ["Return the day of the year.", "Examples"]}, {"name": "pandas.Timestamp.dayofweek", "path": "reference/api/pandas.timestamp.dayofweek", "type": "Pandas arrays", "text": ["Return day of the week.", "Examples"]}, {"name": "pandas.Timestamp.dayofyear", "path": "reference/api/pandas.timestamp.dayofyear", "type": "Pandas arrays", "text": ["Return the day of the year.", "Examples"]}, {"name": "pandas.Timestamp.days_in_month", "path": "reference/api/pandas.timestamp.days_in_month", "type": "Pandas arrays", "text": ["Return the number of days in the month.", "Examples"]}, {"name": "pandas.Timestamp.daysinmonth", "path": "reference/api/pandas.timestamp.daysinmonth", "type": "Pandas arrays", "text": ["Return the number of days in the month.", "Examples"]}, {"name": "pandas.Timestamp.dst", "path": "reference/api/pandas.timestamp.dst", "type": "Pandas arrays", "text": ["Return self.tzinfo.dst(self)."]}, {"name": "pandas.Timestamp.floor", "path": "reference/api/pandas.timestamp.floor", "type": "Pandas arrays", "text": ["Return a new Timestamp floored to this resolution.", "Frequency string indicating the flooring resolution.", "The behavior is as follows:", "bool contains flags to determine if time is dst or not (note that this flag is only applicable for ambiguous fall dst dates).", "\u2018NaT\u2019 will return NaT for an ambiguous time.", "\u2018raise\u2019 will raise an AmbiguousTimeError for an ambiguous time.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST.", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time.", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time.", "\u2018NaT\u2019 will return NaT where there are nonexistent times.", "timedelta objects will shift nonexistent times by the timedelta.", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "Notes", "If the Timestamp has a timezone, flooring will take place relative to the local (\u201cwall\u201d) time and re-localized to the same timezone. When flooring near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.", "Examples", "Create a timestamp object:", "A timestamp can be floored using multiple frequency units:", "freq can also be a multiple of a single unit, like \u20185T\u2019 (i.e. 5 minutes):", "or a combination of multiple units, like \u20181H30T\u2019 (i.e. 1 hour and 30 minutes):", "Analogous for pd.NaT:", "When rounding near a daylight savings time transition, use ambiguous or nonexistent to control how the timestamp should be re-localized."]}, {"name": "pandas.Timestamp.fold", "path": "reference/api/pandas.timestamp.fold", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timestamp.freq", "path": "reference/api/pandas.timestamp.freq", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timestamp.freqstr", "path": "reference/api/pandas.timestamp.freqstr", "type": "Pandas arrays", "text": ["Return the total number of days in the month."]}, {"name": "pandas.Timestamp.fromisocalendar", "path": "reference/api/pandas.timestamp.fromisocalendar", "type": "Pandas arrays", "text": ["int, int, int -> Construct a date from the ISO year, week number and weekday.", "This is the inverse of the date.isocalendar() function"]}, {"name": "pandas.Timestamp.fromisoformat", "path": "reference/api/pandas.timestamp.fromisoformat", "type": "Pandas arrays", "text": ["string -> datetime from datetime.isoformat() output"]}, {"name": "pandas.Timestamp.fromordinal", "path": "reference/api/pandas.timestamp.fromordinal", "type": "Pandas arrays", "text": ["Passed an ordinal, translate and convert to a ts. Note: by definition there cannot be any tz info on the ordinal itself.", "Date corresponding to a proleptic Gregorian ordinal.", "Offset to apply to the Timestamp.", "Time zone for the Timestamp.", "Examples"]}, {"name": "pandas.Timestamp.fromtimestamp", "path": "reference/api/pandas.timestamp.fromtimestamp", "type": "Pandas arrays", "text": ["Transform timestamp[, tz] to tz\u2019s local time from POSIX timestamp.", "Examples", "Note that the output may change depending on your local time."]}, {"name": "pandas.Timestamp.hour", "path": "reference/api/pandas.timestamp.hour", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timestamp.is_leap_year", "path": "reference/api/pandas.timestamp.is_leap_year", "type": "Pandas arrays", "text": ["Return True if year is a leap year.", "Examples"]}, {"name": "pandas.Timestamp.is_month_end", "path": "reference/api/pandas.timestamp.is_month_end", "type": "Pandas arrays", "text": ["Return True if date is last day of month.", "Examples"]}, {"name": "pandas.Timestamp.is_month_start", "path": "reference/api/pandas.timestamp.is_month_start", "type": "Pandas arrays", "text": ["Return True if date is first day of month.", "Examples"]}, {"name": "pandas.Timestamp.is_quarter_end", "path": "reference/api/pandas.timestamp.is_quarter_end", "type": "Pandas arrays", "text": ["Return True if date is last day of the quarter.", "Examples"]}, {"name": "pandas.Timestamp.is_quarter_start", "path": "reference/api/pandas.timestamp.is_quarter_start", "type": "Pandas arrays", "text": ["Return True if date is first day of the quarter.", "Examples"]}, {"name": "pandas.Timestamp.is_year_end", "path": "reference/api/pandas.timestamp.is_year_end", "type": "Pandas arrays", "text": ["Return True if date is last day of the year.", "Examples"]}, {"name": "pandas.Timestamp.is_year_start", "path": "reference/api/pandas.timestamp.is_year_start", "type": "Pandas arrays", "text": ["Return True if date is first day of the year.", "Examples"]}, {"name": "pandas.Timestamp.isocalendar", "path": "reference/api/pandas.timestamp.isocalendar", "type": "Pandas arrays", "text": ["Return a 3-tuple containing ISO year, week number, and weekday."]}, {"name": "pandas.Timestamp.isoformat", "path": "reference/api/pandas.timestamp.isoformat", "type": "Pandas arrays", "text": ["Return the time formatted according to ISO 8610.", "The full format looks like \u2018YYYY-MM-DD HH:MM:SS.mmmmmmnnn\u2019. By default, the fractional part is omitted if self.microsecond == 0 and self.nanosecond == 0.", "If self.tzinfo is not None, the UTC offset is also attached, giving giving a full format of \u2018YYYY-MM-DD HH:MM:SS.mmmmmmnnn+HH:MM\u2019.", "String used as the separator between the date and time.", "Specifies the number of additional terms of the time to include. The valid values are \u2018auto\u2019, \u2018hours\u2019, \u2018minutes\u2019, \u2018seconds\u2019, \u2018milliseconds\u2019, \u2018microseconds\u2019, and \u2018nanoseconds\u2019.", "Examples"]}, {"name": "pandas.Timestamp.isoweekday", "path": "reference/api/pandas.timestamp.isoweekday", "type": "Pandas arrays", "text": ["Return the day of the week represented by the date. Monday == 1 \u2026 Sunday == 7."]}, {"name": "pandas.Timestamp.max", "path": "reference/api/pandas.timestamp.max", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timestamp.microsecond", "path": "reference/api/pandas.timestamp.microsecond", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timestamp.min", "path": "reference/api/pandas.timestamp.min", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timestamp.minute", "path": "reference/api/pandas.timestamp.minute", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timestamp.month", "path": "reference/api/pandas.timestamp.month", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timestamp.month_name", "path": "reference/api/pandas.timestamp.month_name", "type": "Pandas arrays", "text": ["Return the month name of the Timestamp with specified locale.", "Locale determining the language in which to return the month name.", "Examples", "Analogous for pd.NaT:"]}, {"name": "pandas.Timestamp.nanosecond", "path": "reference/api/pandas.timestamp.nanosecond", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timestamp.normalize", "path": "reference/api/pandas.timestamp.normalize", "type": "Pandas arrays", "text": ["Normalize Timestamp to midnight, preserving tz information.", "Examples"]}, {"name": "pandas.Timestamp.now", "path": "reference/api/pandas.timestamp.now", "type": "Pandas arrays", "text": ["Return new Timestamp object representing current time local to tz.", "Timezone to localize to.", "Examples", "Analogous for pd.NaT:"]}, {"name": "pandas.Timestamp.quarter", "path": "reference/api/pandas.timestamp.quarter", "type": "Pandas arrays", "text": ["Return the quarter of the year.", "Examples"]}, {"name": "pandas.Timestamp.replace", "path": "reference/api/pandas.timestamp.replace", "type": "Pandas arrays", "text": ["Implements datetime.replace, handles nanoseconds.", "Examples", "Create a timestamp object:", "Replace year and the hour:", "Replace timezone (not a conversion):", "Analogous for pd.NaT:"]}, {"name": "pandas.Timestamp.resolution", "path": "reference/api/pandas.timestamp.resolution", "type": "Input/output", "text": []}, {"name": "pandas.Timestamp.round", "path": "reference/api/pandas.timestamp.round", "type": "Pandas arrays", "text": ["Round the Timestamp to the specified resolution.", "Frequency string indicating the rounding resolution.", "The behavior is as follows:", "bool contains flags to determine if time is dst or not (note that this flag is only applicable for ambiguous fall dst dates).", "\u2018NaT\u2019 will return NaT for an ambiguous time.", "\u2018raise\u2019 will raise an AmbiguousTimeError for an ambiguous time.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST.", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time.", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time.", "\u2018NaT\u2019 will return NaT where there are nonexistent times.", "timedelta objects will shift nonexistent times by the timedelta.", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "Notes", "If the Timestamp has a timezone, rounding will take place relative to the local (\u201cwall\u201d) time and re-localized to the same timezone. When rounding near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.", "Examples", "Create a timestamp object:", "A timestamp can be rounded using multiple frequency units:", "freq can also be a multiple of a single unit, like \u20185T\u2019 (i.e. 5 minutes):", "or a combination of multiple units, like \u20181H30T\u2019 (i.e. 1 hour and 30 minutes):", "Analogous for pd.NaT:", "When rounding near a daylight savings time transition, use ambiguous or nonexistent to control how the timestamp should be re-localized."]}, {"name": "pandas.Timestamp.second", "path": "reference/api/pandas.timestamp.second", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timestamp.strftime", "path": "reference/api/pandas.timestamp.strftime", "type": "Pandas arrays", "text": ["Return a string representing the given POSIX timestamp controlled by an explicit format string.", "Format string to convert Timestamp to string. See strftime documentation for more information on the format string: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.", "Examples"]}, {"name": "pandas.Timestamp.strptime", "path": "reference/api/pandas.timestamp.strptime", "type": "Pandas arrays", "text": ["Function is not implemented. Use pd.to_datetime()."]}, {"name": "pandas.Timestamp.time", "path": "reference/api/pandas.timestamp.time", "type": "Pandas arrays", "text": ["Return time object with same time but with tzinfo=None."]}, {"name": "pandas.Timestamp.timestamp", "path": "reference/api/pandas.timestamp.timestamp", "type": "Pandas arrays", "text": ["Return POSIX timestamp as float.", "Examples"]}, {"name": "pandas.Timestamp.timetuple", "path": "reference/api/pandas.timestamp.timetuple", "type": "Pandas arrays", "text": ["Return time tuple, compatible with time.localtime()."]}, {"name": "pandas.Timestamp.timetz", "path": "reference/api/pandas.timestamp.timetz", "type": "Pandas arrays", "text": ["Return time object with same time and tzinfo."]}, {"name": "pandas.Timestamp.to_datetime64", "path": "reference/api/pandas.timestamp.to_datetime64", "type": "Pandas arrays", "text": ["Return a numpy.datetime64 object with \u2018ns\u2019 precision."]}, {"name": "pandas.Timestamp.to_julian_date", "path": "reference/api/pandas.timestamp.to_julian_date", "type": "Pandas arrays", "text": ["Convert TimeStamp to a Julian Date. 0 Julian date is noon January 1, 4713 BC.", "Examples"]}, {"name": "pandas.Timestamp.to_numpy", "path": "reference/api/pandas.timestamp.to_numpy", "type": "Pandas arrays", "text": ["Convert the Timestamp to a NumPy datetime64.", "New in version 0.25.0.", "This is an alias method for Timestamp.to_datetime64(). The dtype and copy parameters are available here only for compatibility. Their values will not affect the return value.", "See also", "Similar method for DatetimeIndex.", "Examples", "Analogous for pd.NaT:"]}, {"name": "pandas.Timestamp.to_period", "path": "reference/api/pandas.timestamp.to_period", "type": "Input/output", "text": ["Return an period of which this timestamp is an observation.", "Examples"]}, {"name": "pandas.Timestamp.to_pydatetime", "path": "reference/api/pandas.timestamp.to_pydatetime", "type": "Pandas arrays", "text": ["Convert a Timestamp object to a native Python datetime object.", "If warn=True, issue a warning if nanoseconds is nonzero.", "Examples", "Analogous for pd.NaT:"]}, {"name": "pandas.Timestamp.today", "path": "reference/api/pandas.timestamp.today", "type": "Pandas arrays", "text": ["Return the current time in the local timezone. This differs from datetime.today() in that it can be localized to a passed timezone.", "Timezone to localize to.", "Examples", "Analogous for pd.NaT:"]}, {"name": "pandas.Timestamp.toordinal", "path": "reference/api/pandas.timestamp.toordinal", "type": "Pandas arrays", "text": ["Return proleptic Gregorian ordinal. January 1 of year 1 is day 1."]}, {"name": "pandas.Timestamp.tz", "path": "reference/api/pandas.timestamp.tz", "type": "Pandas arrays", "text": ["Alias for tzinfo.", "Examples"]}, {"name": "pandas.Timestamp.tz_convert", "path": "reference/api/pandas.timestamp.tz_convert", "type": "Pandas arrays", "text": ["Convert timezone-aware Timestamp to another time zone.", "Time zone for time which Timestamp will be converted to. None will remove timezone holding UTC time.", "If Timestamp is tz-naive.", "Examples", "Create a timestamp object with UTC timezone:", "Change to Tokyo timezone:", "Can also use astimezone:", "Analogous for pd.NaT:"]}, {"name": "pandas.Timestamp.tz_localize", "path": "reference/api/pandas.timestamp.tz_localize", "type": "Pandas arrays", "text": ["Convert naive Timestamp to local time zone, or remove timezone from timezone-aware Timestamp.", "Time zone for time which Timestamp will be converted to. None will remove timezone holding local time.", "When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled.", "The behavior is as follows:", "bool contains flags to determine if time is dst or not (note that this flag is only applicable for ambiguous fall dst dates).", "\u2018NaT\u2019 will return NaT for an ambiguous time.", "\u2018raise\u2019 will raise an AmbiguousTimeError for an ambiguous time.", "A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST.", "The behavior is as follows:", "\u2018shift_forward\u2019 will shift the nonexistent time forward to the closest existing time.", "\u2018shift_backward\u2019 will shift the nonexistent time backward to the closest existing time.", "\u2018NaT\u2019 will return NaT where there are nonexistent times.", "timedelta objects will shift nonexistent times by the timedelta.", "\u2018raise\u2019 will raise an NonExistentTimeError if there are nonexistent times.", "If the Timestamp is tz-aware and tz is not None.", "Examples", "Create a naive timestamp object:", "Add \u2018Europe/Stockholm\u2019 as timezone:", "Analogous for pd.NaT:"]}, {"name": "pandas.Timestamp.tzinfo", "path": "reference/api/pandas.timestamp.tzinfo", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timestamp.tzname", "path": "reference/api/pandas.timestamp.tzname", "type": "Pandas arrays", "text": ["Return self.tzinfo.tzname(self)."]}, {"name": "pandas.Timestamp.utcfromtimestamp", "path": "reference/api/pandas.timestamp.utcfromtimestamp", "type": "Pandas arrays", "text": ["Construct a naive UTC datetime from a POSIX timestamp.", "Examples"]}, {"name": "pandas.Timestamp.utcnow", "path": "reference/api/pandas.timestamp.utcnow", "type": "Pandas arrays", "text": ["Return a new Timestamp representing UTC day and time.", "Examples"]}, {"name": "pandas.Timestamp.utcoffset", "path": "reference/api/pandas.timestamp.utcoffset", "type": "Data offsets", "text": ["Return self.tzinfo.utcoffset(self)."]}, {"name": "pandas.Timestamp.utctimetuple", "path": "reference/api/pandas.timestamp.utctimetuple", "type": "Pandas arrays", "text": ["Return UTC time tuple, compatible with time.localtime()."]}, {"name": "pandas.Timestamp.value", "path": "reference/api/pandas.timestamp.value", "type": "Pandas arrays", "text": []}, {"name": "pandas.Timestamp.week", "path": "reference/api/pandas.timestamp.week", "type": "Pandas arrays", "text": ["Return the week number of the year.", "Examples"]}, {"name": "pandas.Timestamp.weekday", "path": "reference/api/pandas.timestamp.weekday", "type": "Pandas arrays", "text": ["Return the day of the week represented by the date. Monday == 0 \u2026 Sunday == 6."]}, {"name": "pandas.Timestamp.weekofyear", "path": "reference/api/pandas.timestamp.weekofyear", "type": "Pandas arrays", "text": ["Return the week number of the year.", "Examples"]}, {"name": "pandas.Timestamp.year", "path": "reference/api/pandas.timestamp.year", "type": "Pandas arrays", "text": []}, {"name": "pandas.to_datetime", "path": "reference/api/pandas.to_datetime", "type": "General functions", "text": ["Convert argument to datetime.", "This function converts a scalar, array-like, Series or DataFrame/dict-like to a pandas datetime object.", "The object to convert to a datetime. If a DataFrame is provided, the method expects minimally the following columns: \"year\", \"month\", \"day\".", "If 'raise', then invalid parsing will raise an exception.", "If 'coerce', then invalid parsing will be set as NaT.", "If 'ignore', then invalid parsing will return the input.", "Specify a date parse order if arg is str or is list-like. If True, parses dates with the day first, e.g. \"10/11/12\" is parsed as 2012-11-10.", "Warning", "dayfirst=True is not strict, but will prefer to parse with day first. If a delimited date string cannot be parsed in accordance with the given dayfirst option, e.g. to_datetime(['31-12-2021']), then a warning will be shown.", "Specify a date parse order if arg is str or is list-like.", "If True parses dates with the year first, e.g. \"10/11/12\" is parsed as 2010-11-12.", "If both dayfirst and yearfirst are True, yearfirst is preceded (same as dateutil).", "Warning", "yearfirst=True is not strict, but will prefer to parse with year first.", "Control timezone-related parsing, localization and conversion.", "If True, the function always returns a timezone-aware UTC-localized Timestamp, Series or DatetimeIndex. To do this, timezone-naive inputs are localized as UTC, while timezone-aware inputs are converted to UTC.", "If False (default), inputs will not be coerced to UTC. Timezone-naive inputs will remain naive, while timezone-aware ones will keep their time offsets. Limitations exist for mixed offsets (typically, daylight savings), see Examples section for details.", "See also: pandas general documentation about timezone conversion and localization.", "The strftime to parse time, e.g. \"%d/%m/%Y\". Note that \"%f\" will parse all the way up to nanoseconds. See strftime documentation for more information on choices.", "Control how format is used:", "If True, require an exact format match.", "If False, allow the format to match anywhere in the target string.", "The unit of the arg (D,s,ms,us,ns) denote the unit, which is an integer or float number. This will be based off the origin. Example, with unit='ms' and origin='unix' (the default), this would calculate the number of milliseconds to the unix epoch start.", "If True and no format is given, attempt to infer the format of the datetime strings based on the first non-NaN element, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the parsing speed by ~5-10x.", "Define the reference date. The numeric values would be parsed as number of units (defined by unit) since this reference date.", "If 'unix' (or POSIX) time; origin is set to 1970-01-01.", "If 'julian', unit must be 'D', and origin is set to beginning of Julian Calendar. Julian day number 0 is assigned to the day starting at noon on January 1, 4713 BC.", "If Timestamp convertible, origin is set to Timestamp identified by origin.", "If True, use a cache of unique, converted dates to apply the datetime conversion. May produce significant speed-up when parsing duplicate date strings, especially ones with timezone offsets. The cache is only used when there are at least 50 values. The presence of out-of-bounds values will render the cache unusable and may slow down parsing.", "Changed in version 0.25.0: changed default value from False to True.", "If parsing succeeded. Return type depends on input (types in parenthesis correspond to fallback in case of unsuccessful timezone or out-of-range timestamp parsing):", "scalar: Timestamp (or datetime.datetime)", "array-like: DatetimeIndex (or Series with object dtype containing datetime.datetime)", "Series: Series of datetime64 dtype (or Series of object dtype containing datetime.datetime)", "DataFrame: Series of datetime64 dtype (or Series of object dtype containing datetime.datetime)", "When parsing a date from string fails.", "When another datetime conversion error happens. For example when one of \u2018year\u2019, \u2018month\u2019, day\u2019 columns is missing in a DataFrame, or when a Timezone-aware datetime.datetime is found in an array-like of mixed time offsets, and utc=False.", "See also", "Cast argument to a specified dtype.", "Convert argument to timedelta.", "Convert dtypes.", "Notes", "Many input types are supported, and lead to different output types:", "scalars can be int, float, str, datetime object (from stdlib datetime module or numpy). They are converted to Timestamp when possible, otherwise they are converted to datetime.datetime. None/NaN/null scalars are converted to NaT.", "array-like can contain int, float, str, datetime objects. They are converted to DatetimeIndex when possible, otherwise they are converted to Index with object dtype, containing datetime.datetime. None/NaN/null entries are converted to NaT in both cases.", "Series are converted to Series with datetime64 dtype when possible, otherwise they are converted to Series with object dtype, containing datetime.datetime. None/NaN/null entries are converted to NaT in both cases.", "DataFrame/dict-like are converted to Series with datetime64 dtype. For each row a datetime is created from assembling the various dataframe columns. Column keys can be common abbreviations like [\u2018year\u2019, \u2018month\u2019, \u2018day\u2019, \u2018minute\u2019, \u2018second\u2019, \u2018ms\u2019, \u2018us\u2019, \u2018ns\u2019]) or plurals of the same.", "The following causes are responsible for datetime.datetime objects being returned (possibly inside an Index or a Series with object dtype) instead of a proper pandas designated type (Timestamp, DatetimeIndex or Series with datetime64 dtype):", "when any input element is before Timestamp.min or after Timestamp.max, see timestamp limitations.", "when utc=False (default) and the input is an array-like or Series containing mixed naive/aware datetime, or aware with mixed time offsets. Note that this happens in the (quite frequent) situation when the timezone has a daylight savings policy. In that case you may wish to use utc=True.", "Examples", "Handling various input formats", "Assembling a datetime from multiple columns of a DataFrame. The keys can be common abbreviations like [\u2018year\u2019, \u2018month\u2019, \u2018day\u2019, \u2018minute\u2019, \u2018second\u2019, \u2018ms\u2019, \u2018us\u2019, \u2018ns\u2019]) or plurals of the same", "Passing infer_datetime_format=True can often-times speedup a parsing if its not an ISO8601 format exactly, but in a regular format.", "Using a unix epoch time", "Warning", "For float arg, precision rounding might happen. To prevent unexpected behavior use a fixed-width exact type.", "Using a non-unix epoch origin", "Non-convertible date/times", "If a date does not meet the timestamp limitations, passing errors='ignore' will return the original input instead of raising any exception.", "Passing errors='coerce' will force an out-of-bounds date to NaT, in addition to forcing non-dates (or non-parseable dates) to NaT.", "Timezones and time offsets", "The default behaviour (utc=False) is as follows:", "Timezone-naive inputs are converted to timezone-naive DatetimeIndex:", "Timezone-aware inputs with constant time offset are converted to timezone-aware DatetimeIndex:", "However, timezone-aware inputs with mixed time offsets (for example issued from a timezone with daylight savings, such as Europe/Paris) are not successfully converted to a DatetimeIndex. Instead a simple Index containing datetime.datetime objects is returned:", "A mix of timezone-aware and timezone-naive inputs is converted to a timezone-aware DatetimeIndex if the offsets of the timezone-aware are constant:", "Finally, mixing timezone-aware strings and datetime.datetime always raises an error, even if the elements all have the same time offset.", "Setting utc=True solves most of the above issues:", "Timezone-naive inputs are localized as UTC", "Timezone-aware inputs are converted to UTC (the output represents the exact same datetime, but viewed from the UTC time offset +00:00).", "Inputs can contain both naive and aware, string or datetime, the above rules still apply"]}, {"name": "pandas.to_numeric", "path": "reference/api/pandas.to_numeric", "type": "General functions", "text": ["Convert argument to a numeric type.", "The default return dtype is float64 or int64 depending on the data supplied. Use the downcast parameter to obtain other dtypes.", "Please note that precision loss may occur if really large numbers are passed in. Due to the internal limitations of ndarray, if numbers smaller than -9223372036854775808 (np.iinfo(np.int64).min) or larger than 18446744073709551615 (np.iinfo(np.uint64).max) are passed in, it is very likely they will be converted to float so that they can stored in an ndarray. These warnings apply similarly to Series since it internally leverages ndarray.", "Argument to be converted.", "If \u2018raise\u2019, then invalid parsing will raise an exception.", "If \u2018coerce\u2019, then invalid parsing will be set as NaN.", "If \u2018ignore\u2019, then invalid parsing will return the input.", "Can be \u2018integer\u2019, \u2018signed\u2019, \u2018unsigned\u2019, or \u2018float\u2019. If not None, and if the data has been successfully cast to a numerical dtype (or if the data was numeric to begin with), downcast that resulting data to the smallest numerical dtype possible according to the following rules:", "\u2018integer\u2019 or \u2018signed\u2019: smallest signed int dtype (min.: np.int8)", "\u2018unsigned\u2019: smallest unsigned int dtype (min.: np.uint8)", "\u2018float\u2019: smallest float dtype (min.: np.float32)", "As this behaviour is separate from the core conversion to numeric values, any errors raised during the downcasting will be surfaced regardless of the value of the \u2018errors\u2019 input.", "In addition, downcasting will only occur if the size of the resulting data\u2019s dtype is strictly larger than the dtype it is to be cast to, so if none of the dtypes checked satisfy that specification, no downcasting will be performed on the data.", "Numeric if parsing succeeded. Return type depends on input. Series if Series, otherwise ndarray.", "See also", "Cast argument to a specified dtype.", "Convert argument to datetime.", "Convert argument to timedelta.", "Cast a numpy array to a specified type.", "Convert dtypes.", "Examples", "Take separate series and convert to numeric, coercing when told to", "Downcasting of nullable integer and floating dtypes is supported:"]}, {"name": "pandas.to_timedelta", "path": "reference/api/pandas.to_timedelta", "type": "General functions", "text": ["Convert argument to timedelta.", "Timedeltas are absolute differences in times, expressed in difference units (e.g. days, hours, minutes, seconds). This method converts an argument from a recognized timedelta format / value into a Timedelta type.", "The data to be converted to timedelta.", "Deprecated since version 1.2: Strings with units \u2018M\u2019, \u2018Y\u2019 and \u2018y\u2019 do not represent unambiguous timedelta values and will be removed in a future version", "Denotes the unit of the arg for numeric arg. Defaults to \"ns\".", "Possible values:", "\u2018W\u2019", "\u2018D\u2019 / \u2018days\u2019 / \u2018day\u2019", "\u2018hours\u2019 / \u2018hour\u2019 / \u2018hr\u2019 / \u2018h\u2019", "\u2018m\u2019 / \u2018minute\u2019 / \u2018min\u2019 / \u2018minutes\u2019 / \u2018T\u2019", "\u2018S\u2019 / \u2018seconds\u2019 / \u2018sec\u2019 / \u2018second\u2019", "\u2018ms\u2019 / \u2018milliseconds\u2019 / \u2018millisecond\u2019 / \u2018milli\u2019 / \u2018millis\u2019 / \u2018L\u2019", "\u2018us\u2019 / \u2018microseconds\u2019 / \u2018microsecond\u2019 / \u2018micro\u2019 / \u2018micros\u2019 / \u2018U\u2019", "\u2018ns\u2019 / \u2018nanoseconds\u2019 / \u2018nano\u2019 / \u2018nanos\u2019 / \u2018nanosecond\u2019 / \u2018N\u2019", "Changed in version 1.1.0: Must not be specified when arg context strings and errors=\"raise\".", "If \u2018raise\u2019, then invalid parsing will raise an exception.", "If \u2018coerce\u2019, then invalid parsing will be set as NaT.", "If \u2018ignore\u2019, then invalid parsing will return the input.", "If parsing succeeded. Return type depends on input:", "list-like: TimedeltaIndex of timedelta64 dtype", "Series: Series of timedelta64 dtype", "scalar: Timedelta", "See also", "Cast argument to a specified dtype.", "Convert argument to datetime.", "Convert dtypes.", "Notes", "If the precision is higher than nanoseconds, the precision of the duration is truncated to nanoseconds for string inputs.", "Examples", "Parsing a single string to a Timedelta:", "Parsing a list or array of strings:", "Converting numbers by specifying the unit keyword argument:"]}, {"name": "pandas.tseries.frequencies.to_offset", "path": "reference/api/pandas.tseries.frequencies.to_offset", "type": "Data offsets", "text": ["Return DateOffset object from string or tuple representation or datetime.timedelta object.", "If freq is an invalid frequency", "See also", "Standard kind of date increment used for a date range.", "Examples"]}, {"name": "pandas.tseries.offsets.BDay", "path": "reference/api/pandas.tseries.offsets.bday", "type": "Data offsets", "text": ["alias of pandas._libs.tslibs.offsets.BusinessDay"]}, {"name": "pandas.tseries.offsets.BMonthBegin", "path": "reference/api/pandas.tseries.offsets.bmonthbegin", "type": "Data offsets", "text": ["alias of pandas._libs.tslibs.offsets.BusinessMonthBegin"]}, {"name": "pandas.tseries.offsets.BMonthEnd", "path": "reference/api/pandas.tseries.offsets.bmonthend", "type": "Data offsets", "text": ["alias of pandas._libs.tslibs.offsets.BusinessMonthEnd"]}, {"name": "pandas.tseries.offsets.BQuarterBegin", "path": "reference/api/pandas.tseries.offsets.bquarterbegin", "type": "Data offsets", "text": ["DateOffset increments between the first business day of each Quarter.", "startingMonth = 1 corresponds to dates like 1/01/2007, 4/01/2007, \u2026 startingMonth = 2 corresponds to dates like 2/01/2007, 5/01/2007, \u2026 startingMonth = 3 corresponds to dates like 3/01/2007, 6/01/2007, \u2026", "Examples", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "startingMonth", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.BQuarterBegin.__call__", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.BQuarterBegin.apply", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.apply_index", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.base", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.BQuarterBegin.copy", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.freqstr", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.is_anchored", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.is_month_end", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.is_month_start", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.is_on_offset", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.is_year_end", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.is_year_start", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.isAnchored", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.kwds", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.n", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.name", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.nanos", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.normalize", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.onOffset", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.rollback", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.BQuarterBegin.rollforward", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.BQuarterBegin.rule_code", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterBegin.startingMonth", "path": "reference/api/pandas.tseries.offsets.bquarterbegin.startingmonth", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd", "path": "reference/api/pandas.tseries.offsets.bquarterend", "type": "Data offsets", "text": ["DateOffset increments between the last business day of each Quarter.", "startingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, \u2026 startingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, \u2026 startingMonth = 3 corresponds to dates like 3/30/2007, 6/29/2007, \u2026", "Examples", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "startingMonth", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.BQuarterEnd.__call__", "path": "reference/api/pandas.tseries.offsets.bquarterend.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.BQuarterEnd.apply", "path": "reference/api/pandas.tseries.offsets.bquarterend.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.apply_index", "path": "reference/api/pandas.tseries.offsets.bquarterend.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.base", "path": "reference/api/pandas.tseries.offsets.bquarterend.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.BQuarterEnd.copy", "path": "reference/api/pandas.tseries.offsets.bquarterend.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.freqstr", "path": "reference/api/pandas.tseries.offsets.bquarterend.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.is_anchored", "path": "reference/api/pandas.tseries.offsets.bquarterend.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.is_month_end", "path": "reference/api/pandas.tseries.offsets.bquarterend.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.is_month_start", "path": "reference/api/pandas.tseries.offsets.bquarterend.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.is_on_offset", "path": "reference/api/pandas.tseries.offsets.bquarterend.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.bquarterend.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.bquarterend.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.is_year_end", "path": "reference/api/pandas.tseries.offsets.bquarterend.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.is_year_start", "path": "reference/api/pandas.tseries.offsets.bquarterend.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.isAnchored", "path": "reference/api/pandas.tseries.offsets.bquarterend.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.kwds", "path": "reference/api/pandas.tseries.offsets.bquarterend.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.n", "path": "reference/api/pandas.tseries.offsets.bquarterend.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.name", "path": "reference/api/pandas.tseries.offsets.bquarterend.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.nanos", "path": "reference/api/pandas.tseries.offsets.bquarterend.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.normalize", "path": "reference/api/pandas.tseries.offsets.bquarterend.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.onOffset", "path": "reference/api/pandas.tseries.offsets.bquarterend.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.rollback", "path": "reference/api/pandas.tseries.offsets.bquarterend.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.BQuarterEnd.rollforward", "path": "reference/api/pandas.tseries.offsets.bquarterend.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.BQuarterEnd.rule_code", "path": "reference/api/pandas.tseries.offsets.bquarterend.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BQuarterEnd.startingMonth", "path": "reference/api/pandas.tseries.offsets.bquarterend.startingmonth", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay", "path": "reference/api/pandas.tseries.offsets.businessday", "type": "Data offsets", "text": ["DateOffset subclass representing possibly n business days.", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "offset", "Alias for self._offset.", "calendar", "freqstr", "holidays", "kwds", "n", "name", "nanos", "normalize", "rule_code", "weekmask", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.BusinessDay.__call__", "path": "reference/api/pandas.tseries.offsets.businessday.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.BusinessDay.apply", "path": "reference/api/pandas.tseries.offsets.businessday.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.apply_index", "path": "reference/api/pandas.tseries.offsets.businessday.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.base", "path": "reference/api/pandas.tseries.offsets.businessday.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.BusinessDay.calendar", "path": "reference/api/pandas.tseries.offsets.businessday.calendar", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.copy", "path": "reference/api/pandas.tseries.offsets.businessday.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.freqstr", "path": "reference/api/pandas.tseries.offsets.businessday.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.holidays", "path": "reference/api/pandas.tseries.offsets.businessday.holidays", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.is_anchored", "path": "reference/api/pandas.tseries.offsets.businessday.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.is_month_end", "path": "reference/api/pandas.tseries.offsets.businessday.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.is_month_start", "path": "reference/api/pandas.tseries.offsets.businessday.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.is_on_offset", "path": "reference/api/pandas.tseries.offsets.businessday.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.businessday.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.businessday.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.is_year_end", "path": "reference/api/pandas.tseries.offsets.businessday.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.is_year_start", "path": "reference/api/pandas.tseries.offsets.businessday.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.isAnchored", "path": "reference/api/pandas.tseries.offsets.businessday.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.kwds", "path": "reference/api/pandas.tseries.offsets.businessday.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.n", "path": "reference/api/pandas.tseries.offsets.businessday.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.name", "path": "reference/api/pandas.tseries.offsets.businessday.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.nanos", "path": "reference/api/pandas.tseries.offsets.businessday.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.normalize", "path": "reference/api/pandas.tseries.offsets.businessday.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.offset", "path": "reference/api/pandas.tseries.offsets.businessday.offset", "type": "Data offsets", "text": ["Alias for self._offset."]}, {"name": "pandas.tseries.offsets.BusinessDay.onOffset", "path": "reference/api/pandas.tseries.offsets.businessday.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.rollback", "path": "reference/api/pandas.tseries.offsets.businessday.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.BusinessDay.rollforward", "path": "reference/api/pandas.tseries.offsets.businessday.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.BusinessDay.rule_code", "path": "reference/api/pandas.tseries.offsets.businessday.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessDay.weekmask", "path": "reference/api/pandas.tseries.offsets.businessday.weekmask", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour", "path": "reference/api/pandas.tseries.offsets.businesshour", "type": "Data offsets", "text": ["DateOffset subclass representing possibly n business hours.", "The number of months represented.", "Normalize start/end dates to midnight before generating date range.", "Weekmask of valid business days, passed to numpy.busdaycalendar.", "Start time of your custom business hour in 24h format.", "End time of your custom business hour in 24h format.", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "next_bday", "Used for moving to next business day.", "offset", "Alias for self._offset.", "calendar", "end", "freqstr", "holidays", "kwds", "n", "name", "nanos", "normalize", "rule_code", "start", "weekmask", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback(other)", "Roll provided date backward to next offset only if not on offset.", "rollforward(other)", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.BusinessHour.__call__", "path": "reference/api/pandas.tseries.offsets.businesshour.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.BusinessHour.apply", "path": "reference/api/pandas.tseries.offsets.businesshour.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.apply_index", "path": "reference/api/pandas.tseries.offsets.businesshour.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.base", "path": "reference/api/pandas.tseries.offsets.businesshour.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.BusinessHour.calendar", "path": "reference/api/pandas.tseries.offsets.businesshour.calendar", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.copy", "path": "reference/api/pandas.tseries.offsets.businesshour.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.end", "path": "reference/api/pandas.tseries.offsets.businesshour.end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.freqstr", "path": "reference/api/pandas.tseries.offsets.businesshour.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.holidays", "path": "reference/api/pandas.tseries.offsets.businesshour.holidays", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.is_anchored", "path": "reference/api/pandas.tseries.offsets.businesshour.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.is_month_end", "path": "reference/api/pandas.tseries.offsets.businesshour.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.is_month_start", "path": "reference/api/pandas.tseries.offsets.businesshour.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.is_on_offset", "path": "reference/api/pandas.tseries.offsets.businesshour.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.businesshour.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.businesshour.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.is_year_end", "path": "reference/api/pandas.tseries.offsets.businesshour.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.is_year_start", "path": "reference/api/pandas.tseries.offsets.businesshour.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.isAnchored", "path": "reference/api/pandas.tseries.offsets.businesshour.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.kwds", "path": "reference/api/pandas.tseries.offsets.businesshour.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.n", "path": "reference/api/pandas.tseries.offsets.businesshour.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.name", "path": "reference/api/pandas.tseries.offsets.businesshour.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.nanos", "path": "reference/api/pandas.tseries.offsets.businesshour.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.next_bday", "path": "reference/api/pandas.tseries.offsets.businesshour.next_bday", "type": "Data offsets", "text": ["Used for moving to next business day."]}, {"name": "pandas.tseries.offsets.BusinessHour.normalize", "path": "reference/api/pandas.tseries.offsets.businesshour.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.offset", "path": "reference/api/pandas.tseries.offsets.businesshour.offset", "type": "Data offsets", "text": ["Alias for self._offset."]}, {"name": "pandas.tseries.offsets.BusinessHour.onOffset", "path": "reference/api/pandas.tseries.offsets.businesshour.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.rollback", "path": "reference/api/pandas.tseries.offsets.businesshour.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset."]}, {"name": "pandas.tseries.offsets.BusinessHour.rollforward", "path": "reference/api/pandas.tseries.offsets.businesshour.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset."]}, {"name": "pandas.tseries.offsets.BusinessHour.rule_code", "path": "reference/api/pandas.tseries.offsets.businesshour.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.start", "path": "reference/api/pandas.tseries.offsets.businesshour.start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessHour.weekmask", "path": "reference/api/pandas.tseries.offsets.businesshour.weekmask", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin", "type": "Data offsets", "text": ["DateOffset of one month at the first business day.", "Examples", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.__call__", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.apply", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.apply_index", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.base", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.copy", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.freqstr", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.is_anchored", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.is_month_end", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.is_month_start", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.is_on_offset", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.is_year_end", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.is_year_start", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.isAnchored", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.kwds", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.n", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.name", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.nanos", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.normalize", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.onOffset", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.rollback", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.rollforward", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.BusinessMonthBegin.rule_code", "path": "reference/api/pandas.tseries.offsets.businessmonthbegin.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd", "path": "reference/api/pandas.tseries.offsets.businessmonthend", "type": "Data offsets", "text": ["DateOffset increments between the last business day of the month.", "Examples", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.__call__", "path": "reference/api/pandas.tseries.offsets.businessmonthend.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.apply", "path": "reference/api/pandas.tseries.offsets.businessmonthend.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.apply_index", "path": "reference/api/pandas.tseries.offsets.businessmonthend.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.base", "path": "reference/api/pandas.tseries.offsets.businessmonthend.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.copy", "path": "reference/api/pandas.tseries.offsets.businessmonthend.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.freqstr", "path": "reference/api/pandas.tseries.offsets.businessmonthend.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.is_anchored", "path": "reference/api/pandas.tseries.offsets.businessmonthend.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.is_month_end", "path": "reference/api/pandas.tseries.offsets.businessmonthend.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.is_month_start", "path": "reference/api/pandas.tseries.offsets.businessmonthend.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.is_on_offset", "path": "reference/api/pandas.tseries.offsets.businessmonthend.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.businessmonthend.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.businessmonthend.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.is_year_end", "path": "reference/api/pandas.tseries.offsets.businessmonthend.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.is_year_start", "path": "reference/api/pandas.tseries.offsets.businessmonthend.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.isAnchored", "path": "reference/api/pandas.tseries.offsets.businessmonthend.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.kwds", "path": "reference/api/pandas.tseries.offsets.businessmonthend.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.n", "path": "reference/api/pandas.tseries.offsets.businessmonthend.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.name", "path": "reference/api/pandas.tseries.offsets.businessmonthend.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.nanos", "path": "reference/api/pandas.tseries.offsets.businessmonthend.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.normalize", "path": "reference/api/pandas.tseries.offsets.businessmonthend.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.onOffset", "path": "reference/api/pandas.tseries.offsets.businessmonthend.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.rollback", "path": "reference/api/pandas.tseries.offsets.businessmonthend.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.rollforward", "path": "reference/api/pandas.tseries.offsets.businessmonthend.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.BusinessMonthEnd.rule_code", "path": "reference/api/pandas.tseries.offsets.businessmonthend.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin", "path": "reference/api/pandas.tseries.offsets.byearbegin", "type": "Data offsets", "text": ["DateOffset increments between the first business day of the year.", "Examples", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "month", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.BYearBegin.__call__", "path": "reference/api/pandas.tseries.offsets.byearbegin.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.BYearBegin.apply", "path": "reference/api/pandas.tseries.offsets.byearbegin.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.apply_index", "path": "reference/api/pandas.tseries.offsets.byearbegin.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.base", "path": "reference/api/pandas.tseries.offsets.byearbegin.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.BYearBegin.copy", "path": "reference/api/pandas.tseries.offsets.byearbegin.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.freqstr", "path": "reference/api/pandas.tseries.offsets.byearbegin.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.is_anchored", "path": "reference/api/pandas.tseries.offsets.byearbegin.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.is_month_end", "path": "reference/api/pandas.tseries.offsets.byearbegin.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.is_month_start", "path": "reference/api/pandas.tseries.offsets.byearbegin.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.is_on_offset", "path": "reference/api/pandas.tseries.offsets.byearbegin.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.byearbegin.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.byearbegin.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.is_year_end", "path": "reference/api/pandas.tseries.offsets.byearbegin.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.is_year_start", "path": "reference/api/pandas.tseries.offsets.byearbegin.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.isAnchored", "path": "reference/api/pandas.tseries.offsets.byearbegin.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.kwds", "path": "reference/api/pandas.tseries.offsets.byearbegin.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.month", "path": "reference/api/pandas.tseries.offsets.byearbegin.month", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.n", "path": "reference/api/pandas.tseries.offsets.byearbegin.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.name", "path": "reference/api/pandas.tseries.offsets.byearbegin.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.nanos", "path": "reference/api/pandas.tseries.offsets.byearbegin.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.normalize", "path": "reference/api/pandas.tseries.offsets.byearbegin.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.onOffset", "path": "reference/api/pandas.tseries.offsets.byearbegin.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearBegin.rollback", "path": "reference/api/pandas.tseries.offsets.byearbegin.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.BYearBegin.rollforward", "path": "reference/api/pandas.tseries.offsets.byearbegin.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.BYearBegin.rule_code", "path": "reference/api/pandas.tseries.offsets.byearbegin.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd", "path": "reference/api/pandas.tseries.offsets.byearend", "type": "Data offsets", "text": ["DateOffset increments between the last business day of the year.", "Examples", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "month", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.BYearEnd.__call__", "path": "reference/api/pandas.tseries.offsets.byearend.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.BYearEnd.apply", "path": "reference/api/pandas.tseries.offsets.byearend.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.apply_index", "path": "reference/api/pandas.tseries.offsets.byearend.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.base", "path": "reference/api/pandas.tseries.offsets.byearend.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.BYearEnd.copy", "path": "reference/api/pandas.tseries.offsets.byearend.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.freqstr", "path": "reference/api/pandas.tseries.offsets.byearend.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.is_anchored", "path": "reference/api/pandas.tseries.offsets.byearend.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.is_month_end", "path": "reference/api/pandas.tseries.offsets.byearend.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.is_month_start", "path": "reference/api/pandas.tseries.offsets.byearend.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.is_on_offset", "path": "reference/api/pandas.tseries.offsets.byearend.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.byearend.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.byearend.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.is_year_end", "path": "reference/api/pandas.tseries.offsets.byearend.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.is_year_start", "path": "reference/api/pandas.tseries.offsets.byearend.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.isAnchored", "path": "reference/api/pandas.tseries.offsets.byearend.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.kwds", "path": "reference/api/pandas.tseries.offsets.byearend.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.month", "path": "reference/api/pandas.tseries.offsets.byearend.month", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.n", "path": "reference/api/pandas.tseries.offsets.byearend.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.name", "path": "reference/api/pandas.tseries.offsets.byearend.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.nanos", "path": "reference/api/pandas.tseries.offsets.byearend.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.normalize", "path": "reference/api/pandas.tseries.offsets.byearend.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.onOffset", "path": "reference/api/pandas.tseries.offsets.byearend.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.BYearEnd.rollback", "path": "reference/api/pandas.tseries.offsets.byearend.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.BYearEnd.rollforward", "path": "reference/api/pandas.tseries.offsets.byearend.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.BYearEnd.rule_code", "path": "reference/api/pandas.tseries.offsets.byearend.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CBMonthBegin", "path": "reference/api/pandas.tseries.offsets.cbmonthbegin", "type": "Data offsets", "text": ["alias of pandas._libs.tslibs.offsets.CustomBusinessMonthBegin"]}, {"name": "pandas.tseries.offsets.CBMonthEnd", "path": "reference/api/pandas.tseries.offsets.cbmonthend", "type": "Data offsets", "text": ["alias of pandas._libs.tslibs.offsets.CustomBusinessMonthEnd"]}, {"name": "pandas.tseries.offsets.CDay", "path": "reference/api/pandas.tseries.offsets.cday", "type": "Data offsets", "text": ["alias of pandas._libs.tslibs.offsets.CustomBusinessDay"]}, {"name": "pandas.tseries.offsets.CustomBusinessDay", "path": "reference/api/pandas.tseries.offsets.custombusinessday", "type": "Data offsets", "text": ["DateOffset subclass representing custom business days excluding holidays.", "Normalize start/end dates to midnight before generating date range.", "Weekmask of valid business days, passed to numpy.busdaycalendar.", "List/array of dates to exclude from the set of valid business days, passed to numpy.busdaycalendar.", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "offset", "Alias for self._offset.", "calendar", "freqstr", "holidays", "kwds", "n", "name", "nanos", "normalize", "rule_code", "weekmask", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.CustomBusinessDay.__call__", "path": "reference/api/pandas.tseries.offsets.custombusinessday.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.CustomBusinessDay.apply", "path": "reference/api/pandas.tseries.offsets.custombusinessday.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.apply_index", "path": "reference/api/pandas.tseries.offsets.custombusinessday.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.base", "path": "reference/api/pandas.tseries.offsets.custombusinessday.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.CustomBusinessDay.calendar", "path": "reference/api/pandas.tseries.offsets.custombusinessday.calendar", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.copy", "path": "reference/api/pandas.tseries.offsets.custombusinessday.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.freqstr", "path": "reference/api/pandas.tseries.offsets.custombusinessday.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.holidays", "path": "reference/api/pandas.tseries.offsets.custombusinessday.holidays", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.is_anchored", "path": "reference/api/pandas.tseries.offsets.custombusinessday.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.is_month_end", "path": "reference/api/pandas.tseries.offsets.custombusinessday.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.is_month_start", "path": "reference/api/pandas.tseries.offsets.custombusinessday.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.is_on_offset", "path": "reference/api/pandas.tseries.offsets.custombusinessday.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.custombusinessday.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.custombusinessday.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.is_year_end", "path": "reference/api/pandas.tseries.offsets.custombusinessday.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.is_year_start", "path": "reference/api/pandas.tseries.offsets.custombusinessday.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.isAnchored", "path": "reference/api/pandas.tseries.offsets.custombusinessday.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.kwds", "path": "reference/api/pandas.tseries.offsets.custombusinessday.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.n", "path": "reference/api/pandas.tseries.offsets.custombusinessday.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.name", "path": "reference/api/pandas.tseries.offsets.custombusinessday.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.nanos", "path": "reference/api/pandas.tseries.offsets.custombusinessday.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.normalize", "path": "reference/api/pandas.tseries.offsets.custombusinessday.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.offset", "path": "reference/api/pandas.tseries.offsets.custombusinessday.offset", "type": "Data offsets", "text": ["Alias for self._offset."]}, {"name": "pandas.tseries.offsets.CustomBusinessDay.onOffset", "path": "reference/api/pandas.tseries.offsets.custombusinessday.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.rollback", "path": "reference/api/pandas.tseries.offsets.custombusinessday.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.CustomBusinessDay.rollforward", "path": "reference/api/pandas.tseries.offsets.custombusinessday.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.CustomBusinessDay.rule_code", "path": "reference/api/pandas.tseries.offsets.custombusinessday.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessDay.weekmask", "path": "reference/api/pandas.tseries.offsets.custombusinessday.weekmask", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour", "path": "reference/api/pandas.tseries.offsets.custombusinesshour", "type": "Data offsets", "text": ["DateOffset subclass representing possibly n custom business days.", "The number of months represented.", "Normalize start/end dates to midnight before generating date range.", "Weekmask of valid business days, passed to numpy.busdaycalendar.", "Start time of your custom business hour in 24h format.", "End time of your custom business hour in 24h format.", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "next_bday", "Used for moving to next business day.", "offset", "Alias for self._offset.", "calendar", "end", "freqstr", "holidays", "kwds", "n", "name", "nanos", "normalize", "rule_code", "start", "weekmask", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback(other)", "Roll provided date backward to next offset only if not on offset.", "rollforward(other)", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.CustomBusinessHour.__call__", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.CustomBusinessHour.apply", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.apply_index", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.base", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.CustomBusinessHour.calendar", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.calendar", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.copy", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.end", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.freqstr", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.holidays", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.holidays", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.is_anchored", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.is_month_end", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.is_month_start", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.is_on_offset", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.is_year_end", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.is_year_start", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.isAnchored", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.kwds", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.n", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.name", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.nanos", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.next_bday", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.next_bday", "type": "Data offsets", "text": ["Used for moving to next business day."]}, {"name": "pandas.tseries.offsets.CustomBusinessHour.normalize", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.offset", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.offset", "type": "Data offsets", "text": ["Alias for self._offset."]}, {"name": "pandas.tseries.offsets.CustomBusinessHour.onOffset", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.rollback", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset."]}, {"name": "pandas.tseries.offsets.CustomBusinessHour.rollforward", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset."]}, {"name": "pandas.tseries.offsets.CustomBusinessHour.rule_code", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.start", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessHour.weekmask", "path": "reference/api/pandas.tseries.offsets.custombusinesshour.weekmask", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin", "type": "Data offsets", "text": ["Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "cbday_roll", "Define default roll function to be called in apply method.", "month_roll", "Define default roll function to be called in apply method.", "offset", "Alias for self._offset.", "calendar", "freqstr", "holidays", "kwds", "m_offset", "n", "name", "nanos", "normalize", "rule_code", "weekmask", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.__call__", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.apply", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.apply_index", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.base", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.calendar", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.calendar", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.cbday_roll", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.cbday_roll", "type": "Data offsets", "text": ["Define default roll function to be called in apply method."]}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.copy", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.freqstr", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.holidays", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.holidays", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.is_anchored", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.is_month_end", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.is_month_start", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.is_on_offset", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.is_year_end", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.is_year_start", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.isAnchored", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.kwds", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.m_offset", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.m_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.month_roll", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.month_roll", "type": "Data offsets", "text": ["Define default roll function to be called in apply method."]}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.n", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.name", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.nanos", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.normalize", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.offset", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.offset", "type": "Data offsets", "text": ["Alias for self._offset."]}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.onOffset", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.rollback", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.rollforward", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.rule_code", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthBegin.weekmask", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthbegin.weekmask", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend", "type": "Data offsets", "text": ["Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "cbday_roll", "Define default roll function to be called in apply method.", "month_roll", "Define default roll function to be called in apply method.", "offset", "Alias for self._offset.", "calendar", "freqstr", "holidays", "kwds", "m_offset", "n", "name", "nanos", "normalize", "rule_code", "weekmask", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.__call__", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.apply", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.apply_index", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.base", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.calendar", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.calendar", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.cbday_roll", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.cbday_roll", "type": "Data offsets", "text": ["Define default roll function to be called in apply method."]}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.copy", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.freqstr", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.holidays", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.holidays", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.is_anchored", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.is_month_end", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.is_month_start", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.is_on_offset", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.is_year_end", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.is_year_start", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.isAnchored", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.kwds", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.m_offset", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.m_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.month_roll", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.month_roll", "type": "Data offsets", "text": ["Define default roll function to be called in apply method."]}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.n", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.name", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.nanos", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.normalize", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.offset", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.offset", "type": "Data offsets", "text": ["Alias for self._offset."]}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.onOffset", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.rollback", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.rollforward", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.rule_code", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.CustomBusinessMonthEnd.weekmask", "path": "reference/api/pandas.tseries.offsets.custombusinessmonthend.weekmask", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset", "path": "reference/api/pandas.tseries.offsets.dateoffset", "type": "Data offsets", "text": ["Standard kind of date increment used for a date range.", "Works exactly like the keyword argument form of relativedelta. Note that the positional argument form of relativedelata is not supported. Use of the keyword n is discouraged\u2013 you would be better off specifying n in the keywords you use, but regardless it is there for you. n is needed for DateOffset subclasses.", "DateOffset works as follows. Each offset specify a set of dates that conform to the DateOffset. For example, Bday defines this set to be the set of dates that are weekdays (M-F). To test if a date is in the set of a DateOffset dateOffset we can use the is_on_offset method: dateOffset.is_on_offset(date).", "If a date is not on a valid date, the rollback and rollforward methods can be used to roll the date to the nearest valid date before/after the date.", "DateOffsets can be created to move dates forward a given number of valid dates. For example, Bday(2) can be added to a date to move it two business days forward. If the date does not start on a valid date, first it is moved to a valid date. Thus pseudo code is:", "date = rollback(date) # does nothing if date is valid return date + <n number of periods>", "When a date offset is created for a negative number of periods, the date is first rolled forward. The pseudo code is:", "date = rollforward(date) # does nothing is date is valid return date + <n number of periods>", "Zero presents a problem. Should it roll forward or back? We arbitrarily have it rollforward:", "date + BDay(0) == BDay.rollforward(date)", "Since 0 is a bit weird, we suggest avoiding its use.", "The number of time periods the offset represents.", "Whether to round the result of a DateOffset addition down to the previous midnight.", "Temporal parameter that add to or replace the offset value.", "Parameters that add to the offset (like Timedelta):", "years", "months", "weeks", "days", "hours", "minutes", "seconds", "microseconds", "nanoseconds", "Parameters that replace the offset value:", "year", "month", "day", "weekday", "hour", "minute", "second", "microsecond", "nanosecond.", "See also", "The relativedelta type is designed to be applied to an existing datetime an can replace specific components of that datetime, or represents an interval of time.", "Examples", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.DateOffset.__call__", "path": "reference/api/pandas.tseries.offsets.dateoffset.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.DateOffset.apply", "path": "reference/api/pandas.tseries.offsets.dateoffset.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.apply_index", "path": "reference/api/pandas.tseries.offsets.dateoffset.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.base", "path": "reference/api/pandas.tseries.offsets.dateoffset.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.DateOffset.copy", "path": "reference/api/pandas.tseries.offsets.dateoffset.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.freqstr", "path": "reference/api/pandas.tseries.offsets.dateoffset.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.is_anchored", "path": "reference/api/pandas.tseries.offsets.dateoffset.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.is_month_end", "path": "reference/api/pandas.tseries.offsets.dateoffset.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.is_month_start", "path": "reference/api/pandas.tseries.offsets.dateoffset.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.is_on_offset", "path": "reference/api/pandas.tseries.offsets.dateoffset.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.dateoffset.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.dateoffset.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.is_year_end", "path": "reference/api/pandas.tseries.offsets.dateoffset.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.is_year_start", "path": "reference/api/pandas.tseries.offsets.dateoffset.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.isAnchored", "path": "reference/api/pandas.tseries.offsets.dateoffset.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.kwds", "path": "reference/api/pandas.tseries.offsets.dateoffset.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.n", "path": "reference/api/pandas.tseries.offsets.dateoffset.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.name", "path": "reference/api/pandas.tseries.offsets.dateoffset.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.nanos", "path": "reference/api/pandas.tseries.offsets.dateoffset.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.normalize", "path": "reference/api/pandas.tseries.offsets.dateoffset.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.onOffset", "path": "reference/api/pandas.tseries.offsets.dateoffset.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.DateOffset.rollback", "path": "reference/api/pandas.tseries.offsets.dateoffset.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.DateOffset.rollforward", "path": "reference/api/pandas.tseries.offsets.dateoffset.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.DateOffset.rule_code", "path": "reference/api/pandas.tseries.offsets.dateoffset.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day", "path": "reference/api/pandas.tseries.offsets.day", "type": "Data offsets", "text": ["Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "delta", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.Day.__call__", "path": "reference/api/pandas.tseries.offsets.day.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.Day.apply", "path": "reference/api/pandas.tseries.offsets.day.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.apply_index", "path": "reference/api/pandas.tseries.offsets.day.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.base", "path": "reference/api/pandas.tseries.offsets.day.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.Day.copy", "path": "reference/api/pandas.tseries.offsets.day.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.delta", "path": "reference/api/pandas.tseries.offsets.day.delta", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.freqstr", "path": "reference/api/pandas.tseries.offsets.day.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.is_anchored", "path": "reference/api/pandas.tseries.offsets.day.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.is_month_end", "path": "reference/api/pandas.tseries.offsets.day.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.is_month_start", "path": "reference/api/pandas.tseries.offsets.day.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.is_on_offset", "path": "reference/api/pandas.tseries.offsets.day.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.day.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.day.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.is_year_end", "path": "reference/api/pandas.tseries.offsets.day.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.is_year_start", "path": "reference/api/pandas.tseries.offsets.day.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.isAnchored", "path": "reference/api/pandas.tseries.offsets.day.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.kwds", "path": "reference/api/pandas.tseries.offsets.day.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.n", "path": "reference/api/pandas.tseries.offsets.day.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.name", "path": "reference/api/pandas.tseries.offsets.day.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.nanos", "path": "reference/api/pandas.tseries.offsets.day.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.normalize", "path": "reference/api/pandas.tseries.offsets.day.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.onOffset", "path": "reference/api/pandas.tseries.offsets.day.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Day.rollback", "path": "reference/api/pandas.tseries.offsets.day.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Day.rollforward", "path": "reference/api/pandas.tseries.offsets.day.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Day.rule_code", "path": "reference/api/pandas.tseries.offsets.day.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter", "path": "reference/api/pandas.tseries.offsets.easter", "type": "Data offsets", "text": ["DateOffset for the Easter holiday using logic defined in dateutil.", "Right now uses the revised method which is valid in years 1583-4099.", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.Easter.__call__", "path": "reference/api/pandas.tseries.offsets.easter.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.Easter.apply", "path": "reference/api/pandas.tseries.offsets.easter.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.apply_index", "path": "reference/api/pandas.tseries.offsets.easter.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.base", "path": "reference/api/pandas.tseries.offsets.easter.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.Easter.copy", "path": "reference/api/pandas.tseries.offsets.easter.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.freqstr", "path": "reference/api/pandas.tseries.offsets.easter.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.is_anchored", "path": "reference/api/pandas.tseries.offsets.easter.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.is_month_end", "path": "reference/api/pandas.tseries.offsets.easter.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.is_month_start", "path": "reference/api/pandas.tseries.offsets.easter.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.is_on_offset", "path": "reference/api/pandas.tseries.offsets.easter.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.easter.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.easter.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.is_year_end", "path": "reference/api/pandas.tseries.offsets.easter.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.is_year_start", "path": "reference/api/pandas.tseries.offsets.easter.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.isAnchored", "path": "reference/api/pandas.tseries.offsets.easter.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.kwds", "path": "reference/api/pandas.tseries.offsets.easter.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.n", "path": "reference/api/pandas.tseries.offsets.easter.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.name", "path": "reference/api/pandas.tseries.offsets.easter.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.nanos", "path": "reference/api/pandas.tseries.offsets.easter.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.normalize", "path": "reference/api/pandas.tseries.offsets.easter.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.onOffset", "path": "reference/api/pandas.tseries.offsets.easter.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Easter.rollback", "path": "reference/api/pandas.tseries.offsets.easter.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Easter.rollforward", "path": "reference/api/pandas.tseries.offsets.easter.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Easter.rule_code", "path": "reference/api/pandas.tseries.offsets.easter.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253", "path": "reference/api/pandas.tseries.offsets.fy5253", "type": "Data offsets", "text": ["Describes 52-53 week fiscal year. This is also known as a 4-4-5 calendar.", "It is used by companies that desire that their fiscal year always end on the same day of the week.", "It is a method of managing accounting periods. It is a common calendar structure for some industries, such as retail, manufacturing and parking industry.", "For more information see: https://en.wikipedia.org/wiki/4-4-5_calendar", "The year may either:", "end on the last X day of the Y month.", "end on the last X day closest to the last day of the Y month.", "X is a specific day of the week. Y is a certain month of the year", "A specific integer for the day of the week.", "0 is Monday", "1 is Tuesday", "2 is Wednesday", "3 is Thursday", "4 is Friday", "5 is Saturday", "6 is Sunday.", "The month in which the fiscal year ends.", "Method of employing 4-4-5 calendar.", "There are two options:", "\u201cnearest\u201d means year end is weekday closest to last day of month in year.", "\u201clast\u201d means year end is final weekday of the final month in fiscal year.", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "startingMonth", "variation", "weekday", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "get_rule_code_suffix", "get_year_end", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.FY5253.__call__", "path": "reference/api/pandas.tseries.offsets.fy5253.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.FY5253.apply", "path": "reference/api/pandas.tseries.offsets.fy5253.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.apply_index", "path": "reference/api/pandas.tseries.offsets.fy5253.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.base", "path": "reference/api/pandas.tseries.offsets.fy5253.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.FY5253.copy", "path": "reference/api/pandas.tseries.offsets.fy5253.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.freqstr", "path": "reference/api/pandas.tseries.offsets.fy5253.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.get_rule_code_suffix", "path": "reference/api/pandas.tseries.offsets.fy5253.get_rule_code_suffix", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.get_year_end", "path": "reference/api/pandas.tseries.offsets.fy5253.get_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.is_anchored", "path": "reference/api/pandas.tseries.offsets.fy5253.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.is_month_end", "path": "reference/api/pandas.tseries.offsets.fy5253.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.is_month_start", "path": "reference/api/pandas.tseries.offsets.fy5253.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.is_on_offset", "path": "reference/api/pandas.tseries.offsets.fy5253.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.fy5253.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.fy5253.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.is_year_end", "path": "reference/api/pandas.tseries.offsets.fy5253.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.is_year_start", "path": "reference/api/pandas.tseries.offsets.fy5253.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.isAnchored", "path": "reference/api/pandas.tseries.offsets.fy5253.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.kwds", "path": "reference/api/pandas.tseries.offsets.fy5253.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.n", "path": "reference/api/pandas.tseries.offsets.fy5253.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.name", "path": "reference/api/pandas.tseries.offsets.fy5253.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.nanos", "path": "reference/api/pandas.tseries.offsets.fy5253.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.normalize", "path": "reference/api/pandas.tseries.offsets.fy5253.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.onOffset", "path": "reference/api/pandas.tseries.offsets.fy5253.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.rollback", "path": "reference/api/pandas.tseries.offsets.fy5253.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.FY5253.rollforward", "path": "reference/api/pandas.tseries.offsets.fy5253.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.FY5253.rule_code", "path": "reference/api/pandas.tseries.offsets.fy5253.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.startingMonth", "path": "reference/api/pandas.tseries.offsets.fy5253.startingmonth", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253.variation", "path": "reference/api/pandas.tseries.offsets.fy5253.variation", "type": "Input/output", "text": []}, {"name": "pandas.tseries.offsets.FY5253.weekday", "path": "reference/api/pandas.tseries.offsets.fy5253.weekday", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter", "path": "reference/api/pandas.tseries.offsets.fy5253quarter", "type": "Data offsets", "text": ["DateOffset increments between business quarter dates for 52-53 week fiscal year (also known as a 4-4-5 calendar).", "It is used by companies that desire that their fiscal year always end on the same day of the week.", "It is a method of managing accounting periods. It is a common calendar structure for some industries, such as retail, manufacturing and parking industry.", "For more information see: https://en.wikipedia.org/wiki/4-4-5_calendar", "The year may either:", "end on the last X day of the Y month.", "end on the last X day closest to the last day of the Y month.", "X is a specific day of the week. Y is a certain month of the year", "startingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, \u2026 startingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, \u2026 startingMonth = 3 corresponds to dates like 3/30/2007, 6/29/2007, \u2026", "A specific integer for the day of the week.", "0 is Monday", "1 is Tuesday", "2 is Wednesday", "3 is Thursday", "4 is Friday", "5 is Saturday", "6 is Sunday.", "The month in which fiscal years end.", "The quarter number that has the leap or 14 week when needed.", "Method of employing 4-4-5 calendar.", "There are two options:", "\u201cnearest\u201d means year end is weekday closest to last day of month in year.", "\u201clast\u201d means year end is final weekday of the final month in fiscal year.", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "n", "name", "nanos", "normalize", "qtr_with_extra_week", "rule_code", "startingMonth", "variation", "weekday", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "get_rule_code_suffix", "get_weeks", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset", "year_has_extra_week"]}, {"name": "pandas.tseries.offsets.FY5253Quarter.__call__", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.FY5253Quarter.apply", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.apply_index", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.base", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.FY5253Quarter.copy", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.freqstr", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.get_rule_code_suffix", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.get_rule_code_suffix", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.get_weeks", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.get_weeks", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.is_anchored", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.is_month_end", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.is_month_start", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.is_on_offset", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.is_year_end", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.is_year_start", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.isAnchored", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.kwds", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.n", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.name", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.nanos", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.normalize", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.onOffset", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.qtr_with_extra_week", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.qtr_with_extra_week", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.rollback", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.FY5253Quarter.rollforward", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.FY5253Quarter.rule_code", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.startingMonth", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.startingmonth", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.variation", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.variation", "type": "Input/output", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.weekday", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.weekday", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.FY5253Quarter.year_has_extra_week", "path": "reference/api/pandas.tseries.offsets.fy5253quarter.year_has_extra_week", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour", "path": "reference/api/pandas.tseries.offsets.hour", "type": "Data offsets", "text": ["Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "delta", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.Hour.__call__", "path": "reference/api/pandas.tseries.offsets.hour.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.Hour.apply", "path": "reference/api/pandas.tseries.offsets.hour.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.apply_index", "path": "reference/api/pandas.tseries.offsets.hour.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.base", "path": "reference/api/pandas.tseries.offsets.hour.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.Hour.copy", "path": "reference/api/pandas.tseries.offsets.hour.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.delta", "path": "reference/api/pandas.tseries.offsets.hour.delta", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.freqstr", "path": "reference/api/pandas.tseries.offsets.hour.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.is_anchored", "path": "reference/api/pandas.tseries.offsets.hour.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.is_month_end", "path": "reference/api/pandas.tseries.offsets.hour.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.is_month_start", "path": "reference/api/pandas.tseries.offsets.hour.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.is_on_offset", "path": "reference/api/pandas.tseries.offsets.hour.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.hour.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.hour.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.is_year_end", "path": "reference/api/pandas.tseries.offsets.hour.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.is_year_start", "path": "reference/api/pandas.tseries.offsets.hour.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.isAnchored", "path": "reference/api/pandas.tseries.offsets.hour.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.kwds", "path": "reference/api/pandas.tseries.offsets.hour.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.n", "path": "reference/api/pandas.tseries.offsets.hour.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.name", "path": "reference/api/pandas.tseries.offsets.hour.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.nanos", "path": "reference/api/pandas.tseries.offsets.hour.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.normalize", "path": "reference/api/pandas.tseries.offsets.hour.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.onOffset", "path": "reference/api/pandas.tseries.offsets.hour.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Hour.rollback", "path": "reference/api/pandas.tseries.offsets.hour.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Hour.rollforward", "path": "reference/api/pandas.tseries.offsets.hour.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Hour.rule_code", "path": "reference/api/pandas.tseries.offsets.hour.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth", "type": "Data offsets", "text": ["Describes monthly dates in last week of month like \u201cthe last Tuesday of each month\u201d.", "A specific integer for the day of the week.", "0 is Monday", "1 is Tuesday", "2 is Wednesday", "3 is Thursday", "4 is Friday", "5 is Saturday", "6 is Sunday.", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "week", "weekday", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.__call__", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.apply", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.apply_index", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.base", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.copy", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.freqstr", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.is_anchored", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.is_month_end", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.is_month_start", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.is_on_offset", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.is_year_end", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.is_year_start", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.isAnchored", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.kwds", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.n", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.name", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.nanos", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.normalize", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.onOffset", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.rollback", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.rollforward", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.rule_code", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.week", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.week", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.LastWeekOfMonth.weekday", "path": "reference/api/pandas.tseries.offsets.lastweekofmonth.weekday", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro", "path": "reference/api/pandas.tseries.offsets.micro", "type": "Data offsets", "text": ["Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "delta", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.Micro.__call__", "path": "reference/api/pandas.tseries.offsets.micro.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.Micro.apply", "path": "reference/api/pandas.tseries.offsets.micro.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.apply_index", "path": "reference/api/pandas.tseries.offsets.micro.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.base", "path": "reference/api/pandas.tseries.offsets.micro.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.Micro.copy", "path": "reference/api/pandas.tseries.offsets.micro.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.delta", "path": "reference/api/pandas.tseries.offsets.micro.delta", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.freqstr", "path": "reference/api/pandas.tseries.offsets.micro.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.is_anchored", "path": "reference/api/pandas.tseries.offsets.micro.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.is_month_end", "path": "reference/api/pandas.tseries.offsets.micro.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.is_month_start", "path": "reference/api/pandas.tseries.offsets.micro.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.is_on_offset", "path": "reference/api/pandas.tseries.offsets.micro.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.micro.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.micro.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.is_year_end", "path": "reference/api/pandas.tseries.offsets.micro.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.is_year_start", "path": "reference/api/pandas.tseries.offsets.micro.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.isAnchored", "path": "reference/api/pandas.tseries.offsets.micro.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.kwds", "path": "reference/api/pandas.tseries.offsets.micro.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.n", "path": "reference/api/pandas.tseries.offsets.micro.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.name", "path": "reference/api/pandas.tseries.offsets.micro.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.nanos", "path": "reference/api/pandas.tseries.offsets.micro.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.normalize", "path": "reference/api/pandas.tseries.offsets.micro.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.onOffset", "path": "reference/api/pandas.tseries.offsets.micro.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Micro.rollback", "path": "reference/api/pandas.tseries.offsets.micro.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Micro.rollforward", "path": "reference/api/pandas.tseries.offsets.micro.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Micro.rule_code", "path": "reference/api/pandas.tseries.offsets.micro.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli", "path": "reference/api/pandas.tseries.offsets.milli", "type": "Data offsets", "text": ["Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "delta", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.Milli.__call__", "path": "reference/api/pandas.tseries.offsets.milli.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.Milli.apply", "path": "reference/api/pandas.tseries.offsets.milli.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.apply_index", "path": "reference/api/pandas.tseries.offsets.milli.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.base", "path": "reference/api/pandas.tseries.offsets.milli.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.Milli.copy", "path": "reference/api/pandas.tseries.offsets.milli.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.delta", "path": "reference/api/pandas.tseries.offsets.milli.delta", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.freqstr", "path": "reference/api/pandas.tseries.offsets.milli.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.is_anchored", "path": "reference/api/pandas.tseries.offsets.milli.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.is_month_end", "path": "reference/api/pandas.tseries.offsets.milli.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.is_month_start", "path": "reference/api/pandas.tseries.offsets.milli.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.is_on_offset", "path": "reference/api/pandas.tseries.offsets.milli.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.milli.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.milli.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.is_year_end", "path": "reference/api/pandas.tseries.offsets.milli.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.is_year_start", "path": "reference/api/pandas.tseries.offsets.milli.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.isAnchored", "path": "reference/api/pandas.tseries.offsets.milli.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.kwds", "path": "reference/api/pandas.tseries.offsets.milli.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.n", "path": "reference/api/pandas.tseries.offsets.milli.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.name", "path": "reference/api/pandas.tseries.offsets.milli.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.nanos", "path": "reference/api/pandas.tseries.offsets.milli.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.normalize", "path": "reference/api/pandas.tseries.offsets.milli.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.onOffset", "path": "reference/api/pandas.tseries.offsets.milli.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Milli.rollback", "path": "reference/api/pandas.tseries.offsets.milli.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Milli.rollforward", "path": "reference/api/pandas.tseries.offsets.milli.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Milli.rule_code", "path": "reference/api/pandas.tseries.offsets.milli.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute", "path": "reference/api/pandas.tseries.offsets.minute", "type": "Data offsets", "text": ["Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "delta", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.Minute.__call__", "path": "reference/api/pandas.tseries.offsets.minute.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.Minute.apply", "path": "reference/api/pandas.tseries.offsets.minute.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.apply_index", "path": "reference/api/pandas.tseries.offsets.minute.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.base", "path": "reference/api/pandas.tseries.offsets.minute.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.Minute.copy", "path": "reference/api/pandas.tseries.offsets.minute.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.delta", "path": "reference/api/pandas.tseries.offsets.minute.delta", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.freqstr", "path": "reference/api/pandas.tseries.offsets.minute.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.is_anchored", "path": "reference/api/pandas.tseries.offsets.minute.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.is_month_end", "path": "reference/api/pandas.tseries.offsets.minute.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.is_month_start", "path": "reference/api/pandas.tseries.offsets.minute.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.is_on_offset", "path": "reference/api/pandas.tseries.offsets.minute.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.minute.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.minute.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.is_year_end", "path": "reference/api/pandas.tseries.offsets.minute.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.is_year_start", "path": "reference/api/pandas.tseries.offsets.minute.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.isAnchored", "path": "reference/api/pandas.tseries.offsets.minute.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.kwds", "path": "reference/api/pandas.tseries.offsets.minute.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.n", "path": "reference/api/pandas.tseries.offsets.minute.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.name", "path": "reference/api/pandas.tseries.offsets.minute.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.nanos", "path": "reference/api/pandas.tseries.offsets.minute.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.normalize", "path": "reference/api/pandas.tseries.offsets.minute.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.onOffset", "path": "reference/api/pandas.tseries.offsets.minute.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Minute.rollback", "path": "reference/api/pandas.tseries.offsets.minute.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Minute.rollforward", "path": "reference/api/pandas.tseries.offsets.minute.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Minute.rule_code", "path": "reference/api/pandas.tseries.offsets.minute.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin", "path": "reference/api/pandas.tseries.offsets.monthbegin", "type": "Data offsets", "text": ["DateOffset of one month at beginning.", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.MonthBegin.__call__", "path": "reference/api/pandas.tseries.offsets.monthbegin.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.MonthBegin.apply", "path": "reference/api/pandas.tseries.offsets.monthbegin.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.apply_index", "path": "reference/api/pandas.tseries.offsets.monthbegin.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.base", "path": "reference/api/pandas.tseries.offsets.monthbegin.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.MonthBegin.copy", "path": "reference/api/pandas.tseries.offsets.monthbegin.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.freqstr", "path": "reference/api/pandas.tseries.offsets.monthbegin.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.is_anchored", "path": "reference/api/pandas.tseries.offsets.monthbegin.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.is_month_end", "path": "reference/api/pandas.tseries.offsets.monthbegin.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.is_month_start", "path": "reference/api/pandas.tseries.offsets.monthbegin.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.is_on_offset", "path": "reference/api/pandas.tseries.offsets.monthbegin.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.monthbegin.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.monthbegin.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.is_year_end", "path": "reference/api/pandas.tseries.offsets.monthbegin.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.is_year_start", "path": "reference/api/pandas.tseries.offsets.monthbegin.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.isAnchored", "path": "reference/api/pandas.tseries.offsets.monthbegin.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.kwds", "path": "reference/api/pandas.tseries.offsets.monthbegin.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.n", "path": "reference/api/pandas.tseries.offsets.monthbegin.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.name", "path": "reference/api/pandas.tseries.offsets.monthbegin.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.nanos", "path": "reference/api/pandas.tseries.offsets.monthbegin.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.normalize", "path": "reference/api/pandas.tseries.offsets.monthbegin.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.onOffset", "path": "reference/api/pandas.tseries.offsets.monthbegin.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthBegin.rollback", "path": "reference/api/pandas.tseries.offsets.monthbegin.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.MonthBegin.rollforward", "path": "reference/api/pandas.tseries.offsets.monthbegin.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.MonthBegin.rule_code", "path": "reference/api/pandas.tseries.offsets.monthbegin.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd", "path": "reference/api/pandas.tseries.offsets.monthend", "type": "Data offsets", "text": ["DateOffset of one month end.", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.MonthEnd.__call__", "path": "reference/api/pandas.tseries.offsets.monthend.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.MonthEnd.apply", "path": "reference/api/pandas.tseries.offsets.monthend.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.apply_index", "path": "reference/api/pandas.tseries.offsets.monthend.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.base", "path": "reference/api/pandas.tseries.offsets.monthend.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.MonthEnd.copy", "path": "reference/api/pandas.tseries.offsets.monthend.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.freqstr", "path": "reference/api/pandas.tseries.offsets.monthend.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.is_anchored", "path": "reference/api/pandas.tseries.offsets.monthend.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.is_month_end", "path": "reference/api/pandas.tseries.offsets.monthend.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.is_month_start", "path": "reference/api/pandas.tseries.offsets.monthend.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.is_on_offset", "path": "reference/api/pandas.tseries.offsets.monthend.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.monthend.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.monthend.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.is_year_end", "path": "reference/api/pandas.tseries.offsets.monthend.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.is_year_start", "path": "reference/api/pandas.tseries.offsets.monthend.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.isAnchored", "path": "reference/api/pandas.tseries.offsets.monthend.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.kwds", "path": "reference/api/pandas.tseries.offsets.monthend.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.n", "path": "reference/api/pandas.tseries.offsets.monthend.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.name", "path": "reference/api/pandas.tseries.offsets.monthend.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.nanos", "path": "reference/api/pandas.tseries.offsets.monthend.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.normalize", "path": "reference/api/pandas.tseries.offsets.monthend.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.onOffset", "path": "reference/api/pandas.tseries.offsets.monthend.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.MonthEnd.rollback", "path": "reference/api/pandas.tseries.offsets.monthend.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.MonthEnd.rollforward", "path": "reference/api/pandas.tseries.offsets.monthend.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.MonthEnd.rule_code", "path": "reference/api/pandas.tseries.offsets.monthend.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano", "path": "reference/api/pandas.tseries.offsets.nano", "type": "Data offsets", "text": ["Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "delta", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.Nano.__call__", "path": "reference/api/pandas.tseries.offsets.nano.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.Nano.apply", "path": "reference/api/pandas.tseries.offsets.nano.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.apply_index", "path": "reference/api/pandas.tseries.offsets.nano.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.base", "path": "reference/api/pandas.tseries.offsets.nano.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.Nano.copy", "path": "reference/api/pandas.tseries.offsets.nano.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.delta", "path": "reference/api/pandas.tseries.offsets.nano.delta", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.freqstr", "path": "reference/api/pandas.tseries.offsets.nano.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.is_anchored", "path": "reference/api/pandas.tseries.offsets.nano.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.is_month_end", "path": "reference/api/pandas.tseries.offsets.nano.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.is_month_start", "path": "reference/api/pandas.tseries.offsets.nano.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.is_on_offset", "path": "reference/api/pandas.tseries.offsets.nano.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.nano.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.nano.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.is_year_end", "path": "reference/api/pandas.tseries.offsets.nano.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.is_year_start", "path": "reference/api/pandas.tseries.offsets.nano.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.isAnchored", "path": "reference/api/pandas.tseries.offsets.nano.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.kwds", "path": "reference/api/pandas.tseries.offsets.nano.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.n", "path": "reference/api/pandas.tseries.offsets.nano.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.name", "path": "reference/api/pandas.tseries.offsets.nano.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.nanos", "path": "reference/api/pandas.tseries.offsets.nano.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.normalize", "path": "reference/api/pandas.tseries.offsets.nano.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.onOffset", "path": "reference/api/pandas.tseries.offsets.nano.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Nano.rollback", "path": "reference/api/pandas.tseries.offsets.nano.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Nano.rollforward", "path": "reference/api/pandas.tseries.offsets.nano.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Nano.rule_code", "path": "reference/api/pandas.tseries.offsets.nano.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin", "path": "reference/api/pandas.tseries.offsets.quarterbegin", "type": "Data offsets", "text": ["DateOffset increments between Quarter start dates.", "startingMonth = 1 corresponds to dates like 1/01/2007, 4/01/2007, \u2026 startingMonth = 2 corresponds to dates like 2/01/2007, 5/01/2007, \u2026 startingMonth = 3 corresponds to dates like 3/01/2007, 6/01/2007, \u2026", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "startingMonth", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.QuarterBegin.__call__", "path": "reference/api/pandas.tseries.offsets.quarterbegin.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.QuarterBegin.apply", "path": "reference/api/pandas.tseries.offsets.quarterbegin.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.apply_index", "path": "reference/api/pandas.tseries.offsets.quarterbegin.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.base", "path": "reference/api/pandas.tseries.offsets.quarterbegin.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.QuarterBegin.copy", "path": "reference/api/pandas.tseries.offsets.quarterbegin.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.freqstr", "path": "reference/api/pandas.tseries.offsets.quarterbegin.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.is_anchored", "path": "reference/api/pandas.tseries.offsets.quarterbegin.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.is_month_end", "path": "reference/api/pandas.tseries.offsets.quarterbegin.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.is_month_start", "path": "reference/api/pandas.tseries.offsets.quarterbegin.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.is_on_offset", "path": "reference/api/pandas.tseries.offsets.quarterbegin.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.quarterbegin.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.quarterbegin.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.is_year_end", "path": "reference/api/pandas.tseries.offsets.quarterbegin.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.is_year_start", "path": "reference/api/pandas.tseries.offsets.quarterbegin.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.isAnchored", "path": "reference/api/pandas.tseries.offsets.quarterbegin.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.kwds", "path": "reference/api/pandas.tseries.offsets.quarterbegin.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.n", "path": "reference/api/pandas.tseries.offsets.quarterbegin.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.name", "path": "reference/api/pandas.tseries.offsets.quarterbegin.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.nanos", "path": "reference/api/pandas.tseries.offsets.quarterbegin.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.normalize", "path": "reference/api/pandas.tseries.offsets.quarterbegin.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.onOffset", "path": "reference/api/pandas.tseries.offsets.quarterbegin.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.rollback", "path": "reference/api/pandas.tseries.offsets.quarterbegin.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.QuarterBegin.rollforward", "path": "reference/api/pandas.tseries.offsets.quarterbegin.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.QuarterBegin.rule_code", "path": "reference/api/pandas.tseries.offsets.quarterbegin.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterBegin.startingMonth", "path": "reference/api/pandas.tseries.offsets.quarterbegin.startingmonth", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd", "path": "reference/api/pandas.tseries.offsets.quarterend", "type": "Data offsets", "text": ["DateOffset increments between Quarter end dates.", "startingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, \u2026 startingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, \u2026 startingMonth = 3 corresponds to dates like 3/31/2007, 6/30/2007, \u2026", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "startingMonth", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.QuarterEnd.__call__", "path": "reference/api/pandas.tseries.offsets.quarterend.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.QuarterEnd.apply", "path": "reference/api/pandas.tseries.offsets.quarterend.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.apply_index", "path": "reference/api/pandas.tseries.offsets.quarterend.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.base", "path": "reference/api/pandas.tseries.offsets.quarterend.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.QuarterEnd.copy", "path": "reference/api/pandas.tseries.offsets.quarterend.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.freqstr", "path": "reference/api/pandas.tseries.offsets.quarterend.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.is_anchored", "path": "reference/api/pandas.tseries.offsets.quarterend.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.is_month_end", "path": "reference/api/pandas.tseries.offsets.quarterend.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.is_month_start", "path": "reference/api/pandas.tseries.offsets.quarterend.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.is_on_offset", "path": "reference/api/pandas.tseries.offsets.quarterend.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.quarterend.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.quarterend.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.is_year_end", "path": "reference/api/pandas.tseries.offsets.quarterend.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.is_year_start", "path": "reference/api/pandas.tseries.offsets.quarterend.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.isAnchored", "path": "reference/api/pandas.tseries.offsets.quarterend.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.kwds", "path": "reference/api/pandas.tseries.offsets.quarterend.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.n", "path": "reference/api/pandas.tseries.offsets.quarterend.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.name", "path": "reference/api/pandas.tseries.offsets.quarterend.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.nanos", "path": "reference/api/pandas.tseries.offsets.quarterend.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.normalize", "path": "reference/api/pandas.tseries.offsets.quarterend.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.onOffset", "path": "reference/api/pandas.tseries.offsets.quarterend.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.rollback", "path": "reference/api/pandas.tseries.offsets.quarterend.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.QuarterEnd.rollforward", "path": "reference/api/pandas.tseries.offsets.quarterend.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.QuarterEnd.rule_code", "path": "reference/api/pandas.tseries.offsets.quarterend.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.QuarterEnd.startingMonth", "path": "reference/api/pandas.tseries.offsets.quarterend.startingmonth", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second", "path": "reference/api/pandas.tseries.offsets.second", "type": "Data offsets", "text": ["Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "delta", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.Second.__call__", "path": "reference/api/pandas.tseries.offsets.second.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.Second.apply", "path": "reference/api/pandas.tseries.offsets.second.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.apply_index", "path": "reference/api/pandas.tseries.offsets.second.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.base", "path": "reference/api/pandas.tseries.offsets.second.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.Second.copy", "path": "reference/api/pandas.tseries.offsets.second.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.delta", "path": "reference/api/pandas.tseries.offsets.second.delta", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.freqstr", "path": "reference/api/pandas.tseries.offsets.second.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.is_anchored", "path": "reference/api/pandas.tseries.offsets.second.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.is_month_end", "path": "reference/api/pandas.tseries.offsets.second.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.is_month_start", "path": "reference/api/pandas.tseries.offsets.second.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.is_on_offset", "path": "reference/api/pandas.tseries.offsets.second.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.second.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.second.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.is_year_end", "path": "reference/api/pandas.tseries.offsets.second.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.is_year_start", "path": "reference/api/pandas.tseries.offsets.second.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.isAnchored", "path": "reference/api/pandas.tseries.offsets.second.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.kwds", "path": "reference/api/pandas.tseries.offsets.second.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.n", "path": "reference/api/pandas.tseries.offsets.second.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.name", "path": "reference/api/pandas.tseries.offsets.second.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.nanos", "path": "reference/api/pandas.tseries.offsets.second.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.normalize", "path": "reference/api/pandas.tseries.offsets.second.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.onOffset", "path": "reference/api/pandas.tseries.offsets.second.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Second.rollback", "path": "reference/api/pandas.tseries.offsets.second.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Second.rollforward", "path": "reference/api/pandas.tseries.offsets.second.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Second.rule_code", "path": "reference/api/pandas.tseries.offsets.second.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin", "path": "reference/api/pandas.tseries.offsets.semimonthbegin", "type": "Data offsets", "text": ["Two DateOffset\u2019s per month repeating on the first day of the month and day_of_month.", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "day_of_month", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.SemiMonthBegin.__call__", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.SemiMonthBegin.apply", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.apply_index", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.base", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.SemiMonthBegin.copy", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.day_of_month", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.day_of_month", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.freqstr", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.is_anchored", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.is_month_end", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.is_month_start", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.is_on_offset", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.is_year_end", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.is_year_start", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.isAnchored", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.kwds", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.n", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.name", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.nanos", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.normalize", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.onOffset", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthBegin.rollback", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.SemiMonthBegin.rollforward", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.SemiMonthBegin.rule_code", "path": "reference/api/pandas.tseries.offsets.semimonthbegin.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd", "path": "reference/api/pandas.tseries.offsets.semimonthend", "type": "Data offsets", "text": ["Two DateOffset\u2019s per month repeating on the last day of the month and day_of_month.", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "day_of_month", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.SemiMonthEnd.__call__", "path": "reference/api/pandas.tseries.offsets.semimonthend.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.SemiMonthEnd.apply", "path": "reference/api/pandas.tseries.offsets.semimonthend.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.apply_index", "path": "reference/api/pandas.tseries.offsets.semimonthend.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.base", "path": "reference/api/pandas.tseries.offsets.semimonthend.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.SemiMonthEnd.copy", "path": "reference/api/pandas.tseries.offsets.semimonthend.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.day_of_month", "path": "reference/api/pandas.tseries.offsets.semimonthend.day_of_month", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.freqstr", "path": "reference/api/pandas.tseries.offsets.semimonthend.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.is_anchored", "path": "reference/api/pandas.tseries.offsets.semimonthend.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.is_month_end", "path": "reference/api/pandas.tseries.offsets.semimonthend.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.is_month_start", "path": "reference/api/pandas.tseries.offsets.semimonthend.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.is_on_offset", "path": "reference/api/pandas.tseries.offsets.semimonthend.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.semimonthend.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.semimonthend.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.is_year_end", "path": "reference/api/pandas.tseries.offsets.semimonthend.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.is_year_start", "path": "reference/api/pandas.tseries.offsets.semimonthend.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.isAnchored", "path": "reference/api/pandas.tseries.offsets.semimonthend.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.kwds", "path": "reference/api/pandas.tseries.offsets.semimonthend.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.n", "path": "reference/api/pandas.tseries.offsets.semimonthend.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.name", "path": "reference/api/pandas.tseries.offsets.semimonthend.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.nanos", "path": "reference/api/pandas.tseries.offsets.semimonthend.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.normalize", "path": "reference/api/pandas.tseries.offsets.semimonthend.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.onOffset", "path": "reference/api/pandas.tseries.offsets.semimonthend.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.SemiMonthEnd.rollback", "path": "reference/api/pandas.tseries.offsets.semimonthend.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.SemiMonthEnd.rollforward", "path": "reference/api/pandas.tseries.offsets.semimonthend.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.SemiMonthEnd.rule_code", "path": "reference/api/pandas.tseries.offsets.semimonthend.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick", "path": "reference/api/pandas.tseries.offsets.tick", "type": "Data offsets", "text": ["Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "delta", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.Tick.__call__", "path": "reference/api/pandas.tseries.offsets.tick.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.Tick.apply", "path": "reference/api/pandas.tseries.offsets.tick.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.apply_index", "path": "reference/api/pandas.tseries.offsets.tick.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.base", "path": "reference/api/pandas.tseries.offsets.tick.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.Tick.copy", "path": "reference/api/pandas.tseries.offsets.tick.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.delta", "path": "reference/api/pandas.tseries.offsets.tick.delta", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.freqstr", "path": "reference/api/pandas.tseries.offsets.tick.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.is_anchored", "path": "reference/api/pandas.tseries.offsets.tick.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.is_month_end", "path": "reference/api/pandas.tseries.offsets.tick.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.is_month_start", "path": "reference/api/pandas.tseries.offsets.tick.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.is_on_offset", "path": "reference/api/pandas.tseries.offsets.tick.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.tick.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.tick.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.is_year_end", "path": "reference/api/pandas.tseries.offsets.tick.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.is_year_start", "path": "reference/api/pandas.tseries.offsets.tick.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.isAnchored", "path": "reference/api/pandas.tseries.offsets.tick.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.kwds", "path": "reference/api/pandas.tseries.offsets.tick.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.n", "path": "reference/api/pandas.tseries.offsets.tick.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.name", "path": "reference/api/pandas.tseries.offsets.tick.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.nanos", "path": "reference/api/pandas.tseries.offsets.tick.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.normalize", "path": "reference/api/pandas.tseries.offsets.tick.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.onOffset", "path": "reference/api/pandas.tseries.offsets.tick.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Tick.rollback", "path": "reference/api/pandas.tseries.offsets.tick.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Tick.rollforward", "path": "reference/api/pandas.tseries.offsets.tick.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Tick.rule_code", "path": "reference/api/pandas.tseries.offsets.tick.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week", "path": "reference/api/pandas.tseries.offsets.week", "type": "Data offsets", "text": ["Weekly offset.", "Always generate specific day of week. 0 for Monday.", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "weekday", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.Week.__call__", "path": "reference/api/pandas.tseries.offsets.week.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.Week.apply", "path": "reference/api/pandas.tseries.offsets.week.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.apply_index", "path": "reference/api/pandas.tseries.offsets.week.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.base", "path": "reference/api/pandas.tseries.offsets.week.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.Week.copy", "path": "reference/api/pandas.tseries.offsets.week.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.freqstr", "path": "reference/api/pandas.tseries.offsets.week.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.is_anchored", "path": "reference/api/pandas.tseries.offsets.week.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.is_month_end", "path": "reference/api/pandas.tseries.offsets.week.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.is_month_start", "path": "reference/api/pandas.tseries.offsets.week.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.is_on_offset", "path": "reference/api/pandas.tseries.offsets.week.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.week.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.week.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.is_year_end", "path": "reference/api/pandas.tseries.offsets.week.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.is_year_start", "path": "reference/api/pandas.tseries.offsets.week.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.isAnchored", "path": "reference/api/pandas.tseries.offsets.week.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.kwds", "path": "reference/api/pandas.tseries.offsets.week.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.n", "path": "reference/api/pandas.tseries.offsets.week.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.name", "path": "reference/api/pandas.tseries.offsets.week.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.nanos", "path": "reference/api/pandas.tseries.offsets.week.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.normalize", "path": "reference/api/pandas.tseries.offsets.week.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.onOffset", "path": "reference/api/pandas.tseries.offsets.week.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.rollback", "path": "reference/api/pandas.tseries.offsets.week.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Week.rollforward", "path": "reference/api/pandas.tseries.offsets.week.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.Week.rule_code", "path": "reference/api/pandas.tseries.offsets.week.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.Week.weekday", "path": "reference/api/pandas.tseries.offsets.week.weekday", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth", "path": "reference/api/pandas.tseries.offsets.weekofmonth", "type": "Data offsets", "text": ["Describes monthly dates like \u201cthe Tuesday of the 2nd week of each month\u201d.", "A specific integer for the week of the month. e.g. 0 is 1st week of month, 1 is the 2nd week, etc.", "A specific integer for the day of the week.", "0 is Monday", "1 is Tuesday", "2 is Wednesday", "3 is Thursday", "4 is Friday", "5 is Saturday", "6 is Sunday.", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "n", "name", "nanos", "normalize", "rule_code", "week", "weekday", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.WeekOfMonth.__call__", "path": "reference/api/pandas.tseries.offsets.weekofmonth.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.WeekOfMonth.apply", "path": "reference/api/pandas.tseries.offsets.weekofmonth.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.apply_index", "path": "reference/api/pandas.tseries.offsets.weekofmonth.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.base", "path": "reference/api/pandas.tseries.offsets.weekofmonth.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.WeekOfMonth.copy", "path": "reference/api/pandas.tseries.offsets.weekofmonth.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.freqstr", "path": "reference/api/pandas.tseries.offsets.weekofmonth.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.is_anchored", "path": "reference/api/pandas.tseries.offsets.weekofmonth.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.is_month_end", "path": "reference/api/pandas.tseries.offsets.weekofmonth.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.is_month_start", "path": "reference/api/pandas.tseries.offsets.weekofmonth.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.is_on_offset", "path": "reference/api/pandas.tseries.offsets.weekofmonth.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.weekofmonth.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.weekofmonth.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.is_year_end", "path": "reference/api/pandas.tseries.offsets.weekofmonth.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.is_year_start", "path": "reference/api/pandas.tseries.offsets.weekofmonth.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.isAnchored", "path": "reference/api/pandas.tseries.offsets.weekofmonth.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.kwds", "path": "reference/api/pandas.tseries.offsets.weekofmonth.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.n", "path": "reference/api/pandas.tseries.offsets.weekofmonth.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.name", "path": "reference/api/pandas.tseries.offsets.weekofmonth.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.nanos", "path": "reference/api/pandas.tseries.offsets.weekofmonth.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.normalize", "path": "reference/api/pandas.tseries.offsets.weekofmonth.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.onOffset", "path": "reference/api/pandas.tseries.offsets.weekofmonth.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.rollback", "path": "reference/api/pandas.tseries.offsets.weekofmonth.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.WeekOfMonth.rollforward", "path": "reference/api/pandas.tseries.offsets.weekofmonth.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.WeekOfMonth.rule_code", "path": "reference/api/pandas.tseries.offsets.weekofmonth.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.week", "path": "reference/api/pandas.tseries.offsets.weekofmonth.week", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.WeekOfMonth.weekday", "path": "reference/api/pandas.tseries.offsets.weekofmonth.weekday", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin", "path": "reference/api/pandas.tseries.offsets.yearbegin", "type": "Data offsets", "text": ["DateOffset increments between calendar year begin dates.", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "month", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.YearBegin.__call__", "path": "reference/api/pandas.tseries.offsets.yearbegin.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.YearBegin.apply", "path": "reference/api/pandas.tseries.offsets.yearbegin.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.apply_index", "path": "reference/api/pandas.tseries.offsets.yearbegin.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.base", "path": "reference/api/pandas.tseries.offsets.yearbegin.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.YearBegin.copy", "path": "reference/api/pandas.tseries.offsets.yearbegin.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.freqstr", "path": "reference/api/pandas.tseries.offsets.yearbegin.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.is_anchored", "path": "reference/api/pandas.tseries.offsets.yearbegin.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.is_month_end", "path": "reference/api/pandas.tseries.offsets.yearbegin.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.is_month_start", "path": "reference/api/pandas.tseries.offsets.yearbegin.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.is_on_offset", "path": "reference/api/pandas.tseries.offsets.yearbegin.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.yearbegin.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.yearbegin.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.is_year_end", "path": "reference/api/pandas.tseries.offsets.yearbegin.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.is_year_start", "path": "reference/api/pandas.tseries.offsets.yearbegin.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.isAnchored", "path": "reference/api/pandas.tseries.offsets.yearbegin.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.kwds", "path": "reference/api/pandas.tseries.offsets.yearbegin.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.month", "path": "reference/api/pandas.tseries.offsets.yearbegin.month", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.n", "path": "reference/api/pandas.tseries.offsets.yearbegin.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.name", "path": "reference/api/pandas.tseries.offsets.yearbegin.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.nanos", "path": "reference/api/pandas.tseries.offsets.yearbegin.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.normalize", "path": "reference/api/pandas.tseries.offsets.yearbegin.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.onOffset", "path": "reference/api/pandas.tseries.offsets.yearbegin.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearBegin.rollback", "path": "reference/api/pandas.tseries.offsets.yearbegin.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.YearBegin.rollforward", "path": "reference/api/pandas.tseries.offsets.yearbegin.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.YearBegin.rule_code", "path": "reference/api/pandas.tseries.offsets.yearbegin.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd", "path": "reference/api/pandas.tseries.offsets.yearend", "type": "Data offsets", "text": ["DateOffset increments between calendar year ends.", "Attributes", "base", "Returns a copy of the calling offset object with n=1 and all other attributes equal.", "freqstr", "kwds", "month", "n", "name", "nanos", "normalize", "rule_code", "Methods", "__call__(*args, **kwargs)", "Call self as a function.", "rollback", "Roll provided date backward to next offset only if not on offset.", "rollforward", "Roll provided date forward to next offset only if not on offset.", "apply", "apply_index", "copy", "isAnchored", "is_anchored", "is_month_end", "is_month_start", "is_on_offset", "is_quarter_end", "is_quarter_start", "is_year_end", "is_year_start", "onOffset"]}, {"name": "pandas.tseries.offsets.YearEnd.__call__", "path": "reference/api/pandas.tseries.offsets.yearend.__call__", "type": "Data offsets", "text": ["Call self as a function."]}, {"name": "pandas.tseries.offsets.YearEnd.apply", "path": "reference/api/pandas.tseries.offsets.yearend.apply", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.apply_index", "path": "reference/api/pandas.tseries.offsets.yearend.apply_index", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.base", "path": "reference/api/pandas.tseries.offsets.yearend.base", "type": "Data offsets", "text": ["Returns a copy of the calling offset object with n=1 and all other attributes equal."]}, {"name": "pandas.tseries.offsets.YearEnd.copy", "path": "reference/api/pandas.tseries.offsets.yearend.copy", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.freqstr", "path": "reference/api/pandas.tseries.offsets.yearend.freqstr", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.is_anchored", "path": "reference/api/pandas.tseries.offsets.yearend.is_anchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.is_month_end", "path": "reference/api/pandas.tseries.offsets.yearend.is_month_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.is_month_start", "path": "reference/api/pandas.tseries.offsets.yearend.is_month_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.is_on_offset", "path": "reference/api/pandas.tseries.offsets.yearend.is_on_offset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.is_quarter_end", "path": "reference/api/pandas.tseries.offsets.yearend.is_quarter_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.is_quarter_start", "path": "reference/api/pandas.tseries.offsets.yearend.is_quarter_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.is_year_end", "path": "reference/api/pandas.tseries.offsets.yearend.is_year_end", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.is_year_start", "path": "reference/api/pandas.tseries.offsets.yearend.is_year_start", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.isAnchored", "path": "reference/api/pandas.tseries.offsets.yearend.isanchored", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.kwds", "path": "reference/api/pandas.tseries.offsets.yearend.kwds", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.month", "path": "reference/api/pandas.tseries.offsets.yearend.month", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.n", "path": "reference/api/pandas.tseries.offsets.yearend.n", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.name", "path": "reference/api/pandas.tseries.offsets.yearend.name", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.nanos", "path": "reference/api/pandas.tseries.offsets.yearend.nanos", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.normalize", "path": "reference/api/pandas.tseries.offsets.yearend.normalize", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.onOffset", "path": "reference/api/pandas.tseries.offsets.yearend.onoffset", "type": "Data offsets", "text": []}, {"name": "pandas.tseries.offsets.YearEnd.rollback", "path": "reference/api/pandas.tseries.offsets.yearend.rollback", "type": "Data offsets", "text": ["Roll provided date backward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.YearEnd.rollforward", "path": "reference/api/pandas.tseries.offsets.yearend.rollforward", "type": "Data offsets", "text": ["Roll provided date forward to next offset only if not on offset.", "Rolled timestamp if not on offset, otherwise unchanged timestamp."]}, {"name": "pandas.tseries.offsets.YearEnd.rule_code", "path": "reference/api/pandas.tseries.offsets.yearend.rule_code", "type": "Data offsets", "text": []}, {"name": "pandas.UInt16Dtype", "path": "reference/api/pandas.uint16dtype", "type": "Pandas arrays", "text": ["An ExtensionDtype for uint16 integer data.", "Changed in version 1.0.0: Now uses pandas.NA as its missing value, rather than numpy.nan.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.UInt32Dtype", "path": "reference/api/pandas.uint32dtype", "type": "Pandas arrays", "text": ["An ExtensionDtype for uint32 integer data.", "Changed in version 1.0.0: Now uses pandas.NA as its missing value, rather than numpy.nan.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.UInt64Dtype", "path": "reference/api/pandas.uint64dtype", "type": "Pandas arrays", "text": ["An ExtensionDtype for uint64 integer data.", "Changed in version 1.0.0: Now uses pandas.NA as its missing value, rather than numpy.nan.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.UInt64Index", "path": "reference/api/pandas.uint64index", "type": "Index Objects", "text": ["Immutable sequence used for indexing and alignment. The basic object storing axis labels for all pandas objects. UInt64Index is a special case of Index with purely unsigned integer labels. .", "Deprecated since version 1.4.0: In pandas v2.0 UInt64Index will be removed and NumericIndex used instead. UInt64Index will remain fully functional for the duration of pandas 1.x.", "Make a copy of input ndarray.", "Name to be stored in the index.", "See also", "The base pandas Index type.", "Index of numpy int/uint/float data.", "Notes", "An Index instance can only contain hashable objects.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.UInt8Dtype", "path": "reference/api/pandas.uint8dtype", "type": "Pandas arrays", "text": ["An ExtensionDtype for uint8 integer data.", "Changed in version 1.0.0: Now uses pandas.NA as its missing value, rather than numpy.nan.", "Attributes", "None", "Methods", "None"]}, {"name": "pandas.unique", "path": "reference/api/pandas.unique", "type": "General functions", "text": ["Return unique values based on a hash table.", "Uniques are returned in order of appearance. This does NOT sort.", "Significantly faster than numpy.unique for long enough sequences. Includes NA values.", "The return can be:", "Index : when the input is an Index", "Categorical : when the input is a Categorical dtype", "ndarray : when the input is a Series/ndarray", "Return numpy.ndarray or ExtensionArray.", "See also", "Return unique values from an Index.", "Return unique values of Series object.", "Examples", "An unordered Categorical will return categories in the order of appearance.", "An ordered Categorical preserves the category ordering.", "An array of tuples"]}, {"name": "pandas.util.hash_array", "path": "reference/api/pandas.util.hash_array", "type": "Pandas arrays", "text": ["Given a 1d array, return an array of deterministic integers.", "Encoding for data & key when strings.", "Hash_key for string key to encode.", "Whether to first categorize object arrays before hashing. This is more efficient when the array contains duplicate values.", "Hashed values, same length as the vals."]}, {"name": "pandas.util.hash_pandas_object", "path": "reference/api/pandas.util.hash_pandas_object", "type": "General functions", "text": ["Return a data hash of the Index/Series/DataFrame.", "Include the index in the hash (if Series/DataFrame).", "Encoding for data & key when strings.", "Hash_key for string key to encode.", "Whether to first categorize object arrays before hashing. This is more efficient when the array contains duplicate values."]}, {"name": "pandas.wide_to_long", "path": "reference/api/pandas.wide_to_long", "type": "General functions", "text": ["Unpivot a DataFrame from wide to long format.", "Less flexible but more user-friendly than melt.", "With stubnames [\u2018A\u2019, \u2018B\u2019], this function expects to find one or more group of columns with format A-suffix1, A-suffix2,\u2026, B-suffix1, B-suffix2,\u2026 You specify what you want to call this suffix in the resulting long format with j (for example j=\u2019year\u2019)", "Each row of these wide variables are assumed to be uniquely identified by i (can be a single column name or a list of column names)", "All remaining variables in the data frame are left intact.", "The wide-format DataFrame.", "The stub name(s). The wide format variables are assumed to start with the stub names.", "Column(s) to use as id variable(s).", "The name of the sub-observation variable. What you wish to name your suffix in the long format.", "A character indicating the separation of the variable names in the wide format, to be stripped from the names in the long format. For example, if your column names are A-suffix1, A-suffix2, you can strip the hyphen by specifying sep=\u2019-\u2019.", "A regular expression capturing the wanted suffixes. \u2018\\d+\u2019 captures numeric suffixes. Suffixes with no numbers could be specified with the negated character class \u2018\\D+\u2019. You can also further disambiguate suffixes, for example, if your wide variables are of the form A-one, B-two,.., and you have an unrelated column A-rating, you can ignore the last one by specifying suffix=\u2019(!?one|two)\u2019. When all suffixes are numeric, they are cast to int64/float64.", "A DataFrame that contains each stub name as a variable, with new index (i, j).", "See also", "Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.", "Create a spreadsheet-style pivot table as a DataFrame.", "Pivot without aggregation that can handle non-numeric data.", "Generalization of pivot that can handle duplicate values for one index/column pair.", "Pivot based on the index values instead of a column.", "Notes", "All extra variables are left untouched. This simply uses pandas.melt under the hood, but is hard-coded to \u201cdo the right thing\u201d in a typical case.", "Examples", "With multiple id columns", "Going from long back to wide just takes some creative use of unstack", "Less wieldy column names are also handled", "If we have many columns, we could also use a regex to find our stubnames and pass that list on to wide_to_long", "All of the above examples have integers as suffixes. It is possible to have non-integers as suffixes."]}, {"name": "Plotting", "path": "reference/plotting", "type": "Plotting", "text": ["The following functions are contained in the pandas.plotting module.", "andrews_curves(frame, class_column[, ax, ...])", "Generate a matplotlib plot of Andrews curves, for visualising clusters of multivariate data.", "autocorrelation_plot(series[, ax])", "Autocorrelation plot for time series.", "bootstrap_plot(series[, fig, size, samples])", "Bootstrap plot on mean, median and mid-range statistics.", "boxplot(data[, column, by, ax, fontsize, ...])", "Make a box plot from DataFrame columns.", "deregister_matplotlib_converters()", "Remove pandas formatters and converters.", "lag_plot(series[, lag, ax])", "Lag plot for time series.", "parallel_coordinates(frame, class_column[, ...])", "Parallel coordinates plotting.", "plot_params", "Stores pandas plotting options.", "radviz(frame, class_column[, ax, color, ...])", "Plot a multidimensional dataset in 2D.", "register_matplotlib_converters()", "Register pandas formatters and converters with matplotlib.", "scatter_matrix(frame[, alpha, figsize, ax, ...])", "Draw a matrix of scatter plots.", "table(ax, data[, rowLabels, colLabels])", "Helper function to convert DataFrame and Series to matplotlib.table."]}, {"name": "Resampling", "path": "reference/resampling", "type": "General functions", "text": ["Resampler objects are returned by resample calls: pandas.DataFrame.resample(), pandas.Series.resample().", "Resampler.__iter__()", "Groupby iterator.", "Resampler.groups", "Dict {group name -> group labels}.", "Resampler.indices", "Dict {group name -> group indices}.", "Resampler.get_group(name[, obj])", "Construct DataFrame from group with provided name.", "Resampler.apply([func])", "Aggregate using one or more operations over the specified axis.", "Resampler.aggregate([func])", "Aggregate using one or more operations over the specified axis.", "Resampler.transform(arg, *args, **kwargs)", "Call function producing a like-indexed Series on each group and return a Series with the transformed values.", "Resampler.pipe(func, *args, **kwargs)", "Apply a function func with arguments to this Resampler object and return the function's result.", "Resampler.ffill([limit])", "Forward fill the values.", "Resampler.backfill([limit])", "Backward fill the new missing values in the resampled data.", "Resampler.bfill([limit])", "Backward fill the new missing values in the resampled data.", "Resampler.pad([limit])", "Forward fill the values.", "Resampler.nearest([limit])", "Resample by using the nearest value.", "Resampler.fillna(method[, limit])", "Fill missing values introduced by upsampling.", "Resampler.asfreq([fill_value])", "Return the values at the new freq, essentially a reindex.", "Resampler.interpolate([method, axis, limit, ...])", "Interpolate values according to different methods.", "Resampler.count()", "Compute count of group, excluding missing values.", "Resampler.nunique([_method])", "Return number of unique elements in the group.", "Resampler.first([_method, min_count])", "Compute first of group values.", "Resampler.last([_method, min_count])", "Compute last of group values.", "Resampler.max([_method, min_count])", "Compute max of group values.", "Resampler.mean([_method])", "Compute mean of groups, excluding missing values.", "Resampler.median([_method])", "Compute median of groups, excluding missing values.", "Resampler.min([_method, min_count])", "Compute min of group values.", "Resampler.ohlc([_method])", "Compute open, high, low and close values of a group, excluding missing values.", "Resampler.prod([_method, min_count])", "Compute prod of group values.", "Resampler.size()", "Compute group sizes.", "Resampler.sem([_method])", "Compute standard error of the mean of groups, excluding missing values.", "Resampler.std([ddof])", "Compute standard deviation of groups, excluding missing values.", "Resampler.sum([_method, min_count])", "Compute sum of group values.", "Resampler.var([ddof])", "Compute variance of groups, excluding missing values.", "Resampler.quantile([q])", "Return value at the given quantile."]}, {"name": "Reshaping and pivot tables", "path": "user_guide/reshaping", "type": "Manual", "text": ["Data is often stored in so-called \u201cstacked\u201d or \u201crecord\u201d format:", "For the curious here is how the above DataFrame was created:", "To select out everything for variable A we could do:", "But suppose we wish to do time series operations with the variables. A better representation would be where the columns are the unique variables and an index of dates identifies individual observations. To reshape the data into this form, we use the DataFrame.pivot() method (also implemented as a top level function pivot()):", "If the values argument is omitted, and the input DataFrame has more than one column of values which are not used as column or index inputs to pivot, then the resulting \u201cpivoted\u201d DataFrame will have hierarchical columns whose topmost level indicates the respective value column:", "You can then select subsets from the pivoted DataFrame:", "Note that this returns a view on the underlying data in the case where the data are homogeneously-typed.", "Note", "pivot() will error with a ValueError: Index contains duplicate\nentries, cannot reshape if the index/column pair is not unique. In this case, consider using pivot_table() which is a generalization of pivot that can handle duplicate values for one index/column pair.", "Closely related to the pivot() method are the related stack() and unstack() methods available on Series and DataFrame. These methods are designed to work together with MultiIndex objects (see the section on hierarchical indexing). Here are essentially what these methods do:", "stack: \u201cpivot\u201d a level of the (possibly hierarchical) column labels, returning a DataFrame with an index with a new inner-most level of row labels.", "unstack: (inverse operation of stack) \u201cpivot\u201d a level of the (possibly hierarchical) row index to the column axis, producing a reshaped DataFrame with a new inner-most level of column labels.", "The clearest way to explain is by example. Let\u2019s take a prior example data set from the hierarchical indexing section:", "The stack function \u201ccompresses\u201d a level in the DataFrame\u2019s columns to produce either:", "A Series, in the case of a simple column Index.", "A DataFrame, in the case of a MultiIndex in the columns.", "If the columns have a MultiIndex, you can choose which level to stack. The stacked level becomes the new lowest level in a MultiIndex on the columns:", "With a \u201cstacked\u201d DataFrame or Series (having a MultiIndex as the index), the inverse operation of stack is unstack, which by default unstacks the last level:", "If the indexes have names, you can use the level names instead of specifying the level numbers:", "Notice that the stack and unstack methods implicitly sort the index levels involved. Hence a call to stack and then unstack, or vice versa, will result in a sorted copy of the original DataFrame or Series:", "The above code will raise a TypeError if the call to sort_index is removed.", "You may also stack or unstack more than one level at a time by passing a list of levels, in which case the end result is as if each level in the list were processed individually.", "The list of levels can contain either level names or level numbers (but not a mixture of the two).", "These functions are intelligent about handling missing data and do not expect each subgroup within the hierarchical index to have the same set of labels. They also can handle the index being unsorted (but you can make it sorted by calling sort_index, of course). Here is a more complex example:", "As mentioned above, stack can be called with a level argument to select which level in the columns to stack:", "Unstacking can result in missing values if subgroups do not have the same set of labels. By default, missing values will be replaced with the default fill value for that data type, NaN for float, NaT for datetimelike, etc. For integer types, by default data will converted to float and missing values will be set to NaN.", "Alternatively, unstack takes an optional fill_value argument, for specifying the value of missing data.", "Unstacking when the columns are a MultiIndex is also careful about doing the right thing:", "The top-level melt() function and the corresponding DataFrame.melt() are useful to massage a DataFrame into a format where one or more columns are identifier variables, while all other columns, considered measured variables, are \u201cunpivoted\u201d to the row axis, leaving just two non-identifier columns, \u201cvariable\u201d and \u201cvalue\u201d. The names of those columns can be customized by supplying the var_name and value_name parameters.", "For instance,", "When transforming a DataFrame using melt(), the index will be ignored. The original index values can be kept around by setting the ignore_index parameter to False (default is True). This will however duplicate them.", "New in version 1.1.0.", "Another way to transform is to use the wide_to_long() panel data convenience function. It is less flexible than melt(), but more user-friendly.", "It should be no shock that combining pivot / stack / unstack with GroupBy and the basic Series and DataFrame statistical functions can produce some very expressive and fast data manipulations.", "While pivot() provides general purpose pivoting with various data types (strings, numerics, etc.), pandas also provides pivot_table() for pivoting with aggregation of numeric data.", "The function pivot_table() can be used to create spreadsheet-style pivot tables. See the cookbook for some advanced strategies.", "It takes a number of arguments:", "data: a DataFrame object.", "values: a column or a list of columns to aggregate.", "index: a column, Grouper, array which has the same length as data, or list of them. Keys to group by on the pivot table index. If an array is passed, it is being used as the same manner as column values.", "columns: a column, Grouper, array which has the same length as data, or list of them. Keys to group by on the pivot table column. If an array is passed, it is being used as the same manner as column values.", "aggfunc: function to use for aggregation, defaulting to numpy.mean.", "Consider a data set like this:", "We can produce pivot tables from this data very easily:", "The result object is a DataFrame having potentially hierarchical indexes on the rows and columns. If the values column name is not given, the pivot table will include all of the data that can be aggregated in an additional level of hierarchy in the columns:", "Also, you can use Grouper for index and columns keywords. For detail of Grouper, see Grouping with a Grouper specification.", "You can render a nice output of the table omitting the missing values by calling to_string if you wish:", "i.e. DataFrame.pivot_table().", "If you pass margins=True to pivot_table, special All columns and rows will be added with partial group aggregates across the categories on the rows and columns:", "Additionally, you can call DataFrame.stack() to display a pivoted DataFrame as having a multi-level index:", "Use crosstab() to compute a cross-tabulation of two (or more) factors. By default crosstab computes a frequency table of the factors unless an array of values and an aggregation function are passed.", "It takes a number of arguments", "index: array-like, values to group by in the rows.", "columns: array-like, values to group by in the columns.", "values: array-like, optional, array of values to aggregate according to the factors.", "aggfunc: function, optional, If no values array is passed, computes a frequency table.", "rownames: sequence, default None, must match number of row arrays passed.", "colnames: sequence, default None, if passed, must match number of column arrays passed.", "margins: boolean, default False, Add row/column margins (subtotals)", "normalize: boolean, {\u2018all\u2019, \u2018index\u2019, \u2018columns\u2019}, or {0,1}, default False. Normalize by dividing all values by the sum of values.", "Any Series passed will have their name attributes used unless row or column names for the cross-tabulation are specified", "For example:", "If crosstab receives only two Series, it will provide a frequency table.", "crosstab can also be implemented to Categorical data.", "If you want to include all of data categories even if the actual data does not contain any instances of a particular category, you should set dropna=False.", "For example:", "Frequency tables can also be normalized to show percentages rather than counts using the normalize argument:", "normalize can also normalize values within each row or within each column:", "crosstab can also be passed a third Series and an aggregation function (aggfunc) that will be applied to the values of the third Series within each group defined by the first two Series:", "Finally, one can also add margins or normalize this output.", "The cut() function computes groupings for the values of the input array and is often used to transform continuous variables to discrete or categorical variables:", "If the bins keyword is an integer, then equal-width bins are formed. Alternatively we can specify custom bin-edges:", "If the bins keyword is an IntervalIndex, then these will be used to bin the passed data.:", "To convert a categorical variable into a \u201cdummy\u201d or \u201cindicator\u201d DataFrame, for example a column in a DataFrame (a Series) which has k distinct values, can derive a DataFrame containing k columns of 1s and 0s using get_dummies():", "Sometimes it\u2019s useful to prefix the column names, for example when merging the result with the original DataFrame:", "This function is often used along with discretization functions like cut:", "See also Series.str.get_dummies.", "get_dummies() also accepts a DataFrame. By default all categorical variables (categorical in the statistical sense, those with object or categorical dtype) are encoded as dummy variables.", "All non-object columns are included untouched in the output. You can control the columns that are encoded with the columns keyword.", "Notice that the B column is still included in the output, it just hasn\u2019t been encoded. You can drop B before calling get_dummies if you don\u2019t want to include it in the output.", "As with the Series version, you can pass values for the prefix and prefix_sep. By default the column name is used as the prefix, and \u2018_\u2019 as the prefix separator. You can specify prefix and prefix_sep in 3 ways:", "string: Use the same value for prefix or prefix_sep for each column to be encoded.", "list: Must be the same length as the number of columns being encoded.", "dict: Mapping column name to prefix.", "Sometimes it will be useful to only keep k-1 levels of a categorical variable to avoid collinearity when feeding the result to statistical models. You can switch to this mode by turn on drop_first.", "When a column contains only one level, it will be omitted in the result.", "By default new columns will have np.uint8 dtype. To choose another dtype, use the dtype argument:", "To encode 1-d values as an enumerated type use factorize():", "Note that factorize is similar to numpy.unique, but differs in its handling of NaN:", "Note", "The following numpy.unique will fail under Python 3 with a TypeError because of an ordering bug. See also here.", "Note", "If you just want to handle one column as a categorical variable (like R\u2019s factor), you can use df[\"cat_col\"] = pd.Categorical(df[\"col\"]) or df[\"cat_col\"] = df[\"col\"].astype(\"category\"). For full docs on Categorical, see the Categorical introduction and the API documentation.", "In this section, we will review frequently asked questions and examples. The column names and relevant column values are named to correspond with how this DataFrame will be pivoted in the answers below.", "Suppose we wanted to pivot df such that the col values are columns, row values are the index, and the mean of val0 are the values? In particular, the resulting DataFrame should look like:", "This solution uses pivot_table(). Also note that aggfunc='mean' is the default. It is included here to be explicit.", "Note that we can also replace the missing values by using the fill_value parameter.", "Also note that we can pass in other aggregation functions as well. For example, we can also pass in sum.", "Another aggregation we can do is calculate the frequency in which the columns and rows occur together a.k.a. \u201ccross tabulation\u201d. To do this, we can pass size to the aggfunc parameter.", "We can also perform multiple aggregations. For example, to perform both a sum and mean, we can pass in a list to the aggfunc argument.", "Note to aggregate over multiple value columns, we can pass in a list to the values parameter.", "Note to subdivide over multiple columns we can pass in a list to the columns parameter.", "New in version 0.25.0.", "Sometimes the values in a column are list-like.", "We can \u2018explode\u2019 the values column, transforming each list-like to a separate row, by using explode(). This will replicate the index values from the original row:", "You can also explode the column in the DataFrame.", "Series.explode() will replace empty lists with np.nan and preserve scalar entries. The dtype of the resulting Series is always object.", "Here is a typical usecase. You have comma separated strings in a column and want to expand this.", "Creating a long form DataFrame is now straightforward using explode and chained operations"]}, {"name": "Scaling to large datasets", "path": "user_guide/scale", "type": "Manual", "text": ["pandas provides data structures for in-memory analytics, which makes using pandas to analyze datasets that are larger than memory datasets somewhat tricky. Even datasets that are a sizable fraction of memory become unwieldy, as some pandas operations need to make intermediate copies.", "This document provides a few recommendations for scaling your analysis to larger datasets. It\u2019s a complement to Enhancing performance, which focuses on speeding up analysis for datasets that fit in memory.", "But first, it\u2019s worth considering not using pandas. pandas isn\u2019t the right tool for all situations. If you\u2019re working with very large datasets and a tool like PostgreSQL fits your needs, then you should probably be using that. Assuming you want or need the expressiveness and power of pandas, let\u2019s carry on.", "Suppose our raw dataset on disk has many columns:", "To load the columns we want, we have two options. Option 1 loads in all the data and then filters to what we need.", "Option 2 only loads the columns we request.", "If we were to measure the memory usage of the two calls, we\u2019d see that specifying columns uses about 1/10th the memory in this case.", "With pandas.read_csv(), you can specify usecols to limit the columns read into memory. Not all file formats that can be read by pandas provide an option to read a subset of columns.", "The default pandas data types are not the most memory efficient. This is especially true for text data columns with relatively few unique values (commonly referred to as \u201clow-cardinality\u201d data). By using more efficient data types, you can store larger datasets in memory.", "Now, let\u2019s inspect the data types and memory usage to see where we should focus our attention.", "The name column is taking up much more memory than any other. It has just a few unique values, so it\u2019s a good candidate for converting to a Categorical. With a Categorical, we store each unique name once and use space-efficient integers to know which specific name is used in each row.", "We can go a bit further and downcast the numeric columns to their smallest types using pandas.to_numeric().", "In all, we\u2019ve reduced the in-memory footprint of this dataset to 1/5 of its original size.", "See Categorical data for more on Categorical and dtypes for an overview of all of pandas\u2019 dtypes.", "Some workloads can be achieved with chunking: splitting a large problem like \u201cconvert this directory of CSVs to parquet\u201d into a bunch of small problems (\u201cconvert this individual CSV file into a Parquet file. Now repeat that for each file in this directory.\u201d). As long as each chunk fits in memory, you can work with datasets that are much larger than memory.", "Note", "Chunking works well when the operation you\u2019re performing requires zero or minimal coordination between chunks. For more complicated workflows, you\u2019re better off using another library.", "Suppose we have an even larger \u201clogical dataset\u201d on disk that\u2019s a directory of parquet files. Each file in the directory represents a different year of the entire dataset.", "Now we\u2019ll implement an out-of-core value_counts. The peak memory usage of this workflow is the single largest chunk, plus a small series storing the unique value counts up to this point. As long as each individual file fits in memory, this will work for arbitrary-sized datasets.", "Some readers, like pandas.read_csv(), offer parameters to control the chunksize when reading a single file.", "Manually chunking is an OK option for workflows that don\u2019t require too sophisticated of operations. Some operations, like groupby, are much harder to do chunkwise. In these cases, you may be better switching to a different library that implements these out-of-core algorithms for you.", "pandas is just one library offering a DataFrame API. Because of its popularity, pandas\u2019 API has become something of a standard that other libraries implement. The pandas documentation maintains a list of libraries implementing a DataFrame API in our ecosystem page.", "For example, Dask, a parallel computing library, has dask.dataframe, a pandas-like API for working with larger than memory datasets in parallel. Dask can use multiple threads or processes on a single machine, or a cluster of machines to process data in parallel.", "We\u2019ll import dask.dataframe and notice that the API feels similar to pandas. We can use Dask\u2019s read_parquet function, but provide a globstring of files to read in.", "Inspecting the ddf object, we see a few things", "There are familiar attributes like .columns and .dtypes", "There are familiar methods like .groupby, .sum, etc.", "There are new attributes like .npartitions and .divisions", "The partitions and divisions are how Dask parallelizes computation. A Dask DataFrame is made up of many pandas DataFrames. A single method call on a Dask DataFrame ends up making many pandas method calls, and Dask knows how to coordinate everything to get the result.", "One major difference: the dask.dataframe API is lazy. If you look at the repr above, you\u2019ll notice that the values aren\u2019t actually printed out; just the column names and dtypes. That\u2019s because Dask hasn\u2019t actually read the data yet. Rather than executing immediately, doing operations build up a task graph.", "Each of these calls is instant because the result isn\u2019t being computed yet. We\u2019re just building up a list of computation to do when someone needs the result. Dask knows that the return type of a pandas.Series.value_counts is a pandas Series with a certain dtype and a certain name. So the Dask version returns a Dask Series with the same dtype and the same name.", "To get the actual result you can call .compute().", "At that point, you get back the same thing you\u2019d get with pandas, in this case a concrete pandas Series with the count of each name.", "Calling .compute causes the full task graph to be executed. This includes reading the data, selecting the columns, and doing the value_counts. The execution is done in parallel where possible, and Dask tries to keep the overall memory footprint small. You can work with datasets that are much larger than memory, as long as each partition (a regular pandas DataFrame) fits in memory.", "By default, dask.dataframe operations use a threadpool to do operations in parallel. We can also connect to a cluster to distribute the work on many machines. In this case we\u2019ll connect to a local \u201ccluster\u201d made up of several processes on this single machine.", "Once this client is created, all of Dask\u2019s computation will take place on the cluster (which is just processes in this case).", "Dask implements the most used parts of the pandas API. For example, we can do a familiar groupby aggregation.", "The grouping and aggregation is done out-of-core and in parallel.", "When Dask knows the divisions of a dataset, certain optimizations are possible. When reading parquet datasets written by dask, the divisions will be known automatically. In this case, since we created the parquet files manually, we need to supply the divisions manually.", "Now we can do things like fast random access with .loc.", "Dask knows to just look in the 3rd partition for selecting values in 2002. It doesn\u2019t need to look at any other data.", "Many workflows involve a large amount of data and processing it in a way that reduces the size to something that fits in memory. In this case, we\u2019ll resample to daily frequency and take the mean. Once we\u2019ve taken the mean, we know the results will fit in memory, so we can safely call compute without running out of memory. At that point it\u2019s just a regular pandas object.", "These Dask examples have all be done using multiple processes on a single machine. Dask can be deployed on a cluster to scale up to even larger datasets.", "You see more dask examples at https://examples.dask.org."]}, {"name": "Series", "path": "reference/series", "type": "General functions", "text": ["Series([data, index, dtype, name, copy, ...])", "One-dimensional ndarray with axis labels (including time series).", "Axes", "Series.index", "The index (axis labels) of the Series.", "Series.array", "The ExtensionArray of the data backing this Series or Index.", "Series.values", "Return Series as ndarray or ndarray-like depending on the dtype.", "Series.dtype", "Return the dtype object of the underlying data.", "Series.shape", "Return a tuple of the shape of the underlying data.", "Series.nbytes", "Return the number of bytes in the underlying data.", "Series.ndim", "Number of dimensions of the underlying data, by definition 1.", "Series.size", "Return the number of elements in the underlying data.", "Series.T", "Return the transpose, which is by definition self.", "Series.memory_usage([index, deep])", "Return the memory usage of the Series.", "Series.hasnans", "Return True if there are any NaNs.", "Series.empty", "Indicator whether Series/DataFrame is empty.", "Series.dtypes", "Return the dtype object of the underlying data.", "Series.name", "Return the name of the Series.", "Series.flags", "Get the properties associated with this pandas object.", "Series.set_flags(*[, copy, ...])", "Return a new object with updated flags.", "Series.astype(dtype[, copy, errors])", "Cast a pandas object to a specified dtype dtype.", "Series.convert_dtypes([infer_objects, ...])", "Convert columns to best possible dtypes using dtypes supporting pd.NA.", "Series.infer_objects()", "Attempt to infer better dtypes for object columns.", "Series.copy([deep])", "Make a copy of this object's indices and data.", "Series.bool()", "Return the bool of a single element Series or DataFrame.", "Series.to_numpy([dtype, copy, na_value])", "A NumPy ndarray representing the values in this Series or Index.", "Series.to_period([freq, copy])", "Convert Series from DatetimeIndex to PeriodIndex.", "Series.to_timestamp([freq, how, copy])", "Cast to DatetimeIndex of Timestamps, at beginning of period.", "Series.to_list()", "Return a list of the values.", "Series.__array__([dtype])", "Return the values as a NumPy array.", "Series.get(key[, default])", "Get item from object for given key (ex: DataFrame column).", "Series.at", "Access a single value for a row/column label pair.", "Series.iat", "Access a single value for a row/column pair by integer position.", "Series.loc", "Access a group of rows and columns by label(s) or a boolean array.", "Series.iloc", "Purely integer-location based indexing for selection by position.", "Series.__iter__()", "Return an iterator of the values.", "Series.items()", "Lazily iterate over (index, value) tuples.", "Series.iteritems()", "Lazily iterate over (index, value) tuples.", "Series.keys()", "Return alias for index.", "Series.pop(item)", "Return item and drops from series.", "Series.item()", "Return the first element of the underlying data as a Python scalar.", "Series.xs(key[, axis, level, drop_level])", "Return cross-section from the Series/DataFrame.", "For more information on .at, .iat, .loc, and .iloc, see the indexing documentation.", "Series.add(other[, level, fill_value, axis])", "Return Addition of series and other, element-wise (binary operator add).", "Series.sub(other[, level, fill_value, axis])", "Return Subtraction of series and other, element-wise (binary operator sub).", "Series.mul(other[, level, fill_value, axis])", "Return Multiplication of series and other, element-wise (binary operator mul).", "Series.div(other[, level, fill_value, axis])", "Return Floating division of series and other, element-wise (binary operator truediv).", "Series.truediv(other[, level, fill_value, axis])", "Return Floating division of series and other, element-wise (binary operator truediv).", "Series.floordiv(other[, level, fill_value, axis])", "Return Integer division of series and other, element-wise (binary operator floordiv).", "Series.mod(other[, level, fill_value, axis])", "Return Modulo of series and other, element-wise (binary operator mod).", "Series.pow(other[, level, fill_value, axis])", "Return Exponential power of series and other, element-wise (binary operator pow).", "Series.radd(other[, level, fill_value, axis])", "Return Addition of series and other, element-wise (binary operator radd).", "Series.rsub(other[, level, fill_value, axis])", "Return Subtraction of series and other, element-wise (binary operator rsub).", "Series.rmul(other[, level, fill_value, axis])", "Return Multiplication of series and other, element-wise (binary operator rmul).", "Series.rdiv(other[, level, fill_value, axis])", "Return Floating division of series and other, element-wise (binary operator rtruediv).", "Series.rtruediv(other[, level, fill_value, axis])", "Return Floating division of series and other, element-wise (binary operator rtruediv).", "Series.rfloordiv(other[, level, fill_value, ...])", "Return Integer division of series and other, element-wise (binary operator rfloordiv).", "Series.rmod(other[, level, fill_value, axis])", "Return Modulo of series and other, element-wise (binary operator rmod).", "Series.rpow(other[, level, fill_value, axis])", "Return Exponential power of series and other, element-wise (binary operator rpow).", "Series.combine(other, func[, fill_value])", "Combine the Series with a Series or scalar according to func.", "Series.combine_first(other)", "Update null elements with value in the same location in 'other'.", "Series.round([decimals])", "Round each value in a Series to the given number of decimals.", "Series.lt(other[, level, fill_value, axis])", "Return Less than of series and other, element-wise (binary operator lt).", "Series.gt(other[, level, fill_value, axis])", "Return Greater than of series and other, element-wise (binary operator gt).", "Series.le(other[, level, fill_value, axis])", "Return Less than or equal to of series and other, element-wise (binary operator le).", "Series.ge(other[, level, fill_value, axis])", "Return Greater than or equal to of series and other, element-wise (binary operator ge).", "Series.ne(other[, level, fill_value, axis])", "Return Not equal to of series and other, element-wise (binary operator ne).", "Series.eq(other[, level, fill_value, axis])", "Return Equal to of series and other, element-wise (binary operator eq).", "Series.product([axis, skipna, level, ...])", "Return the product of the values over the requested axis.", "Series.dot(other)", "Compute the dot product between the Series and the columns of other.", "Series.apply(func[, convert_dtype, args])", "Invoke function on values of Series.", "Series.agg([func, axis])", "Aggregate using one or more operations over the specified axis.", "Series.aggregate([func, axis])", "Aggregate using one or more operations over the specified axis.", "Series.transform(func[, axis])", "Call func on self producing a Series with the same axis shape as self.", "Series.map(arg[, na_action])", "Map values of Series according to an input mapping or function.", "Series.groupby([by, axis, level, as_index, ...])", "Group Series using a mapper or by a Series of columns.", "Series.rolling(window[, min_periods, ...])", "Provide rolling window calculations.", "Series.expanding([min_periods, center, ...])", "Provide expanding window calculations.", "Series.ewm([com, span, halflife, alpha, ...])", "Provide exponentially weighted (EW) calculations.", "Series.pipe(func, *args, **kwargs)", "Apply chainable functions that expect Series or DataFrames.", "Series.abs()", "Return a Series/DataFrame with absolute numeric value of each element.", "Series.all([axis, bool_only, skipna, level])", "Return whether all elements are True, potentially over an axis.", "Series.any([axis, bool_only, skipna, level])", "Return whether any element is True, potentially over an axis.", "Series.autocorr([lag])", "Compute the lag-N autocorrelation.", "Series.between(left, right[, inclusive])", "Return boolean Series equivalent to left <= series <= right.", "Series.clip([lower, upper, axis, inplace])", "Trim values at input threshold(s).", "Series.corr(other[, method, min_periods])", "Compute correlation with other Series, excluding missing values.", "Series.count([level])", "Return number of non-NA/null observations in the Series.", "Series.cov(other[, min_periods, ddof])", "Compute covariance with Series, excluding missing values.", "Series.cummax([axis, skipna])", "Return cumulative maximum over a DataFrame or Series axis.", "Series.cummin([axis, skipna])", "Return cumulative minimum over a DataFrame or Series axis.", "Series.cumprod([axis, skipna])", "Return cumulative product over a DataFrame or Series axis.", "Series.cumsum([axis, skipna])", "Return cumulative sum over a DataFrame or Series axis.", "Series.describe([percentiles, include, ...])", "Generate descriptive statistics.", "Series.diff([periods])", "First discrete difference of element.", "Series.factorize([sort, na_sentinel])", "Encode the object as an enumerated type or categorical variable.", "Series.kurt([axis, skipna, level, numeric_only])", "Return unbiased kurtosis over requested axis.", "Series.mad([axis, skipna, level])", "Return the mean absolute deviation of the values over the requested axis.", "Series.max([axis, skipna, level, numeric_only])", "Return the maximum of the values over the requested axis.", "Series.mean([axis, skipna, level, numeric_only])", "Return the mean of the values over the requested axis.", "Series.median([axis, skipna, level, ...])", "Return the median of the values over the requested axis.", "Series.min([axis, skipna, level, numeric_only])", "Return the minimum of the values over the requested axis.", "Series.mode([dropna])", "Return the mode(s) of the Series.", "Series.nlargest([n, keep])", "Return the largest n elements.", "Series.nsmallest([n, keep])", "Return the smallest n elements.", "Series.pct_change([periods, fill_method, ...])", "Percentage change between the current and a prior element.", "Series.prod([axis, skipna, level, ...])", "Return the product of the values over the requested axis.", "Series.quantile([q, interpolation])", "Return value at the given quantile.", "Series.rank([axis, method, numeric_only, ...])", "Compute numerical data ranks (1 through n) along axis.", "Series.sem([axis, skipna, level, ddof, ...])", "Return unbiased standard error of the mean over requested axis.", "Series.skew([axis, skipna, level, numeric_only])", "Return unbiased skew over requested axis.", "Series.std([axis, skipna, level, ddof, ...])", "Return sample standard deviation over requested axis.", "Series.sum([axis, skipna, level, ...])", "Return the sum of the values over the requested axis.", "Series.var([axis, skipna, level, ddof, ...])", "Return unbiased variance over requested axis.", "Series.kurtosis([axis, skipna, level, ...])", "Return unbiased kurtosis over requested axis.", "Series.unique()", "Return unique values of Series object.", "Series.nunique([dropna])", "Return number of unique elements in the object.", "Series.is_unique", "Return boolean if values in the object are unique.", "Series.is_monotonic", "Return boolean if values in the object are monotonic_increasing.", "Series.is_monotonic_increasing", "Alias for is_monotonic.", "Series.is_monotonic_decreasing", "Return boolean if values in the object are monotonic_decreasing.", "Series.value_counts([normalize, sort, ...])", "Return a Series containing counts of unique values.", "Series.align(other[, join, axis, level, ...])", "Align two objects on their axes with the specified join method.", "Series.drop([labels, axis, index, columns, ...])", "Return Series with specified index labels removed.", "Series.droplevel(level[, axis])", "Return Series/DataFrame with requested index / column level(s) removed.", "Series.drop_duplicates([keep, inplace])", "Return Series with duplicate values removed.", "Series.duplicated([keep])", "Indicate duplicate Series values.", "Series.equals(other)", "Test whether two objects contain the same elements.", "Series.first(offset)", "Select initial periods of time series data based on a date offset.", "Series.head([n])", "Return the first n rows.", "Series.idxmax([axis, skipna])", "Return the row label of the maximum value.", "Series.idxmin([axis, skipna])", "Return the row label of the minimum value.", "Series.isin(values)", "Whether elements in Series are contained in values.", "Series.last(offset)", "Select final periods of time series data based on a date offset.", "Series.reindex(*args, **kwargs)", "Conform Series to new index with optional filling logic.", "Series.reindex_like(other[, method, copy, ...])", "Return an object with matching indices as other object.", "Series.rename([index, axis, copy, inplace, ...])", "Alter Series index labels or name.", "Series.rename_axis([mapper, index, columns, ...])", "Set the name of the axis for the index or columns.", "Series.reset_index([level, drop, name, inplace])", "Generate a new DataFrame or Series with the index reset.", "Series.sample([n, frac, replace, weights, ...])", "Return a random sample of items from an axis of object.", "Series.set_axis(labels[, axis, inplace])", "Assign desired index to given axis.", "Series.take(indices[, axis, is_copy])", "Return the elements in the given positional indices along an axis.", "Series.tail([n])", "Return the last n rows.", "Series.truncate([before, after, axis, copy])", "Truncate a Series or DataFrame before and after some index value.", "Series.where(cond[, other, inplace, axis, ...])", "Replace values where the condition is False.", "Series.mask(cond[, other, inplace, axis, ...])", "Replace values where the condition is True.", "Series.add_prefix(prefix)", "Prefix labels with string prefix.", "Series.add_suffix(suffix)", "Suffix labels with string suffix.", "Series.filter([items, like, regex, axis])", "Subset the dataframe rows or columns according to the specified index labels.", "Series.backfill([axis, inplace, limit, downcast])", "Synonym for DataFrame.fillna() with method='bfill'.", "Series.bfill([axis, inplace, limit, downcast])", "Synonym for DataFrame.fillna() with method='bfill'.", "Series.dropna([axis, inplace, how])", "Return a new Series with missing values removed.", "Series.ffill([axis, inplace, limit, downcast])", "Synonym for DataFrame.fillna() with method='ffill'.", "Series.fillna([value, method, axis, ...])", "Fill NA/NaN values using the specified method.", "Series.interpolate([method, axis, limit, ...])", "Fill NaN values using an interpolation method.", "Series.isna()", "Detect missing values.", "Series.isnull()", "Series.isnull is an alias for Series.isna.", "Series.notna()", "Detect existing (non-missing) values.", "Series.notnull()", "Series.notnull is an alias for Series.notna.", "Series.pad([axis, inplace, limit, downcast])", "Synonym for DataFrame.fillna() with method='ffill'.", "Series.replace([to_replace, value, inplace, ...])", "Replace values given in to_replace with value.", "Series.argsort([axis, kind, order])", "Return the integer indices that would sort the Series values.", "Series.argmin([axis, skipna])", "Return int position of the smallest value in the Series.", "Series.argmax([axis, skipna])", "Return int position of the largest value in the Series.", "Series.reorder_levels(order)", "Rearrange index levels using input order.", "Series.sort_values([axis, ascending, ...])", "Sort by the values.", "Series.sort_index([axis, level, ascending, ...])", "Sort Series by index labels.", "Series.swaplevel([i, j, copy])", "Swap levels i and j in a MultiIndex.", "Series.unstack([level, fill_value])", "Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.", "Series.explode([ignore_index])", "Transform each element of a list-like to a row.", "Series.searchsorted(value[, side, sorter])", "Find indices where elements should be inserted to maintain order.", "Series.ravel([order])", "Return the flattened underlying data as an ndarray.", "Series.repeat(repeats[, axis])", "Repeat elements of a Series.", "Series.squeeze([axis])", "Squeeze 1 dimensional axis objects into scalars.", "Series.view([dtype])", "Create a new view of the Series.", "Series.append(to_append[, ignore_index, ...])", "Concatenate two or more Series.", "Series.compare(other[, align_axis, ...])", "Compare to another Series and show the differences.", "Series.update(other)", "Modify Series in place using values from passed Series.", "Series.asfreq(freq[, method, how, ...])", "Convert time series to specified frequency.", "Series.asof(where[, subset])", "Return the last row(s) without any NaNs before where.", "Series.shift([periods, freq, axis, fill_value])", "Shift index by desired number of periods with an optional time freq.", "Series.first_valid_index()", "Return index for first non-NA value or None, if no NA value is found.", "Series.last_valid_index()", "Return index for last non-NA value or None, if no NA value is found.", "Series.resample(rule[, axis, closed, label, ...])", "Resample time-series data.", "Series.tz_convert(tz[, axis, level, copy])", "Convert tz-aware axis to target time zone.", "Series.tz_localize(tz[, axis, level, copy, ...])", "Localize tz-naive index of a Series or DataFrame to target time zone.", "Series.at_time(time[, asof, axis])", "Select values at particular time of day (e.g., 9:30AM).", "Series.between_time(start_time, end_time[, ...])", "Select values between particular times of the day (e.g., 9:00-9:30 AM).", "Series.tshift([periods, freq, axis])", "(DEPRECATED) Shift the time index, using the index's frequency if available.", "Series.slice_shift([periods, axis])", "(DEPRECATED) Equivalent to shift without copying data.", "pandas provides dtype-specific methods under various accessors. These are separate namespaces within Series that only apply to specific data types.", "Data Type", "Accessor", "Datetime, Timedelta, Period", "dt", "String", "str", "Categorical", "cat", "Sparse", "sparse", "Series.dt can be used to access the values of the series as datetimelike and return several properties. These can be accessed like Series.dt.<property>.", "Series.dt.date", "Returns numpy array of python datetime.date objects.", "Series.dt.time", "Returns numpy array of datetime.time objects.", "Series.dt.timetz", "Returns numpy array of datetime.time objects with timezone information.", "Series.dt.year", "The year of the datetime.", "Series.dt.month", "The month as January=1, December=12.", "Series.dt.day", "The day of the datetime.", "Series.dt.hour", "The hours of the datetime.", "Series.dt.minute", "The minutes of the datetime.", "Series.dt.second", "The seconds of the datetime.", "Series.dt.microsecond", "The microseconds of the datetime.", "Series.dt.nanosecond", "The nanoseconds of the datetime.", "Series.dt.week", "(DEPRECATED) The week ordinal of the year.", "Series.dt.weekofyear", "(DEPRECATED) The week ordinal of the year.", "Series.dt.dayofweek", "The day of the week with Monday=0, Sunday=6.", "Series.dt.day_of_week", "The day of the week with Monday=0, Sunday=6.", "Series.dt.weekday", "The day of the week with Monday=0, Sunday=6.", "Series.dt.dayofyear", "The ordinal day of the year.", "Series.dt.day_of_year", "The ordinal day of the year.", "Series.dt.quarter", "The quarter of the date.", "Series.dt.is_month_start", "Indicates whether the date is the first day of the month.", "Series.dt.is_month_end", "Indicates whether the date is the last day of the month.", "Series.dt.is_quarter_start", "Indicator for whether the date is the first day of a quarter.", "Series.dt.is_quarter_end", "Indicator for whether the date is the last day of a quarter.", "Series.dt.is_year_start", "Indicate whether the date is the first day of a year.", "Series.dt.is_year_end", "Indicate whether the date is the last day of the year.", "Series.dt.is_leap_year", "Boolean indicator if the date belongs to a leap year.", "Series.dt.daysinmonth", "The number of days in the month.", "Series.dt.days_in_month", "The number of days in the month.", "Series.dt.tz", "Return the timezone.", "Series.dt.freq", "Return the frequency object for this PeriodArray.", "Series.dt.to_period(*args, **kwargs)", "Cast to PeriodArray/Index at a particular frequency.", "Series.dt.to_pydatetime()", "Return the data as an array of datetime.datetime objects.", "Series.dt.tz_localize(*args, **kwargs)", "Localize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index.", "Series.dt.tz_convert(*args, **kwargs)", "Convert tz-aware Datetime Array/Index from one time zone to another.", "Series.dt.normalize(*args, **kwargs)", "Convert times to midnight.", "Series.dt.strftime(*args, **kwargs)", "Convert to Index using specified date_format.", "Series.dt.round(*args, **kwargs)", "Perform round operation on the data to the specified freq.", "Series.dt.floor(*args, **kwargs)", "Perform floor operation on the data to the specified freq.", "Series.dt.ceil(*args, **kwargs)", "Perform ceil operation on the data to the specified freq.", "Series.dt.month_name(*args, **kwargs)", "Return the month names of the DateTimeIndex with specified locale.", "Series.dt.day_name(*args, **kwargs)", "Return the day names of the DateTimeIndex with specified locale.", "Series.dt.qyear", "Series.dt.start_time", "Series.dt.end_time", "Series.dt.days", "Number of days for each element.", "Series.dt.seconds", "Number of seconds (>= 0 and less than 1 day) for each element.", "Series.dt.microseconds", "Number of microseconds (>= 0 and less than 1 second) for each element.", "Series.dt.nanoseconds", "Number of nanoseconds (>= 0 and less than 1 microsecond) for each element.", "Series.dt.components", "Return a Dataframe of the components of the Timedeltas.", "Series.dt.to_pytimedelta()", "Return an array of native datetime.timedelta objects.", "Series.dt.total_seconds(*args, **kwargs)", "Return total duration of each element expressed in seconds.", "Series.str can be used to access the values of the series as strings and apply several methods to it. These can be accessed like Series.str.<function/property>.", "Series.str.capitalize()", "Convert strings in the Series/Index to be capitalized.", "Series.str.casefold()", "Convert strings in the Series/Index to be casefolded.", "Series.str.cat([others, sep, na_rep, join])", "Concatenate strings in the Series/Index with given separator.", "Series.str.center(width[, fillchar])", "Pad left and right side of strings in the Series/Index.", "Series.str.contains(pat[, case, flags, na, ...])", "Test if pattern or regex is contained within a string of a Series or Index.", "Series.str.count(pat[, flags])", "Count occurrences of pattern in each string of the Series/Index.", "Series.str.decode(encoding[, errors])", "Decode character string in the Series/Index using indicated encoding.", "Series.str.encode(encoding[, errors])", "Encode character string in the Series/Index using indicated encoding.", "Series.str.endswith(pat[, na])", "Test if the end of each string element matches a pattern.", "Series.str.extract(pat[, flags, expand])", "Extract capture groups in the regex pat as columns in a DataFrame.", "Series.str.extractall(pat[, flags])", "Extract capture groups in the regex pat as columns in DataFrame.", "Series.str.find(sub[, start, end])", "Return lowest indexes in each strings in the Series/Index.", "Series.str.findall(pat[, flags])", "Find all occurrences of pattern or regular expression in the Series/Index.", "Series.str.fullmatch(pat[, case, flags, na])", "Determine if each string entirely matches a regular expression.", "Series.str.get(i)", "Extract element from each component at specified position.", "Series.str.index(sub[, start, end])", "Return lowest indexes in each string in Series/Index.", "Series.str.join(sep)", "Join lists contained as elements in the Series/Index with passed delimiter.", "Series.str.len()", "Compute the length of each element in the Series/Index.", "Series.str.ljust(width[, fillchar])", "Pad right side of strings in the Series/Index.", "Series.str.lower()", "Convert strings in the Series/Index to lowercase.", "Series.str.lstrip([to_strip])", "Remove leading characters.", "Series.str.match(pat[, case, flags, na])", "Determine if each string starts with a match of a regular expression.", "Series.str.normalize(form)", "Return the Unicode normal form for the strings in the Series/Index.", "Series.str.pad(width[, side, fillchar])", "Pad strings in the Series/Index up to width.", "Series.str.partition([sep, expand])", "Split the string at the first occurrence of sep.", "Series.str.removeprefix(prefix)", "Remove a prefix from an object series.", "Series.str.removesuffix(suffix)", "Remove a suffix from an object series.", "Series.str.repeat(repeats)", "Duplicate each string in the Series or Index.", "Series.str.replace(pat, repl[, n, case, ...])", "Replace each occurrence of pattern/regex in the Series/Index.", "Series.str.rfind(sub[, start, end])", "Return highest indexes in each strings in the Series/Index.", "Series.str.rindex(sub[, start, end])", "Return highest indexes in each string in Series/Index.", "Series.str.rjust(width[, fillchar])", "Pad left side of strings in the Series/Index.", "Series.str.rpartition([sep, expand])", "Split the string at the last occurrence of sep.", "Series.str.rstrip([to_strip])", "Remove trailing characters.", "Series.str.slice([start, stop, step])", "Slice substrings from each element in the Series or Index.", "Series.str.slice_replace([start, stop, repl])", "Replace a positional slice of a string with another value.", "Series.str.split([pat, n, expand, regex])", "Split strings around given separator/delimiter.", "Series.str.rsplit([pat, n, expand])", "Split strings around given separator/delimiter.", "Series.str.startswith(pat[, na])", "Test if the start of each string element matches a pattern.", "Series.str.strip([to_strip])", "Remove leading and trailing characters.", "Series.str.swapcase()", "Convert strings in the Series/Index to be swapcased.", "Series.str.title()", "Convert strings in the Series/Index to titlecase.", "Series.str.translate(table)", "Map all characters in the string through the given mapping table.", "Series.str.upper()", "Convert strings in the Series/Index to uppercase.", "Series.str.wrap(width, **kwargs)", "Wrap strings in Series/Index at specified line width.", "Series.str.zfill(width)", "Pad strings in the Series/Index by prepending '0' characters.", "Series.str.isalnum()", "Check whether all characters in each string are alphanumeric.", "Series.str.isalpha()", "Check whether all characters in each string are alphabetic.", "Series.str.isdigit()", "Check whether all characters in each string are digits.", "Series.str.isspace()", "Check whether all characters in each string are whitespace.", "Series.str.islower()", "Check whether all characters in each string are lowercase.", "Series.str.isupper()", "Check whether all characters in each string are uppercase.", "Series.str.istitle()", "Check whether all characters in each string are titlecase.", "Series.str.isnumeric()", "Check whether all characters in each string are numeric.", "Series.str.isdecimal()", "Check whether all characters in each string are decimal.", "Series.str.get_dummies([sep])", "Return DataFrame of dummy/indicator variables for Series.", "Categorical-dtype specific methods and attributes are available under the Series.cat accessor.", "Series.cat.categories", "The categories of this categorical.", "Series.cat.ordered", "Whether the categories have an ordered relationship.", "Series.cat.codes", "Return Series of codes as well as the index.", "Series.cat.rename_categories(*args, **kwargs)", "Rename categories.", "Series.cat.reorder_categories(*args, **kwargs)", "Reorder categories as specified in new_categories.", "Series.cat.add_categories(*args, **kwargs)", "Add new categories.", "Series.cat.remove_categories(*args, **kwargs)", "Remove the specified categories.", "Series.cat.remove_unused_categories(*args, ...)", "Remove categories which are not used.", "Series.cat.set_categories(*args, **kwargs)", "Set the categories to the specified new_categories.", "Series.cat.as_ordered(*args, **kwargs)", "Set the Categorical to be ordered.", "Series.cat.as_unordered(*args, **kwargs)", "Set the Categorical to be unordered.", "Sparse-dtype specific methods and attributes are provided under the Series.sparse accessor.", "Series.sparse.npoints", "The number of non- fill_value points.", "Series.sparse.density", "The percent of non- fill_value points, as decimal.", "Series.sparse.fill_value", "Elements in data that are fill_value are not stored.", "Series.sparse.sp_values", "An ndarray containing the non- fill_value values.", "Series.sparse.from_coo(A[, dense_index])", "Create a Series with sparse values from a scipy.sparse.coo_matrix.", "Series.sparse.to_coo([row_levels, ...])", "Create a scipy.sparse.coo_matrix from a Series with MultiIndex.", "Flags refer to attributes of the pandas object. Properties of the dataset (like the date is was recorded, the URL it was accessed from, etc.) should be stored in Series.attrs.", "Flags(obj, *, allows_duplicate_labels)", "Flags that apply to pandas objects.", "Series.attrs is a dictionary for storing global metadata for this Series.", "Warning", "Series.attrs is considered experimental and may change without warning.", "Series.attrs", "Dictionary of global attributes of this dataset.", "Series.plot is both a callable method and a namespace attribute for specific plotting methods of the form Series.plot.<kind>.", "Series.plot([kind, ax, figsize, ....])", "Series plotting accessor and method", "Series.plot.area([x, y])", "Draw a stacked area plot.", "Series.plot.bar([x, y])", "Vertical bar plot.", "Series.plot.barh([x, y])", "Make a horizontal bar plot.", "Series.plot.box([by])", "Make a box plot of the DataFrame columns.", "Series.plot.density([bw_method, ind])", "Generate Kernel Density Estimate plot using Gaussian kernels.", "Series.plot.hist([by, bins])", "Draw one histogram of the DataFrame's columns.", "Series.plot.kde([bw_method, ind])", "Generate Kernel Density Estimate plot using Gaussian kernels.", "Series.plot.line([x, y])", "Plot Series or DataFrame as lines.", "Series.plot.pie(**kwargs)", "Generate a pie plot.", "Series.hist([by, ax, grid, xlabelsize, ...])", "Draw histogram of the input series using matplotlib.", "Series.to_pickle(path[, compression, ...])", "Pickle (serialize) object to file.", "Series.to_csv([path_or_buf, sep, na_rep, ...])", "Write object to a comma-separated values (csv) file.", "Series.to_dict([into])", "Convert Series to {label -> value} dict or dict-like object.", "Series.to_excel(excel_writer[, sheet_name, ...])", "Write object to an Excel sheet.", "Series.to_frame([name])", "Convert Series to DataFrame.", "Series.to_xarray()", "Return an xarray object from the pandas object.", "Series.to_hdf(path_or_buf, key[, mode, ...])", "Write the contained data to an HDF5 file using HDFStore.", "Series.to_sql(name, con[, schema, ...])", "Write records stored in a DataFrame to a SQL database.", "Series.to_json([path_or_buf, orient, ...])", "Convert the object to a JSON string.", "Series.to_string([buf, na_rep, ...])", "Render a string representation of the Series.", "Series.to_clipboard([excel, sep])", "Copy object to the system clipboard.", "Series.to_latex([buf, columns, col_space, ...])", "Render object to a LaTeX tabular, longtable, or nested table.", "Series.to_markdown([buf, mode, index, ...])", "Print Series in Markdown-friendly format."]}, {"name": "Sparse data structures", "path": "user_guide/sparse", "type": "Manual", "text": ["pandas provides data structures for efficiently storing sparse data. These are not necessarily sparse in the typical \u201cmostly 0\u201d. Rather, you can view these objects as being \u201ccompressed\u201d where any data matching a specific value (NaN / missing value, though any value can be chosen, including 0) is omitted. The compressed values are not actually stored in the array.", "Notice the dtype, Sparse[float64, nan]. The nan means that elements in the array that are nan aren\u2019t actually stored, only the non-nan elements are. Those non-nan elements have a float64 dtype.", "The sparse objects exist for memory efficiency reasons. Suppose you had a large, mostly NA DataFrame:", "As you can see, the density (% of values that have not been \u201ccompressed\u201d) is extremely low. This sparse object takes up much less memory on disk (pickled) and in the Python interpreter.", "Functionally, their behavior should be nearly identical to their dense counterparts.", "arrays.SparseArray is a ExtensionArray for storing an array of sparse values (see dtypes for more on extension arrays). It is a 1-dimensional ndarray-like object storing only values distinct from the fill_value:", "A sparse array can be converted to a regular (dense) ndarray with numpy.asarray()", "The SparseArray.dtype property stores two pieces of information", "The dtype of the non-sparse values", "The scalar fill value", "A SparseDtype may be constructed by passing only a dtype", "in which case a default fill value will be used (for NumPy dtypes this is often the \u201cmissing\u201d value for that dtype). To override this default an explicit fill value may be passed instead", "Finally, the string alias 'Sparse[dtype]' may be used to specify a sparse dtype in many places", "pandas provides a .sparse accessor, similar to .str for string data, .cat for categorical data, and .dt for datetime-like data. This namespace provides attributes and methods that are specific to sparse data.", "This accessor is available only on data with SparseDtype, and on the Series class itself for creating a Series with sparse data from a scipy COO matrix with.", "New in version 0.25.0.", "A .sparse accessor has been added for DataFrame as well. See Sparse accessor for more.", "You can apply NumPy ufuncs to SparseArray and get a SparseArray as a result.", "The ufunc is also applied to fill_value. This is needed to get the correct dense result.", "Note", "SparseSeries and SparseDataFrame were removed in pandas 1.0.0. This migration guide is present to aid in migrating from previous versions.", "In older versions of pandas, the SparseSeries and SparseDataFrame classes (documented below) were the preferred way to work with sparse data. With the advent of extension arrays, these subclasses are no longer needed. Their purpose is better served by using a regular Series or DataFrame with sparse values instead.", "Note", "There\u2019s no performance or memory penalty to using a Series or DataFrame with sparse values, rather than a SparseSeries or SparseDataFrame.", "This section provides some guidance on migrating your code to the new style. As a reminder, you can use the Python warnings module to control warnings. But we recommend modifying your code, rather than ignoring the warning.", "Construction", "From an array-like, use the regular Series or DataFrame constructors with SparseArray values.", "From a SciPy sparse matrix, use DataFrame.sparse.from_spmatrix(),", "Conversion", "From sparse to dense, use the .sparse accessors", "From dense to sparse, use DataFrame.astype() with a SparseDtype.", "Sparse Properties", "Sparse-specific properties, like density, are available on the .sparse accessor.", "General differences", "In a SparseDataFrame, all columns were sparse. A DataFrame can have a mixture of sparse and dense columns. As a consequence, assigning new columns to a DataFrame with sparse values will not automatically convert the input to be sparse.", "Instead, you\u2019ll need to ensure that the values being assigned are sparse", "The SparseDataFrame.default_kind and SparseDataFrame.default_fill_value attributes have no replacement.", "Use DataFrame.sparse.from_spmatrix() to create a DataFrame with sparse values from a sparse matrix.", "New in version 0.25.0.", "All sparse formats are supported, but matrices that are not in COOrdinate format will be converted, copying data as needed. To convert back to sparse SciPy matrix in COO format, you can use the DataFrame.sparse.to_coo() method:", "Series.sparse.to_coo() is implemented for transforming a Series with sparse values indexed by a MultiIndex to a scipy.sparse.coo_matrix.", "The method requires a MultiIndex with two or more levels.", "In the example below, we transform the Series to a sparse representation of a 2-d array by specifying that the first and second MultiIndex levels define labels for the rows and the third and fourth levels define labels for the columns. We also specify that the column and row labels should be sorted in the final sparse representation.", "Specifying different row and column labels (and not sorting them) yields a different sparse matrix:", "A convenience method Series.sparse.from_coo() is implemented for creating a Series with sparse values from a scipy.sparse.coo_matrix.", "The default behaviour (with dense_index=False) simply returns a Series containing only the non-null entries.", "Specifying dense_index=True will result in an index that is the Cartesian product of the row and columns coordinates of the matrix. Note that this will consume a significant amount of memory (relative to dense_index=False) if the sparse matrix is large (and sparse) enough."]}, {"name": "Style", "path": "reference/style", "type": "Style", "text": ["Styler objects are returned by pandas.DataFrame.style.", "Styler(data[, precision, table_styles, ...])", "Helps style a DataFrame or Series according to the data with HTML and CSS.", "Styler.from_custom_template(searchpath[, ...])", "Factory function for creating a subclass of Styler.", "Styler.env", "Styler.template_html", "Styler.template_html_style", "Styler.template_html_table", "Styler.template_latex", "Styler.loader", "Styler.apply(func[, axis, subset])", "Apply a CSS-styling function column-wise, row-wise, or table-wise.", "Styler.applymap(func[, subset])", "Apply a CSS-styling function elementwise.", "Styler.apply_index(func[, axis, level])", "Apply a CSS-styling function to the index or column headers, level-wise.", "Styler.applymap_index(func[, axis, level])", "Apply a CSS-styling function to the index or column headers, elementwise.", "Styler.format([formatter, subset, na_rep, ...])", "Format the text display value of cells.", "Styler.format_index([formatter, axis, ...])", "Format the text display value of index labels or column headers.", "Styler.hide([subset, axis, level, names])", "Hide the entire index / column headers, or specific rows / columns from display.", "Styler.set_td_classes(classes)", "Set the DataFrame of strings added to the class attribute of <td> HTML elements.", "Styler.set_table_styles([table_styles, ...])", "Set the table styles included within the <style> HTML element.", "Styler.set_table_attributes(attributes)", "Set the table attributes added to the <table> HTML element.", "Styler.set_tooltips(ttips[, props, css_class])", "Set the DataFrame of strings on Styler generating :hover tooltips.", "Styler.set_caption(caption)", "Set the text added to a <caption> HTML element.", "Styler.set_sticky([axis, pixel_size, levels])", "Add CSS to permanently display the index or column headers in a scrolling frame.", "Styler.set_properties([subset])", "Set defined CSS-properties to each <td> HTML element within the given subset.", "Styler.set_uuid(uuid)", "Set the uuid applied to id attributes of HTML elements.", "Styler.clear()", "Reset the Styler, removing any previously applied styles.", "Styler.pipe(func, *args, **kwargs)", "Apply func(self, *args, **kwargs), and return the result.", "Styler.highlight_null([null_color, subset, ...])", "Highlight missing values with a style.", "Styler.highlight_max([subset, color, axis, ...])", "Highlight the maximum with a style.", "Styler.highlight_min([subset, color, axis, ...])", "Highlight the minimum with a style.", "Styler.highlight_between([subset, color, ...])", "Highlight a defined range with a style.", "Styler.highlight_quantile([subset, color, ...])", "Highlight values defined by a quantile with a style.", "Styler.background_gradient([cmap, low, ...])", "Color the background in a gradient style.", "Styler.text_gradient([cmap, low, high, ...])", "Color the text in a gradient style.", "Styler.bar([subset, axis, color, cmap, ...])", "Draw bar chart in the cell backgrounds.", "Styler.to_html([buf, table_uuid, ...])", "Write Styler to a file, buffer or string in HTML-CSS format.", "Styler.to_latex([buf, column_format, ...])", "Write Styler to a file, buffer or string in LaTeX format.", "Styler.to_excel(excel_writer[, sheet_name, ...])", "Write Styler to an Excel sheet.", "Styler.export()", "Export the styles applied to the current Styler.", "Styler.use(styles)", "Set the styles on the current Styler."]}, {"name": "Table Visualization", "path": "user_guide/style", "type": "Manual", "text": ["This section demonstrates visualization of tabular data using the Styler class. For information on visualization with charting please see Chart Visualization. This document is written as a Jupyter Notebook, and can be viewed or downloaded here.", "Styling should be performed after the data in a DataFrame has been processed. The Styler creates an HTML <table> and leverages CSS styling language to manipulate many parameters including colors, fonts, borders, background, etc. See here for more information on styling HTML tables. This allows a lot of flexibility out of the box, and even enables web developers to integrate DataFrames into their exiting user interface designs.", "The DataFrame.style attribute is a property that returns a Styler object. It has a _repr_html_ method defined on it so they are rendered automatically in Jupyter Notebook.", "The above output looks very similar to the standard DataFrame HTML representation. But the HTML here has already attached some CSS classes to each cell, even if we haven\u2019t yet created any styles. We can view these by calling the .to_html() method, which returns the raw HTML as string, which is useful for further processing or adding to a file - read on in More about CSS and HTML. Below we will show how we can use these to format the DataFrame to be more communicative. For example how we can build s:", "Before adding styles it is useful to show that the Styler can distinguish the display value from the actual value, in both datavlaues and index or columns headers. To control the display value, the text is printed in each cell as string, and we can use the .format() and .format_index() methods to manipulate this according to a format spec string or a callable that takes a single value and returns a string. It is possible to define this for the whole table, or index, or for individual columns, or MultiIndex levels.", "Additionally, the format function has a precision argument to specifically help formatting floats, as well as decimal and thousands separators to support other locales, an na_rep argument to display missing data, and an escape argument to help displaying safe-HTML or safe-LaTeX. The default formatter is configured to adopt pandas\u2019 styler.format.precision option, controllable using with pd.option_context('format.precision', 2):", "Using Styler to manipulate the display is a useful feature because maintaining the indexing and datavalues for other purposes gives greater control. You do not have to overwrite your DataFrame to display it how you like. Here is an example of using the formatting functions whilst still relying on the underlying data for indexing and calculations.", "The index and column headers can be completely hidden, as well subselecting rows or columns that one wishes to exclude. Both these options are performed using the same methods.", "The index can be hidden from rendering by calling .hide() without any arguments, which might be useful if your index is integer based. Similarly column headers can be hidden by calling .hide(axis=\u201ccolumns\u201d) without any further arguments.", "Specific rows or columns can be hidden from rendering by calling the same .hide() method and passing in a row/column label, a list-like or a slice of row/column labels to for the subset argument.", "Hiding does not change the integer arrangement of CSS classes, e.g. hiding the first two columns of a DataFrame means the column class indexing will still start at col2, since col0 and col1 are simply ignored.", "We can update our Styler object from before to hide some data and format the values.", "There are 3 primary methods of adding custom CSS styles to Styler:", "Using .set_table_styles() to control broader areas of the table with specified internal CSS. Although table styles allow the flexibility to add CSS selectors and properties controlling all individual parts of the table, they are unwieldy for individual cell specifications. Also, note that table styles cannot be exported to Excel.", "Using .set_td_classes() to directly link either external CSS classes to your data cells or link the internal CSS classes created by .set_table_styles(). See here. These cannot be used on column header rows or indexes, and also won\u2019t export to Excel.", "Using the .apply() and .applymap() functions to add direct internal CSS to specific data cells. See here. As of v1.4.0 there are also methods that work directly on column header rows or indexes; .apply_index() and .applymap_index(). Note that only these methods add styles that will export to Excel. These methods work in a similar way to DataFrame.apply() and DataFrame.applymap().", "Table styles are flexible enough to control all individual parts of the table, including column headers and indexes. However, they can be unwieldy to type for individual data cells or for any kind of conditional formatting, so we recommend that table styles are used for broad styling, such as entire rows or columns at a time.", "Table styles are also used to control features which can apply to the whole table at once such as creating a generic hover functionality. The :hover pseudo-selector, as well as other pseudo-selectors, can only be used this way.", "To replicate the normal format of CSS selectors and properties (attribute value pairs), e.g.", "the necessary format to pass styles to .set_table_styles() is as a list of dicts, each with a CSS-selector tag and CSS-properties. Properties can either be a list of 2-tuples, or a regular CSS-string, for example:", "Next we just add a couple more styling artifacts targeting specific parts of the table. Be careful here, since we are chaining methods we need to explicitly instruct the method not to overwrite the existing styles.", "As a convenience method (since version 1.2.0) we can also pass a dict to .set_table_styles() which contains row or column keys. Behind the scenes Styler just indexes the keys and adds relevant .col<m> or .row<n> classes as necessary to the given CSS selectors.", "If you have designed a website then it is likely you will already have an external CSS file that controls the styling of table and cell objects within it. You may want to use these native files rather than duplicate all the CSS in python (and duplicate any maintenance work).", "It is very easy to add a class to the main <table> using .set_table_attributes(). This method can also attach inline styles - read more in CSS Hierarchies.", "New in version 1.2.0", "The .set_td_classes() method accepts a DataFrame with matching indices and columns to the underlying Styler\u2019s DataFrame. That DataFrame will contain strings as css-classes to add to individual data cells: the <td> elements of the <table>. Rather than use external CSS we will create our classes internally and add them to table style. We will save adding the borders until the section on tooltips.", "We use the following methods to pass your style functions. Both of those methods take a function (and some other keyword arguments) and apply it to the DataFrame in a certain way, rendering CSS styles.", ".applymap() (elementwise): accepts a function that takes a single value and returns a string with the CSS attribute-value pair.", ".apply() (column-/row-/table-wise): accepts a function that takes a Series or DataFrame and returns a Series, DataFrame, or numpy array with an identical shape where each element is a string with a CSS attribute-value pair. This method passes each column or row of your DataFrame one-at-a-time or the entire table at once, depending on the axis keyword argument. For columnwise use axis=0, rowwise use axis=1, and for the entire table at once use axis=None.", "This method is powerful for applying multiple, complex logic to data cells. We create a new DataFrame to demonstrate this.", "For example we can build a function that colors text if it is negative, and chain this with a function that partially fades cells of negligible value. Since this looks at each element in turn we use applymap.", "We can also build a function that highlights the maximum value across rows, cols, and the DataFrame all at once. In this case we use apply. Below we highlight the maximum in a column.", "We can use the same function across the different axes, highlighting here the DataFrame maximum in purple, and row maximums in pink.", "This last example shows how some styles have been overwritten by others. In general the most recent style applied is active but you can read more in the section on CSS hierarchies. You can also apply these styles to more granular parts of the DataFrame - read more in section on subset slicing.", "It is possible to replicate some of this functionality using just classes but it can be more cumbersome. See item 3) of Optimization", "Debugging Tip: If you\u2019re having trouble writing your style function, try just passing it into DataFrame.apply. Internally, Styler.apply uses DataFrame.apply so the result should be the same, and with DataFrame.apply you will be able to inspect the CSS string output of your intended function in each cell.", "Similar application is acheived for headers by using:", ".applymap_index() (elementwise): accepts a function that takes a single value and returns a string with the CSS attribute-value pair.", ".apply_index() (level-wise): accepts a function that takes a Series and returns a Series, or numpy array with an identical shape where each element is a string with a CSS attribute-value pair. This method passes each level of your Index one-at-a-time. To style the index use axis=0 and to style the column headers use axis=1.", "You can select a level of a MultiIndex but currently no similar subset application is available for these methods.", "Table captions can be added with the .set_caption() method. You can use table styles to control the CSS relevant to the caption.", "Adding tooltips (since version 1.3.0) can be done using the .set_tooltips() method in the same way you can add CSS classes to data cells by providing a string based DataFrame with intersecting indices and columns. You don\u2019t have to specify a css_class name or any css props for the tooltips, since there are standard defaults, but the option is there if you want more visual control.", "The only thing left to do for our table is to add the highlighting borders to draw the audience attention to the tooltips. We will create internal CSS classes as before using table styles. Setting classes always overwrites so we need to make sure we add the previous classes.", "The examples we have shown so far for the Styler.apply and Styler.applymap functions have not demonstrated the use of the subset argument. This is a useful argument which permits a lot of flexibility: it allows you to apply styles to specific rows or columns, without having to code that logic into your style function.", "The value passed to subset behaves similar to slicing a DataFrame;", "A scalar is treated as a column label", "A list (or Series or NumPy array) is treated as multiple column labels", "A tuple is treated as (row_indexer, column_indexer)", "Consider using pd.IndexSlice to construct the tuple for the last one. We will create a MultiIndexed DataFrame to demonstrate the functionality.", "We will use subset to highlight the maximum in the third and fourth columns with red text. We will highlight the subset sliced region in yellow.", "If combined with the IndexSlice as suggested then it can index across both dimensions with greater flexibility.", "This also provides the flexibility to sub select rows when used with the axis=1.", "There is also scope to provide conditional filtering.", "Suppose we want to highlight the maximum across columns 2 and 4 only in the case that the sum of columns 1 and 3 is less than -2.0 (essentially excluding rows (:,'r2')).", "Only label-based slicing is supported right now, not positional, and not callables.", "If your style function uses a subset or axis keyword argument, consider wrapping your function in a functools.partial, partialing out that keyword.", "Generally, for smaller tables and most cases, the rendered HTML does not need to be optimized, and we don\u2019t really recommend it. There are two cases where it is worth considering:", "If you are rendering and styling a very large HTML table, certain browsers have performance issues.", "If you are using Styler to dynamically create part of online user interfaces and want to improve network performance.", "Here we recommend the following steps to implement:", "Ignore the uuid and set cell_ids to False. This will prevent unnecessary HTML.", "This is sub-optimal:", "This is better:", "Use table styles where possible (e.g. for all cells or rows or columns at a time) since the CSS is nearly always more efficient than other formats.", "This is sub-optimal:", "This is better:", "For large DataFrames where the same style is applied to many cells it can be more efficient to declare the styles as classes and then apply those classes to data cells, rather than directly applying styles to cells. It is, however, probably still easier to use the Styler function api when you are not concerned about optimization.", "This is sub-optimal:", "This is better:", "Tooltips require cell_ids to work and they generate extra HTML elements for every data cell.", "You can remove unnecessary HTML, or shorten the default class names by replacing the default css dict. You can read a little more about CSS below.", "Some styling functions are common enough that we\u2019ve \u201cbuilt them in\u201d to the Styler, so you don\u2019t have to write them and apply them yourself. The current list of such functions is:", ".highlight_null: for use with identifying missing data.", ".highlight_min and .highlight_max: for use with identifying extremeties in data.", ".highlight_between and .highlight_quantile: for use with identifying classes within data.", ".background_gradient: a flexible method for highlighting cells based or their, or other, values on a numeric scale.", ".text_gradient: similar method for highlighting text based on their, or other, values on a numeric scale.", ".bar: to display mini-charts within cell backgrounds.", "The individual documentation on each function often gives more examples of their arguments.", "This method accepts ranges as float, or NumPy arrays or Series provided the indexes match.", "Useful for detecting the highest or lowest percentile values", "You can create \u201cheatmaps\u201d with the background_gradient and text_gradient methods. These require matplotlib, and we\u2019ll use Seaborn to get a nice colormap.", ".background_gradient and .text_gradient have a number of keyword arguments to customise the gradients and colors. See the documentation.", "Use Styler.set_properties when the style doesn\u2019t actually depend on the values. This is just a simple wrapper for .applymap where the function returns the same properties for all cells.", "You can include \u201cbar charts\u201d in your DataFrame.", "Additional keyword arguments give more control on centering and positioning, and you can pass a list of [color_negative, color_positive] to highlight lower and higher values or a matplotlib colormap.", "To showcase an example here\u2019s how you can change the above with the new align option, combined with setting vmin and vmax limits, the width of the figure, and underlying css props of cells, leaving space to display the text and the bars. We also use text_gradient to color the text the same as the bars using a matplotlib colormap (although in this case the visualization is probably better without this additional effect).", "The following example aims to give a highlight of the behavior of the new align options:", "Say you have a lovely style built up for a DataFrame, and now you want to apply the same style to a second DataFrame. Export the style with df1.style.export, and import it on the second DataFrame with df1.style.set", "Notice that you\u2019re able to share the styles even though they\u2019re data aware. The styles are re-evaluated on the new DataFrame they\u2019ve been used upon.", "DataFrame only (use Series.to_frame().style)", "The index and columns do not need to be unique, but certain styling functions can only work with unique indexes.", "No large repr, and construction performance isn\u2019t great; although we have some HTML optimizations", "You can only apply styles, you can\u2019t insert new HTML entities, except via subclassing.", "Here are a few interesting examples.", "Styler interacts pretty well with widgets. If you\u2019re viewing this online instead of running the notebook yourself, you\u2019re missing out on interactively adjusting the color palette.", "If you display a large matrix or DataFrame in a notebook, but you want to always see the column and row headers you can use the .set_sticky method which manipulates the table styles CSS.", "It is also possible to stick MultiIndexes and even only specific levels.", "Suppose you have to display HTML within HTML, that can be a bit of pain when the renderer can\u2019t distinguish. You can use the escape formatting option to handle this, and even use it within a formatter that contains HTML itself.", "Some support (since version 0.20.0) is available for exporting styled DataFrames to Excel worksheets using the OpenPyXL or XlsxWriter engines. CSS2.2 properties handled include:", "background-color", "color", "font-family", "font-style", "font-weight", "text-align", "text-decoration", "vertical-align", "white-space: nowrap", "Currently broken: border-style, border-width, border-color and their {top, right, bottom, left variants}", "Only CSS2 named colors and hex colors of the form #rgb or #rrggbb are currently supported.", "The following pseudo CSS properties are also available to set excel specific style properties:", "number-format", "Table level styles, and data cell CSS-classes are not included in the export to Excel: individual cells must have their properties mapped by the Styler.apply and/or Styler.applymap methods.", "A screenshot of the output:", "There is support (since version 1.3.0) to export Styler to LaTeX. The documentation for the .to_latex method gives further detail and numerous examples.", "Cascading Style Sheet (CSS) language, which is designed to influence how a browser renders HTML elements, has its own peculiarities. It never reports errors: it just silently ignores them and doesn\u2019t render your objects how you intend so can sometimes be frustrating. Here is a very brief primer on how Styler creates HTML and interacts with CSS, with advice on common pitfalls to avoid.", "The precise structure of the CSS class attached to each cell is as follows.", "Cells with Index and Column names include index_name and level<k> where k is its level in a MultiIndex", "Index label cells include", "row_heading", "level<k> where k is the level in a MultiIndex", "row<m> where m is the numeric position of the row", "Column label cells include", "col_heading", "level<k> where k is the level in a MultiIndex", "col<n> where n is the numeric position of the column", "Data cells include", "data", "row<m>, where m is the numeric position of the cell.", "col<n>, where n is the numeric position of the cell.", "Blank cells include blank", "Trimmed cells include col_trim or row_trim", "The structure of the id is T_uuid_level<k>_row<m>_col<n> where level<k> is used only on headings, and headings will only have either row<m> or col<n> whichever is needed. By default we\u2019ve also prepended each row/column identifier with a UUID unique to each DataFrame so that the style from one doesn\u2019t collide with the styling from another within the same notebook or page. You can read more about the use of UUIDs in Optimization.", "We can see example of the HTML by calling the .to_html() method.", "The examples have shown that when CSS styles overlap, the one that comes last in the HTML render, takes precedence. So the following yield different results:", "This is only true for CSS rules that are equivalent in hierarchy, or importance. You can read more about CSS specificity here but for our purposes it suffices to summarize the key points:", "A CSS importance score for each HTML element is derived by starting at zero and adding:", "1000 for an inline style attribute", "100 for each ID", "10 for each attribute, class or pseudo-class", "1 for each element name or pseudo-element", "Let\u2019s use this to describe the action of the following configurations", "This text is red because the generated selector #T_a_ td is worth 101 (ID plus element), whereas #T_a_row0_col0 is only worth 100 (ID), so is considered inferior even though in the HTML it comes after the previous.", "In the above case the text is blue because the selector #T_b_ .cls-1 is worth 110 (ID plus class), which takes precendence.", "Now we have created another table style this time the selector T_c_ td.data (ID plus element plus class) gets bumped up to 111.", "If your style fails to be applied, and its really frustrating, try the !important trump card.", "Finally got that green text after all!", "The core of pandas is, and will remain, its \u201chigh-performance, easy-to-use data structures\u201d. With that in mind, we hope that DataFrame.style accomplishes two goals", "Provide an API that is pleasing to use interactively and is \u201cgood enough\u201d for many tasks", "Provide the foundations for dedicated libraries to build on", "If you build a great library on top of this, let us know and we\u2019ll link to it.", "If the default template doesn\u2019t quite suit your needs, you can subclass Styler and extend or override the template. We\u2019ll show an example of extending the default template to insert a custom header before each table.", "We\u2019ll use the following template:", "Now that we\u2019ve created a template, we need to set up a subclass of Styler that knows about it.", "Notice that we include the original loader in our environment\u2019s loader. That\u2019s because we extend the original template, so the Jinja environment needs to be able to find it.", "Now we can use that custom styler. It\u2019s __init__ takes a DataFrame.", "Our custom template accepts a table_title keyword. We can provide the value in the .to_html method.", "For convenience, we provide the Styler.from_custom_template method that does the same as the custom subclass.", "Here\u2019s the template structure for the both the style generation template and the table generation template:", "Style template:", "Table template:", "See the template in the GitHub repo for more details."]}, {"name": "Time deltas", "path": "user_guide/timedeltas", "type": "Manual", "text": ["Timedeltas are differences in times, expressed in difference units, e.g. days, hours, minutes, seconds. They can be both positive and negative.", "Timedelta is a subclass of datetime.timedelta, and behaves in a similar manner, but allows compatibility with np.timedelta64 types as well as a host of custom representation, parsing, and attributes.", "You can construct a Timedelta scalar through various arguments, including ISO 8601 Duration strings.", "DateOffsets (Day, Hour, Minute, Second, Milli, Micro, Nano) can also be used in construction.", "Further, operations among the scalars yield another scalar Timedelta.", "Using the top-level pd.to_timedelta, you can convert a scalar, array, list, or Series from a recognized timedelta format / value into a Timedelta type. It will construct Series if the input is a Series, a scalar if the input is scalar-like, otherwise it will output a TimedeltaIndex.", "You can parse a single string to a Timedelta:", "or a list/array of strings:", "The unit keyword argument specifies the unit of the Timedelta if the input is numeric:", "Warning", "If a string or array of strings is passed as an input then the unit keyword argument will be ignored. If a string without units is passed then the default unit of nanoseconds is assumed.", "pandas represents Timedeltas in nanosecond resolution using 64 bit integers. As such, the 64 bit integer limits determine the Timedelta limits.", "You can operate on Series/DataFrames and construct timedelta64[ns] Series through subtraction operations on datetime64[ns] Series, or Timestamps.", "Operations with scalars from a timedelta64[ns] series:", "Series of timedeltas with NaT values are supported:", "Elements can be set to NaT using np.nan analogously to datetimes:", "Operands can also appear in a reversed order (a singular object operated with a Series):", "min, max and the corresponding idxmin, idxmax operations are supported on frames:", "min, max, idxmin, idxmax operations are supported on Series as well. A scalar result will be a Timedelta.", "You can fillna on timedeltas, passing a timedelta to get a particular value.", "You can also negate, multiply and use abs on Timedeltas:", "Numeric reduction operation for timedelta64[ns] will return Timedelta objects. As usual NaT are skipped during evaluation.", "Timedelta Series, TimedeltaIndex, and Timedelta scalars can be converted to other \u2018frequencies\u2019 by dividing by another timedelta, or by astyping to a specific timedelta type. These operations yield Series and propagate NaT -> nan. Note that division by the NumPy scalar is true division, while astyping is equivalent of floor division.", "Dividing or multiplying a timedelta64[ns] Series by an integer or integer Series yields another timedelta64[ns] dtypes Series.", "Rounded division (floor-division) of a timedelta64[ns] Series by a scalar Timedelta gives a series of integers.", "The mod (%) and divmod operations are defined for Timedelta when operating with another timedelta-like or with a numeric argument.", "You can access various components of the Timedelta or TimedeltaIndex directly using the attributes days,seconds,microseconds,nanoseconds. These are identical to the values returned by datetime.timedelta, in that, for example, the .seconds attribute represents the number of seconds >= 0 and < 1 day. These are signed according to whether the Timedelta is signed.", "These operations can also be directly accessed via the .dt property of the Series as well.", "Note", "Note that the attributes are NOT the displayed values of the Timedelta. Use .components to retrieve the displayed values.", "For a Series:", "You can access the value of the fields for a scalar Timedelta directly.", "You can use the .components property to access a reduced form of the timedelta. This returns a DataFrame indexed similarly to the Series. These are the displayed values of the Timedelta.", "You can convert a Timedelta to an ISO 8601 Duration string with the .isoformat method", "To generate an index with time delta, you can use either the TimedeltaIndex or the timedelta_range() constructor.", "Using TimedeltaIndex you can pass string-like, Timedelta, timedelta, or np.timedelta64 objects. Passing np.nan/pd.NaT/nat will represent missing values.", "The string \u2018infer\u2019 can be passed in order to set the frequency of the index as the inferred frequency upon creation:", "Similar to date_range(), you can construct regular ranges of a TimedeltaIndex using timedelta_range(). The default frequency for timedelta_range is calendar day:", "Various combinations of start, end, and periods can be used with timedelta_range:", "The freq parameter can passed a variety of frequency aliases:", "Specifying start, end, and periods will generate a range of evenly spaced timedeltas from start to end inclusively, with periods number of elements in the resulting TimedeltaIndex:", "Similarly to other of the datetime-like indices, DatetimeIndex and PeriodIndex, you can use TimedeltaIndex as the index of pandas objects.", "Selections work similarly, with coercion on string-likes and slices:", "Furthermore you can use partial string selection and the range will be inferred:", "Finally, the combination of TimedeltaIndex with DatetimeIndex allow certain combination operations that are NaT preserving:", "Similarly to frequency conversion on a Series above, you can convert these indices to yield another Index.", "Scalars type ops work as well. These can potentially return a different type of index.", "Similar to timeseries resampling, we can resample with a TimedeltaIndex."]}, {"name": "Time series / date functionality", "path": "user_guide/timeseries", "type": "Manual", "text": ["pandas contains extensive capabilities and features for working with time series data for all domains. Using the NumPy datetime64 and timedelta64 dtypes, pandas has consolidated a large number of features from other Python libraries like scikits.timeseries as well as created a tremendous amount of new functionality for manipulating time series data.", "For example, pandas supports:", "Parsing time series information from various sources and formats", "Generate sequences of fixed-frequency dates and time spans", "Manipulating and converting date times with timezone information", "Resampling or converting a time series to a particular frequency", "Performing date and time arithmetic with absolute or relative time increments", "pandas provides a relatively compact and self-contained set of tools for performing the above tasks and more.", "pandas captures 4 general time related concepts:", "Date times: A specific date and time with timezone support. Similar to datetime.datetime from the standard library.", "Time deltas: An absolute time duration. Similar to datetime.timedelta from the standard library.", "Time spans: A span of time defined by a point in time and its associated frequency.", "Date offsets: A relative time duration that respects calendar arithmetic. Similar to dateutil.relativedelta.relativedelta from the dateutil package.", "Concept", "Scalar Class", "Array Class", "pandas Data Type", "Primary Creation Method", "Date times", "Timestamp", "DatetimeIndex", "datetime64[ns] or datetime64[ns, tz]", "to_datetime or date_range", "Time deltas", "Timedelta", "TimedeltaIndex", "timedelta64[ns]", "to_timedelta or timedelta_range", "Time spans", "Period", "PeriodIndex", "period[freq]", "Period or period_range", "Date offsets", "DateOffset", "None", "None", "DateOffset", "For time series data, it\u2019s conventional to represent the time component in the index of a Series or DataFrame so manipulations can be performed with respect to the time element.", "However, Series and DataFrame can directly also support the time component as data itself.", "Series and DataFrame have extended data type support and functionality for datetime, timedelta and Period data when passed into those constructors. DateOffset data however will be stored as object data.", "Lastly, pandas represents null date times, time deltas, and time spans as NaT which is useful for representing missing or null date like values and behaves similar as np.nan does for float data.", "Timestamped data is the most basic type of time series data that associates values with points in time. For pandas objects it means using the points in time.", "However, in many cases it is more natural to associate things like change variables with a time span instead. The span represented by Period can be specified explicitly, or inferred from datetime string format.", "For example:", "Timestamp and Period can serve as an index. Lists of Timestamp and Period are automatically coerced to DatetimeIndex and PeriodIndex respectively.", "pandas allows you to capture both representations and convert between them. Under the hood, pandas represents timestamps using instances of Timestamp and sequences of timestamps using instances of DatetimeIndex. For regular time spans, pandas uses Period objects for scalar values and PeriodIndex for sequences of spans. Better support for irregular intervals with arbitrary start and end points are forth-coming in future releases.", "To convert a Series or list-like object of date-like objects e.g. strings, epochs, or a mixture, you can use the to_datetime function. When passed a Series, this returns a Series (with the same index), while a list-like is converted to a DatetimeIndex:", "If you use dates which start with the day first (i.e. European style), you can pass the dayfirst flag:", "Warning", "You see in the above example that dayfirst isn\u2019t strict. If a date can\u2019t be parsed with the day being first it will be parsed as if dayfirst were False, and in the case of parsing delimited date strings (e.g. 31-12-2012) then a warning will also be raised.", "If you pass a single string to to_datetime, it returns a single Timestamp. Timestamp can also accept string input, but it doesn\u2019t accept string parsing options like dayfirst or format, so use to_datetime if these are required.", "You can also use the DatetimeIndex constructor directly:", "The string \u2018infer\u2019 can be passed in order to set the frequency of the index as the inferred frequency upon creation:", "In addition to the required datetime string, a format argument can be passed to ensure specific parsing. This could also potentially speed up the conversion considerably.", "For more information on the choices available when specifying the format option, see the Python datetime documentation.", "You can also pass a DataFrame of integer or string columns to assemble into a Series of Timestamps.", "You can pass only the columns that you need to assemble.", "pd.to_datetime looks for standard designations of the datetime component in the column names, including:", "required: year, month, day", "optional: hour, minute, second, millisecond, microsecond, nanosecond", "The default behavior, errors='raise', is to raise when unparsable:", "Pass errors='ignore' to return the original input when unparsable:", "Pass errors='coerce' to convert unparsable data to NaT (not a time):", "pandas supports converting integer or float epoch times to Timestamp and DatetimeIndex. The default unit is nanoseconds, since that is how Timestamp objects are stored internally. However, epochs are often stored in another unit which can be specified. These are computed from the starting point specified by the origin parameter.", "Note", "The unit parameter does not use the same strings as the format parameter that was discussed above). The available units are listed on the documentation for pandas.to_datetime().", "Changed in version 1.0.0.", "Constructing a Timestamp or DatetimeIndex with an epoch timestamp with the tz argument specified will raise a ValueError. If you have epochs in wall time in another timezone, you can read the epochs as timezone-naive timestamps and then localize to the appropriate timezone:", "Note", "Epoch times will be rounded to the nearest nanosecond.", "Warning", "Conversion of float epoch times can lead to inaccurate and unexpected results. Python floats have about 15 digits precision in decimal. Rounding during conversion from float to high precision Timestamp is unavoidable. The only way to achieve exact precision is to use a fixed-width types (e.g. an int64).", "See also", "Using the origin Parameter", "To invert the operation from above, namely, to convert from a Timestamp to a \u2018unix\u2019 epoch:", "We subtract the epoch (midnight at January 1, 1970 UTC) and then floor divide by the \u201cunit\u201d (1 second).", "Using the origin parameter, one can specify an alternative starting point for creation of a DatetimeIndex. For example, to use 1960-01-01 as the starting date:", "The default is set at origin='unix', which defaults to 1970-01-01 00:00:00. Commonly called \u2018unix epoch\u2019 or POSIX time.", "To generate an index with timestamps, you can use either the DatetimeIndex or Index constructor and pass in a list of datetime objects:", "In practice this becomes very cumbersome because we often need a very long index with a large number of timestamps. If we need timestamps on a regular frequency, we can use the date_range() and bdate_range() functions to create a DatetimeIndex. The default frequency for date_range is a calendar day while the default for bdate_range is a business day:", "Convenience functions like date_range and bdate_range can utilize a variety of frequency aliases:", "date_range and bdate_range make it easy to generate a range of dates using various combinations of parameters like start, end, periods, and freq. The start and end dates are strictly inclusive, so dates outside of those specified will not be generated:", "Specifying start, end, and periods will generate a range of evenly spaced dates from start to end inclusively, with periods number of elements in the resulting DatetimeIndex:", "bdate_range can also generate a range of custom frequency dates by using the weekmask and holidays parameters. These parameters will only be used if a custom frequency string is passed.", "See also", "Custom business days", "Since pandas represents timestamps in nanosecond resolution, the time span that can be represented using a 64-bit integer is limited to approximately 584 years:", "See also", "Representing out-of-bounds spans", "One of the main uses for DatetimeIndex is as an index for pandas objects. The DatetimeIndex class contains many time series related optimizations:", "A large range of dates for various offsets are pre-computed and cached under the hood in order to make generating subsequent date ranges very fast (just have to grab a slice).", "Fast shifting using the shift method on pandas objects.", "Unioning of overlapping DatetimeIndex objects with the same frequency is very fast (important for fast data alignment).", "Quick access to date fields via properties such as year, month, etc.", "Regularization functions like snap and very fast asof logic.", "DatetimeIndex objects have all the basic functionality of regular Index objects, and a smorgasbord of advanced time series specific methods for easy frequency processing.", "See also", "Reindexing methods", "Note", "While pandas does not force you to have a sorted date index, some of these methods may have unexpected or incorrect behavior if the dates are unsorted.", "DatetimeIndex can be used like a regular index and offers all of its intelligent functionality like selection, slicing, etc.", "Dates and strings that parse to timestamps can be passed as indexing parameters:", "To provide convenience for accessing longer time series, you can also pass in the year or year and month as strings:", "This type of slicing will work on a DataFrame with a DatetimeIndex as well. Since the partial string selection is a form of label slicing, the endpoints will be included. This would include matching times on an included date:", "Warning", "Indexing DataFrame rows with a single string with getitem (e.g. frame[dtstring]) is deprecated starting with pandas 1.2.0 (given the ambiguity whether it is indexing the rows or selecting a column) and will be removed in a future version. The equivalent with .loc (e.g. frame.loc[dtstring]) is still supported.", "This starts on the very first time in the month, and includes the last date and time for the month:", "This specifies a stop time that includes all of the times on the last day:", "This specifies an exact stop time (and is not the same as the above):", "We are stopping on the included end-point as it is part of the index:", "DatetimeIndex partial string indexing also works on a DataFrame with a MultiIndex:", "New in version 0.25.0.", "Slicing with string indexing also honors UTC offset.", "The same string used as an indexing parameter can be treated either as a slice or as an exact match depending on the resolution of the index. If the string is less accurate than the index, it will be treated as a slice, otherwise as an exact match.", "Consider a Series object with a minute resolution index:", "A timestamp string less accurate than a minute gives a Series object.", "A timestamp string with minute resolution (or more accurate), gives a scalar instead, i.e. it is not casted to a slice.", "If index resolution is second, then the minute-accurate timestamp gives a Series.", "If the timestamp string is treated as a slice, it can be used to index DataFrame with .loc[] as well.", "Warning", "However, if the string is treated as an exact match, the selection in DataFrame\u2019s [] will be column-wise and not row-wise, see Indexing Basics. For example dft_minute['2011-12-31 23:59'] will raise KeyError as '2012-12-31 23:59' has the same resolution as the index and there is no column with such name:", "To always have unambiguous selection, whether the row is treated as a slice or a single selection, use .loc.", "Note also that DatetimeIndex resolution cannot be less precise than day.", "As discussed in previous section, indexing a DatetimeIndex with a partial string depends on the \u201caccuracy\u201d of the period, in other words how specific the interval is in relation to the resolution of the index. In contrast, indexing with Timestamp or datetime objects is exact, because the objects have exact meaning. These also follow the semantics of including both endpoints.", "These Timestamp and datetime objects have exact hours, minutes, and seconds, even though they were not explicitly specified (they are 0).", "With no defaults.", "A truncate() convenience function is provided that is similar to slicing. Note that truncate assumes a 0 value for any unspecified date component in a DatetimeIndex in contrast to slicing which returns any partially matching dates:", "Even complicated fancy indexing that breaks the DatetimeIndex frequency regularity will result in a DatetimeIndex, although frequency is lost:", "There are several time/date properties that one can access from Timestamp or a collection of timestamps like a DatetimeIndex.", "Property", "Description", "year", "The year of the datetime", "month", "The month of the datetime", "day", "The days of the datetime", "hour", "The hour of the datetime", "minute", "The minutes of the datetime", "second", "The seconds of the datetime", "microsecond", "The microseconds of the datetime", "nanosecond", "The nanoseconds of the datetime", "date", "Returns datetime.date (does not contain timezone information)", "time", "Returns datetime.time (does not contain timezone information)", "timetz", "Returns datetime.time as local time with timezone information", "dayofyear", "The ordinal day of year", "day_of_year", "The ordinal day of year", "weekofyear", "The week ordinal of the year", "week", "The week ordinal of the year", "dayofweek", "The number of the day of the week with Monday=0, Sunday=6", "day_of_week", "The number of the day of the week with Monday=0, Sunday=6", "weekday", "The number of the day of the week with Monday=0, Sunday=6", "quarter", "Quarter of the date: Jan-Mar = 1, Apr-Jun = 2, etc.", "days_in_month", "The number of days in the month of the datetime", "is_month_start", "Logical indicating if first day of month (defined by frequency)", "is_month_end", "Logical indicating if last day of month (defined by frequency)", "is_quarter_start", "Logical indicating if first day of quarter (defined by frequency)", "is_quarter_end", "Logical indicating if last day of quarter (defined by frequency)", "is_year_start", "Logical indicating if first day of year (defined by frequency)", "is_year_end", "Logical indicating if last day of year (defined by frequency)", "is_leap_year", "Logical indicating if the date belongs to a leap year", "Furthermore, if you have a Series with datetimelike values, then you can access these properties via the .dt accessor, as detailed in the section on .dt accessors.", "New in version 1.1.0.", "You may obtain the year, week and day components of the ISO year from the ISO 8601 standard:", "In the preceding examples, frequency strings (e.g. 'D') were used to specify a frequency that defined:", "how the date times in DatetimeIndex were spaced when using date_range()", "the frequency of a Period or PeriodIndex", "These frequency strings map to a DateOffset object and its subclasses. A DateOffset is similar to a Timedelta that represents a duration of time but follows specific calendar duration rules. For example, a Timedelta day will always increment datetimes by 24 hours, while a DateOffset day will increment datetimes to the same time the next day whether a day represents 23, 24 or 25 hours due to daylight savings time. However, all DateOffset subclasses that are an hour or smaller (Hour, Minute, Second, Milli, Micro, Nano) behave like Timedelta and respect absolute time.", "The basic DateOffset acts similar to dateutil.relativedelta (relativedelta documentation) that shifts a date time by the corresponding calendar duration specified. The arithmetic operator (+) can be used to perform the shift.", "Most DateOffsets have associated frequencies strings, or offset aliases, that can be passed into freq keyword arguments. The available date offsets and associated frequency strings can be found below:", "Date Offset", "Frequency String", "Description", "DateOffset", "None", "Generic offset class, defaults to absolute 24 hours", "BDay or BusinessDay", "'B'", "business day (weekday)", "CDay or CustomBusinessDay", "'C'", "custom business day", "Week", "'W'", "one week, optionally anchored on a day of the week", "WeekOfMonth", "'WOM'", "the x-th day of the y-th week of each month", "LastWeekOfMonth", "'LWOM'", "the x-th day of the last week of each month", "MonthEnd", "'M'", "calendar month end", "MonthBegin", "'MS'", "calendar month begin", "BMonthEnd or BusinessMonthEnd", "'BM'", "business month end", "BMonthBegin or BusinessMonthBegin", "'BMS'", "business month begin", "CBMonthEnd or CustomBusinessMonthEnd", "'CBM'", "custom business month end", "CBMonthBegin or CustomBusinessMonthBegin", "'CBMS'", "custom business month begin", "SemiMonthEnd", "'SM'", "15th (or other day_of_month) and calendar month end", "SemiMonthBegin", "'SMS'", "15th (or other day_of_month) and calendar month begin", "QuarterEnd", "'Q'", "calendar quarter end", "QuarterBegin", "'QS'", "calendar quarter begin", "BQuarterEnd", "'BQ", "business quarter end", "BQuarterBegin", "'BQS'", "business quarter begin", "FY5253Quarter", "'REQ'", "retail (aka 52-53 week) quarter", "YearEnd", "'A'", "calendar year end", "YearBegin", "'AS' or 'BYS'", "calendar year begin", "BYearEnd", "'BA'", "business year end", "BYearBegin", "'BAS'", "business year begin", "FY5253", "'RE'", "retail (aka 52-53 week) year", "Easter", "None", "Easter holiday", "BusinessHour", "'BH'", "business hour", "CustomBusinessHour", "'CBH'", "custom business hour", "Day", "'D'", "one absolute day", "Hour", "'H'", "one hour", "Minute", "'T' or 'min'", "one minute", "Second", "'S'", "one second", "Milli", "'L' or 'ms'", "one millisecond", "Micro", "'U' or 'us'", "one microsecond", "Nano", "'N'", "one nanosecond", "DateOffsets additionally have rollforward() and rollback() methods for moving a date forward or backward respectively to a valid offset date relative to the offset. For example, business offsets will roll dates that land on the weekends (Saturday and Sunday) forward to Monday since business offsets operate on the weekdays.", "These operations preserve time (hour, minute, etc) information by default. To reset time to midnight, use normalize() before or after applying the operation (depending on whether you want the time information included in the operation).", "Some of the offsets can be \u201cparameterized\u201d when created to result in different behaviors. For example, the Week offset for generating weekly data accepts a weekday parameter which results in the generated dates always lying on a particular day of the week:", "The normalize option will be effective for addition and subtraction.", "Another example is parameterizing YearEnd with the specific ending month:", "Offsets can be used with either a Series or DatetimeIndex to apply the offset to each element.", "If the offset class maps directly to a Timedelta (Day, Hour, Minute, Second, Micro, Milli, Nano) it can be used exactly like a Timedelta - see the Timedelta section for more examples.", "Note that some offsets (such as BQuarterEnd) do not have a vectorized implementation. They can still be used but may calculate significantly slower and will show a PerformanceWarning", "The CDay or CustomBusinessDay class provides a parametric BusinessDay class which can be used to create customized business day calendars which account for local holidays and local weekend conventions.", "As an interesting example, let\u2019s look at Egypt where a Friday-Saturday weekend is observed.", "Let\u2019s map to the weekday names:", "Holiday calendars can be used to provide the list of holidays. See the holiday calendar section for more information.", "Monthly offsets that respect a certain holiday calendar can be defined in the usual way.", "Note", "The frequency string \u2018C\u2019 is used to indicate that a CustomBusinessDay DateOffset is used, it is important to note that since CustomBusinessDay is a parameterised type, instances of CustomBusinessDay may differ and this is not detectable from the \u2018C\u2019 frequency string. The user therefore needs to ensure that the \u2018C\u2019 frequency string is used consistently within the user\u2019s application.", "The BusinessHour class provides a business hour representation on BusinessDay, allowing to use specific start and end times.", "By default, BusinessHour uses 9:00 - 17:00 as business hours. Adding BusinessHour will increment Timestamp by hourly frequency. If target Timestamp is out of business hours, move to the next business hour then increment it. If the result exceeds the business hours end, the remaining hours are added to the next business day.", "You can also specify start and end time by keywords. The argument must be a str with an hour:minute representation or a datetime.time instance. Specifying seconds, microseconds and nanoseconds as business hour results in ValueError.", "Passing start time later than end represents midnight business hour. In this case, business hour exceeds midnight and overlap to the next day. Valid business hours are distinguished by whether it started from valid BusinessDay.", "Applying BusinessHour.rollforward and rollback to out of business hours results in the next business hour start or previous day\u2019s end. Different from other offsets, BusinessHour.rollforward may output different results from apply by definition.", "This is because one day\u2019s business hour end is equal to next day\u2019s business hour start. For example, under the default business hours (9:00 - 17:00), there is no gap (0 minutes) between 2014-08-01 17:00 and 2014-08-04 09:00.", "BusinessHour regards Saturday and Sunday as holidays. To use arbitrary holidays, you can use CustomBusinessHour offset, as explained in the following subsection.", "The CustomBusinessHour is a mixture of BusinessHour and CustomBusinessDay which allows you to specify arbitrary holidays. CustomBusinessHour works as the same as BusinessHour except that it skips specified custom holidays.", "You can use keyword arguments supported by either BusinessHour and CustomBusinessDay.", "A number of string aliases are given to useful common time series frequencies. We will refer to these aliases as offset aliases.", "Alias", "Description", "B", "business day frequency", "C", "custom business day frequency", "D", "calendar day frequency", "W", "weekly frequency", "M", "month end frequency", "SM", "semi-month end frequency (15th and end of month)", "BM", "business month end frequency", "CBM", "custom business month end frequency", "MS", "month start frequency", "SMS", "semi-month start frequency (1st and 15th)", "BMS", "business month start frequency", "CBMS", "custom business month start frequency", "Q", "quarter end frequency", "BQ", "business quarter end frequency", "QS", "quarter start frequency", "BQS", "business quarter start frequency", "A, Y", "year end frequency", "BA, BY", "business year end frequency", "AS, YS", "year start frequency", "BAS, BYS", "business year start frequency", "BH", "business hour frequency", "H", "hourly frequency", "T, min", "minutely frequency", "S", "secondly frequency", "L, ms", "milliseconds", "U, us", "microseconds", "N", "nanoseconds", "Note", "When using the offset aliases above, it should be noted that functions such as date_range(), bdate_range(), will only return timestamps that are in the interval defined by start_date and end_date. If the start_date does not correspond to the frequency, the returned timestamps will start at the next valid timestamp, same for end_date, the returned timestamps will stop at the previous valid timestamp.", "For example, for the offset MS, if the start_date is not the first of the month, the returned timestamps will start with the first day of the next month. If end_date is not the first day of a month, the last returned timestamp will be the first day of the corresponding month.", "We can see in the above example date_range() and bdate_range() will only return the valid timestamps between the start_date and end_date. If these are not valid timestamps for the given frequency it will roll to the next value for start_date (respectively previous for the end_date)", "As we have seen previously, the alias and the offset instance are fungible in most functions:", "You can combine together day and intraday offsets:", "For some frequencies you can specify an anchoring suffix:", "Alias", "Description", "W-SUN", "weekly frequency (Sundays). Same as \u2018W\u2019", "W-MON", "weekly frequency (Mondays)", "W-TUE", "weekly frequency (Tuesdays)", "W-WED", "weekly frequency (Wednesdays)", "W-THU", "weekly frequency (Thursdays)", "W-FRI", "weekly frequency (Fridays)", "W-SAT", "weekly frequency (Saturdays)", "(B)Q(S)-DEC", "quarterly frequency, year ends in December. Same as \u2018Q\u2019", "(B)Q(S)-JAN", "quarterly frequency, year ends in January", "(B)Q(S)-FEB", "quarterly frequency, year ends in February", "(B)Q(S)-MAR", "quarterly frequency, year ends in March", "(B)Q(S)-APR", "quarterly frequency, year ends in April", "(B)Q(S)-MAY", "quarterly frequency, year ends in May", "(B)Q(S)-JUN", "quarterly frequency, year ends in June", "(B)Q(S)-JUL", "quarterly frequency, year ends in July", "(B)Q(S)-AUG", "quarterly frequency, year ends in August", "(B)Q(S)-SEP", "quarterly frequency, year ends in September", "(B)Q(S)-OCT", "quarterly frequency, year ends in October", "(B)Q(S)-NOV", "quarterly frequency, year ends in November", "(B)A(S)-DEC", "annual frequency, anchored end of December. Same as \u2018A\u2019", "(B)A(S)-JAN", "annual frequency, anchored end of January", "(B)A(S)-FEB", "annual frequency, anchored end of February", "(B)A(S)-MAR", "annual frequency, anchored end of March", "(B)A(S)-APR", "annual frequency, anchored end of April", "(B)A(S)-MAY", "annual frequency, anchored end of May", "(B)A(S)-JUN", "annual frequency, anchored end of June", "(B)A(S)-JUL", "annual frequency, anchored end of July", "(B)A(S)-AUG", "annual frequency, anchored end of August", "(B)A(S)-SEP", "annual frequency, anchored end of September", "(B)A(S)-OCT", "annual frequency, anchored end of October", "(B)A(S)-NOV", "annual frequency, anchored end of November", "These can be used as arguments to date_range, bdate_range, constructors for DatetimeIndex, as well as various other timeseries-related functions in pandas.", "For those offsets that are anchored to the start or end of specific frequency (MonthEnd, MonthBegin, WeekEnd, etc), the following rules apply to rolling forward and backwards.", "When n is not 0, if the given date is not on an anchor point, it snapped to the next(previous) anchor point, and moved |n|-1 additional steps forwards or backwards.", "If the given date is on an anchor point, it is moved |n| points forwards or backwards.", "For the case when n=0, the date is not moved if on an anchor point, otherwise it is rolled forward to the next anchor point.", "Holidays and calendars provide a simple way to define holiday rules to be used with CustomBusinessDay or in other analysis that requires a predefined set of holidays. The AbstractHolidayCalendar class provides all the necessary methods to return a list of holidays and only rules need to be defined in a specific holiday calendar class. Furthermore, the start_date and end_date class attributes determine over what date range holidays are generated. These should be overwritten on the AbstractHolidayCalendar class to have the range apply to all calendar subclasses. USFederalHolidayCalendar is the only calendar that exists and primarily serves as an example for developing other calendars.", "For holidays that occur on fixed dates (e.g., US Memorial Day or July 4th) an observance rule determines when that holiday is observed if it falls on a weekend or some other non-observed day. Defined observance rules are:", "Rule", "Description", "nearest_workday", "move Saturday to Friday and Sunday to Monday", "sunday_to_monday", "move Sunday to following Monday", "next_monday_or_tuesday", "move Saturday to Monday and Sunday/Monday to Tuesday", "previous_friday", "move Saturday and Sunday to previous Friday\u201d", "next_monday", "move Saturday and Sunday to following Monday", "An example of how holidays and holiday calendars are defined:", "weekday=MO(2) is same as 2 * Week(weekday=2)", "Using this calendar, creating an index or doing offset arithmetic skips weekends and holidays (i.e., Memorial Day/July 4th). For example, the below defines a custom business day offset using the ExampleCalendar. Like any other offset, it can be used to create a DatetimeIndex or added to datetime or Timestamp objects.", "Ranges are defined by the start_date and end_date class attributes of AbstractHolidayCalendar. The defaults are shown below.", "These dates can be overwritten by setting the attributes as datetime/Timestamp/string.", "Every calendar class is accessible by name using the get_calendar function which returns a holiday class instance. Any imported calendar class will automatically be available by this function. Also, HolidayCalendarFactory provides an easy interface to create calendars that are combinations of calendars or calendars with additional rules.", "One may want to shift or lag the values in a time series back and forward in time. The method for this is shift(), which is available on all of the pandas objects.", "The shift method accepts an freq argument which can accept a DateOffset class or other timedelta-like object or also an offset alias.", "When freq is specified, shift method changes all the dates in the index rather than changing the alignment of the data and the index:", "Note that with when freq is specified, the leading entry is no longer NaN because the data is not being realigned.", "The primary function for changing frequencies is the asfreq() method. For a DatetimeIndex, this is basically just a thin, but convenient wrapper around reindex() which generates a date_range and calls reindex.", "asfreq provides a further convenience so you can specify an interpolation method for any gaps that may appear after the frequency conversion.", "Related to asfreq and reindex is fillna(), which is documented in the missing data section.", "DatetimeIndex can be converted to an array of Python native datetime.datetime objects using the to_pydatetime method.", "pandas has a simple, powerful, and efficient functionality for performing resampling operations during frequency conversion (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications.", "resample() is a time-based groupby, followed by a reduction method on each of its groups. See some cookbook examples for some advanced strategies.", "The resample() method can be used directly from DataFrameGroupBy objects, see the groupby docs.", "The resample function is very flexible and allows you to specify many different parameters to control the frequency conversion and resampling operation.", "Any function available via dispatching is available as a method of the returned object, including sum, mean, std, sem, max, min, median, first, last, ohlc:", "For downsampling, closed can be set to \u2018left\u2019 or \u2018right\u2019 to specify which end of the interval is closed:", "Parameters like label are used to manipulate the resulting labels. label specifies whether the result is labeled with the beginning or the end of the interval.", "Warning", "The default values for label and closed is \u2018left\u2019 for all frequency offsets except for \u2018M\u2019, \u2018A\u2019, \u2018Q\u2019, \u2018BM\u2019, \u2018BA\u2019, \u2018BQ\u2019, and \u2018W\u2019 which all have a default of \u2018right\u2019.", "This might unintendedly lead to looking ahead, where the value for a later time is pulled back to a previous time as in the following example with the BusinessDay frequency:", "Notice how the value for Sunday got pulled back to the previous Friday. To get the behavior where the value for Sunday is pushed to Monday, use instead", "The axis parameter can be set to 0 or 1 and allows you to resample the specified axis for a DataFrame.", "kind can be set to \u2018timestamp\u2019 or \u2018period\u2019 to convert the resulting index to/from timestamp and time span representations. By default resample retains the input representation.", "convention can be set to \u2018start\u2019 or \u2018end\u2019 when resampling period data (detail below). It specifies how low frequency periods are converted to higher frequency periods.", "For upsampling, you can specify a way to upsample and the limit parameter to interpolate over the gaps that are created:", "Sparse timeseries are the ones where you have a lot fewer points relative to the amount of time you are looking to resample. Naively upsampling a sparse series can potentially generate lots of intermediate values. When you don\u2019t want to use a method to fill these values, e.g. fill_method is None, then intermediate values will be filled with NaN.", "Since resample is a time-based groupby, the following is a method to efficiently resample only the groups that are not all NaN.", "If we want to resample to the full range of the series:", "We can instead only resample those groups where we have points as follows:", "Similar to the aggregating API, groupby API, and the window API, a Resampler can be selectively resampled.", "Resampling a DataFrame, the default will be to act on all columns with the same function.", "We can select a specific column or columns using standard getitem.", "You can pass a list or dict of functions to do aggregation with, outputting a DataFrame:", "On a resampled DataFrame, you can pass a list of functions to apply to each column, which produces an aggregated result with a hierarchical index:", "By passing a dict to aggregate you can apply a different aggregation to the columns of a DataFrame:", "The function names can also be strings. In order for a string to be valid it must be implemented on the resampled object:", "Furthermore, you can also specify multiple aggregation functions for each column separately.", "If a DataFrame does not have a datetimelike index, but instead you want to resample based on datetimelike column in the frame, it can passed to the on keyword.", "Similarly, if you instead want to resample by a datetimelike level of MultiIndex, its name or location can be passed to the level keyword.", "With the Resampler object in hand, iterating through the grouped data is very natural and functions similarly to itertools.groupby():", "See Iterating through groups or Resampler.__iter__ for more.", "New in version 1.1.0.", "The bins of the grouping are adjusted based on the beginning of the day of the time series starting point. This works well with frequencies that are multiples of a day (like 30D) or that divide a day evenly (like 90s or 1min). This can create inconsistencies with some frequencies that do not meet this criteria. To change this behavior you can specify a fixed Timestamp with the argument origin.", "For example:", "Here we can see that, when using origin with its default value ('start_day'), the result after '2000-10-02 00:00:00' are not identical depending on the start of time series:", "Here we can see that, when setting origin to 'epoch', the result after '2000-10-02 00:00:00' are identical depending on the start of time series:", "If needed you can use a custom timestamp for origin:", "If needed you can just adjust the bins with an offset Timedelta that would be added to the default origin. Those two examples are equivalent for this time series:", "Note the use of 'start' for origin on the last example. In that case, origin will be set to the first value of the timeseries.", "New in version 1.3.0.", "Instead of adjusting the beginning of bins, sometimes we need to fix the end of the bins to make a backward resample with a given freq. The backward resample sets closed to 'right' by default since the last value should be considered as the edge point for the last bin.", "We can set origin to 'end'. The value for a specific Timestamp index stands for the resample result from the current Timestamp minus freq to the current Timestamp with a right close.", "Besides, in contrast with the 'start_day' option, end_day is supported. This will set the origin as the ceiling midnight of the largest Timestamp.", "The above result uses 2000-10-02 00:29:00 as the last bin\u2019s right edge since the following computation.", "Regular intervals of time are represented by Period objects in pandas while sequences of Period objects are collected in a PeriodIndex, which can be created with the convenience function period_range.", "A Period represents a span of time (e.g., a day, a month, a quarter, etc). You can specify the span via freq keyword using a frequency alias like below. Because freq represents a span of Period, it cannot be negative like \u201c-3D\u201d.", "Adding and subtracting integers from periods shifts the period by its own frequency. Arithmetic is not allowed between Period with different freq (span).", "If Period freq is daily or higher (D, H, T, S, L, U, N), offsets and timedelta-like can be added if the result can have the same freq. Otherwise, ValueError will be raised.", "If Period has other frequencies, only the same offsets can be added. Otherwise, ValueError will be raised.", "Taking the difference of Period instances with the same frequency will return the number of frequency units between them:", "Regular sequences of Period objects can be collected in a PeriodIndex, which can be constructed using the period_range convenience function:", "The PeriodIndex constructor can also be used directly:", "Passing multiplied frequency outputs a sequence of Period which has multiplied span.", "If start or end are Period objects, they will be used as anchor endpoints for a PeriodIndex with frequency matching that of the PeriodIndex constructor.", "Just like DatetimeIndex, a PeriodIndex can also be used to index pandas objects:", "PeriodIndex supports addition and subtraction with the same rule as Period.", "PeriodIndex has its own dtype named period, refer to Period Dtypes.", "PeriodIndex has a custom period dtype. This is a pandas extension dtype similar to the timezone aware dtype (datetime64[ns, tz]).", "The period dtype holds the freq attribute and is represented with period[freq] like period[D] or period[M], using frequency strings.", "The period dtype can be used in .astype(...). It allows one to change the freq of a PeriodIndex like .asfreq() and convert a DatetimeIndex to PeriodIndex like to_period():", "PeriodIndex now supports partial string slicing with non-monotonic indexes.", "New in version 1.1.0.", "You can pass in dates and strings to Series and DataFrame with PeriodIndex, in the same manner as DatetimeIndex. For details, refer to DatetimeIndex Partial String Indexing.", "Passing a string representing a lower frequency than PeriodIndex returns partial sliced data.", "As with DatetimeIndex, the endpoints will be included in the result. The example below slices data starting from 10:00 to 11:59.", "The frequency of Period and PeriodIndex can be converted via the asfreq method. Let\u2019s start with the fiscal year 2011, ending in December:", "We can convert it to a monthly frequency. Using the how parameter, we can specify whether to return the starting or ending month:", "The shorthands \u2018s\u2019 and \u2018e\u2019 are provided for convenience:", "Converting to a \u201csuper-period\u201d (e.g., annual frequency is a super-period of quarterly frequency) automatically returns the super-period that includes the input period:", "Note that since we converted to an annual frequency that ends the year in November, the monthly period of December 2011 is actually in the 2012 A-NOV period.", "Period conversions with anchored frequencies are particularly useful for working with various quarterly data common to economics, business, and other fields. Many organizations define quarters relative to the month in which their fiscal year starts and ends. Thus, first quarter of 2011 could start in 2010 or a few months into 2011. Via anchored frequencies, pandas works for all quarterly frequencies Q-JAN through Q-DEC.", "Q-DEC define regular calendar quarters:", "Q-MAR defines fiscal year end in March:", "Timestamped data can be converted to PeriodIndex-ed data using to_period and vice-versa using to_timestamp:", "Remember that \u2018s\u2019 and \u2018e\u2019 can be used to return the timestamps at the start or end of the period:", "Converting between period and timestamp enables some convenient arithmetic functions to be used. In the following example, we convert a quarterly frequency with year ending in November to 9am of the end of the month following the quarter end:", "If you have data that is outside of the Timestamp bounds, see Timestamp limitations, then you can use a PeriodIndex and/or Series of Periods to do computations.", "To convert from an int64 based YYYYMMDD representation.", "These can easily be converted to a PeriodIndex:", "pandas provides rich support for working with timestamps in different time zones using the pytz and dateutil libraries or datetime.timezone objects from the standard library.", "By default, pandas objects are time zone unaware:", "To localize these dates to a time zone (assign a particular time zone to a naive date), you can use the tz_localize method or the tz keyword argument in date_range(), Timestamp, or DatetimeIndex. You can either pass pytz or dateutil time zone objects or Olson time zone database strings. Olson time zone strings will return pytz time zone objects by default. To return dateutil time zone objects, append dateutil/ before the string.", "In pytz you can find a list of common (and less common) time zones using from pytz import common_timezones, all_timezones.", "dateutil uses the OS time zones so there isn\u2019t a fixed list available. For common zones, the names are the same as pytz.", "New in version 0.25.0.", "Note that the UTC time zone is a special case in dateutil and should be constructed explicitly as an instance of dateutil.tz.tzutc. You can also construct other time zones objects explicitly first.", "To convert a time zone aware pandas object from one time zone to another, you can use the tz_convert method.", "Note", "When using pytz time zones, DatetimeIndex will construct a different time zone object than a Timestamp for the same time zone input. A DatetimeIndex can hold a collection of Timestamp objects that may have different UTC offsets and cannot be succinctly represented by one pytz time zone instance while one Timestamp represents one point in time with a specific UTC offset.", "Warning", "Be wary of conversions between libraries. For some time zones, pytz and dateutil have different definitions of the zone. This is more of a problem for unusual time zones than for \u2018standard\u2019 zones like US/Eastern.", "Warning", "Be aware that a time zone definition across versions of time zone libraries may not be considered equal. This may cause problems when working with stored data that is localized using one version and operated on with a different version. See here for how to handle such a situation.", "Warning", "For pytz time zones, it is incorrect to pass a time zone object directly into the datetime.datetime constructor (e.g., datetime.datetime(2011, 1, 1, tzinfo=pytz.timezone('US/Eastern')). Instead, the datetime needs to be localized using the localize method on the pytz time zone object.", "Warning", "Be aware that for times in the future, correct conversion between time zones (and UTC) cannot be guaranteed by any time zone library because a timezone\u2019s offset from UTC may be changed by the respective government.", "Warning", "If you are using dates beyond 2038-01-18, due to current deficiencies in the underlying libraries caused by the year 2038 problem, daylight saving time (DST) adjustments to timezone aware dates will not be applied. If and when the underlying libraries are fixed, the DST transitions will be applied.", "For example, for two dates that are in British Summer Time (and so would normally be GMT+1), both the following asserts evaluate as true:", "Under the hood, all timestamps are stored in UTC. Values from a time zone aware DatetimeIndex or Timestamp will have their fields (day, hour, minute, etc.) localized to the time zone. However, timestamps with the same UTC value are still considered to be equal even if they are in different time zones:", "Operations between Series in different time zones will yield UTC Series, aligning the data on the UTC timestamps:", "To remove time zone information, use tz_localize(None) or tz_convert(None). tz_localize(None) will remove the time zone yielding the local time representation. tz_convert(None) will remove the time zone after converting to UTC time.", "New in version 1.1.0.", "For ambiguous times, pandas supports explicitly specifying the keyword-only fold argument. Due to daylight saving time, one wall clock time can occur twice when shifting from summer to winter time; fold describes whether the datetime-like corresponds to the first (0) or the second time (1) the wall clock hits the ambiguous time. Fold is supported only for constructing from naive datetime.datetime (see datetime documentation for details) or from Timestamp or for constructing from components (see below). Only dateutil timezones are supported (see dateutil documentation for dateutil methods that deal with ambiguous datetimes) as pytz timezones do not support fold (see pytz documentation for details on how pytz deals with ambiguous datetimes). To localize an ambiguous datetime with pytz, please use Timestamp.tz_localize(). In general, we recommend to rely on Timestamp.tz_localize() when localizing ambiguous datetimes if you need direct control over how they are handled.", "tz_localize may not be able to determine the UTC offset of a timestamp because daylight savings time (DST) in a local time zone causes some times to occur twice within one day (\u201cclocks fall back\u201d). The following options are available:", "'raise': Raises a pytz.AmbiguousTimeError (the default behavior)", "'infer': Attempt to determine the correct offset base on the monotonicity of the timestamps", "'NaT': Replaces ambiguous times with NaT", "bool: True represents a DST time, False represents non-DST time. An array-like of bool values is supported for a sequence of times.", "This will fail as there are ambiguous times ('11/06/2011 01:00')", "Handle these ambiguous times by specifying the following.", "A DST transition may also shift the local time ahead by 1 hour creating nonexistent local times (\u201cclocks spring forward\u201d). The behavior of localizing a timeseries with nonexistent times can be controlled by the nonexistent argument. The following options are available:", "'raise': Raises a pytz.NonExistentTimeError (the default behavior)", "'NaT': Replaces nonexistent times with NaT", "'shift_forward': Shifts nonexistent times forward to the closest real time", "'shift_backward': Shifts nonexistent times backward to the closest real time", "timedelta object: Shifts nonexistent times by the timedelta duration", "Localization of nonexistent times will raise an error by default.", "Transform nonexistent times to NaT or shift the times.", "A Series with time zone naive values is represented with a dtype of datetime64[ns].", "A Series with a time zone aware values is represented with a dtype of datetime64[ns, tz] where tz is the time zone", "Both of these Series time zone information can be manipulated via the .dt accessor, see the dt accessor section.", "For example, to localize and convert a naive stamp to time zone aware.", "Time zone information can also be manipulated using the astype method. This method can convert between different timezone-aware dtypes.", "Note", "Using Series.to_numpy() on a Series, returns a NumPy array of the data. NumPy does not currently support time zones (even though it is printing in the local time zone!), therefore an object array of Timestamps is returned for time zone aware data:", "By converting to an object array of Timestamps, it preserves the time zone information. For example, when converting back to a Series:", "However, if you want an actual NumPy datetime64[ns] array (with the values converted to UTC) instead of an array of objects, you can specify the dtype argument:"]}, {"name": "User Guide", "path": "user_guide/index", "type": "Manual", "text": ["The User Guide covers all of pandas by topic area. Each of the subsections introduces a topic (such as \u201cworking with missing data\u201d), and discusses how pandas approaches the problem, with many examples throughout.", "Users brand-new to pandas should start with 10 minutes to pandas.", "For a high level summary of the pandas fundamentals, see Intro to data structures and Essential basic functionality.", "Further information on any specific method can be obtained in the API reference."]}, {"name": "Window", "path": "reference/window", "type": "Window", "text": ["Rolling objects are returned by .rolling calls: pandas.DataFrame.rolling(), pandas.Series.rolling(), etc. Expanding objects are returned by .expanding calls: pandas.DataFrame.expanding(), pandas.Series.expanding(), etc. ExponentialMovingWindow objects are returned by .ewm calls: pandas.DataFrame.ewm(), pandas.Series.ewm(), etc.", "Rolling.count()", "Calculate the rolling count of non NaN observations.", "Rolling.sum(*args[, engine, engine_kwargs])", "Calculate the rolling sum.", "Rolling.mean(*args[, engine, engine_kwargs])", "Calculate the rolling mean.", "Rolling.median([engine, engine_kwargs])", "Calculate the rolling median.", "Rolling.var([ddof, engine, engine_kwargs])", "Calculate the rolling variance.", "Rolling.std([ddof, engine, engine_kwargs])", "Calculate the rolling standard deviation.", "Rolling.min(*args[, engine, engine_kwargs])", "Calculate the rolling minimum.", "Rolling.max(*args[, engine, engine_kwargs])", "Calculate the rolling maximum.", "Rolling.corr([other, pairwise, ddof])", "Calculate the rolling correlation.", "Rolling.cov([other, pairwise, ddof])", "Calculate the rolling sample covariance.", "Rolling.skew(**kwargs)", "Calculate the rolling unbiased skewness.", "Rolling.kurt(**kwargs)", "Calculate the rolling Fisher's definition of kurtosis without bias.", "Rolling.apply(func[, raw, engine, ...])", "Calculate the rolling custom aggregation function.", "Rolling.aggregate(func, *args, **kwargs)", "Aggregate using one or more operations over the specified axis.", "Rolling.quantile(quantile[, interpolation])", "Calculate the rolling quantile.", "Rolling.sem([ddof])", "Calculate the rolling standard error of mean.", "Rolling.rank([method, ascending, pct])", "Calculate the rolling rank.", "Window.mean(*args, **kwargs)", "Calculate the rolling weighted window mean.", "Window.sum(*args, **kwargs)", "Calculate the rolling weighted window sum.", "Window.var([ddof])", "Calculate the rolling weighted window variance.", "Window.std([ddof])", "Calculate the rolling weighted window standard deviation.", "Expanding.count()", "Calculate the expanding count of non NaN observations.", "Expanding.sum(*args[, engine, engine_kwargs])", "Calculate the expanding sum.", "Expanding.mean(*args[, engine, engine_kwargs])", "Calculate the expanding mean.", "Expanding.median([engine, engine_kwargs])", "Calculate the expanding median.", "Expanding.var([ddof, engine, engine_kwargs])", "Calculate the expanding variance.", "Expanding.std([ddof, engine, engine_kwargs])", "Calculate the expanding standard deviation.", "Expanding.min(*args[, engine, engine_kwargs])", "Calculate the expanding minimum.", "Expanding.max(*args[, engine, engine_kwargs])", "Calculate the expanding maximum.", "Expanding.corr([other, pairwise, ddof])", "Calculate the expanding correlation.", "Expanding.cov([other, pairwise, ddof])", "Calculate the expanding sample covariance.", "Expanding.skew(**kwargs)", "Calculate the expanding unbiased skewness.", "Expanding.kurt(**kwargs)", "Calculate the expanding Fisher's definition of kurtosis without bias.", "Expanding.apply(func[, raw, engine, ...])", "Calculate the expanding custom aggregation function.", "Expanding.aggregate(func, *args, **kwargs)", "Aggregate using one or more operations over the specified axis.", "Expanding.quantile(quantile[, interpolation])", "Calculate the expanding quantile.", "Expanding.sem([ddof])", "Calculate the expanding standard error of mean.", "Expanding.rank([method, ascending, pct])", "Calculate the expanding rank.", "ExponentialMovingWindow.mean(*args[, ...])", "Calculate the ewm (exponential weighted moment) mean.", "ExponentialMovingWindow.sum(*args[, engine, ...])", "Calculate the ewm (exponential weighted moment) sum.", "ExponentialMovingWindow.std([bias])", "Calculate the ewm (exponential weighted moment) standard deviation.", "ExponentialMovingWindow.var([bias])", "Calculate the ewm (exponential weighted moment) variance.", "ExponentialMovingWindow.corr([other, pairwise])", "Calculate the ewm (exponential weighted moment) sample correlation.", "ExponentialMovingWindow.cov([other, ...])", "Calculate the ewm (exponential weighted moment) sample covariance.", "Base class for defining custom window boundaries.", "api.indexers.BaseIndexer([index_array, ...])", "Base class for window bounds calculations.", "api.indexers.FixedForwardWindowIndexer([...])", "Creates window boundaries for fixed-length windows that include the current row.", "api.indexers.VariableOffsetWindowIndexer([...])", "Calculate window boundaries based on a non-fixed offset such as a BusinessDay."]}, {"name": "Windowing Operations", "path": "user_guide/window", "type": "Manual", "text": ["pandas contains a compact set of APIs for performing windowing operations - an operation that performs an aggregation over a sliding partition of values. The API functions similarly to the groupby API in that Series and DataFrame call the windowing method with necessary parameters and then subsequently call the aggregation function.", "The windows are comprised by looking back the length of the window from the current observation. The result above can be derived by taking the sum of the following windowed partitions of data:", "pandas supports 4 types of windowing operations:", "Rolling window: Generic fixed or variable sliding window over the values.", "Weighted window: Weighted, non-rectangular window supplied by the scipy.signal library.", "Expanding window: Accumulating window over the values.", "Exponentially Weighted window: Accumulating and exponentially weighted window over the values.", "Concept", "Method", "Returned Object", "Supports time-based windows", "Supports chained groupby", "Supports table method", "Supports online operations", "Rolling window", "rolling", "Rolling", "Yes", "Yes", "Yes (as of version 1.3)", "No", "Weighted window", "rolling", "Window", "No", "No", "No", "No", "Expanding window", "expanding", "Expanding", "No", "Yes", "Yes (as of version 1.3)", "No", "Exponentially Weighted window", "ewm", "ExponentialMovingWindow", "No", "Yes (as of version 1.2)", "No", "Yes (as of version 1.3)", "As noted above, some operations support specifying a window based on a time offset:", "Additionally, some methods support chaining a groupby operation with a windowing operation which will first group the data by the specified keys and then perform a windowing operation per group.", "Note", "Windowing operations currently only support numeric data (integer and float) and will always return float64 values.", "Warning", "Some windowing aggregation, mean, sum, var and std methods may suffer from numerical imprecision due to the underlying windowing algorithms accumulating sums. When values differ with magnitude \\(1/np.finfo(np.double).eps\\) this results in truncation. It must be noted, that large values may have an impact on windows, which do not include these values. Kahan summation is used to compute the rolling sums to preserve accuracy as much as possible.", "New in version 1.3.0.", "Some windowing operations also support the method='table' option in the constructor which performs the windowing operation over an entire DataFrame instead of a single column or row at a time. This can provide a useful performance benefit for a DataFrame with many columns or rows (with the corresponding axis argument) or the ability to utilize other columns during the windowing operation. The method='table' option can only be used if engine='numba' is specified in the corresponding method call.", "For example, a weighted mean calculation can be calculated with apply() by specifying a separate column of weights.", "New in version 1.3.", "Some windowing operations also support an online method after constructing a windowing object which returns a new object that supports passing in new DataFrame or Series objects to continue the windowing calculation with the new values (i.e. online calculations).", "The methods on this new windowing objects must call the aggregation method first to \u201cprime\u201d the initial state of the online calculation. Then, new DataFrame or Series objects can be passed in the update argument to continue the windowing calculation.", "All windowing operations support a min_periods argument that dictates the minimum amount of non-np.nan values a window must have; otherwise, the resulting value is np.nan. min_periods defaults to 1 for time-based windows and window for fixed windows", "Additionally, all windowing operations supports the aggregate method for returning a result of multiple aggregations applied to a window.", "Generic rolling windows support specifying windows as a fixed number of observations or variable number of observations based on an offset. If a time based offset is provided, the corresponding time based index must be monotonic.", "For all supported aggregation functions, see Rolling window functions.", "By default the labels are set to the right edge of the window, but a center keyword is available so the labels can be set at the center.", "This can also be applied to datetime-like indices.", "New in version 1.3.0.", "The inclusion of the interval endpoints in rolling window calculations can be specified with the closed parameter:", "Value", "Behavior", "'right'", "close right endpoint", "'left'", "close left endpoint", "'both'", "close both endpoints", "'neither'", "open endpoints", "For example, having the right endpoint open is useful in many problems that require that there is no contamination from present information back to past information. This allows the rolling window to compute statistics \u201cup to that point in time\u201d, but not including that point in time.", "New in version 1.0.", "In addition to accepting an integer or offset as a window argument, rolling also accepts a BaseIndexer subclass that allows a user to define a custom method for calculating window bounds. The BaseIndexer subclass will need to define a get_window_bounds method that returns a tuple of two arrays, the first being the starting indices of the windows and second being the ending indices of the windows. Additionally, num_values, min_periods, center, closed and will automatically be passed to get_window_bounds and the defined method must always accept these arguments.", "For example, if we have the following DataFrame", "and we want to use an expanding window where use_expanding is True otherwise a window of size 1, we can create the following BaseIndexer subclass:", "You can view other examples of BaseIndexer subclasses here", "New in version 1.1.", "One subclass of note within those examples is the VariableOffsetWindowIndexer that allows rolling operations over a non-fixed offset like a BusinessDay.", "For some problems knowledge of the future is available for analysis. For example, this occurs when each data point is a full time series read from an experiment, and the task is to extract underlying conditions. In these cases it can be useful to perform forward-looking rolling window computations. FixedForwardWindowIndexer class is available for this purpose. This BaseIndexer subclass implements a closed fixed-width forward-looking rolling window, and we can use it as follows:", "We can also achieve this by using slicing, applying rolling aggregation, and then flipping the result as shown in example below:", "The apply() function takes an extra func argument and performs generic rolling computations. The func argument should be a single function that produces a single value from an ndarray input. raw specifies whether the windows are cast as Series objects (raw=False) or ndarray objects (raw=True).", "New in version 1.0.", "Additionally, apply() can leverage Numba if installed as an optional dependency. The apply aggregation can be executed using Numba by specifying engine='numba' and engine_kwargs arguments (raw must also be set to True). See enhancing performance with Numba for general usage of the arguments and performance considerations.", "Numba will be applied in potentially two routines:", "If func is a standard Python function, the engine will JIT the passed function. func can also be a JITed function in which case the engine will not JIT the function again.", "The engine will JIT the for loop where the apply function is applied to each window.", "The engine_kwargs argument is a dictionary of keyword arguments that will be passed into the numba.jit decorator. These keyword arguments will be applied to both the passed function (if a standard Python function) and the apply for loop over each window.", "New in version 1.3.0.", "mean, median, max, min, and sum also support the engine and engine_kwargs arguments.", "cov() and corr() can compute moving window statistics about two Series or any combination of DataFrame/Series or DataFrame/DataFrame. Here is the behavior in each case:", "two Series: compute the statistic for the pairing.", "DataFrame/Series: compute the statistics for each column of the DataFrame with the passed Series, thus returning a DataFrame.", "DataFrame/DataFrame: by default compute the statistic for matching column names, returning a DataFrame. If the keyword argument pairwise=True is passed then computes the statistic for each pair of columns, returning a DataFrame with a MultiIndex whose values are the dates in question (see the next section).", "For example:", "In financial data analysis and other fields it\u2019s common to compute covariance and correlation matrices for a collection of time series. Often one is also interested in moving-window covariance and correlation matrices. This can be done by passing the pairwise keyword argument, which in the case of DataFrame inputs will yield a MultiIndexed DataFrame whose index are the dates in question. In the case of a single DataFrame argument the pairwise argument can even be omitted:", "Note", "Missing values are ignored and each entry is computed using the pairwise complete observations. Please see the covariance section for caveats associated with this method of calculating covariance and correlation matrices.", "The win_type argument in .rolling generates a weighted windows that are commonly used in filtering and spectral estimation. win_type must be string that corresponds to a scipy.signal window function. Scipy must be installed in order to use these windows, and supplementary arguments that the Scipy window methods take must be specified in the aggregation function.", "For all supported aggregation functions, see Weighted window functions.", "An expanding window yields the value of an aggregation statistic with all the data available up to that point in time. Since these calculations are a special case of rolling statistics, they are implemented in pandas such that the following two calls are equivalent:", "For all supported aggregation functions, see Expanding window functions.", "An exponentially weighted window is similar to an expanding window but with each prior point being exponentially weighted down relative to the current point.", "In general, a weighted moving average is calculated as", "where \\(x_t\\) is the input, \\(y_t\\) is the result and the \\(w_i\\) are the weights.", "For all supported aggregation functions, see Exponentially-weighted window functions.", "The EW functions support two variants of exponential weights. The default, adjust=True, uses the weights \\(w_i = (1 - \\alpha)^i\\) which gives", "When adjust=False is specified, moving averages are calculated as", "which is equivalent to using weights", "Note", "These equations are sometimes written in terms of \\(\\alpha' = 1 - \\alpha\\), e.g.", "The difference between the above two variants arises because we are dealing with series which have finite history. Consider a series of infinite history, with adjust=True:", "Noting that the denominator is a geometric series with initial term equal to 1 and a ratio of \\(1 - \\alpha\\) we have", "which is the same expression as adjust=False above and therefore shows the equivalence of the two variants for infinite series. When adjust=False, we have \\(y_0 = x_0\\) and \\(y_t = \\alpha x_t + (1 - \\alpha) y_{t-1}\\). Therefore, there is an assumption that \\(x_0\\) is not an ordinary value but rather an exponentially weighted moment of the infinite series up to that point.", "One must have \\(0 < \\alpha \\leq 1\\), and while it is possible to pass \\(\\alpha\\) directly, it\u2019s often easier to think about either the span, center of mass (com) or half-life of an EW moment:", "One must specify precisely one of span, center of mass, half-life and alpha to the EW functions:", "Span corresponds to what is commonly called an \u201cN-day EW moving average\u201d.", "Center of mass has a more physical interpretation and can be thought of in terms of span: \\(c = (s - 1) / 2\\).", "Half-life is the period of time for the exponential weight to reduce to one half.", "Alpha specifies the smoothing factor directly.", "New in version 1.1.0.", "You can also specify halflife in terms of a timedelta convertible unit to specify the amount of time it takes for an observation to decay to half its value when also specifying a sequence of times.", "The following formula is used to compute exponentially weighted mean with an input vector of times:", "ExponentialMovingWindow also has an ignore_na argument, which determines how intermediate null values affect the calculation of the weights. When ignore_na=False (the default), weights are calculated based on absolute positions, so that intermediate null values affect the result. When ignore_na=True, weights are calculated by ignoring intermediate null values. For example, assuming adjust=True, if ignore_na=False, the weighted average of 3, NaN, 5 would be calculated as", "Whereas if ignore_na=True, the weighted average would be calculated as", "The var(), std(), and cov() functions have a bias argument, specifying whether the result should contain biased or unbiased statistics. For example, if bias=True, ewmvar(x) is calculated as ewmvar(x) = ewma(x**2) - ewma(x)**2; whereas if bias=False (the default), the biased variance statistics are scaled by debiasing factors", "(For \\(w_i = 1\\), this reduces to the usual \\(N / (N - 1)\\) factor, with \\(N = t + 1\\).) See Weighted Sample Variance on Wikipedia for further details."]}, {"name": "Working with missing data", "path": "user_guide/missing_data", "type": "Manual", "text": ["In this section, we will discuss missing (also referred to as NA) values in pandas.", "Note", "The choice of using NaN internally to denote missing data was largely for simplicity and performance reasons. Starting from pandas 1.0, some optional data types start experimenting with a native NA scalar using a mask-based approach. See here for more.", "See the cookbook for some advanced strategies.", "As data comes in many shapes and forms, pandas aims to be flexible with regard to handling missing data. While NaN is the default missing value marker for reasons of computational speed and convenience, we need to be able to easily detect this value with data of different types: floating point, integer, boolean, and general object. In many cases, however, the Python None will arise and we wish to also consider that \u201cmissing\u201d or \u201cnot available\u201d or \u201cNA\u201d.", "Note", "If you want to consider inf and -inf to be \u201cNA\u201d in computations, you can set pandas.options.mode.use_inf_as_na = True.", "To make detecting missing values easier (and across different array dtypes), pandas provides the isna() and notna() functions, which are also methods on Series and DataFrame objects:", "Warning", "One has to be mindful that in Python (and NumPy), the nan's don\u2019t compare equal, but None's do. Note that pandas/NumPy uses the fact that np.nan != np.nan, and treats None like np.nan.", "So as compared to above, a scalar equality comparison versus a None/np.nan doesn\u2019t provide useful information.", "Because NaN is a float, a column of integers with even one missing values is cast to floating-point dtype (see Support for integer NA for more). pandas provides a nullable integer array, which can be used by explicitly requesting the dtype:", "Alternatively, the string alias dtype='Int64' (note the capital \"I\") can be used.", "See Nullable integer data type for more.", "For datetime64[ns] types, NaT represents missing values. This is a pseudo-native sentinel value that can be represented by NumPy in a singular dtype (datetime64[ns]). pandas objects provide compatibility between NaT and NaN.", "You can insert missing values by simply assigning to containers. The actual missing value used will be chosen based on the dtype.", "For example, numeric containers will always use NaN regardless of the missing value type chosen:", "Likewise, datetime containers will always use NaT.", "For object containers, pandas will use the value given:", "Missing values propagate naturally through arithmetic operations between pandas objects.", "The descriptive statistics and computational methods discussed in the data structure overview (and listed here and here) are all written to account for missing data. For example:", "When summing data, NA (missing) values will be treated as zero.", "If the data are all NA, the result will be 0.", "Cumulative methods like cumsum() and cumprod() ignore NA values by default, but preserve them in the resulting arrays. To override this behaviour and include NA values, use skipna=False.", "Warning", "This behavior is now standard as of v0.22.0 and is consistent with the default in numpy; previously sum/prod of all-NA or empty Series/DataFrames would return NaN. See v0.22.0 whatsnew for more.", "The sum of an empty or all-NA Series or column of a DataFrame is 0.", "The product of an empty or all-NA Series or column of a DataFrame is 1.", "NA groups in GroupBy are automatically excluded. This behavior is consistent with R, for example:", "See the groupby section here for more information.", "pandas objects are equipped with various data manipulation methods for dealing with missing data.", "fillna() can \u201cfill in\u201d NA values with non-NA data in a couple of ways, which we illustrate:", "Replace NA with a scalar value", "Fill gaps forward or backward", "Using the same filling arguments as reindexing, we can propagate non-NA values forward or backward:", "Limit the amount of filling", "If we only want consecutive gaps filled up to a certain number of data points, we can use the limit keyword:", "To remind you, these are the available filling methods:", "Method", "Action", "pad / ffill", "Fill values forward", "bfill / backfill", "Fill values backward", "With time series data, using pad/ffill is extremely common so that the \u201clast known value\u201d is available at every time point.", "ffill() is equivalent to fillna(method='ffill') and bfill() is equivalent to fillna(method='bfill')", "You can also fillna using a dict or Series that is alignable. The labels of the dict or index of the Series must match the columns of the frame you wish to fill. The use case of this is to fill a DataFrame with the mean of that column.", "Same result as above, but is aligning the \u2018fill\u2019 value which is a Series in this case.", "You may wish to simply exclude labels from a data set which refer to missing data. To do this, use dropna():", "An equivalent dropna() is available for Series. DataFrame.dropna has considerably more options than Series.dropna, which can be examined in the API.", "Both Series and DataFrame objects have interpolate() that, by default, performs linear interpolation at missing data points.", "Index aware interpolation is available via the method keyword:", "For a floating-point index, use method='values':", "You can also interpolate with a DataFrame:", "The method argument gives access to fancier interpolation methods. If you have scipy installed, you can pass the name of a 1-d interpolation routine to method. You\u2019ll want to consult the full scipy interpolation documentation and reference guide for details. The appropriate interpolation method will depend on the type of data you are working with.", "If you are dealing with a time series that is growing at an increasing rate, method='quadratic' may be appropriate.", "If you have values approximating a cumulative distribution function, then method='pchip' should work well.", "To fill missing values with goal of smooth plotting, consider method='akima'.", "Warning", "These methods require scipy.", "When interpolating via a polynomial or spline approximation, you must also specify the degree or order of the approximation:", "Compare several methods:", "Another use case is interpolation at new values. Suppose you have 100 observations from some distribution. And let\u2019s suppose that you\u2019re particularly interested in what\u2019s happening around the middle. You can mix pandas\u2019 reindex and interpolate methods to interpolate at the new values.", "Like other pandas fill methods, interpolate() accepts a limit keyword argument. Use this argument to limit the number of consecutive NaN values filled since the last valid observation:", "By default, NaN values are filled in a forward direction. Use limit_direction parameter to fill backward or from both directions.", "By default, NaN values are filled whether they are inside (surrounded by) existing valid values, or outside existing valid values. The limit_area parameter restricts filling to either inside or outside values.", "Often times we want to replace arbitrary values with other values.", "replace() in Series and replace() in DataFrame provides an efficient yet flexible way to perform such replacements.", "For a Series, you can replace a single value or a list of values by another value:", "You can replace a list of values by a list of other values:", "You can also specify a mapping dict:", "For a DataFrame, you can specify individual values by column:", "Instead of replacing with specified values, you can treat all given values as missing and interpolate over them:", "Note", "Python strings prefixed with the r character such as r'hello world' are so-called \u201craw\u201d strings. They have different semantics regarding backslashes than strings without this prefix. Backslashes in raw strings will be interpreted as an escaped backslash, e.g., r'\\' == '\\\\'. You should read about them if this is unclear.", "Replace the \u2018.\u2019 with NaN (str -> str):", "Now do it with a regular expression that removes surrounding whitespace (regex -> regex):", "Replace a few different values (list -> list):", "list of regex -> list of regex:", "Only search in column 'b' (dict -> dict):", "Same as the previous example, but use a regular expression for searching instead (dict of regex -> dict):", "You can pass nested dictionaries of regular expressions that use regex=True:", "Alternatively, you can pass the nested dictionary like so:", "You can also use the group of a regular expression match when replacing (dict of regex -> dict of regex), this works for lists as well.", "You can pass a list of regular expressions, of which those that match will be replaced with a scalar (list of regex -> regex).", "All of the regular expression examples can also be passed with the to_replace argument as the regex argument. In this case the value argument must be passed explicitly by name or regex must be a nested dictionary. The previous example, in this case, would then be:", "This can be convenient if you do not want to pass regex=True every time you want to use a regular expression.", "Note", "Anywhere in the above replace examples that you see a regular expression a compiled regular expression is valid as well.", "replace() is similar to fillna().", "Replacing more than one value is possible by passing a list.", "You can also operate on the DataFrame in place:", "While pandas supports storing arrays of integer and boolean type, these types are not capable of storing missing data. Until we can switch to using a native NA type in NumPy, we\u2019ve established some \u201ccasting rules\u201d. When a reindexing operation introduces missing data, the Series will be cast according to the rules introduced in the table below.", "data type", "Cast to", "integer", "float", "boolean", "object", "float", "no cast", "object", "no cast", "For example:", "Ordinarily NumPy will complain if you try to use an object array (even if it contains boolean values) instead of a boolean array to get or set values from an ndarray (e.g. selecting values based on some criteria). If a boolean vector contains NAs, an exception will be generated:", "However, these can be filled in using fillna() and it will work fine:", "pandas provides a nullable integer dtype, but you must explicitly request it when creating the series or column. Notice that we use a capital \u201cI\u201d in the dtype=\"Int64\".", "See Nullable integer data type for more.", "Warning", "Experimental: the behaviour of pd.NA can still change without warning.", "New in version 1.0.0.", "Starting from pandas 1.0, an experimental pd.NA value (singleton) is available to represent scalar missing values. At this moment, it is used in the nullable integer, boolean and dedicated string data types as the missing value indicator.", "The goal of pd.NA is provide a \u201cmissing\u201d indicator that can be used consistently across data types (instead of np.nan, None or pd.NaT depending on the data type).", "For example, when having missing values in a Series with the nullable integer dtype, it will use pd.NA:", "Currently, pandas does not yet use those data types by default (when creating a DataFrame or Series, or when reading in data), so you need to specify the dtype explicitly. An easy way to convert to those dtypes is explained here.", "In general, missing values propagate in operations involving pd.NA. When one of the operands is unknown, the outcome of the operation is also unknown.", "For example, pd.NA propagates in arithmetic operations, similarly to np.nan:", "There are a few special cases when the result is known, even when one of the operands is NA.", "In equality and comparison operations, pd.NA also propagates. This deviates from the behaviour of np.nan, where comparisons with np.nan always return False.", "To check if a value is equal to pd.NA, the isna() function can be used:", "An exception on this basic propagation rule are reductions (such as the mean or the minimum), where pandas defaults to skipping missing values. See above for more.", "For logical operations, pd.NA follows the rules of the three-valued logic (or Kleene logic, similarly to R, SQL and Julia). This logic means to only propagate missing values when it is logically required.", "For example, for the logical \u201cor\u201d operation (|), if one of the operands is True, we already know the result will be True, regardless of the other value (so regardless the missing value would be True or False). In this case, pd.NA does not propagate:", "On the other hand, if one of the operands is False, the result depends on the value of the other operand. Therefore, in this case pd.NA propagates:", "The behaviour of the logical \u201cand\u201d operation (&) can be derived using similar logic (where now pd.NA will not propagate if one of the operands is already False):", "Since the actual value of an NA is unknown, it is ambiguous to convert NA to a boolean value. The following raises an error:", "This also means that pd.NA cannot be used in a context where it is evaluated to a boolean, such as if condition: ... where condition can potentially be pd.NA. In such cases, isna() can be used to check for pd.NA or condition being pd.NA can be avoided, for example by filling missing values beforehand.", "A similar situation occurs when using Series or DataFrame objects in if statements, see Using if/truth statements with pandas.", "pandas.NA implements NumPy\u2019s __array_ufunc__ protocol. Most ufuncs work with NA, and generally return NA:", "Warning", "Currently, ufuncs involving an ndarray and NA will return an object-dtype filled with NA values.", "The return type here may change to return a different array type in the future.", "See DataFrame interoperability with NumPy functions for more on ufuncs.", "If you have a DataFrame or Series using traditional types that have missing data represented using np.nan, there are convenience methods convert_dtypes() in Series and convert_dtypes() in DataFrame that can convert data to use the newer dtypes for integers, strings and booleans listed here. This is especially helpful after reading in data sets when letting the readers such as read_csv() and read_excel() infer default dtypes.", "In this example, while the dtypes of all columns are changed, we show the results for the first 10 columns."]}, {"name": "Working with text data", "path": "user_guide/text", "type": "Manual", "text": ["New in version 1.0.0.", "There are two ways to store text data in pandas:", "object -dtype NumPy array.", "StringDtype extension type.", "We recommend using StringDtype to store text data.", "Prior to pandas 1.0, object dtype was the only option. This was unfortunate for many reasons:", "You can accidentally store a mixture of strings and non-strings in an object dtype array. It\u2019s better to have a dedicated dtype.", "object dtype breaks dtype-specific operations like DataFrame.select_dtypes(). There isn\u2019t a clear way to select just text while excluding non-text but still object-dtype columns.", "When reading code, the contents of an object dtype array is less clear than 'string'.", "Currently, the performance of object dtype arrays of strings and arrays.StringArray are about the same. We expect future enhancements to significantly increase the performance and lower the memory overhead of StringArray.", "Warning", "StringArray is currently considered experimental. The implementation and parts of the API may change without warning.", "For backwards-compatibility, object dtype remains the default type we infer a list of strings to", "To explicitly request string dtype, specify the dtype", "Or astype after the Series or DataFrame is created", "Changed in version 1.1.0.", "You can also use StringDtype/\"string\" as the dtype on non-string data and it will be converted to string dtype:", "or convert from existing pandas data:", "These are places where the behavior of StringDtype objects differ from object dtype", "For StringDtype, string accessor methods that return numeric output will always return a nullable integer dtype, rather than either int or float dtype, depending on the presence of NA values. Methods returning boolean output will return a nullable boolean dtype.", "Both outputs are Int64 dtype. Compare that with object-dtype", "When NA values are present, the output dtype is float64. Similarly for methods returning boolean values.", "Some string methods, like Series.str.decode() are not available on StringArray because StringArray only holds strings, not bytes.", "In comparison operations, arrays.StringArray and Series backed by a StringArray will return an object with BooleanDtype, rather than a bool dtype object. Missing values in a StringArray will propagate in comparison operations, rather than always comparing unequal like numpy.nan.", "Everything else that follows in the rest of this document applies equally to string and object dtype.", "Series and Index are equipped with a set of string processing methods that make it easy to operate on each element of the array. Perhaps most importantly, these methods exclude missing/NA values automatically. These are accessed via the str attribute and generally have names matching the equivalent (scalar) built-in string methods:", "The string methods on Index are especially useful for cleaning up or transforming DataFrame columns. For instance, you may have columns with leading or trailing whitespace:", "Since df.columns is an Index object, we can use the .str accessor", "These string methods can then be used to clean up the columns as needed. Here we are removing leading and trailing whitespaces, lower casing all names, and replacing any remaining whitespaces with underscores:", "Note", "If you have a Series where lots of elements are repeated (i.e. the number of unique elements in the Series is a lot smaller than the length of the Series), it can be faster to convert the original Series to one of type category and then use .str.<method> or .dt.<property> on that. The performance difference comes from the fact that, for Series of type category, the string operations are done on the .categories and not on each element of the Series.", "Please note that a Series of type category with string .categories has some limitations in comparison to Series of type string (e.g. you can\u2019t add strings to each other: s + \" \" + s won\u2019t work if s is a Series of type category). Also, .str methods which operate on elements of type list are not available on such a Series.", "Warning", "Before v.0.25.0, the .str-accessor did only the most rudimentary type checks. Starting with v.0.25.0, the type of the Series is inferred and the allowed types (i.e. strings) are enforced more rigorously.", "Generally speaking, the .str accessor is intended to work only on strings. With very few exceptions, other uses are not supported, and may be disabled at a later point.", "Methods like split return a Series of lists:", "Elements in the split lists can be accessed using get or [] notation:", "It is easy to expand this to return a DataFrame using expand.", "When original Series has StringDtype, the output columns will all be StringDtype as well.", "It is also possible to limit the number of splits:", "rsplit is similar to split except it works in the reverse direction, i.e., from the end of the string to the beginning of the string:", "replace optionally uses regular expressions:", "Warning", "Some caution must be taken when dealing with regular expressions! The current behavior is to treat single character patterns as literal strings, even when regex is set to True. This behavior is deprecated and will be removed in a future version so that the regex keyword is always respected.", "Changed in version 1.2.0.", "If you want literal replacement of a string (equivalent to str.replace()), you can set the optional regex parameter to False, rather than escaping each character. In this case both pat and repl must be strings:", "The replace method can also take a callable as replacement. It is called on every pat using re.sub(). The callable should expect one positional argument (a regex object) and return a string.", "The replace method also accepts a compiled regular expression object from re.compile() as a pattern. All flags should be included in the compiled regular expression object.", "Including a flags argument when calling replace with a compiled regular expression object will raise a ValueError.", "removeprefix and removesuffix have the same effect as str.removeprefix and str.removesuffix added in Python 3.9 <https://docs.python.org/3/library/stdtypes.html#str.removeprefix>`__:", "New in version 1.4.0.", "There are several ways to concatenate a Series or Index, either with itself or others, all based on cat(), resp. Index.str.cat.", "The content of a Series (or Index) can be concatenated:", "If not specified, the keyword sep for the separator defaults to the empty string, sep='':", "By default, missing values are ignored. Using na_rep, they can be given a representation:", "The first argument to cat() can be a list-like object, provided that it matches the length of the calling Series (or Index).", "Missing values on either side will result in missing values in the result as well, unless na_rep is specified:", "The parameter others can also be two-dimensional. In this case, the number or rows must match the lengths of the calling Series (or Index).", "For concatenation with a Series or DataFrame, it is possible to align the indexes before concatenation by setting the join-keyword.", "Warning", "If the join keyword is not passed, the method cat() will currently fall back to the behavior before version 0.23.0 (i.e. no alignment), but a FutureWarning will be raised if any of the involved indexes differ, since this default will change to join='left' in a future version.", "The usual options are available for join (one of 'left', 'outer', 'inner', 'right'). In particular, alignment also means that the different lengths do not need to coincide anymore.", "The same alignment can be used when others is a DataFrame:", "Several array-like items (specifically: Series, Index, and 1-dimensional variants of np.ndarray) can be combined in a list-like container (including iterators, dict-views, etc.).", "All elements without an index (e.g. np.ndarray) within the passed list-like must match in length to the calling Series (or Index), but Series and Index may have arbitrary length (as long as alignment is not disabled with join=None):", "If using join='right' on a list-like of others that contains different indexes, the union of these indexes will be used as the basis for the final concatenation:", "You can use [] notation to directly index by position locations. If you index past the end of the string, the result will be a NaN.", "Warning", "Before version 0.23, argument expand of the extract method defaulted to False. When expand=False, expand returns a Series, Index, or DataFrame, depending on the subject and regular expression pattern. When expand=True, it always returns a DataFrame, which is more consistent and less confusing from the perspective of a user. expand=True has been the default since version 0.23.0.", "The extract method accepts a regular expression with at least one capture group.", "Extracting a regular expression with more than one group returns a DataFrame with one column per group.", "Elements that do not match return a row filled with NaN. Thus, a Series of messy strings can be \u201cconverted\u201d into a like-indexed Series or DataFrame of cleaned-up or more useful strings, without necessitating get() to access tuples or re.match objects. The dtype of the result is always object, even if no match is found and the result only contains NaN.", "Named groups like", "and optional groups like", "can also be used. Note that any capture group names in the regular expression will be used for column names; otherwise capture group numbers will be used.", "Extracting a regular expression with one group returns a DataFrame with one column if expand=True.", "It returns a Series if expand=False.", "Calling on an Index with a regex with exactly one capture group returns a DataFrame with one column if expand=True.", "It returns an Index if expand=False.", "Calling on an Index with a regex with more than one capture group returns a DataFrame if expand=True.", "It raises ValueError if expand=False.", "The table below summarizes the behavior of extract(expand=False) (input subject in first column, number of groups in regex in first row)", "1 group", ">1 group", "Index", "Index", "ValueError", "Series", "Series", "DataFrame", "Unlike extract (which returns only the first match),", "the extractall method returns every match. The result of extractall is always a DataFrame with a MultiIndex on its rows. The last level of the MultiIndex is named match and indicates the order in the subject.", "When each subject string in the Series has exactly one match,", "then extractall(pat).xs(0, level='match') gives the same result as extract(pat).", "Index also supports .str.extractall. It returns a DataFrame which has the same result as a Series.str.extractall with a default index (starts from 0).", "You can check whether elements contain a pattern:", "Or whether elements match a pattern:", "New in version 1.1.0.", "Note", "The distinction between match, fullmatch, and contains is strictness: fullmatch tests whether the entire string matches the regular expression; match tests whether there is a match of the regular expression that begins at the first character of the string; and contains tests whether there is a match of the regular expression at any position within the string.", "The corresponding functions in the re package for these three match modes are re.fullmatch, re.match, and re.search, respectively.", "Methods like match, fullmatch, contains, startswith, and endswith take an extra na argument so missing values can be considered True or False:", "You can extract dummy variables from string columns. For example if they are separated by a '|':", "String Index also supports get_dummies which returns a MultiIndex.", "See also get_dummies().", "Method", "Description", "cat()", "Concatenate strings", "split()", "Split strings on delimiter", "rsplit()", "Split strings on delimiter working from the end of the string", "get()", "Index into each element (retrieve i-th element)", "join()", "Join strings in each element of the Series with passed separator", "get_dummies()", "Split strings on the delimiter returning DataFrame of dummy variables", "contains()", "Return boolean array if each string contains pattern/regex", "replace()", "Replace occurrences of pattern/regex/string with some other string or the return value of a callable given the occurrence", "removeprefix()", "Remove prefix from string, i.e. only remove if string starts with prefix.", "removesuffix()", "Remove suffix from string, i.e. only remove if string ends with suffix.", "repeat()", "Duplicate values (s.str.repeat(3) equivalent to x * 3)", "pad()", "Add whitespace to left, right, or both sides of strings", "center()", "Equivalent to str.center", "ljust()", "Equivalent to str.ljust", "rjust()", "Equivalent to str.rjust", "zfill()", "Equivalent to str.zfill", "wrap()", "Split long strings into lines with length less than a given width", "slice()", "Slice each string in the Series", "slice_replace()", "Replace slice in each string with passed value", "count()", "Count occurrences of pattern", "startswith()", "Equivalent to str.startswith(pat) for each element", "endswith()", "Equivalent to str.endswith(pat) for each element", "findall()", "Compute list of all occurrences of pattern/regex for each string", "match()", "Call re.match on each element, returning matched groups as list", "extract()", "Call re.search on each element, returning DataFrame with one row for each element and one column for each regex capture group", "extractall()", "Call re.findall on each element, returning DataFrame with one row for each match and one column for each regex capture group", "len()", "Compute string lengths", "strip()", "Equivalent to str.strip", "rstrip()", "Equivalent to str.rstrip", "lstrip()", "Equivalent to str.lstrip", "partition()", "Equivalent to str.partition", "rpartition()", "Equivalent to str.rpartition", "lower()", "Equivalent to str.lower", "casefold()", "Equivalent to str.casefold", "upper()", "Equivalent to str.upper", "find()", "Equivalent to str.find", "rfind()", "Equivalent to str.rfind", "index()", "Equivalent to str.index", "rindex()", "Equivalent to str.rindex", "capitalize()", "Equivalent to str.capitalize", "swapcase()", "Equivalent to str.swapcase", "normalize()", "Return Unicode normal form. Equivalent to unicodedata.normalize", "translate()", "Equivalent to str.translate", "isalnum()", "Equivalent to str.isalnum", "isalpha()", "Equivalent to str.isalpha", "isdigit()", "Equivalent to str.isdigit", "isspace()", "Equivalent to str.isspace", "islower()", "Equivalent to str.islower", "isupper()", "Equivalent to str.isupper", "istitle()", "Equivalent to str.istitle", "isnumeric()", "Equivalent to str.isnumeric", "isdecimal()", "Equivalent to str.isdecimal"]}]