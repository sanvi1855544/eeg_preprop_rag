<h1>sklearn.linear_model.lars_path_gram</h1> <dl class="py function"> <dt id="sklearn.linear_model.lars_path_gram">
<code>sklearn.linear_model.lars_path_gram(Xy, Gram, *, n_samples, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=2.220446049250313e-16, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_least_angle.py#L178"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>lars_path in the sufficient stats mode [1]</p> <p>The optimization objective for the case method=’lasso’ is:</p> <pre data-language="python">(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
</pre> <p>in the case of method=’lars’, the objective function is only known in the form of an implicit equation (see discussion in [1])</p> <p>Read more in the <a class="reference internal" href="../linear_model#least-angle-regression"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xyarray-like of shape (n_samples,) or (n_samples, n_targets)</code> </dt>
<dd>
<p>Xy = np.dot(X.T, y).</p> </dd> <dt>
<code>Gramarray-like of shape (n_features, n_features)</code> </dt>
<dd>
<p>Gram = np.dot(X.T * X).</p> </dd> <dt>
<code>n_samplesint or float</code> </dt>
<dd>
<p>Equivalent size of sample.</p> </dd> <dt>
<code>max_iterint, default=500</code> </dt>
<dd>
<p>Maximum number of iterations to perform, set to infinity for no limit.</p> </dd> <dt>
<code>alpha_minfloat, default=0</code> </dt>
<dd>
<p>Minimum correlation along the path. It corresponds to the regularization parameter alpha parameter in the Lasso.</p> </dd> <dt>
<code>method{‘lar’, ‘lasso’}, default=’lar’</code> </dt>
<dd>
<p>Specifies the returned model. Select <code>'lar'</code> for Least Angle Regression, <code>'lasso'</code> for the Lasso.</p> </dd> <dt>
<code>copy_Xbool, default=True</code> </dt>
<dd>
<p>If <code>False</code>, <code>X</code> is overwritten.</p> </dd> <dt>
<code>epsfloat, default=np.finfo(float).eps</code> </dt>
<dd>
<p>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the <code>tol</code> parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.</p> </dd> <dt>
<code>copy_Grambool, default=True</code> </dt>
<dd>
<p>If <code>False</code>, <code>Gram</code> is overwritten.</p> </dd> <dt>
<code>verboseint, default=0</code> </dt>
<dd>
<p>Controls output verbosity.</p> </dd> <dt>
<code>return_pathbool, default=True</code> </dt>
<dd>
<p>If <code>return_path==True</code> returns the entire path, else returns only the last point of the path.</p> </dd> <dt>
<code>return_n_iterbool, default=False</code> </dt>
<dd>
<p>Whether to return the number of iterations.</p> </dd> <dt>
<code>positivebool, default=False</code> </dt>
<dd>
<p>Restrict coefficients to be &gt;= 0. This option is only allowed with method ‘lasso’. Note that the model coefficients will not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (<code>alphas_[alphas_ &gt; 0.].min()</code> when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent lasso_path function.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>alphasarray-like of shape (n_alphas + 1,)</code> </dt>
<dd>
<p>Maximum of covariances (in absolute value) at each iteration. <code>n_alphas</code> is either <code>max_iter</code>, <code>n_features</code> or the number of nodes in the path with <code>alpha &gt;= alpha_min</code>, whichever is smaller.</p> </dd> <dt>
<code>activearray-like of shape (n_alphas,)</code> </dt>
<dd>
<p>Indices of active variables at the end of the path.</p> </dd> <dt>
<code>coefsarray-like of shape (n_features, n_alphas + 1)</code> </dt>
<dd>
<p>Coefficients along the path</p> </dd> <dt>
<code>n_iterint</code> </dt>
<dd>
<p>Number of iterations run. Returned only if return_n_iter is set to True.</p> </dd> </dl> </dd> </dl> <div class="admonition seealso"> <p class="admonition-title">See also</p> <dl class="simple"> <dt>
 <a class="reference internal" href="sklearn.linear_model.lars_path#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code>lars_path</code></a>
</dt>
 <dt>
 <a class="reference internal" href="sklearn.linear_model.lasso_path#sklearn.linear_model.lasso_path" title="sklearn.linear_model.lasso_path"><code>lasso_path</code></a>
</dt>
 <dt>
<code>lasso_path_gram</code> </dt>
 <dt>
 <a class="reference internal" href="sklearn.linear_model.lassolars#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code>LassoLars</code></a>
</dt>
 <dt>
 <a class="reference internal" href="sklearn.linear_model.lars#sklearn.linear_model.Lars" title="sklearn.linear_model.Lars"><code>Lars</code></a>
</dt>
 <dt>
 <a class="reference internal" href="sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code>LassoLarsCV</code></a>
</dt>
 <dt>
 <a class="reference internal" href="sklearn.linear_model.larscv#sklearn.linear_model.LarsCV" title="sklearn.linear_model.LarsCV"><code>LarsCV</code></a>
</dt>
 <dt>
 <a class="reference internal" href="sklearn.decomposition.sparse_encode#sklearn.decomposition.sparse_encode" title="sklearn.decomposition.sparse_encode"><code>sklearn.decomposition.sparse_encode</code></a>
</dt>
 </dl> </div> <h4 class="rubric">References</h4> <dl class="citation"> <dt class="label" id="r34229eeff553-1">
<code>1</code> </dt> <dd>
<p>“Least Angle Regression”, Efron et al. <a class="reference external" href="http://statweb.stanford.edu/~tibs/ftp/lars.pdf">http://statweb.stanford.edu/~tibs/ftp/lars.pdf</a></p> </dd> <dt class="label" id="r34229eeff553-2">
<code>2</code> </dt> <dd>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Least-angle_regression">Wikipedia entry on the Least-angle regression</a></p> </dd> <dt class="label" id="r34229eeff553-3">
<code>3</code> </dt> <dd>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Lasso_(statistics)">Wikipedia entry on the Lasso</a></p> </dd> </dl> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2020 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/0.24/modules/generated/sklearn.linear_model.lars_path_gram.html" class="_attribution-link">https://scikit-learn.org/0.24/modules/generated/sklearn.linear_model.lars_path_gram.html</a>
  </p>
</div>
