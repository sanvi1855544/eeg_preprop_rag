<h1>sklearn.decomposition.non_negative_factorization</h1> <dl class="py function"> <dt id="sklearn.decomposition.non_negative_factorization">
<code>sklearn.decomposition.non_negative_factorization(X, W=None, H=None, n_components=None, *, init='warn', update_H=True, solver='cd', beta_loss='frobenius', tol=0.0001, max_iter=200, alpha=0.0, l1_ratio=0.0, regularization=None, random_state=None, verbose=0, shuffle=False)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/decomposition/_nmf.py#L853"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Compute Non-negative Matrix Factorization (NMF).</p> <p>Find two non-negative matrices (W, H) whose product approximates the non- negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction.</p> <p>The objective function is:</p>  <div class="math notranslate nohighlight"> \[ \begin{align}\begin{aligned}0.5 * ||X - WH||_{Fro}^2 + alpha * l1_{ratio} * ||vec(W)||_1\\+ alpha * l1_{ratio} * ||vec(H)||_1\\+ 0.5 * alpha * (1 - l1_{ratio}) * ||W||_{Fro}^2\\+ 0.5 * alpha * (1 - l1_{ratio}) * ||H||_{Fro}^2\end{aligned}\end{align} \]</div>  <p>Where:</p> <p><span class="math notranslate nohighlight">\(||A||_{Fro}^2 = \sum_{i,j} A_{ij}^2\)</span> (Frobenius norm)</p> <p><span class="math notranslate nohighlight">\(||vec(A)||_1 = \sum_{i,j} abs(A_{ij})\)</span> (Elementwise L1 norm)</p> <p>For multiplicative-update (‘mu’) solver, the Frobenius norm <span class="math notranslate nohighlight">\((0.5 * ||X - WH||_{Fro}^2)\)</span> can be changed into another beta-divergence loss, by changing the beta_loss parameter.</p> <p>The objective function is minimized with an alternating minimization of W and H. If H is given and update_H=False, it solves for W only.</p> <dl class="field-list"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl> <dt>
<code>Xarray-like of shape (n_samples, n_features)</code> </dt>
<dd>
<p>Constant matrix.</p> </dd> <dt>
<code>Warray-like of shape (n_samples, n_components), default=None</code> </dt>
<dd>
<p>If init=’custom’, it is used as initial guess for the solution.</p> </dd> <dt>
<code>Harray-like of shape (n_components, n_features), default=None</code> </dt>
<dd>
<p>If init=’custom’, it is used as initial guess for the solution. If update_H=False, it is used as a constant, to solve for W only.</p> </dd> <dt>
<code>n_componentsint, default=None</code> </dt>
<dd>
<p>Number of components, if n_components is not set all features are kept.</p> </dd> <dt>
<code>init{‘random’, ‘nndsvd’, ‘nndsvda’, ‘nndsvdar’, ‘custom’}, default=None</code> </dt>
<dd>
<p>Method used to initialize the procedure.</p> <p>Valid options:</p> <ul class="simple"> <li>None: ‘nndsvd’ if n_components &lt; n_features, otherwise ‘random’.</li> <li>
<dl class="simple"> <dt>‘random’: non-negative random matrices, scaled with:</dt>
<dd>
<p>sqrt(X.mean() / n_components)</p> </dd> </dl> </li> <li>
<dl class="simple"> <dt>‘nndsvd’: Nonnegative Double Singular Value Decomposition (NNDSVD)</dt>
<dd>
<p>initialization (better for sparseness)</p> </dd> </dl> </li> <li>
<dl class="simple"> <dt>‘nndsvda’: NNDSVD with zeros filled with the average of X</dt>
<dd>
<p>(better when sparsity is not desired)</p> </dd> </dl> </li> <li>
<dl class="simple"> <dt>‘nndsvdar’: NNDSVD with zeros filled with small random values</dt>
<dd>
<p>(generally faster, less accurate alternative to NNDSVDa for when sparsity is not desired)</p> </dd> </dl> </li> <li>‘custom’: use custom matrices W and H if <code>update_H=True</code>. If <code>update_H=False</code>, then only custom matrix H is used.</li> </ul> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 0.23: </span>The default value of <code>init</code> changed from ‘random’ to None in 0.23.</p> </div> </dd> <dt>
<code>update_Hbool, default=True</code> </dt>
<dd>
<p>Set to True, both W and H will be estimated from initial guesses. Set to False, only W will be estimated.</p> </dd> <dt>
<code>solver{‘cd’, ‘mu’}, default=’cd’</code> </dt>
<dd>
<p>Numerical solver to use:</p> <ul class="simple"> <li>
<dl class="simple"> <dt>‘cd’ is a Coordinate Descent solver that uses Fast Hierarchical</dt>
<dd>
<p>Alternating Least Squares (Fast HALS).</p> </dd> </dl> </li> <li>‘mu’ is a Multiplicative Update solver.</li> </ul> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.17: </span>Coordinate Descent solver.</p> </div> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.19: </span>Multiplicative Update solver.</p> </div> </dd> <dt>
<code>beta_lossfloat or {‘frobenius’, ‘kullback-leibler’, ‘itakura-saito’}, default=’frobenius’</code> </dt>
<dd>
<p>Beta divergence to be minimized, measuring the distance between X and the dot product WH. Note that values different from ‘frobenius’ (or 2) and ‘kullback-leibler’ (or 1) lead to significantly slower fits. Note that for beta_loss &lt;= 0 (or ‘itakura-saito’), the input matrix X cannot contain zeros. Used only in ‘mu’ solver.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.19.</span></p> </div> </dd> <dt>
<code>tolfloat, default=1e-4</code> </dt>
<dd>
<p>Tolerance of the stopping condition.</p> </dd> <dt>
<code>max_iterint, default=200</code> </dt>
<dd>
<p>Maximum number of iterations before timing out.</p> </dd> <dt>
<code>alphafloat, default=0.</code> </dt>
<dd>
<p>Constant that multiplies the regularization terms.</p> </dd> <dt>
<code>l1_ratiofloat, default=0.</code> </dt>
<dd>
<p>The regularization mixing parameter, with 0 &lt;= l1_ratio &lt;= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2.</p> </dd> <dt>
<code>regularization{‘both’, ‘components’, ‘transformation’}, default=None</code> </dt>
<dd>
<p>Select whether the regularization affects the components (H), the transformation (W), both or none of them.</p> </dd> <dt>
<code>random_stateint, RandomState instance or None, default=None</code> </dt>
<dd>
<p>Used for NMF initialisation (when <code>init</code> == ‘nndsvdar’ or ‘random’), and in Coordinate Descent. Pass an int for reproducible results across multiple function calls. See <a class="reference internal" href="https://scikit-learn.org/0.24/glossary.html#term-random_state"><span class="xref std std-term">Glossary</span></a>.</p> </dd> <dt>
<code>verboseint, default=0</code> </dt>
<dd>
<p>The verbosity level.</p> </dd> <dt>
<code>shufflebool, default=False</code> </dt>
<dd>
<p>If true, randomize the order of coordinates in the CD solver.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>Wndarray of shape (n_samples, n_components)</code> </dt>
<dd>
<p>Solution to the non-negative least squares problem.</p> </dd> <dt>
<code>Hndarray of shape (n_components, n_features)</code> </dt>
<dd>
<p>Solution to the non-negative least squares problem.</p> </dd> <dt>
<code>n_iterint</code> </dt>
<dd>
<p>Actual number of iterations.</p> </dd> </dl> </dd> </dl> <h4 class="rubric">References</h4> <p>Cichocki, Andrzej, and P. H. A. N. Anh-Huy. “Fast local algorithms for large scale nonnegative matrix and tensor factorizations.” IEICE transactions on fundamentals of electronics, communications and computer sciences 92.3: 708-721, 2009.</p> <p>Fevotte, C., &amp; Idier, J. (2011). Algorithms for nonnegative matrix factorization with the beta-divergence. Neural Computation, 23(9).</p> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])
&gt;&gt;&gt; from sklearn.decomposition import non_negative_factorization
&gt;&gt;&gt; W, H, n_iter = non_negative_factorization(X, n_components=2,
... init='random', random_state=0)
</pre> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2020 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/0.24/modules/generated/sklearn.decomposition.non_negative_factorization.html" class="_attribution-link">https://scikit-learn.org/0.24/modules/generated/sklearn.decomposition.non_negative_factorization.html</a>
  </p>
</div>
