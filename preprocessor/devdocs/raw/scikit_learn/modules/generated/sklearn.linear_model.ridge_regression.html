<h1>sklearn.linear_model.ridge_regression</h1> <dl class="py function"> <dt id="sklearn.linear_model.ridge_regression">
<code>sklearn.linear_model.ridge_regression(X, y, alpha, *, sample_weight=None, solver='auto', max_iter=None, tol=0.001, verbose=0, random_state=None, return_n_iter=False, return_intercept=False, check_input=True)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_ridge.py#L238"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Solve the ridge equation by the method of normal equations.</p> <p>Read more in the <a class="reference internal" href="../linear_model#ridge-regression"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl> <dt>
<code>X{ndarray, sparse matrix, LinearOperator} of shape (n_samples, n_features)</code> </dt>
<dd>
<p>Training data</p> </dd> <dt>
<code>yndarray of shape (n_samples,) or (n_samples, n_targets)</code> </dt>
<dd>
<p>Target values</p> </dd> <dt>
<code>alphafloat or array-like of shape (n_targets,)</code> </dt>
<dd>
<p>Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to <code>1 / (2C)</code> in other linear models such as <a class="reference internal" href="sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code>LogisticRegression</code></a> or <a class="reference internal" href="sklearn.svm.linearsvc#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code>LinearSVC</code></a>. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number.</p> </dd> <dt>
<code>sample_weightfloat or array-like of shape (n_samples,), default=None</code> </dt>
<dd>
<p>Individual weights for each sample. If given a float, every sample will have the same weight. If sample_weight is not None and solver=’auto’, the solver will be set to ‘cholesky’.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.17.</span></p> </div> </dd> <dt>
<code>solver{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’}, default=’auto’</code> </dt>
<dd>
<p>Solver to use in the computational routines:</p> <ul class="simple"> <li>‘auto’ chooses the solver automatically based on the type of data.</li> <li>‘svd’ uses a Singular Value Decomposition of X to compute the Ridge coefficients. More stable for singular matrices than ‘cholesky’.</li> <li>‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solution via a Cholesky decomposition of dot(X.T, X)</li> <li>‘sparse_cg’ uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than ‘cholesky’ for large-scale data (possibility to set <code>tol</code> and <code>max_iter</code>).</li> <li>‘lsqr’ uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative procedure.</li> <li>‘sag’ uses a Stochastic Average Gradient descent, and ‘saga’ uses its improved, unbiased version named SAGA. Both methods also use an iterative procedure, and are often faster than other solvers when both n_samples and n_features are large. Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.</li> </ul> <p>All last five solvers support both dense and sparse data. However, only ‘sag’ and ‘sparse_cg’ supports sparse input when <code>fit_intercept</code> is True.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p> </div> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.19: </span>SAGA solver.</p> </div> </dd> <dt>
<code>max_iterint, default=None</code> </dt>
<dd>
<p>Maximum number of iterations for conjugate gradient solver. For the ‘sparse_cg’ and ‘lsqr’ solvers, the default value is determined by scipy.sparse.linalg. For ‘sag’ and saga solver, the default value is 1000.</p> </dd> <dt>
<code>tolfloat, default=1e-3</code> </dt>
<dd>
<p>Precision of the solution.</p> </dd> <dt>
<code>verboseint, default=0</code> </dt>
<dd>
<p>Verbosity level. Setting verbose &gt; 0 will display additional information depending on the solver used.</p> </dd> <dt>
<code>random_stateint, RandomState instance, default=None</code> </dt>
<dd>
<p>Used when <code>solver</code> == ‘sag’ or ‘saga’ to shuffle the data. See <a class="reference internal" href="https://scikit-learn.org/0.24/glossary.html#term-random_state"><span class="xref std std-term">Glossary</span></a> for details.</p> </dd> <dt>
<code>return_n_iterbool, default=False</code> </dt>
<dd>
<p>If True, the method also returns <code>n_iter</code>, the actual number of iteration performed by the solver.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.17.</span></p> </div> </dd> <dt>
<code>return_interceptbool, default=False</code> </dt>
<dd>
<p>If True and if X is sparse, the method also returns the intercept, and the solver is automatically changed to ‘sag’. This is only a temporary fix for fitting the intercept with sparse data. For dense data, use sklearn.linear_model._preprocess_data before your regression.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.17.</span></p> </div> </dd> <dt>
<code>check_inputbool, default=True</code> </dt>
<dd>
<p>If False, the input arrays X and y will not be checked.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.21.</span></p> </div> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>coefndarray of shape (n_features,) or (n_targets, n_features)</code> </dt>
<dd>
<p>Weight vector(s).</p> </dd> <dt>
<code>n_iterint, optional</code> </dt>
<dd>
<p>The actual number of iteration performed by the solver. Only returned if <code>return_n_iter</code> is True.</p> </dd> <dt>
<code>interceptfloat or ndarray of shape (n_targets,)</code> </dt>
<dd>
<p>The intercept of the model. Only returned if <code>return_intercept</code> is True and if X is a scipy sparse array.</p> </dd> </dl> </dd> </dl> <h4 class="rubric">Notes</h4> <p>This function won’t compute the intercept.</p> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2020 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/0.24/modules/generated/sklearn.linear_model.ridge_regression.html" class="_attribution-link">https://scikit-learn.org/0.24/modules/generated/sklearn.linear_model.ridge_regression.html</a>
  </p>
</div>
