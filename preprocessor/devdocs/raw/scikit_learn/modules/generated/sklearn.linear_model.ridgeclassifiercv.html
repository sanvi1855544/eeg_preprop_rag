<h1>sklearn.linear_model.RidgeClassifierCV</h1> <dl class="py class"> <dt id="sklearn.linear_model.RidgeClassifierCV">
<code>class sklearn.linear_model.RidgeClassifierCV(alphas=0.1, 1.0, 10.0, *, fit_intercept=True, normalize=False, scoring=None, cv=None, class_weight=None, store_cv_values=False)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_ridge.py#L1797"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Ridge classifier with built-in cross-validation.</p> <p>See glossary entry for <a class="reference internal" href="https://scikit-learn.org/0.24/glossary.html#term-cross-validation-estimator"><span class="xref std std-term">cross-validation estimator</span></a>.</p> <p>By default, it performs Leave-One-Out Cross-Validation. Currently, only the n_features &gt; n_samples case is handled efficiently.</p> <p>Read more in the <a class="reference internal" href="../linear_model#ridge-regression"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl> <dt>
<code>alphasndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)</code> </dt>
<dd>
<p>Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to <code>1 / (2C)</code> in other linear models such as <a class="reference internal" href="sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code>LogisticRegression</code></a> or <a class="reference internal" href="sklearn.svm.linearsvc#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code>LinearSVC</code></a>.</p> </dd> <dt>
<code>fit_interceptbool, default=True</code> </dt>
<dd>
<p>Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).</p> </dd> <dt>
<code>normalizebool, default=False</code> </dt>
<dd>
<p>This parameter is ignored when <code>fit_intercept</code> is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use <a class="reference internal" href="sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code>StandardScaler</code></a> before calling <code>fit</code> on an estimator with <code>normalize=False</code>.</p> </dd> <dt>
<code>scoringstring, callable, default=None</code> </dt>
<dd>
<p>A string (see model evaluation documentation) or a scorer callable object / function with signature <code>scorer(estimator, X, y)</code>.</p> </dd> <dt>
<code>cvint, cross-validation generator or an iterable, default=None</code> </dt>
<dd>
<p>Determines the cross-validation splitting strategy. Possible inputs for cv are:</p> <ul class="simple"> <li>None, to use the efficient Leave-One-Out cross-validation</li> <li>integer, to specify the number of folds.</li> <li>
<a class="reference internal" href="https://scikit-learn.org/0.24/glossary.html#term-CV-splitter"><span class="xref std std-term">CV splitter</span></a>,</li> <li>An iterable yielding (train, test) splits as arrays of indices.</li> </ul> <p>Refer <a class="reference internal" href="../cross_validation#cross-validation"><span class="std std-ref">User Guide</span></a> for the various cross-validation strategies that can be used here.</p> </dd> <dt>
<code>class_weightdict or ‘balanced’, default=None</code> </dt>
<dd>
<p>Weights associated with classes in the form <code>{class_label: weight}</code>. If not given, all classes are supposed to have weight one.</p> <p>The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as <code>n_samples / (n_classes * np.bincount(y))</code></p> </dd> <dt>
<code>store_cv_valuesbool, default=False</code> </dt>
<dd>
<p>Flag indicating if the cross-validation values corresponding to each alpha should be stored in the <code>cv_values_</code> attribute (see below). This flag is only compatible with <code>cv=None</code> (i.e. using Leave-One-Out Cross-Validation).</p> </dd> </dl> </dd> <dt class="field-even">Attributes</dt> <dd class="field-even">
<dl> <dt>
<code>cv_values_ndarray of shape (n_samples, n_targets, n_alphas), optional</code> </dt>
<dd>
<p>Cross-validation values for each alpha (if <code>store_cv_values=True</code> and <code>cv=None</code>). After <code>fit()</code> has been called, this attribute will contain the mean squared errors (by default) or the values of the <code>{loss,score}_func</code> function (if provided in the constructor). This attribute exists only when <code>store_cv_values</code> is True.</p> </dd> <dt>
<code>coef_ndarray of shape (1, n_features) or (n_targets, n_features)</code> </dt>
<dd>
<p>Coefficient of the features in the decision function.</p> <p><code>coef_</code> is of shape (1, n_features) when the given problem is binary.</p> </dd> <dt>
<code>intercept_float or ndarray of shape (n_targets,)</code> </dt>
<dd>
<p>Independent term in decision function. Set to 0.0 if <code>fit_intercept = False</code>.</p> </dd> <dt>
<code>alpha_float</code> </dt>
<dd>
<p>Estimated regularization parameter.</p> </dd> <dt>
<code>best_score_float</code> </dt>
<dd>
<p>Score of base estimator with best alpha.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.23.</span></p> </div> </dd> <dt>
<code>classes_ndarray of shape (n_classes,)</code> </dt>
<dd>
<p>The classes labels.</p> </dd> </dl> </dd> </dl> <div class="admonition seealso"> <p class="admonition-title">See also</p> <dl class="simple"> <dt>
 <a class="reference internal" href="sklearn.linear_model.ridge#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code>Ridge</code></a>
</dt>
<dd>
<p>Ridge regression.</p> </dd> <dt>
 <a class="reference internal" href="sklearn.linear_model.ridgeclassifier#sklearn.linear_model.RidgeClassifier" title="sklearn.linear_model.RidgeClassifier"><code>RidgeClassifier</code></a>
</dt>
<dd>
<p>Ridge classifier.</p> </dd> <dt>
 <a class="reference internal" href="sklearn.linear_model.ridgecv#sklearn.linear_model.RidgeCV" title="sklearn.linear_model.RidgeCV"><code>RidgeCV</code></a>
</dt>
<dd>
<p>Ridge regression with built-in cross validation.</p> </dd> </dl> </div> <h4 class="rubric">Notes</h4> <p>For multi-class classification, n_class classifiers are trained in a one-versus-all approach. Concretely, this is implemented by taking advantage of the multi-variate response support in Ridge.</p> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from sklearn.datasets import load_breast_cancer
&gt;&gt;&gt; from sklearn.linear_model import RidgeClassifierCV
&gt;&gt;&gt; X, y = load_breast_cancer(return_X_y=True)
&gt;&gt;&gt; clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
&gt;&gt;&gt; clf.score(X, y)
0.9630...
</pre> <h4 class="rubric">Methods</h4> <table class="longtable docutils align-default">   <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.RidgeClassifierCV.decision_function" title="sklearn.linear_model.RidgeClassifierCV.decision_function"><code>decision_function</code></a>(X)</p></td> <td><p>Predict confidence scores for samples.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.RidgeClassifierCV.fit" title="sklearn.linear_model.RidgeClassifierCV.fit"><code>fit</code></a>(X, y[, sample_weight])</p></td> <td><p>Fit Ridge classifier with cv.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.RidgeClassifierCV.get_params" title="sklearn.linear_model.RidgeClassifierCV.get_params"><code>get_params</code></a>([deep])</p></td> <td><p>Get parameters for this estimator.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.RidgeClassifierCV.predict" title="sklearn.linear_model.RidgeClassifierCV.predict"><code>predict</code></a>(X)</p></td> <td><p>Predict class labels for samples in X.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.RidgeClassifierCV.score" title="sklearn.linear_model.RidgeClassifierCV.score"><code>score</code></a>(X, y[, sample_weight])</p></td> <td><p>Return the mean accuracy on the given test data and labels.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.RidgeClassifierCV.set_params" title="sklearn.linear_model.RidgeClassifierCV.set_params"><code>set_params</code></a>(**params)</p></td> <td><p>Set the parameters of this estimator.</p></td> </tr>  </table> <dl class="py method"> <dt id="sklearn.linear_model.RidgeClassifierCV.decision_function">
<code>decision_function(X)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_base.py#L263"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Predict confidence scores for samples.</p> <p>The confidence score for a sample is proportional to the signed distance of that sample to the hyperplane.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xarray-like or sparse matrix, shape (n_samples, n_features)</code> </dt>
<dd>
<p>Samples.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)</dt>
<dd>
<p>Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where &gt;0 means this class would be predicted.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.RidgeClassifierCV.fit">
<code>fit(X, y, sample_weight=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_ridge.py#L1921"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fit Ridge classifier with cv.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xndarray of shape (n_samples, n_features)</code> </dt>
<dd>
<p>Training vectors, where n_samples is the number of samples and n_features is the number of features. When using GCV, will be cast to float64 if necessary.</p> </dd> <dt>
<code>yndarray of shape (n_samples,)</code> </dt>
<dd>
<p>Target values. Will be cast to X’s dtype if necessary.</p> </dd> <dt>
<code>sample_weightfloat or ndarray of shape (n_samples,), default=None</code> </dt>
<dd>
<p>Individual weights for each sample. If given a float, every sample will have the same weight.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>selfobject</code> </dt>
 </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.RidgeClassifierCV.get_params">
<code>get_params(deep=True)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/base.py#L178"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Get parameters for this estimator.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>deepbool, default=True</code> </dt>
<dd>
<p>If True, will return the parameters for this estimator and contained subobjects that are estimators.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>paramsdict</code> </dt>
<dd>
<p>Parameter names mapped to their values.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.RidgeClassifierCV.predict">
<code>predict(X)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_base.py#L295"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Predict class labels for samples in X.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xarray-like or sparse matrix, shape (n_samples, n_features)</code> </dt>
<dd>
<p>Samples.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>Carray, shape [n_samples]</code> </dt>
<dd>
<p>Predicted class label per sample.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.RidgeClassifierCV.score">
<code>score(X, y, sample_weight=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/base.py#L475"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Return the mean accuracy on the given test data and labels.</p> <p>In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xarray-like of shape (n_samples, n_features)</code> </dt>
<dd>
<p>Test samples.</p> </dd> <dt>
<code>yarray-like of shape (n_samples,) or (n_samples, n_outputs)</code> </dt>
<dd>
<p>True labels for <code>X</code>.</p> </dd> <dt>
<code>sample_weightarray-like of shape (n_samples,), default=None</code> </dt>
<dd>
<p>Sample weights.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>scorefloat</code> </dt>
<dd>
<p>Mean accuracy of <code>self.predict(X)</code> wrt. <code>y</code>.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.RidgeClassifierCV.set_params">
<code>set_params(**params)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/base.py#L202"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as <a class="reference internal" href="sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code>Pipeline</code></a>). The latter have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it’s possible to update each component of a nested object.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>**paramsdict</code> </dt>
<dd>
<p>Estimator parameters.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>selfestimator instance</code> </dt>
<dd>
<p>Estimator instance.</p> </dd> </dl> </dd> </dl> </dd>
</dl> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2020 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/0.24/modules/generated/sklearn.linear_model.RidgeClassifierCV.html" class="_attribution-link">https://scikit-learn.org/0.24/modules/generated/sklearn.linear_model.RidgeClassifierCV.html</a>
  </p>
</div>
