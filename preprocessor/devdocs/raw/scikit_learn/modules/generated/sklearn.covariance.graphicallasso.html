<h1>sklearn.covariance.GraphicalLasso</h1> <dl class="py class"> <dt id="sklearn.covariance.GraphicalLasso">
<code>class sklearn.covariance.GraphicalLasso(alpha=0.01, *, mode='cd', tol=0.0001, enet_tol=0.0001, max_iter=100, verbose=False, assume_centered=False)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/covariance/_graph_lasso.py#L287"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sparse inverse covariance estimation with an l1-penalized estimator.</p> <p>Read more in the <a class="reference internal" href="../covariance#sparse-inverse-covariance"><span class="std std-ref">User Guide</span></a>.</p> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version v0.20: </span>GraphLasso has been renamed to GraphicalLasso</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>alphafloat, default=0.01</code> </dt>
<dd>
<p>The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance. Range is (0, inf].</p> </dd> <dt>
<code>mode{‘cd’, ‘lars’}, default=’cd’</code> </dt>
<dd>
<p>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p &gt; n. Elsewhere prefer cd which is more numerically stable.</p> </dd> <dt>
<code>tolfloat, default=1e-4</code> </dt>
<dd>
<p>The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped. Range is (0, inf].</p> </dd> <dt>
<code>enet_tolfloat, default=1e-4</code> </dt>
<dd>
<p>The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode=’cd’. Range is (0, inf].</p> </dd> <dt>
<code>max_iterint, default=100</code> </dt>
<dd>
<p>The maximum number of iterations.</p> </dd> <dt>
<code>verbosebool, default=False</code> </dt>
<dd>
<p>If verbose is True, the objective function and dual gap are plotted at each iteration.</p> </dd> <dt>
<code>assume_centeredbool, default=False</code> </dt>
<dd>
<p>If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data are centered before computation.</p> </dd> </dl> </dd> <dt class="field-even">Attributes</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>location_ndarray of shape (n_features,)</code> </dt>
<dd>
<p>Estimated location, i.e. the estimated mean.</p> </dd> <dt>
<code>covariance_ndarray of shape (n_features, n_features)</code> </dt>
<dd>
<p>Estimated covariance matrix</p> </dd> <dt>
<code>precision_ndarray of shape (n_features, n_features)</code> </dt>
<dd>
<p>Estimated pseudo inverse matrix.</p> </dd> <dt>
<code>n_iter_int</code> </dt>
<dd>
<p>Number of iterations run.</p> </dd> </dl> </dd> </dl> <div class="admonition seealso"> <p class="admonition-title">See also</p> <dl class="simple"> <dt>
<code>graphical_lasso,</code> <a class="reference internal" href="sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV" title="sklearn.covariance.GraphicalLassoCV"><code>GraphicalLassoCV</code></a>
</dt>
 </dl> </div> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.covariance import GraphicalLasso
&gt;&gt;&gt; true_cov = np.array([[0.8, 0.0, 0.2, 0.0],
...                      [0.0, 0.4, 0.0, 0.0],
...                      [0.2, 0.0, 0.3, 0.1],
...                      [0.0, 0.0, 0.1, 0.7]])
&gt;&gt;&gt; np.random.seed(0)
&gt;&gt;&gt; X = np.random.multivariate_normal(mean=[0, 0, 0, 0],
...                                   cov=true_cov,
...                                   size=200)
&gt;&gt;&gt; cov = GraphicalLasso().fit(X)
&gt;&gt;&gt; np.around(cov.covariance_, decimals=3)
array([[0.816, 0.049, 0.218, 0.019],
       [0.049, 0.364, 0.017, 0.034],
       [0.218, 0.017, 0.322, 0.093],
       [0.019, 0.034, 0.093, 0.69 ]])
&gt;&gt;&gt; np.around(cov.location_, decimals=3)
array([0.073, 0.04 , 0.038, 0.143])
</pre> <h4 class="rubric">Methods</h4> <table class="longtable docutils align-default">   <tr>
<td><p><a class="reference internal" href="#sklearn.covariance.GraphicalLasso.error_norm" title="sklearn.covariance.GraphicalLasso.error_norm"><code>error_norm</code></a>(comp_cov[, norm, scaling, squared])</p></td> <td><p>Computes the Mean Squared Error between two covariance estimators.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.covariance.GraphicalLasso.fit" title="sklearn.covariance.GraphicalLasso.fit"><code>fit</code></a>(X[, y])</p></td> <td><p>Fits the GraphicalLasso model to X.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.covariance.GraphicalLasso.get_params" title="sklearn.covariance.GraphicalLasso.get_params"><code>get_params</code></a>([deep])</p></td> <td><p>Get parameters for this estimator.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.covariance.GraphicalLasso.get_precision" title="sklearn.covariance.GraphicalLasso.get_precision"><code>get_precision</code></a>()</p></td> <td><p>Getter for the precision matrix.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.covariance.GraphicalLasso.mahalanobis" title="sklearn.covariance.GraphicalLasso.mahalanobis"><code>mahalanobis</code></a>(X)</p></td> <td><p>Computes the squared Mahalanobis distances of given observations.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.covariance.GraphicalLasso.score" title="sklearn.covariance.GraphicalLasso.score"><code>score</code></a>(X_test[, y])</p></td> <td><p>Computes the log-likelihood of a Gaussian data set with <code>self.covariance_</code> as an estimator of its covariance matrix.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.covariance.GraphicalLasso.set_params" title="sklearn.covariance.GraphicalLasso.set_params"><code>set_params</code></a>(**params)</p></td> <td><p>Set the parameters of this estimator.</p></td> </tr>  </table> <dl class="py method"> <dt id="sklearn.covariance.GraphicalLasso.error_norm">
<code>error_norm(comp_cov, norm='frobenius', scaling=True, squared=True)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/covariance/_empirical_covariance.py#L245"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius norm).</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>comp_covarray-like of shape (n_features, n_features)</code> </dt>
<dd>
<p>The covariance to compare with.</p> </dd> <dt>
<code>norm{“frobenius”, “spectral”}, default=”frobenius”</code> </dt>
<dd>
<p>The type of norm used to compute the error. Available error types: - ‘frobenius’ (default): sqrt(tr(A^t.A)) - ‘spectral’: sqrt(max(eigenvalues(A^t.A)) where A is the error <code>(comp_cov - self.covariance_)</code>.</p> </dd> <dt>
<code>scalingbool, default=True</code> </dt>
<dd>
<p>If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled.</p> </dd> <dt>
<code>squaredbool, default=True</code> </dt>
<dd>
<p>Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>resultfloat</code> </dt>
<dd>
<p>The Mean Squared Error (in the sense of the Frobenius norm) between <code>self</code> and <code>comp_cov</code> covariance estimators.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.covariance.GraphicalLasso.fit">
<code>fit(X, y=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/covariance/_graph_lasso.py#L380"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fits the GraphicalLasso model to X.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xarray-like of shape (n_samples, n_features)</code> </dt>
<dd>
<p>Data from which to compute the covariance estimate</p> </dd> <dt>
<code>yIgnored</code> </dt>
<dd>
<p>Not used, present for API consistency by convention.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>selfobject</code> </dt>
 </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.covariance.GraphicalLasso.get_params">
<code>get_params(deep=True)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/base.py#L178"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Get parameters for this estimator.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>deepbool, default=True</code> </dt>
<dd>
<p>If True, will return the parameters for this estimator and contained subobjects that are estimators.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>paramsdict</code> </dt>
<dd>
<p>Parameter names mapped to their values.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.covariance.GraphicalLasso.get_precision">
<code>get_precision()</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/covariance/_empirical_covariance.py#L174"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Getter for the precision matrix.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>precision_array-like of shape (n_features, n_features)</code> </dt>
<dd>
<p>The precision matrix associated to the current covariance object.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.covariance.GraphicalLasso.mahalanobis">
<code>mahalanobis(X)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/covariance/_empirical_covariance.py#L297"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Computes the squared Mahalanobis distances of given observations.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xarray-like of shape (n_samples, n_features)</code> </dt>
<dd>
<p>The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>distndarray of shape (n_samples,)</code> </dt>
<dd>
<p>Squared Mahalanobis distances of the observations.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.covariance.GraphicalLasso.score">
<code>score(X_test, y=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/covariance/_empirical_covariance.py#L216"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Computes the log-likelihood of a Gaussian data set with <code>self.covariance_</code> as an estimator of its covariance matrix.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>X_testarray-like of shape (n_samples, n_features)</code> </dt>
<dd>
<p>Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering).</p> </dd> <dt>
<code>yIgnored</code> </dt>
<dd>
<p>Not used, present for API consistency by convention.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>resfloat</code> </dt>
<dd>
<p>The likelihood of the data set with <code>self.covariance_</code> as an estimator of its covariance matrix.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.covariance.GraphicalLasso.set_params">
<code>set_params(**params)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/base.py#L202"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as <a class="reference internal" href="sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code>Pipeline</code></a>). The latter have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it’s possible to update each component of a nested object.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>**paramsdict</code> </dt>
<dd>
<p>Estimator parameters.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>selfestimator instance</code> </dt>
<dd>
<p>Estimator instance.</p> </dd> </dl> </dd> </dl> </dd>
</dl> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2020 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/0.24/modules/generated/sklearn.covariance.GraphicalLasso.html" class="_attribution-link">https://scikit-learn.org/0.24/modules/generated/sklearn.covariance.GraphicalLasso.html</a>
  </p>
</div>
