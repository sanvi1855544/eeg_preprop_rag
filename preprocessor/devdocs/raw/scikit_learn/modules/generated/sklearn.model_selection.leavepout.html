<h1>sklearn.model_selection.LeavePOut</h1> <dl class="py class"> <dt id="sklearn.model_selection.LeavePOut">
<code>class sklearn.model_selection.LeavePOut(p)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/model_selection/_split.py#L188"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Leave-P-Out cross-validator</p> <p>Provides train/test indices to split data in train/test sets. This results in testing on all distinct samples of size p, while the remaining n - p samples form the training set in each iteration.</p> <p>Note: <code>LeavePOut(p)</code> is NOT equivalent to <code>KFold(n_splits=n_samples // p)</code> which creates non-overlapping test sets.</p> <p>Due to the high number of iterations which grows combinatorically with the number of samples this cross-validation method can be very costly. For large datasets one should favor <a class="reference internal" href="sklearn.model_selection.kfold#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code>KFold</code></a>, <a class="reference internal" href="sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold" title="sklearn.model_selection.StratifiedKFold"><code>StratifiedKFold</code></a> or <a class="reference internal" href="sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit" title="sklearn.model_selection.ShuffleSplit"><code>ShuffleSplit</code></a>.</p> <p>Read more in the <a class="reference internal" href="../cross_validation#leave-p-out"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>pint</code> </dt>
<dd>
<p>Size of the test sets. Must be strictly less than the number of samples.</p> </dd> </dl> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.model_selection import LeavePOut
&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
&gt;&gt;&gt; y = np.array([1, 2, 3, 4])
&gt;&gt;&gt; lpo = LeavePOut(2)
&gt;&gt;&gt; lpo.get_n_splits(X)
6
&gt;&gt;&gt; print(lpo)
LeavePOut(p=2)
&gt;&gt;&gt; for train_index, test_index in lpo.split(X):
...     print("TRAIN:", train_index, "TEST:", test_index)
...     X_train, X_test = X[train_index], X[test_index]
...     y_train, y_test = y[train_index], y[test_index]
TRAIN: [2 3] TEST: [0 1]
TRAIN: [1 3] TEST: [0 2]
TRAIN: [1 2] TEST: [0 3]
TRAIN: [0 3] TEST: [1 2]
TRAIN: [0 2] TEST: [1 3]
TRAIN: [0 1] TEST: [2 3]
</pre> <h4 class="rubric">Methods</h4> <table class="longtable docutils align-default">   <tr>
<td><p><a class="reference internal" href="#sklearn.model_selection.LeavePOut.get_n_splits" title="sklearn.model_selection.LeavePOut.get_n_splits"><code>get_n_splits</code></a>(X[, y, groups])</p></td> <td><p>Returns the number of splitting iterations in the cross-validator</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.model_selection.LeavePOut.split" title="sklearn.model_selection.LeavePOut.split"><code>split</code></a>(X[, y, groups])</p></td> <td><p>Generate indices to split data into training and test set.</p></td> </tr>  </table> <dl class="py method"> <dt id="sklearn.model_selection.LeavePOut.get_n_splits">
<code>get_n_splits(X, y=None, groups=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/model_selection/_split.py#L247"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the number of splitting iterations in the cross-validator</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xarray-like of shape (n_samples, n_features)</code> </dt>
<dd>
<p>Training data, where n_samples is the number of samples and n_features is the number of features.</p> </dd> <dt>
<code>yobject</code> </dt>
<dd>
<p>Always ignored, exists for compatibility.</p> </dd> <dt>
<code>groupsobject</code> </dt>
<dd>
<p>Always ignored, exists for compatibility.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.model_selection.LeavePOut.split">
<code>split(X, y=None, groups=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/model_selection/_split.py#L54"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Generate indices to split data into training and test set.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xarray-like of shape (n_samples, n_features)</code> </dt>
<dd>
<p>Training data, where n_samples is the number of samples and n_features is the number of features.</p> </dd> <dt>
<code>yarray-like of shape (n_samples,)</code> </dt>
<dd>
<p>The target variable for supervised learning problems.</p> </dd> <dt>
<code>groupsarray-like of shape (n_samples,), default=None</code> </dt>
<dd>
<p>Group labels for the samples used while splitting the dataset into train/test set.</p> </dd> </dl> </dd> <dt class="field-even">Yields</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>trainndarray</code> </dt>
<dd>
<p>The training set indices for that split.</p> </dd> <dt>
<code>testndarray</code> </dt>
<dd>
<p>The testing set indices for that split.</p> </dd> </dl> </dd> </dl> </dd>
</dl> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2020 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/0.24/modules/generated/sklearn.model_selection.LeavePOut.html" class="_attribution-link">https://scikit-learn.org/0.24/modules/generated/sklearn.model_selection.LeavePOut.html</a>
  </p>
</div>
