<h1>sklearn.metrics.normalized_mutual_info_score</h1> <dl class="py function"> <dt id="sklearn.metrics.normalized_mutual_info_score">
<code>sklearn.metrics.normalized_mutual_info_score(labels_true, labels_pred, *, average_method='arithmetic')</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/metrics/cluster/_supervised.py#L923"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Normalized Mutual Information between two clusterings.</p> <p>Normalized Mutual Information (NMI) is a normalization of the Mutual Information (MI) score to scale the results between 0 (no mutual information) and 1 (perfect correlation). In this function, mutual information is normalized by some generalized mean of <code>H(labels_true)</code> and <code>H(labels_pred))</code>, defined by the <code>average_method</code>.</p> <p>This measure is not adjusted for chance. Therefore <a class="reference internal" href="sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score" title="sklearn.metrics.adjusted_mutual_info_score"><code>adjusted_mutual_info_score</code></a> might be preferred.</p> <p>This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won’t change the score value in any way.</p> <p>This metric is furthermore symmetric: switching <code>label_true</code> with <code>label_pred</code> will return the same score value. This can be useful to measure the agreement of two independent label assignments strategies on the same dataset when the real ground truth is not known.</p> <p>Read more in the <a class="reference internal" href="../clustering#mutual-info-score"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl> <dt>
<code>labels_trueint array, shape = [n_samples]</code> </dt>
<dd>
<p>A clustering of the data into disjoint subsets.</p> </dd> <dt>
<code>labels_predint array-like of shape (n_samples,)</code> </dt>
<dd>
<p>A clustering of the data into disjoint subsets.</p> </dd> <dt>
<code>average_methodstr, default=’arithmetic’</code> </dt>
<dd>
<p>How to compute the normalizer in the denominator. Possible options are ‘min’, ‘geometric’, ‘arithmetic’, and ‘max’.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.20.</span></p> </div> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code>average_method</code> changed from ‘geometric’ to ‘arithmetic’.</p> </div> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>nmifloat</code> </dt>
<dd>
<p>score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling</p> </dd> </dl> </dd> </dl> <div class="admonition seealso"> <p class="admonition-title">See also</p> <dl class="simple"> <dt>
 <a class="reference internal" href="sklearn.metrics.v_measure_score#sklearn.metrics.v_measure_score" title="sklearn.metrics.v_measure_score"><code>v_measure_score</code></a>
</dt>
<dd>
<p>V-Measure (NMI with arithmetic mean option).</p> </dd> <dt>
 <a class="reference internal" href="sklearn.metrics.adjusted_rand_score#sklearn.metrics.adjusted_rand_score" title="sklearn.metrics.adjusted_rand_score"><code>adjusted_rand_score</code></a>
</dt>
<dd>
<p>Adjusted Rand Index.</p> </dd> <dt>
 <a class="reference internal" href="sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score" title="sklearn.metrics.adjusted_mutual_info_score"><code>adjusted_mutual_info_score</code></a>
</dt>
<dd>
<p>Adjusted Mutual Information (adjusted against chance).</p> </dd> </dl> </div> <h4 class="rubric">Examples</h4> <p>Perfect labelings are both homogeneous and complete, hence have score 1.0:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.metrics.cluster import normalized_mutual_info_score
&gt;&gt;&gt; normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
... 
1.0
&gt;&gt;&gt; normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
... 
1.0
</pre> <p>If classes members are completely split across different clusters, the assignment is totally in-complete, hence the NMI is null:</p> <pre data-language="python">&gt;&gt;&gt; normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])
... 
0.0
</pre> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2020 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/0.24/modules/generated/sklearn.metrics.normalized_mutual_info_score.html" class="_attribution-link">https://scikit-learn.org/0.24/modules/generated/sklearn.metrics.normalized_mutual_info_score.html</a>
  </p>
</div>
