<h1>sklearn.preprocessing.power_transform</h1> <dl class="py function"> <dt id="sklearn.preprocessing.power_transform">
<code>sklearn.preprocessing.power_transform(X, method='yeo-johnson', *, standardize=True, copy=True)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/preprocessing/_data.py#L3299"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired.</p> <p>Currently, power_transform supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood.</p> <p>Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data.</p> <p>By default, zero-mean, unit-variance normalization is applied to the transformed data.</p> <p>Read more in the <a class="reference internal" href="../preprocessing#preprocessing-transformer"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl> <dt>
<code>Xarray-like of shape (n_samples, n_features)</code> </dt>
<dd>
<p>The data to be transformed using a power transformation.</p> </dd> <dt>
<code>method{‘yeo-johnson’, ‘box-cox’}, default=’yeo-johnson’</code> </dt>
<dd>
<p>The power transform method. Available methods are:</p> <ul class="simple"> <li>‘yeo-johnson’ <a class="reference internal" href="#r742a88cfa144-1" id="id1">[1]</a>, works with positive and negative values</li> <li>‘box-cox’ <a class="reference internal" href="#r742a88cfa144-2" id="id2">[2]</a>, only works with strictly positive values</li> </ul> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 0.23: </span>The default value of the <code>method</code> parameter changed from ‘box-cox’ to ‘yeo-johnson’ in 0.23.</p> </div> </dd> <dt>
<code>standardizebool, default=True</code> </dt>
<dd>
<p>Set to True to apply zero-mean, unit-variance normalization to the transformed output.</p> </dd> <dt>
<code>copybool, default=True</code> </dt>
<dd>
<p>Set to False to perform inplace computation during transformation.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>X_transndarray of shape (n_samples, n_features)</code> </dt>
<dd>
<p>The transformed data.</p> </dd> </dl> </dd> </dl> <div class="admonition seealso"> <p class="admonition-title">See also</p> <dl class="simple"> <dt>
 <a class="reference internal" href="sklearn.preprocessing.powertransformer#sklearn.preprocessing.PowerTransformer" title="sklearn.preprocessing.PowerTransformer"><code>PowerTransformer</code></a>
</dt>
<dd>
<p>Equivalent transformation with the Transformer API (e.g. as part of a preprocessing <a class="reference internal" href="sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code>Pipeline</code></a>).</p> </dd> <dt>
 <a class="reference internal" href="sklearn.preprocessing.quantile_transform#sklearn.preprocessing.quantile_transform" title="sklearn.preprocessing.quantile_transform"><code>quantile_transform</code></a>
</dt>
<dd>
<p>Maps data to a standard normal distribution with the parameter <code>output_distribution='normal'</code>.</p> </dd> </dl> </div> <h4 class="rubric">Notes</h4> <p>NaNs are treated as missing values: disregarded in <code>fit</code>, and maintained in <code>transform</code>.</p> <p>For a comparison of the different scalers, transformers, and normalizers, see <a class="reference internal" href="../../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py"><span class="std std-ref">examples/preprocessing/plot_all_scaling.py</span></a>.</p> <h4 class="rubric">References</h4> <dl class="citation"> <dt class="label" id="r742a88cfa144-1">
<code>1</code> </dt> <dd>
<p>I.K. Yeo and R.A. Johnson, “A new family of power transformations to improve normality or symmetry.” Biometrika, 87(4), pp.954-959, (2000).</p> </dd> <dt class="label" id="r742a88cfa144-2">
<code>2</code> </dt> <dd>
<p>G.E.P. Box and D.R. Cox, “An Analysis of Transformations”, Journal of the Royal Statistical Society B, 26, 211-252 (1964).</p> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.preprocessing import power_transform
&gt;&gt;&gt; data = [[1, 2], [3, 2], [4, 5]]
&gt;&gt;&gt; print(power_transform(data, method='box-cox'))
[[-1.332... -0.707...]
 [ 0.256... -0.707...]
 [ 1.076...  1.414...]]
</pre> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Risk of data leak. Do not use <a class="reference internal" href="#sklearn.preprocessing.power_transform" title="sklearn.preprocessing.power_transform"><code>power_transform</code></a> unless you know what you are doing. A common mistake is to apply it to the entire data <em>before</em> splitting into training and test sets. This will bias the model evaluation because information would have leaked from the test set to the training set. In general, we recommend using <a class="reference internal" href="sklearn.preprocessing.powertransformer#sklearn.preprocessing.PowerTransformer" title="sklearn.preprocessing.PowerTransformer"><code>PowerTransformer</code></a> within a <a class="reference internal" href="../compose#pipeline"><span class="std std-ref">Pipeline</span></a> in order to prevent most risks of data leaking, e.g.: <code>pipe = make_pipeline(PowerTransformer(),
LogisticRegression())</code>.</p> </div> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2020 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/0.24/modules/generated/sklearn.preprocessing.power_transform.html" class="_attribution-link">https://scikit-learn.org/0.24/modules/generated/sklearn.preprocessing.power_transform.html</a>
  </p>
</div>
