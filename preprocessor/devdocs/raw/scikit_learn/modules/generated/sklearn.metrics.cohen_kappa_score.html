<h1>sklearn.metrics.cohen_kappa_score</h1> <dl class="py function"> <dt id="sklearn.metrics.cohen_kappa_score">
<code>sklearn.metrics.cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/metrics/_classification.py#L560"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Cohen’s kappa: a statistic that measures inter-annotator agreement.</p> <p>This function computes Cohen’s kappa <a class="reference internal" href="#r219a3b9132e1-1" id="id1">[1]</a>, a score that expresses the level of agreement between two annotators on a classification problem. It is defined as</p> <div class="math notranslate nohighlight"> \[\kappa = (p_o - p_e) / (1 - p_e)\]</div> <p>where <span class="math notranslate nohighlight">\(p_o\)</span> is the empirical probability of agreement on the label assigned to any sample (the observed agreement ratio), and <span class="math notranslate nohighlight">\(p_e\)</span> is the expected agreement when both annotators assign labels randomly. <span class="math notranslate nohighlight">\(p_e\)</span> is estimated using a per-annotator empirical prior over the class labels <a class="reference internal" href="#r219a3b9132e1-2" id="id2">[2]</a>.</p> <p>Read more in the <a class="reference internal" href="../model_evaluation#cohen-kappa"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>y1array of shape (n_samples,)</code> </dt>
<dd>
<p>Labels assigned by the first annotator.</p> </dd> <dt>
<code>y2array of shape (n_samples,)</code> </dt>
<dd>
<p>Labels assigned by the second annotator. The kappa statistic is symmetric, so swapping <code>y1</code> and <code>y2</code> doesn’t change the value.</p> </dd> <dt>
<code>labelsarray-like of shape (n_classes,), default=None</code> </dt>
<dd>
<p>List of labels to index the matrix. This may be used to select a subset of labels. If None, all labels that appear at least once in <code>y1</code> or <code>y2</code> are used.</p> </dd> <dt>
<code>weights{‘linear’, ‘quadratic’}, default=None</code> </dt>
<dd>
<p>Weighting type to calculate the score. None means no weighted; “linear” means linear weighted; “quadratic” means quadratic weighted.</p> </dd> <dt>
<code>sample_weightarray-like of shape (n_samples,), default=None</code> </dt>
<dd>
<p>Sample weights.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>kappafloat</code> </dt>
<dd>
<p>The kappa statistic, which is a number between -1 and 1. The maximum value means complete agreement; zero or lower means chance agreement.</p> </dd> </dl> </dd> </dl> <h4 class="rubric">References</h4> <dl class="citation"> <dt class="label" id="r219a3b9132e1-1">
<code>1</code> </dt> <dd>
<p>J. Cohen (1960). “A coefficient of agreement for nominal scales”. Educational and Psychological Measurement 20(1):37-46. doi:10.1177/001316446002000104.</p> </dd> <dt class="label" id="r219a3b9132e1-2">
<code>2</code> </dt> <dd>
<p><a class="reference external" href="https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2">R. Artstein and M. Poesio (2008). “Inter-coder agreement for computational linguistics”. Computational Linguistics 34(4):555-596</a>.</p> </dd> <dt class="label" id="r219a3b9132e1-3">
<code>3</code> </dt> <dd>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Wikipedia entry for the Cohen’s kappa</a>.</p> </dd> </dl> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2020 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/0.24/modules/generated/sklearn.metrics.cohen_kappa_score.html" class="_attribution-link">https://scikit-learn.org/0.24/modules/generated/sklearn.metrics.cohen_kappa_score.html</a>
  </p>
</div>
