<h1>sklearn.linear_model.LogisticRegressionCV</h1> <dl class="py class"> <dt id="sklearn.linear_model.LogisticRegressionCV">
<code>class sklearn.linear_model.LogisticRegressionCV(*, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class='auto', random_state=None, l1_ratios=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_logistic.py#L1502"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Logistic Regression CV (aka logit, MaxEnt) classifier.</p> <p>See glossary entry for <a class="reference internal" href="https://scikit-learn.org/0.24/glossary.html#term-cross-validation-estimator"><span class="xref std std-term">cross-validation estimator</span></a>.</p> <p>This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. Elastic-Net penalty is only supported by the saga solver.</p> <p>For the grid of <code>Cs</code> values and <code>l1_ratios</code> values, the best hyperparameter is selected by the cross-validator <a class="reference internal" href="sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold" title="sklearn.model_selection.StratifiedKFold"><code>StratifiedKFold</code></a>, but it can be changed using the <a class="reference internal" href="https://scikit-learn.org/0.24/glossary.html#term-cv"><span class="xref std std-term">cv</span></a> parameter. The ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ solvers can warm-start the coefficients (see <a class="reference internal" href="https://scikit-learn.org/0.24/glossary.html#term-warm_start"><span class="xref std std-term">Glossary</span></a>).</p> <p>Read more in the <a class="reference internal" href="../linear_model#logistic-regression"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl> <dt>
<code>Csint or list of floats, default=10</code> </dt>
<dd>
<p>Each of the values in Cs describes the inverse of regularization strength. If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4. Like in support vector machines, smaller values specify stronger regularization.</p> </dd> <dt>
<code>fit_interceptbool, default=True</code> </dt>
<dd>
<p>Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function.</p> </dd> <dt>
<code>cvint or cross-validation generator, default=None</code> </dt>
<dd>
<p>The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module <a class="reference internal" href="../classes#module-sklearn.model_selection" title="sklearn.model_selection"><code>sklearn.model_selection</code></a> module for the list of possible cross-validation objects.</p> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 0.22: </span><code>cv</code> default value if None changed from 3-fold to 5-fold.</p> </div> </dd> <dt>
<code>dualbool, default=False</code> </dt>
<dd>
<p>Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples &gt; n_features.</p> </dd> <dt>
<code>penalty{‘l1’, ‘l2’, ‘elasticnet’}, default=’l2’</code> </dt>
<dd>
<p>Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties. ‘elasticnet’ is only supported by the ‘saga’ solver.</p> </dd> <dt>
<code>scoringstr or callable, default=None</code> </dt>
<dd>
<p>A string (see model evaluation documentation) or a scorer callable object / function with signature <code>scorer(estimator, X, y)</code>. For a list of scoring functions that can be used, look at <a class="reference internal" href="../classes#module-sklearn.metrics" title="sklearn.metrics"><code>sklearn.metrics</code></a>. The default scoring option used is ‘accuracy’.</p> </dd> <dt>
<code>solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’</code> </dt>
<dd>
<p>Algorithm to use in the optimization problem.</p> <ul class="simple"> <li>For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.</li> <li>For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.</li> <li>‘newton-cg’, ‘lbfgs’ and ‘sag’ only handle L2 penalty, whereas ‘liblinear’ and ‘saga’ handle L1 penalty.</li> <li>‘liblinear’ might be slower in LogisticRegressionCV because it does not handle warm-starting.</li> </ul> <p>Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p> </div> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.19: </span>SAGA solver.</p> </div> </dd> <dt>
<code>tolfloat, default=1e-4</code> </dt>
<dd>
<p>Tolerance for stopping criteria.</p> </dd> <dt>
<code>max_iterint, default=100</code> </dt>
<dd>
<p>Maximum number of iterations of the optimization algorithm.</p> </dd> <dt>
<code>class_weightdict or ‘balanced’, default=None</code> </dt>
<dd>
<p>Weights associated with classes in the form <code>{class_label: weight}</code>. If not given, all classes are supposed to have weight one.</p> <p>The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as <code>n_samples / (n_classes * np.bincount(y))</code>.</p> <p>Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.17: </span>class_weight == ‘balanced’</p> </div> </dd> <dt>
<code>n_jobsint, default=None</code> </dt>
<dd>
<p>Number of CPU cores used during the cross-validation loop. <code>None</code> means 1 unless in a <a class="reference external" href="https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend" title="(in joblib v1.1.0.dev0)"><code>joblib.parallel_backend</code></a> context. <code>-1</code> means using all processors. See <a class="reference internal" href="https://scikit-learn.org/0.24/glossary.html#term-n_jobs"><span class="xref std std-term">Glossary</span></a> for more details.</p> </dd> <dt>
<code>verboseint, default=0</code> </dt>
<dd>
<p>For the ‘liblinear’, ‘sag’ and ‘lbfgs’ solvers set verbose to any positive number for verbosity.</p> </dd> <dt>
<code>refitbool, default=True</code> </dt>
<dd>
<p>If set to True, the scores are averaged across all folds, and the coefs and the C that corresponds to the best score is taken, and a final refit is done using these parameters. Otherwise the coefs, intercepts and C that correspond to the best scores across folds are averaged.</p> </dd> <dt>
<code>intercept_scalingfloat, default=1</code> </dt>
<dd>
<p>Useful only when the solver ‘liblinear’ is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a “synthetic” feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes <code>intercept_scaling * synthetic_feature_weight</code>.</p> <p>Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.</p> </dd> <dt>
<code>multi_class{‘auto, ‘ovr’, ‘multinomial’}, default=’auto’</code> </dt>
<dd>
<p>If the option chosen is ‘ovr’, then a binary problem is fit for each label. For ‘multinomial’ the loss minimised is the multinomial loss fit across the entire probability distribution, <em>even when the data is binary</em>. ‘multinomial’ is unavailable when solver=’liblinear’. ‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’, and otherwise selects ‘multinomial’.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.18: </span>Stochastic Average Gradient descent solver for ‘multinomial’ case.</p> </div> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 0.22: </span>Default changed from ‘ovr’ to ‘auto’ in 0.22.</p> </div> </dd> <dt>
<code>random_stateint, RandomState instance, default=None</code> </dt>
<dd>
<p>Used when <code>solver='sag'</code>, ‘saga’ or ‘liblinear’ to shuffle the data. Note that this only applies to the solver and not the cross-validation generator. See <a class="reference internal" href="https://scikit-learn.org/0.24/glossary.html#term-random_state"><span class="xref std std-term">Glossary</span></a> for details.</p> </dd> <dt>
<code>l1_ratioslist of float, default=None</code> </dt>
<dd>
<p>The list of Elastic-Net mixing parameter, with <code>0 &lt;= l1_ratio &lt;= 1</code>. Only used if <code>penalty='elasticnet'</code>. A value of 0 is equivalent to using <code>penalty='l2'</code>, while 1 is equivalent to using <code>penalty='l1'</code>. For <code>0 &lt; l1_ratio &lt;1</code>, the penalty is a combination of L1 and L2.</p> </dd> </dl> </dd> <dt class="field-even">Attributes</dt> <dd class="field-even">
<dl> <dt>
<code>classes_ndarray of shape (n_classes, )</code> </dt>
<dd>
<p>A list of class labels known to the classifier.</p> </dd> <dt>
<code>coef_ndarray of shape (1, n_features) or (n_classes, n_features)</code> </dt>
<dd>
<p>Coefficient of the features in the decision function.</p> <p><code>coef_</code> is of shape (1, n_features) when the given problem is binary.</p> </dd> <dt>
<code>intercept_ndarray of shape (1,) or (n_classes,)</code> </dt>
<dd>
<p>Intercept (a.k.a. bias) added to the decision function.</p> <p>If <code>fit_intercept</code> is set to False, the intercept is set to zero. <code>intercept_</code> is of shape(1,) when the problem is binary.</p> </dd> <dt>
<code>Cs_ndarray of shape (n_cs)</code> </dt>
<dd>
<p>Array of C i.e. inverse of regularization parameter values used for cross-validation.</p> </dd> <dt>
<code>l1_ratios_ndarray of shape (n_l1_ratios)</code> </dt>
<dd>
<p>Array of l1_ratios used for cross-validation. If no l1_ratio is used (i.e. penalty is not ‘elasticnet’), this is set to <code>[None]</code></p> </dd> <dt>
<code>coefs_paths_ndarray of shape (n_folds, n_cs, n_features) or (n_folds, n_cs, n_features + 1)</code> </dt>
<dd>
<p>dict with classes as the keys, and the path of coefficients obtained during cross-validating across each fold and then across each Cs after doing an OvR for the corresponding class as values. If the ‘multi_class’ option is set to ‘multinomial’, then the coefs_paths are the coefficients corresponding to each class. Each dict value has shape <code>(n_folds, n_cs, n_features)</code> or <code>(n_folds, n_cs, n_features + 1)</code> depending on whether the intercept is fit or not. If <code>penalty='elasticnet'</code>, the shape is <code>(n_folds, n_cs, n_l1_ratios_, n_features)</code> or <code>(n_folds, n_cs, n_l1_ratios_, n_features + 1)</code>.</p> </dd> <dt>
<code>scores_dict</code> </dt>
<dd>
<p>dict with classes as the keys, and the values as the grid of scores obtained during cross-validating each fold, after doing an OvR for the corresponding class. If the ‘multi_class’ option given is ‘multinomial’ then the same scores are repeated across all classes, since this is the multinomial class. Each dict value has shape <code>(n_folds, n_cs</code> or <code>(n_folds, n_cs, n_l1_ratios)</code> if <code>penalty='elasticnet'</code>.</p> </dd> <dt>
<code>C_ndarray of shape (n_classes,) or (n_classes - 1,)</code> </dt>
<dd>
<p>Array of C that maps to the best scores across every class. If refit is set to False, then for each class, the best C is the average of the C’s that correspond to the best scores for each fold. <code>C_</code> is of shape(n_classes,) when the problem is binary.</p> </dd> <dt>
<code>l1_ratio_ndarray of shape (n_classes,) or (n_classes - 1,)</code> </dt>
<dd>
<p>Array of l1_ratio that maps to the best scores across every class. If refit is set to False, then for each class, the best l1_ratio is the average of the l1_ratio’s that correspond to the best scores for each fold. <code>l1_ratio_</code> is of shape(n_classes,) when the problem is binary.</p> </dd> <dt>
<code>n_iter_ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)</code> </dt>
<dd>
<p>Actual number of iterations for all classes, folds and Cs. In the binary or multinomial cases, the first dimension is equal to 1. If <code>penalty='elasticnet'</code>, the shape is <code>(n_classes, n_folds,
n_cs, n_l1_ratios)</code> or <code>(1, n_folds, n_cs, n_l1_ratios)</code>.</p> </dd> </dl> </dd> </dl> <div class="admonition seealso"> <p class="admonition-title">See also</p> <dl class="simple"> <dt>
 <a class="reference internal" href="sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code>LogisticRegression</code></a>
</dt>
 </dl> </div> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from sklearn.datasets import load_iris
&gt;&gt;&gt; from sklearn.linear_model import LogisticRegressionCV
&gt;&gt;&gt; X, y = load_iris(return_X_y=True)
&gt;&gt;&gt; clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)
&gt;&gt;&gt; clf.predict(X[:2, :])
array([0, 0])
&gt;&gt;&gt; clf.predict_proba(X[:2, :]).shape
(2, 3)
&gt;&gt;&gt; clf.score(X, y)
0.98...
</pre> <h4 class="rubric">Methods</h4> <table class="longtable docutils align-default">   <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.decision_function" title="sklearn.linear_model.LogisticRegressionCV.decision_function"><code>decision_function</code></a>(X)</p></td> <td><p>Predict confidence scores for samples.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.densify" title="sklearn.linear_model.LogisticRegressionCV.densify"><code>densify</code></a>()</p></td> <td><p>Convert coefficient matrix to dense array format.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.fit" title="sklearn.linear_model.LogisticRegressionCV.fit"><code>fit</code></a>(X, y[, sample_weight])</p></td> <td><p>Fit the model according to the given training data.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.get_params" title="sklearn.linear_model.LogisticRegressionCV.get_params"><code>get_params</code></a>([deep])</p></td> <td><p>Get parameters for this estimator.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.predict" title="sklearn.linear_model.LogisticRegressionCV.predict"><code>predict</code></a>(X)</p></td> <td><p>Predict class labels for samples in X.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.predict_log_proba" title="sklearn.linear_model.LogisticRegressionCV.predict_log_proba"><code>predict_log_proba</code></a>(X)</p></td> <td><p>Predict logarithm of probability estimates.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.predict_proba" title="sklearn.linear_model.LogisticRegressionCV.predict_proba"><code>predict_proba</code></a>(X)</p></td> <td><p>Probability estimates.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.score" title="sklearn.linear_model.LogisticRegressionCV.score"><code>score</code></a>(X, y[, sample_weight])</p></td> <td><p>Returns the score using the <code>scoring</code> option on the given test data and labels.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.set_params" title="sklearn.linear_model.LogisticRegressionCV.set_params"><code>set_params</code></a>(**params)</p></td> <td><p>Set the parameters of this estimator.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.sparsify" title="sklearn.linear_model.LogisticRegressionCV.sparsify"><code>sparsify</code></a>()</p></td> <td><p>Convert coefficient matrix to sparse format.</p></td> </tr>  </table> <dl class="py method"> <dt id="sklearn.linear_model.LogisticRegressionCV.decision_function">
<code>decision_function(X)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_base.py#L263"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Predict confidence scores for samples.</p> <p>The confidence score for a sample is proportional to the signed distance of that sample to the hyperplane.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xarray-like or sparse matrix, shape (n_samples, n_features)</code> </dt>
<dd>
<p>Samples.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)</dt>
<dd>
<p>Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where &gt;0 means this class would be predicted.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.LogisticRegressionCV.densify">
<code>densify()</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_base.py#L339"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Convert coefficient matrix to dense array format.</p> <p>Converts the <code>coef_</code> member (back) to a numpy.ndarray. This is the default format of <code>coef_</code> and is required for fitting, so calling this method is only required on models that have previously been sparsified; otherwise, it is a no-op.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<dl class="simple"> <dt>self</dt>
<dd>
<p>Fitted estimator.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.LogisticRegressionCV.fit">
<code>fit(X, y, sample_weight=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_logistic.py#L1769"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fit the model according to the given training data.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>X{array-like, sparse matrix} of shape (n_samples, n_features)</code> </dt>
<dd>
<p>Training vector, where n_samples is the number of samples and n_features is the number of features.</p> </dd> <dt>
<code>yarray-like of shape (n_samples,)</code> </dt>
<dd>
<p>Target vector relative to X.</p> </dd> <dt>
<code>sample_weightarray-like of shape (n_samples,) default=None</code> </dt>
<dd>
<p>Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>selfobject</code> </dt>
 </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.LogisticRegressionCV.get_params">
<code>get_params(deep=True)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/base.py#L178"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Get parameters for this estimator.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>deepbool, default=True</code> </dt>
<dd>
<p>If True, will return the parameters for this estimator and contained subobjects that are estimators.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>paramsdict</code> </dt>
<dd>
<p>Parameter names mapped to their values.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.LogisticRegressionCV.predict">
<code>predict(X)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_base.py#L295"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Predict class labels for samples in X.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xarray-like or sparse matrix, shape (n_samples, n_features)</code> </dt>
<dd>
<p>Samples.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>Carray, shape [n_samples]</code> </dt>
<dd>
<p>Predicted class label per sample.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.LogisticRegressionCV.predict_log_proba">
<code>predict_log_proba(X)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_logistic.py#L1480"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Predict logarithm of probability estimates.</p> <p>The returned estimates for all classes are ordered by the label of classes.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xarray-like of shape (n_samples, n_features)</code> </dt>
<dd>
<p>Vector to be scored, where <code>n_samples</code> is the number of samples and <code>n_features</code> is the number of features.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>Tarray-like of shape (n_samples, n_classes)</code> </dt>
<dd>
<p>Returns the log-probability of the sample for each class in the model, where classes are ordered as they are in <code>self.classes_</code>.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.LogisticRegressionCV.predict_proba">
<code>predict_proba(X)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_logistic.py#L1437"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Probability estimates.</p> <p>The returned estimates for all classes are ordered by the label of classes.</p> <p>For a multi_class problem, if multi_class is set to be “multinomial” the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xarray-like of shape (n_samples, n_features)</code> </dt>
<dd>
<p>Vector to be scored, where <code>n_samples</code> is the number of samples and <code>n_features</code> is the number of features.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>Tarray-like of shape (n_samples, n_classes)</code> </dt>
<dd>
<p>Returns the probability of the sample for each class in the model, where classes are ordered as they are in <code>self.classes_</code>.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.LogisticRegressionCV.score">
<code>score(X, y, sample_weight=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_logistic.py#L2064"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the score using the <code>scoring</code> option on the given test data and labels.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xarray-like of shape (n_samples, n_features)</code> </dt>
<dd>
<p>Test samples.</p> </dd> <dt>
<code>yarray-like of shape (n_samples,)</code> </dt>
<dd>
<p>True labels for X.</p> </dd> <dt>
<code>sample_weightarray-like of shape (n_samples,), default=None</code> </dt>
<dd>
<p>Sample weights.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>scorefloat</code> </dt>
<dd>
<p>Score of self.predict(X) wrt. y.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.LogisticRegressionCV.set_params">
<code>set_params(**params)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/base.py#L202"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as <a class="reference internal" href="sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code>Pipeline</code></a>). The latter have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it’s possible to update each component of a nested object.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>**paramsdict</code> </dt>
<dd>
<p>Estimator parameters.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>selfestimator instance</code> </dt>
<dd>
<p>Estimator instance.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.LogisticRegressionCV.sparsify">
<code>sparsify()</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_base.py#L359"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Convert coefficient matrix to sparse format.</p> <p>Converts the <code>coef_</code> member to a scipy.sparse matrix, which for L1-regularized models can be much more memory- and storage-efficient than the usual numpy.ndarray representation.</p> <p>The <code>intercept_</code> member is not converted.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<dl class="simple"> <dt>self</dt>
<dd>
<p>Fitted estimator.</p> </dd> </dl> </dd> </dl> <h4 class="rubric">Notes</h4> <p>For non-sparse models, i.e. when there are not many zeros in <code>coef_</code>, this may actually <em>increase</em> memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with <code>(coef_ == 0).sum()</code>, must be more than 50% for this to provide significant benefits.</p> <p>After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.</p> </dd>
</dl> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2020 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/0.24/modules/generated/sklearn.linear_model.LogisticRegressionCV.html" class="_attribution-link">https://scikit-learn.org/0.24/modules/generated/sklearn.linear_model.LogisticRegressionCV.html</a>
  </p>
</div>
