<h1>sklearn.linear_model.Lars</h1> <dl class="py class"> <dt id="sklearn.linear_model.Lars">
<code>class sklearn.linear_model.Lars(*, fit_intercept=True, verbose=False, normalize=True, precompute='auto', n_nonzero_coefs=500, eps=2.220446049250313e-16, copy_X=True, fit_path=True, jitter=None, random_state=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_least_angle.py#L804"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Least Angle Regression model a.k.a. LAR</p> <p>Read more in the <a class="reference internal" href="../linear_model#least-angle-regression"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl> <dt>
<code>fit_interceptbool, default=True</code> </dt>
<dd>
<p>Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).</p> </dd> <dt>
<code>verbosebool or int, default=False</code> </dt>
<dd>
<p>Sets the verbosity amount.</p> </dd> <dt>
<code>normalizebool, default=True</code> </dt>
<dd>
<p>This parameter is ignored when <code>fit_intercept</code> is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use <a class="reference internal" href="sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code>StandardScaler</code></a> before calling <code>fit</code> on an estimator with <code>normalize=False</code>.</p> </dd> <dt>
<code>precomputebool, ‘auto’ or array-like , default=’auto’</code> </dt>
<dd>
<p>Whether to use a precomputed Gram matrix to speed up calculations. If set to <code>'auto'</code> let us decide. The Gram matrix can also be passed as argument.</p> </dd> <dt>
<code>n_nonzero_coefsint, default=500</code> </dt>
<dd>
<p>Target number of non-zero coefficients. Use <code>np.inf</code> for no limit.</p> </dd> <dt>
<code>epsfloat, default=np.finfo(float).eps</code> </dt>
<dd>
<p>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the <code>tol</code> parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.</p> </dd> <dt>
<code>copy_Xbool, default=True</code> </dt>
<dd>
<p>If <code>True</code>, X will be copied; else, it may be overwritten.</p> </dd> <dt>
<code>fit_pathbool, default=True</code> </dt>
<dd>
<p>If True the full path is stored in the <code>coef_path_</code> attribute. If you compute the solution for a large problem or many targets, setting <code>fit_path</code> to <code>False</code> will lead to a speedup, especially with a small alpha.</p> </dd> <dt>
<code>jitterfloat, default=None</code> </dt>
<dd>
<p>Upper bound on a uniform noise parameter to be added to the <code>y</code> values, to satisfy the model’s assumption of one-at-a-time computations. Might help with stability.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.23.</span></p> </div> </dd> <dt>
<code>random_stateint, RandomState instance or None, default=None</code> </dt>
<dd>
<p>Determines random number generation for jittering. Pass an int for reproducible output across multiple function calls. See <a class="reference internal" href="https://scikit-learn.org/0.24/glossary.html#term-random_state"><span class="xref std std-term">Glossary</span></a>. Ignored if <code>jitter</code> is None.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.23.</span></p> </div> </dd> </dl> </dd> <dt class="field-even">Attributes</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>alphas_array-like of shape (n_alphas + 1,) or list of such arrays</code> </dt>
<dd>
<p>Maximum of covariances (in absolute value) at each iteration. <code>n_alphas</code> is either <code>max_iter</code>, <code>n_features</code> or the number of nodes in the path with <code>alpha &gt;= alpha_min</code>, whichever is smaller. If this is a list of array-like, the length of the outer list is <code>n_targets</code>.</p> </dd> <dt>
<code>active_list of shape (n_alphas,) or list of such lists</code> </dt>
<dd>
<p>Indices of active variables at the end of the path. If this is a list of list, the length of the outer list is <code>n_targets</code>.</p> </dd> <dt>
<code>coef_path_array-like of shape (n_features, n_alphas + 1) or list of such arrays</code> </dt>
<dd>
<p>The varying values of the coefficients along the path. It is not present if the <code>fit_path</code> parameter is <code>False</code>. If this is a list of array-like, the length of the outer list is <code>n_targets</code>.</p> </dd> <dt>
<code>coef_array-like of shape (n_features,) or (n_targets, n_features)</code> </dt>
<dd>
<p>Parameter vector (w in the formulation formula).</p> </dd> <dt>
<code>intercept_float or array-like of shape (n_targets,)</code> </dt>
<dd>
<p>Independent term in decision function.</p> </dd> <dt>
<code>n_iter_array-like or int</code> </dt>
<dd>
<p>The number of iterations taken by lars_path to find the grid of alphas for each target.</p> </dd> </dl> </dd> </dl> <div class="admonition seealso"> <p class="admonition-title">See also</p> <dl class="simple"> <dt>
<code>lars_path,</code> <a class="reference internal" href="sklearn.linear_model.larscv#sklearn.linear_model.LarsCV" title="sklearn.linear_model.LarsCV"><code>LarsCV</code></a>
</dt>
 <dt>
 <a class="reference internal" href="sklearn.decomposition.sparse_encode#sklearn.decomposition.sparse_encode" title="sklearn.decomposition.sparse_encode"><code>sklearn.decomposition.sparse_encode</code></a>
</dt>
 </dl> </div> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; reg = linear_model.Lars(n_nonzero_coefs=1)
&gt;&gt;&gt; reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])
Lars(n_nonzero_coefs=1)
&gt;&gt;&gt; print(reg.coef_)
[ 0. -1.11...]
</pre> <h4 class="rubric">Methods</h4> <table class="longtable docutils align-default">   <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.Lars.fit" title="sklearn.linear_model.Lars.fit"><code>fit</code></a>(X, y[, Xy])</p></td> <td><p>Fit the model using X, y as training data.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.Lars.get_params" title="sklearn.linear_model.Lars.get_params"><code>get_params</code></a>([deep])</p></td> <td><p>Get parameters for this estimator.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.Lars.predict" title="sklearn.linear_model.Lars.predict"><code>predict</code></a>(X)</p></td> <td><p>Predict using the linear model.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.Lars.score" title="sklearn.linear_model.Lars.score"><code>score</code></a>(X, y[, sample_weight])</p></td> <td><p>Return the coefficient of determination <span class="math notranslate nohighlight">\(R^2\)</span> of the prediction.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.Lars.set_params" title="sklearn.linear_model.Lars.set_params"><code>set_params</code></a>(**params)</p></td> <td><p>Set the parameters of this estimator.</p></td> </tr>  </table> <dl class="py method"> <dt id="sklearn.linear_model.Lars.fit">
<code>fit(X, y, Xy=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_least_angle.py#L997"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fit the model using X, y as training data.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xarray-like of shape (n_samples, n_features)</code> </dt>
<dd>
<p>Training data.</p> </dd> <dt>
<code>yarray-like of shape (n_samples,) or (n_samples, n_targets)</code> </dt>
<dd>
<p>Target values.</p> </dd> <dt>
<code>Xyarray-like of shape (n_samples,) or (n_samples, n_targets), default=None</code> </dt>
<dd>
<p>Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>selfobject</code> </dt>
<dd>
<p>returns an instance of self.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.Lars.get_params">
<code>get_params(deep=True)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/base.py#L178"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Get parameters for this estimator.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>deepbool, default=True</code> </dt>
<dd>
<p>If True, will return the parameters for this estimator and contained subobjects that are estimators.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>paramsdict</code> </dt>
<dd>
<p>Parameter names mapped to their values.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.Lars.predict">
<code>predict(X)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_base.py#L224"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Predict using the linear model.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xarray-like or sparse matrix, shape (n_samples, n_features)</code> </dt>
<dd>
<p>Samples.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>Carray, shape (n_samples,)</code> </dt>
<dd>
<p>Returns predicted values.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.Lars.score">
<code>score(X, y, sample_weight=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/base.py#L510"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Return the coefficient of determination <span class="math notranslate nohighlight">\(R^2\)</span> of the prediction.</p> <p>The coefficient <span class="math notranslate nohighlight">\(R^2\)</span> is defined as <span class="math notranslate nohighlight">\((1 - \frac{u}{v})\)</span>, where <span class="math notranslate nohighlight">\(u\)</span> is the residual sum of squares <code>((y_true - y_pred)
** 2).sum()</code> and <span class="math notranslate nohighlight">\(v\)</span> is the total sum of squares <code>((y_true -
y_true.mean()) ** 2).sum()</code>. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of <code>y</code>, disregarding the input features, would get a <span class="math notranslate nohighlight">\(R^2\)</span> score of 0.0.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>Xarray-like of shape (n_samples, n_features)</code> </dt>
<dd>
<p>Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape <code>(n_samples, n_samples_fitted)</code>, where <code>n_samples_fitted</code> is the number of samples used in the fitting for the estimator.</p> </dd> <dt>
<code>yarray-like of shape (n_samples,) or (n_samples, n_outputs)</code> </dt>
<dd>
<p>True values for <code>X</code>.</p> </dd> <dt>
<code>sample_weightarray-like of shape (n_samples,), default=None</code> </dt>
<dd>
<p>Sample weights.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>scorefloat</code> </dt>
<dd>
<p><span class="math notranslate nohighlight">\(R^2\)</span> of <code>self.predict(X)</code> wrt. <code>y</code>.</p> </dd> </dl> </dd> </dl> <h4 class="rubric">Notes</h4> <p>The <span class="math notranslate nohighlight">\(R^2\)</span> score used when calling <code>score</code> on a regressor uses <code>multioutput='uniform_average'</code> from version 0.23 to keep consistent with default value of <a class="reference internal" href="sklearn.metrics.r2_score#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code>r2_score</code></a>. This influences the <code>score</code> method of all the multioutput regressors (except for <a class="reference internal" href="sklearn.multioutput.multioutputregressor#sklearn.multioutput.MultiOutputRegressor" title="sklearn.multioutput.MultiOutputRegressor"><code>MultiOutputRegressor</code></a>).</p> </dd>
</dl> <dl class="py method"> <dt id="sklearn.linear_model.Lars.set_params">
<code>set_params(**params)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/base.py#L202"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as <a class="reference internal" href="sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code>Pipeline</code></a>). The latter have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it’s possible to update each component of a nested object.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<code>**paramsdict</code> </dt>
<dd>
<p>Estimator parameters.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>selfestimator instance</code> </dt>
<dd>
<p>Estimator instance.</p> </dd> </dl> </dd> </dl> </dd>
</dl> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2020 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/0.24/modules/generated/sklearn.linear_model.Lars.html" class="_attribution-link">https://scikit-learn.org/0.24/modules/generated/sklearn.linear_model.Lars.html</a>
  </p>
</div>
