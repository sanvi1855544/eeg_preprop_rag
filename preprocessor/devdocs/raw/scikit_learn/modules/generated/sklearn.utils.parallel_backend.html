<h1>sklearn.utils.parallel_backend</h1> <dl class="py function"> <dt id="sklearn.utils.parallel_backend">
<code>sklearn.utils.parallel_backend(backend, n_jobs=- 1, inner_max_num_threads=None, **backend_params)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/../../miniconda/envs/testenv/lib/python3.9/site-packages/joblib/parallel.py#L122"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Change the default backend used by Parallel inside a with block.</p> <p>If <code>backend</code> is a string it must match a previously registered implementation using the <code>register_parallel_backend</code> function.</p> <p>By default the following backends are available:</p> <ul class="simple"> <li>‘loky’: single-host, process-based parallelism (used by default),</li> <li>‘threading’: single-host, thread-based parallelism,</li> <li>‘multiprocessing’: legacy single-host, process-based parallelism.</li> </ul> <p>‘loky’ is recommended to run functions that manipulate Python objects. ‘threading’ is a low-overhead alternative that is most efficient for functions that release the Global Interpreter Lock: e.g. I/O-bound code or CPU-bound code in a few calls to native code that explicitly releases the GIL.</p> <p>In addition, if the <code>dask</code> and <code>distributed</code> Python packages are installed, it is possible to use the ‘dask’ backend for better scheduling of nested parallel calls without over-subscription and potentially distribute parallel calls over a networked cluster of several hosts.</p> <p>It is also possible to use the distributed ‘ray’ backend for distributing the workload to a cluster of nodes. To use the ‘ray’ joblib backend add the following lines:</p> <pre data-language="python">&gt;&gt;&gt; from ray.util.joblib import register_ray  
&gt;&gt;&gt; register_ray()  
&gt;&gt;&gt; with parallel_backend("ray"):  
...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))
[-1, -2, -3, -4, -5]
</pre> <p>Alternatively the backend can be passed directly as an instance.</p> <p>By default all available workers will be used (<code>n_jobs=-1</code>) unless the caller passes an explicit value for the <code>n_jobs</code> parameter.</p> <p>This is an alternative to passing a <code>backend='backend_name'</code> argument to the <code>Parallel</code> class constructor. It is particularly useful when calling into library code that uses joblib internally but does not expose the backend argument in its own API.</p> <pre data-language="python">&gt;&gt;&gt; from operator import neg
&gt;&gt;&gt; with parallel_backend('threading'):
...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))
...
[-1, -2, -3, -4, -5]
</pre> <p>Warning: this function is experimental and subject to change in a future version of joblib.</p> <p>Joblib also tries to limit the oversubscription by limiting the number of threads usable in some third-party library threadpools like OpenBLAS, MKL or OpenMP. The default limit in each worker is set to <code>max(cpu_count() // effective_n_jobs, 1)</code> but this limit can be overwritten with the <code>inner_max_num_threads</code> argument which will be used to set this limit in the child processes.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.10.</span></p> </div> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2020 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/0.24/modules/generated/sklearn.utils.parallel_backend.html" class="_attribution-link">https://scikit-learn.org/0.24/modules/generated/sklearn.utils.parallel_backend.html</a>
  </p>
</div>
