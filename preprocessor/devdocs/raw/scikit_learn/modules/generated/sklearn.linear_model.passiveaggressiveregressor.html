<h1>sklearn.linear_model.PassiveAggressiveRegressor</h1> <dl class="py function"> <dt id="sklearn.linear_model.PassiveAggressiveRegressor">
<code>sklearn.linear_model.PassiveAggressiveRegressor(*, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='epsilon_insensitive', epsilon=0.1, random_state=None, warm_start=False, average=False)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/linear_model/_passive_aggressive.py#L261"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Passive Aggressive Regressor</p> <p>Read more in the <a class="reference internal" href="../linear_model#passive-aggressive"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl> <dt>
<code>Cfloat, default=1.0</code> </dt>
<dd>
<p>Maximum step size (regularization). Defaults to 1.0.</p> </dd> <dt>
<code>fit_interceptbool, default=True</code> </dt>
<dd>
<p>Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True.</p> </dd> <dt>
<code>max_iterint, default=1000</code> </dt>
<dd>
<p>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the <code>fit</code> method, and not the <code>partial_fit</code> method.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.19.</span></p> </div> </dd> <dt>
<code>tolfloat or None, default=1e-3</code> </dt>
<dd>
<p>The stopping criterion. If it is not None, the iterations will stop when (loss &gt; previous_loss - tol).</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.19.</span></p> </div> </dd> <dt>
<code>early_stoppingbool, default=False</code> </dt>
<dd>
<p>Whether to use early stopping to terminate training when validation. score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.20.</span></p> </div> </dd> <dt>
<code>validation_fractionfloat, default=0.1</code> </dt>
<dd>
<p>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.20.</span></p> </div> </dd> <dt>
<code>n_iter_no_changeint, default=5</code> </dt>
<dd>
<p>Number of iterations with no improvement to wait before early stopping.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.20.</span></p> </div> </dd> <dt>
<code>shufflebool, default=True</code> </dt>
<dd>
<p>Whether or not the training data should be shuffled after each epoch.</p> </dd> <dt>
<code>verboseinteger, default=0</code> </dt>
<dd>
<p>The verbosity level</p> </dd> <dt>
<code>lossstring, default=”epsilon_insensitive”</code> </dt>
<dd>
<p>The loss function to be used: epsilon_insensitive: equivalent to PA-I in the reference paper. squared_epsilon_insensitive: equivalent to PA-II in the reference paper.</p> </dd> <dt>
<code>epsilonfloat, default=DEFAULT_EPSILON</code> </dt>
<dd>
<p>If the difference between the current prediction and the correct label is below this threshold, the model is not updated.</p> </dd> <dt>
<code>random_stateint, RandomState instance, default=None</code> </dt>
<dd>
<p>Used to shuffle the training data, when <code>shuffle</code> is set to <code>True</code>. Pass an int for reproducible output across multiple function calls. See <a class="reference internal" href="https://scikit-learn.org/0.24/glossary.html#term-random_state"><span class="xref std std-term">Glossary</span></a>.</p> </dd> <dt>
<code>warm_startbool, default=False</code> </dt>
<dd>
<p>When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See <a class="reference internal" href="https://scikit-learn.org/0.24/glossary.html#term-warm_start"><span class="xref std std-term">the Glossary</span></a>.</p> <p>Repeatedly calling fit or partial_fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled.</p> </dd> <dt>
<code>averagebool or int, default=False</code> </dt>
<dd>
<p>When set to True, computes the averaged SGD weights and stores the result in the <code>coef_</code> attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.19: </span>parameter <em>average</em> to use weights averaging in SGD</p> </div> </dd> </dl> </dd> <dt class="field-even">Attributes</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>coef_array, shape = [1, n_features] if n_classes == 2 else [n_classes, n_features]</code> </dt>
<dd>
<p>Weights assigned to the features.</p> </dd> <dt>
<code>intercept_array, shape = [1] if n_classes == 2 else [n_classes]</code> </dt>
<dd>
<p>Constants in decision function.</p> </dd> <dt>
<code>n_iter_int</code> </dt>
<dd>
<p>The actual number of iterations to reach the stopping criterion.</p> </dd> <dt>
<code>t_int</code> </dt>
<dd>
<p>Number of weight updates performed during training. Same as <code>(n_iter_ * n_samples)</code>.</p> </dd> </dl> </dd> </dl> <div class="admonition seealso"> <p class="admonition-title">See also</p> <dl class="simple"> <dt>
 <a class="reference internal" href="sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code>SGDRegressor</code></a>
</dt>
 </dl> </div> <h4 class="rubric">References</h4> <p>Online Passive-Aggressive Algorithms &lt;<a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf">http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf</a>&gt; K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)</p> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from sklearn.linear_model import PassiveAggressiveRegressor
&gt;&gt;&gt; from sklearn.datasets import make_regression
</pre> <pre data-language="python">&gt;&gt;&gt; X, y = make_regression(n_features=4, random_state=0)
&gt;&gt;&gt; regr = PassiveAggressiveRegressor(max_iter=100, random_state=0,
... tol=1e-3)
&gt;&gt;&gt; regr.fit(X, y)
PassiveAggressiveRegressor(max_iter=100, random_state=0)
&gt;&gt;&gt; print(regr.coef_)
[20.48736655 34.18818427 67.59122734 87.94731329]
&gt;&gt;&gt; print(regr.intercept_)
[-0.02306214]
&gt;&gt;&gt; print(regr.predict([[0, 0, 0, 0]]))
[-0.02306214]
</pre> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2020 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/0.24/modules/generated/sklearn.linear_model.PassiveAggressiveRegressor.html" class="_attribution-link">https://scikit-learn.org/0.24/modules/generated/sklearn.linear_model.PassiveAggressiveRegressor.html</a>
  </p>
</div>
